{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion Pipeline"
      ],
      "metadata": {
        "id": "oERHyDCI1RUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Parsing"
      ],
      "metadata": {
        "id": "Gcyhxul94eOk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2417cb7",
        "outputId": "c23e107f-39a2-4c3d-bd2a-2584b9c1d979"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install langchain langchain-core langchain-community pypdf pymupdf sentence-transformers faiss-cpu chromadb langchain-groq python-dotenv typesense langchain_openai langgraph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting typesense\n",
            "  Downloading typesense-1.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading typesense-1.3.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.5-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=150e8820481bc11de43d40b385ece8df870f3a6c2a1e688140b8a810fbda425a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pypdf, pymupdf, pybase64, opentelemetry-proto, mypy-extensions, marshmallow, humanfriendly, httptools, faiss-cpu, bcrypt, backoff, watchfiles, typing-inspect, requests, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, typesense, posthog, opentelemetry-semantic-conventions, onnxruntime, groq, dataclasses-json, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-core, langchain-text-splitters, langchain_openai, langchain-groq, chromadb, langchain-classic, langchain-community\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.4.0 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 faiss-cpu-1.13.2 groq-0.37.1 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.5 langchain-groq-1.1.1 langchain-text-splitters-1.1.0 langchain_openai-1.1.6 marshmallow-3.26.2 mypy-extensions-1.1.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pymupdf-1.26.7 pypdf-6.5.0 pypika-0.48.9 pyproject_hooks-1.2.0 requests-2.32.5 typesense-1.3.0 typing-inspect-0.9.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Structure\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "j-QI6LqG1uWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Document(\n",
        "    page_content=\"Machine learning is fun\",\n",
        "    metadata = {                                                      # to apply filters\n",
        "        \"source\" : \"example.txt\",\n",
        "        \"page_no\" : 1,\n",
        "        \"author\" : \"firdous\",\n",
        "        \"date_created\" : \"2025-01-01\",\n",
        "    }\n",
        ")\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3zpO-_Q5ou2",
        "outputId": "6febf4cc-ef57-4425-e3fd-ed365c469fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'example.txt', 'page_no': 1, 'author': 'firdous', 'date_created': '2025-01-01'}, page_content='Machine learning is fun')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create simple txt file\n",
        "import os\n",
        "os.makedirs('data/text_files',exist_ok=True)"
      ],
      "metadata": {
        "id": "MroCTKT46URZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts={\n",
        "    \"data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
        "\n",
        "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
        "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
        "programming languages in the world.\n",
        "\n",
        "Key Features:\n",
        "- Easy to learn and use\n",
        "- Extensive standard library\n",
        "- Cross-platform compatibility\n",
        "- Strong community support\n",
        "\n",
        "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
        "\n",
        "    \"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
        "\n",
        "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
        "from experience without being explicitly programmed. It focuses on developing computer programs\n",
        "that can access data and use it to learn for themselves.\n",
        "\n",
        "Types of Machine Learning:\n",
        "1. Supervised Learning: Learning with labeled data\n",
        "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
        "3. Reinforcement Learning: Learning through rewards and penalties\n",
        "\n",
        "Applications include image recognition, speech processing, and recommendation systems\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "for filepath,content in sample_texts.items():\n",
        "  with open(filepath,'w',encoding=\"utf-8\") as f:\n",
        "    f.write(content)\n",
        "print(\"Sample text files created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhAq9eBt6mZP",
        "outputId": "cb9c9a72-a9eb-41cf-8496-2c9fb1c488aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text files created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TextLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "loader = TextLoader('data/text_files/python_intro.txt')\n",
        "document = loader.load()\n",
        "print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubv_HdNqvxBd",
        "outputId": "efb1a282-e970-428e-af9a-5035b022e102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DirectoryLoader\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "dir_loader = DirectoryLoader(\n",
        "    'data/text_files',\n",
        "    glob = \"**/*.txt\",\n",
        "    loader_cls = TextLoader,\n",
        "    loader_kwargs={'encoding' : 'utf-8'},\n",
        "    show_progress = True\n",
        ")\n",
        "\n",
        "documents = dir_loader.load()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tojSHzaZwLkc",
        "outputId": "d92fb86a-fd3b-48d9-dd65-192880c2862a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 2069.73it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
              " Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_community.document_loaders import PyMuPDFLoader,PyPDFLoader #PyMuPDF better to PyPDF\n",
        "\n",
        "dir_loader = DirectoryLoader(\n",
        "    'data/pdf_files',\n",
        "    glob = \"**/*.pdf\",\n",
        "    loader_cls = PyMuPDFLoader,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "documents = dir_loader.load()\n",
        "documents\n",
        "# Visit langchain documentation for other types of documents loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYNXf9-ZxU-_",
        "outputId": "4626b667-eb82-4173-b813-0038a4a03330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:09<00:00,  2.40s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 0}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 1}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 2}, page_content='MAKING SENSE OF\\nDATA I'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 3}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 4}, page_content='MAKING SENSE OF\\nDATA I\\nA Practical Guide to Exploratory\\nData Analysis and Data Mining\\nSecond Edition\\nGLENN J. MYATT\\nWAYNE P. JOHNSON'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 5}, page_content='Copyright © 2014 by John Wiley & Sons, Inc. All rights reserved\\nPublished by John Wiley & Sons, Inc., Hoboken, New Jersey\\nPublished simultaneously in Canada\\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form\\nor by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as\\npermitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior\\nwritten permission of the Publisher, or authorization through payment of the appropriate per-copy fee\\nto the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400,\\nfax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission\\nshould be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street,\\nHoboken, NJ07030, (201) 748-6011, fax (201) 748-6008, or online at\\nhttp://www.wiley.com/go/permission.\\nLimit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts\\nin preparing this book, they make no representations or warranties with respect to the accuracy or\\ncompleteness of the contents of this book and specifically disclaim any implied warranties of\\nmerchantability or fitness for a particular purpose. No warranty may be created or extended by sales\\nrepresentatives or written sales materials. The advice and strategies contained herein may not be\\nsuitable for your situation. You should consult with a professional where appropriate. Neither the\\npublisher nor author shall be liable for any loss of profit or any other commercial damages, including\\nbut not limited to special, incidental, consequential, or other damages.\\nFor general information on our other products and services or for technical support, please contact\\nour Customer Care Department within the United States at (800) 762-2974, outside the United States\\nat (317) 572-3993 or fax (317) 572-4002.\\nWiley also publishes its books in a variety of electronic formats. Some content that appears in print\\nmay not be available in electronic formats. For more information about Wiley products, visit our web\\nsite at www.wiley.com.\\nLibrary of Congress Cataloging-in-Publication Data:\\nMyatt, Glenn J., 1969–\\n[Making sense of data]\\nMaking sense of data I : a practical guide to exploratory data analysis and data mining /\\nGlenn J. Myatt, Wayne P. Johnson. – Second edition.\\npages cm\\nRevised edition of: Making sense of data. c2007.\\nIncludes bibliographical references and index.\\nISBN 978-1-118-40741-7 (paper)\\n1. Data mining.\\n2. Mathematical statistics.\\nI. Johnson, Wayne P.\\nII. Title.\\nQA276.M92 2014\\n006.3′12–dc23\\n2014007303\\nPrinted in the United States of America\\nISBN: 9781118407417\\n10 9 8 7 6 5 4 3 2 1'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 6}, page_content='CONTENTS\\nPREFACE\\nix\\n1\\nINTRODUCTION\\n1\\n1.1\\nOverview / 1\\n1.2\\nSources of Data / 2\\n1.3\\nProcess for Making Sense of Data / 3\\n1.4\\nOverview of Book / 13\\n1.5\\nSummary / 16\\nFurther Reading / 16\\n2\\nDESCRIBING DATA\\n17\\n2.1\\nOverview / 17\\n2.2\\nObservations and Variables / 18\\n2.3\\nTypes of Variables / 20\\n2.4\\nCentral Tendency / 22\\n2.5\\nDistribution of the Data / 24\\n2.6\\nConfidence Intervals / 36\\n2.7\\nHypothesis Tests / 40\\nExercises / 42\\nFurther Reading / 45\\nv'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 7}, page_content='vi\\nCONTENTS\\n3\\nPREPARING DATA TABLES\\n47\\n3.1\\nOverview / 47\\n3.2\\nCleaning the Data / 48\\n3.3\\nRemoving Observations and Variables / 49\\n3.4\\nGenerating Consistent Scales Across Variables / 49\\n3.5\\nNew Frequency Distribution / 51\\n3.6\\nConverting Text to Numbers / 52\\n3.7\\nConverting Continuous Data to Categories / 53\\n3.8\\nCombining Variables / 54\\n3.9\\nGenerating Groups / 54\\n3.10\\nPreparing Unstructured Data / 55\\nExercises / 57\\nFurther Reading / 57\\n4\\nUNDERSTANDING RELATIONSHIPS\\n59\\n4.1\\nOverview / 59\\n4.2\\nVisualizing Relationships Between Variables / 60\\n4.3\\nCalculating Metrics About Relationships / 69\\nExercises / 81\\nFurther Reading / 82\\n5\\nIDENTIFYING AND UNDERSTANDING GROUPS\\n83\\n5.1\\nOverview / 83\\n5.2\\nClustering / 88\\n5.3\\nAssociation Rules / 111\\n5.4\\nLearning Decision Trees from Data / 122\\nExercises / 137\\nFurther Reading / 140\\n6\\nBUILDING MODELS FROM DATA\\n141\\n6.1\\nOverview / 141\\n6.2\\nLinear Regression / 149\\n6.3\\nLogistic Regression / 161\\n6.4\\nk-Nearest Neighbors / 167'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 8}, page_content='CONTENTS\\nvii\\n6.5\\nClassification and Regression Trees / 172\\n6.6\\nOther Approaches / 178\\nExercises / 179\\nFurther Reading / 182\\nAPPENDIX A ANSWERS TO EXERCISES\\n185\\nAPPENDIX B HANDS-ON TUTORIALS\\n191\\nB.1\\nTutorial Overview / 191\\nB.2\\nAccess and Installation / 191\\nB.3\\nSoftware Overview / 192\\nB.4\\nReading in Data / 193\\nB.5\\nPreparation Tools / 195\\nB.6\\nTables and Graph Tools / 199\\nB.7\\nStatistics Tools / 202\\nB.8\\nGrouping Tools / 204\\nB.9\\nModels Tools / 207\\nB.10\\nApply Model / 211\\nB.11\\nExercises / 211\\nBIBLIOGRAPHY\\n227\\nINDEX\\n231'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 9}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 10}, page_content='PREFACE\\nAn unprecedented amount of data is being generated at increasingly rapid\\nrates in many disciplines. Every day retail companies collect data on sales\\ntransactions, organizations log mouse clicks made on their websites, and\\nbiologists generate millions of pieces of information related to genes.\\nIt is practically impossible to make sense of data sets containing more\\nthan a handful of data points without the help of computer programs.\\nMany free and commercial software programs exist to sift through data,\\nsuch as spreadsheet applications, data visualization software, statistical\\npackages and scripting languages, and data mining tools. Deciding what\\nsoftware to use is just one of the many questions that must be considered\\nin exploratory data analysis or data mining projects. Translating the raw\\ndata collected in various ways into actionable information requires an\\nunderstanding of exploratory data analysis and data mining methods and\\noften an appreciation of the subject matter, business processes, software\\ndeployment, project management methods, change management issues,\\nand so on.\\nThe purpose of this book is to describe a practical approach for making\\nsense out of data. A step-by-step process is introduced, which is designed\\nto walk you through the steps and issues that you will face in data analysis\\nor data mining projects. It covers the more common tasks relating to\\nthe analysis of data including (1) how to prepare data prior to analysis,\\n(2) how to generate summaries of the data, (3) how to identify non-trivial\\nix'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 11}, page_content='x\\nPREFACE\\nfacts, patterns, and relationships in the data, and (4) how to create models\\nfrom the data to better understand the data and make predictions.\\nThe process outlined in the book starts by understanding the problem\\nyou are trying to solve, what data will be used and how, who will use\\nthe information generated, and how it will be delivered to them, and the\\nspecific and measurable success criteria against which the project will be\\nevaluated.\\nThe type of data collected and the quality of this data will directly impact\\nthe usefulness of the results. Ideally, the data will have been carefully col-\\nlected to answer the specific questions defined at the start of the project. In\\npractice, you are often dealing with data generated for an entirely different\\npurpose. In this situation, it is necessary to thoroughly understand and\\nprepare the data for the new questions being posed. This is often one of the\\nmost time-consuming parts of the data mining process where many issues\\nneed to be carefully adressed.\\nThe analysis can begin once the data has been collected and prepared.\\nThe choice of methods used to analyze the data depends on many factors,\\nincluding the problem definition and the type of the data that has been\\ncollected. Although many methods might solve your problem, you may\\nnot know which one works best until you have experimented with the\\nalternatives. Throughout the technical sections, issues relating to when\\nyou would apply the different methods along with how you could optimize\\nthe results are discussed.\\nAfter the data is analyzed, it needs to be delivered to your target audience.\\nThis might be as simple as issuing a report or as complex as implementing\\nand deploying new software to automatically reapply the analysis as new\\ndata becomes available. Beyond the technical challenges, if the solution\\nchanges the way its intended audience operates on a daily basis, it will need\\nto be managed. It will be important to understand how well the solution\\nimplemented in the field actually solves the original business problem.\\nLarger projects are increasingly implemented by interdisciplinary teams\\ninvolving subject matter experts, business analysts, statisticians or data\\nmining experts, IT professionals, and project managers. This book is aimed\\nat the entire interdisciplinary team and addresses issues and technical\\nsolutions relating to data analysis or data mining projects. The book also\\nserves as an introductory textbook for students of any discipline, both\\nundergraduate and graduate, who wish to understand exploratory data\\nanalysis and data mining processes and methods.\\nThe book covers a series of topics relating to the process of making sense\\nof data, including the data mining process and how to describe data table\\nelements (i.e., observations and variables), preparing data prior to analysis,'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 12}, page_content='PREFACE\\nxi\\nvisualizing and describing relationships between variables, identifying and\\nmaking statements about groups of observations, extracting interesting\\nrules, and building mathematical models that can be used to understand\\nthe data and make predictions.\\nThe book focuses on practical approaches and covers information on\\nhow the techniques operate as well as suggestions for when and how to use\\nthe different methods. Each chapter includes a “Further Reading” section\\nthat highlights additional books and online resources that provide back-\\nground as well as more in-depth coverage of the material. At the end of\\nselected chapters are a set of exercises designed to help in understanding\\nthe chapter’s material. The appendix covers a series of practical tutorials\\nthat make use of the freely available Traceis software developed to accom-\\npany the book, which is available from the book’s website: http://www.\\nmakingsenseofdata.com; however, the tutorials could be used with other\\navailable software. Finally, a deck of slides has been developed to accom-\\npany the book’s material and is available on request from the book’s\\nauthors.\\nThe authors wish to thank Chelsey Hill-Esler, Dr. McCullough, and\\nVinod Chandnani for their help with the book.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 13}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 14}, page_content='CHAPTER 1\\nINTRODUCTION\\n1.1 OVERVIEW\\nAlmost every discipline from biology and economics to engineering and\\nmarketing measures, gathers, and stores data in some digital form. Retail\\ncompanies store information on sales transactions, insurance companies\\nkeep track of insurance claims, and meteorological organizations measure\\nand collect data concerning weather conditions. Timely and well-founded\\ndecisions need to be made using the information collected. These deci-\\nsions will be used to maximize sales, improve research and development\\nprojects, and trim costs. Retail companies must determine which prod-\\nucts in their stores are under- or over-performing as well as understand the\\npreferences of their customers; insurance companies need to identify activ-\\nities associated with fraudulent claims; and meteorological organizations\\nattempt to predict future weather conditions.\\nData are being produced at faster rates due to the explosion of internet-\\nrelated information and the increased use of operational systems to collect\\nbusiness, engineering and scientific data, and measurements from sensors\\nor monitors. It is a trend that will continue into the foreseeable future. The\\nchallenges of handling and making sense of this information are significant\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n1'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 15}, page_content='2\\nINTRODUCTION\\nbecause of the increasing volume of data, the complexity that arises from\\nthe diverse types of information that are collected, and the reliability of the\\ndata collected.\\nThe process of taking raw data and converting it into meaningful infor-\\nmation necessary to make decisions is the focus of this book. The following\\nsections in this chapter outline the major steps in a data analysis or data\\nmining project from defining the problem to the deployment of the results.\\nThe process provides a framework for executing projects related to data\\nmining or data analysis. It includes a discussion of the steps and challenges\\nof (1) defining the project, (2) preparing data for analysis, (3) selecting\\ndata analysis or data mining approaches that may include performing an\\noptimization of the analysis to refine the results, and (4) deploying and\\nmeasuring the results to ensure that any expected benefits are realized.\\nThe chapter also includes an outline of topics covered in this book and the\\nsupporting resources that can be used alongside the book’s content.\\n1.2 SOURCES OF DATA\\nThere are many different sources of data as well as methods used to collect\\nthe data. Surveys or polls are valuable approaches for gathering data to\\nanswer specific questions. An interview using a set of predefined questions\\nis often conducted over the phone, in person, or over the internet. It is used\\nto elicit information on people’s opinions, preferences, and behavior. For\\nexample, a poll may be used to understand how a population of eligible\\nvoters will cast their vote in an upcoming election. The specific questions\\nalong with the target population should be clearly defined prior to the inter-\\nviews. Any bias in the survey should be eliminated by selecting a random\\nsample of the target population. For example, bias can be introduced in\\nsituations where only those responding to the questionnaire are included\\nin the survey, since this group may not be representative of a random sam-\\nple of the entire population. The questionnaire should not contain leading\\nquestions—questions that favor a particular response. Other factors which\\nmight result in segments of the total population being excluded should also\\nbe considered, such as the time of day the survey or poll was conducted.\\nA well-designed survey or poll can provide an accurate and cost-effective\\napproach to understanding opinions or needs across a large group of indi-\\nviduals without the need to survey everyone in the target population.\\nExperiments measure and collect data to answer specific questions in a\\nhighly controlled manner. The data collected should be reliably measured;\\nin other words, repeating the measurement should not result in substantially'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 16}, page_content='PROCESS FOR MAKING SENSE OF DATA\\n3\\ndifferent values. Experiments attempt to understand cause-and-effect phe-\\nnomena by controlling other factors that may be important. For example,\\nwhen studying the effects of a new drug, a double-blind study is typically\\nused. The sample of patients selected to take part in the study is divided\\ninto two groups. The new drug is delivered to one group, whereas a placebo\\n(a sugar pill) is given to the other group. To avoid a bias in the study on\\nthe part of the patient or the doctor, neither the patient nor the doctor\\nadministering the treatment knows which group a patient belongs to. In\\ncertain situations it is impossible to conduct a controlled experiment on\\neither logistical or ethical grounds. In these situations a large number of\\nobservations are measured and care is taken when interpreting the results.\\nFor example, it would not be ethical to set up a controlled experiment to\\ntest whether smoking causes health problems.\\nAs part of the daily operations of an organization, data is collected\\nfor a variety of reasons. Operational databases contain ongoing business\\ntransactions and are accessed and updated regularly. Examples include\\nsupply chain and logistics management systems, customer relationship\\nmanagement databases (CRM), and enterprise resource planning databases\\n(ERP). An organization may also be automatically monitoring operational\\nprocesses with sensors, such as the performance of various nodes in a\\ncommunications network. A data warehouse is a copy of data gathered\\nfrom other sources within an organization that is appropriately prepared for\\nmaking decisions. It is not updated as frequently as operational databases.\\nDatabases are also used to house historical polls, surveys, and experiments.\\nIn many cases data from in-house sources may not be sufficient to answer\\nthe questions now being asked of it. In these cases, the internal data can\\nbe augmented with data from other sources such as information collected\\nfrom the web or literature.\\n1.3 PROCESS FOR MAKING SENSE OF DATA\\n1.3.1 Overview\\nFollowing a predefined process will ensure that issues are addressed and\\nappropriate steps are taken. For exploratory data analysis and data mining\\nprojects, you should carefully think through the following steps, which are\\nsummarized here and expanded in the following sections:\\n1. Problem definition and planning: The problem to be solved and the\\nprojected deliverables should be clearly defined and planned, and an\\nappropriate team should be assembled to perform the analysis.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 17}, page_content='4\\nINTRODUCTION\\nFIGURE 1.1\\nSummary of a general framework for a data analysis project.\\n2. Data preparation: Prior to starting a data analysis or data min-\\ning project, the data should be collected, characterized, cleaned,\\ntransformed, and partitioned into an appropriate form for further\\nprocessing.\\n3. Analysis: Based on the information from steps 1 and 2, appropriate\\ndata analysis and data mining techniques should be selected. These\\nmethods often need to be optimized to obtain the best results.\\n4. Deployment: The results from step 3 should be communicated and/or\\ndeployed to obtain the projected benefits identified at the start of the\\nproject.\\nFigure 1.1 summarizes this process. Although it is usual to follow the\\norder described, there will be interactions between the different steps that\\nmay require work completed in earlier phases to be revised. For example,\\nit may be necessary to return to the data preparation (step 2) while imple-\\nmenting the data analysis (step 3) in order to make modifications based on\\nwhat is being learned.\\n1.3.2 Problem Definition and Planning\\nThe first step in a data analysis or data mining project is to describe\\nthe problem being addressed and generate a plan. The following section\\naddresses a number of issues to consider in this first phase. These issues\\nare summarized in Figure 1.2.\\nFIGURE 1.2\\nSummary of some of the issues to consider when defining and\\nplanning a data analysis project.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 18}, page_content='PROCESS FOR MAKING SENSE OF DATA\\n5\\nIt is important to document the business or scientific problem to be\\nsolved along with relevant background information. In certain situations,\\nhowever, it may not be possible or even desirable to know precisely the sort\\nof information that will be generated from the project. These more open-\\nended projects will often generate questions by exploring large databases.\\nBut even in these cases, identifying the business or scientific problem\\ndriving the analysis will help to constrain and focus the work. To illus-\\ntrate, an e-commerce company wishes to embark on a project to redesign\\ntheir website in order to generate additional revenue. Before starting this\\npotentially costly project, the organization decides to perform data anal-\\nysis or data mining of available web-related information. The results of\\nthis analysis will then be used to influence and prioritize this redesign. A\\ngeneral problem statement, such as “make recommendations to improve\\nsales on the website,” along with relevant background information should\\nbe documented.\\nThis broad statement of the problem is useful as a headline; however,\\nthis description should be divided into a series of clearly defined deliver-\\nables that ultimately solve the broader issue. These include: (1) categorize\\nwebsite users based on demographic information; (2) categorize users of\\nthe website based on browsing patterns; and (3) determine if there are any\\nrelationships between these demographic and/or browsing patterns and\\npurchasing habits. This information can then be used to tailor the site to\\nspecific groups of users or improve how their customers purchase based\\non the usage patterns found in the analysis. In addition to understanding\\nwhat type of information will be generated, it is also useful to know how\\nit will be delivered. Will the solution be a report, a computer program to\\nbe used for making predictions, or a set of business rules? Defining these\\ndeliverables will set the expectations for those working on the project and\\nfor its stakeholders, such as the management sponsoring the project.\\nThe success criteria related to the project’s objective should ideally be\\ndefined in ways that can be measured. For example, a criterion might be to\\nincrease revenue or reduce costs by a specific amount. This type of criteria\\ncan often be directly related to the performance level of a computational\\nmodel generated from the data. For example, when developing a compu-\\ntational model that will be used to make numeric projections, it is useful\\nto understand the required level of accuracy. Understanding this will help\\nprioritize the types of methods adopted or the time or approach used in\\noptimizations. For example, a credit card company that is losing customers\\nto other companies may set a business objective to reduce the turnover\\nrate by 10%. They know that if they are able to identify customers likely\\nto switch to a competitor, they have an opportunity to improve retention'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 19}, page_content='6\\nINTRODUCTION\\nthrough additional marketing. To identify these customers, the company\\ndecides to build a predictive model and the accuracy of its predictions will\\naffect the level of retention that can be achieved.\\nIt is also important to understand the consequences of answering ques-\\ntions incorrectly. For example, when predicting tornadoes, there are two\\npossible prediction errors: (1) incorrectly predicting a tornado would strike\\nand (2) incorrectly predicting there would be no tornado. The consequence\\nof scenario (2) is that a tornado hits with no warning. In this case, affected\\nneighborhoods and emergency crews would not be prepared and the con-\\nsequences might be catastrophic. The consequence of scenario (1) is less\\nsevere than scenario (2) since loss of life is more costly than the incon-\\nvenience to neighborhoods and emergency services that prepared for a\\ntornado that did not hit. There are often different business consequences\\nrelated to different types of prediction errors, such as incorrectly predicting\\na positive outcome or incorrectly predicting a negative one.\\nThere may be restrictions concerning what resources are available for\\nuse in the project or other constraints that influence how the project pro-\\nceeds, such as limitations on available data as well as computational hard-\\nware or software that can be used. Issues related to use of the data, such as\\nprivacy or legal issues, should be identified and documented. For example,\\na data set containing personal information on customers’ shopping habits\\ncould be used in a data mining project. However, if the results could be\\ntraced to specific individuals, the resulting findings should be anonymized.\\nThere may also be limitations on the amount of time available to a compu-\\ntational algorithm to make a prediction. To illustrate, suppose a web-based\\ndata mining application or service that dynamically suggests alternative\\nproducts to customers while they are browsing items in an online store is\\nto be developed. Because certain data mining or modeling methods take\\na long time to generate an answer, these approaches should be avoided if\\nsuggestions must be generated rapidly (within a few seconds) otherwise the\\ncustomer will become frustrated and shop elsewhere. Finally, other restric-\\ntions relating to business issues include the window of opportunity available\\nfor the deliverables. For example, a company may wish to develop and use\\na predictive model to prioritize a new type of shampoo for testing. In this\\nscenario, the project is being driven by competitive intelligence indicating\\nthat another company is developing a similar shampoo and the company\\nthat is first to market the product will have a significant advantage. There-\\nfore, the time to generate the model may be an important factor since there\\nis only a small window of opportunity based on business considerations.\\nCross-disciplinary teams solve complex problems by looking at the\\ndata from different perspectives. Because of the range of expertise often'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 20}, page_content='PROCESS FOR MAKING SENSE OF DATA\\n7\\nrequired, teams are essential—especially for large-scale projects—and it\\nis helpful to consider the different roles needed for an interdisciplinary\\nteam. A project leader plans and directs a project, and monitors its results.\\nDomain experts provide specific knowledge of the subject matter or busi-\\nness problems, including (1) how the data was collected, (2) what the\\ndata values mean, (3) the accuracy of the data, (4) how to interpret the\\nresults of the analysis, and (5) the business issues being addressed by\\nthe project. Data analysis/mining experts are familiar with statistics, data\\nanalysis methods, and data mining approaches as well as issues relating\\nto data preparation. An IT specialist has expertise in integrating data sets\\n(e.g., accessing databases, joining tables, pivoting tables) as well as knowl-\\nedge of software and hardware issues important for implementation and\\ndeployment. End users use information derived from the data routinely or\\nfrom a one-off analysis to help them make decisions. A single member\\nof the team may take on multiple roles such as the role of project leader\\nand data analysis/mining expert, or several individuals may be responsible\\nfor a single role. For example, a team may include multiple subject matter\\nexperts, where one individual has knowledge of how the data was measured\\nand another has knowledge of how it can be interpreted. Other individuals,\\nsuch as the project sponsor, who have an interest in the project should be\\nincluded as interested parties at appropriate times throughout the project.\\nFor example, representatives from the finance group may be involved if the\\nsolution proposes a change to a business process with important financial\\nimplications.\\nDifferent individuals will play active roles at different times. It is desir-\\nable to involve all parties in the project definition phase. In the data prepa-\\nration phase, the IT expert plays an important role in integrating the data in\\na form that can be processed. During this phase, the data analysis/mining\\nexpert and the subject matter expert/business analyst will also be working\\nclosely together to clean and categorize the data. The data analysis/mining\\nexpert should be primarily responsible for ensuring that the data is trans-\\nformed into a form appropriate for analysis. The analysis phase is primarily\\nthe responsibility of the data analysis/mining expert with input from the\\nsubject matter expert or business analyst. The IT expert can provide a valu-\\nable hardware and software support role throughout the project and will\\nplay a critical role in situations where the output of the analysis is to be\\nintegrated within an operational system.\\nWith cross-disciplinary teams, communicating within the group may\\nbe challenging from time-to-time due to the disparate backgrounds of the\\nmembers of the group. A useful way of facilitating communication is to\\ndefine and share glossaries defining terms familiar to the subject matter'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 21}, page_content='8\\nINTRODUCTION\\nexperts or to the data analysis/data mining experts. Team meetings to share\\ninformation are also essential for communication purposes.\\nThe extent of the project plan depends on the size and scope of the\\nproject. A timetable of events should be put together that includes the\\npreparation, implementation, and deployment phases (summarized in Sec-\\ntions 1.3.3, 1.3.4, and 1.3.5). Time should be built into the timetable for\\nreviews after each phase. At the end of the project, a valuable exercise that\\nprovides insight for future projects is to spend time evaluating what did and\\ndid not work. Progress will be iterative and not strictly sequential, moving\\nbetween phases of the process as new questions arise. If there are high-risk\\nsteps in the process, these should be identified and contingencies for them\\nadded to the plan. Tasks with dependencies and contingencies should be\\ndocumented using timelines or standard project management support tools\\nsuch as Gantt charts. Based on the plan, budgets and success criteria can\\nbe developed to compare costs against benefits. This will help determine\\nthe feasibility of the project and whether the project should move forward.\\n1.3.3 Data Preparation\\nIn many projects, understanding the data and getting it ready for analysis\\nis the most time-consuming step in the process, since the data is usually\\nintegrated from many sources, with different representations and formats.\\nFigure 1.3 illustrates some of the steps required for preparing a data set.\\nIn situations where the data has been collected for a different purpose, the\\ndata will need to be transformed into an appropriate form for analysis.\\nFor example, the data may be in the form of a series of documents that\\nrequires it to be extracted from the text of the document and converted\\nto a tabular form that is amenable for data analysis. The data should\\nbe prepared to mirror as closely as possible the target population about\\nwhich new questions will be asked. Since multiple sources of data may be\\nused, care must be taken not to introduce errors when these sources are\\nbrought together. Retaining information about the source is useful both for\\nbookkeeping and for interpreting the results.\\nFIGURE 1.3\\nSummary of steps to consider when preparing the data.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 22}, page_content='PROCESS FOR MAKING SENSE OF DATA\\n9\\nIt is important to characterize the types of attributes that have been col-\\nlected over the different items in the data set. For example, do the attributes\\nrepresent discrete categories such as color or gender or are they numeric\\nvalues of attributes such as temperature or weight? This categorization\\nhelps identify unexpected values. In looking at the numeric attribute weight\\ncollected for a set of people, if an item has the value “low” then we need\\nto either replace this erroneous value or remove the entire record for that\\nperson. Another possible error occurs in values for observations that lie\\noutside the typical range for an attribute. For example, a person assigned\\na weight of 3,000 lb is likely the result of a typing error made during\\ndata collection. This categorization is also essential when selecting the\\nappropriate data analysis or data mining approach to use.\\nIn addition to addressing the mistakes or inconsistencies in data collec-\\ntion, it may be important to change the data to make it more amenable for\\ndata analysis. The transformations should be done without losing impor-\\ntant information. For example, if a data mining approach requires that all\\nattributes have a consistent range, the data will need to be appropriately\\nmodified. The data may also need to be divided into subsets or filtered based\\non specific criteria to make it amenable to answering the problems outlined\\nat the beginning of the project. Multiple approaches to understanding and\\npreparing data are discussed in Chapters 2 and 3.\\n1.3.4 Analysis\\nAs discussed earlier, an initial examination of the data is important in\\nunderstanding the type of information that has been collected and the\\nmeaning of the data. In combination with information from the problem\\ndefinition, this categorization will determine the type of data analysis and\\ndata mining approaches to use. Figure 1.4 summarizes some of the main\\nanalysis approaches to consider.\\nFIGURE 1.4\\nSummary of tasks to consider when analyzing the data.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 23}, page_content='10\\nINTRODUCTION\\nOne common category of analysis tasks provides summarizations and\\nstatements about the data. Summarization is a process by which data is\\nreduced for interpretation without sacrificing important information. Sum-\\nmaries can be developed for the data as a whole or in part. For example, a\\nretail company that collected data on its transactions could develop sum-\\nmaries of the total sales transactions. In addition, the company could also\\ngenerate summaries of transactions by products or stores. It may be impor-\\ntant to make statements with measures of confidence about the entire data\\nset or groups within the data. For example, if you wish to make a statement\\nconcerning the performance of a particular store with slightly lower net\\nrevenue than other stores it is being compared to, you need to know if it is\\nreally underperforming or just within an expected range of performance.\\nData visualization, such as charts and summary tables, is an important tool\\nused alongside summarization methods to present broad conclusions and\\nmake statements about the data with measures of confidence. These are\\ndiscussed in Chapters 2 and 4.\\nA second category of tasks focuses on the identification of important\\nfacts, relationships, anomalies, or trends in the data. Discovering this infor-\\nmation often involves looking at the data in many ways using a combi-\\nnation of data visualization, data analysis, and data mining methods. For\\nexample, a retail company may want to understand customer profiles and\\nother facts that lead to the purchase of certain product lines. Cluster-\\ning is a data analysis method used to group together items with simi-\\nlar attributes. This approach is outlined in Chapter 5. Other data mining\\nmethods, such as decision trees or association rules (also described in\\nChapter 5), automatically extract important facts or rules from the data.\\nThese data mining approaches—describing, looking for relationships, and\\ngrouping—combined with data visualization provide the foundation for\\nbasic exploratory analysis.\\nA third category of tasks involves the development of mathematical\\nmodels that encode relationships in the data. These models are useful\\nfor gaining an understanding of the data and for making predictions. To\\nillustrate, suppose a retail company wants to predict whether specific con-\\nsumers may be interested in buying a particular product. One approach\\nto this problem is to collect historical data containing different customer\\nattributes, such as the customer’s age, gender, the location where they live,\\nand so on, as well as which products the customer has purchased in the\\npast. Using these attributes, a mathematical model can be built that encodes\\nimportant relationships in the data. It may be that female customers between\\n20 and 35 that live in specific areas are more likely to buy the product. Since\\nthese relationships are described in the model, it can be used to examine a'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 24}, page_content='PROCESS FOR MAKING SENSE OF DATA\\n11\\nlist of prospective customers that also contain information on age, gender,\\nlocation, and so on, to make predictions of those most likely to buy the\\nproduct. The individuals predicted by the model as buyers of the product\\nmight become the focus of a targeted marketing campaign. Models can\\nbe built to predict continuous data values (regression models) or categori-\\ncal data (classification models). Simple methods to generate these models\\ninclude linear regression, logistic regression, classification and regression\\ntrees, and k-nearest neighbors. These techniques are discussed in Chapter\\n6 along with summaries of other approaches. The selection of the methods\\nis often driven by the type of data being analyzed as well as the problem\\nbeing solved. Some approaches generate solutions that are straightforward\\nto interpret and explain which may be important for examining specific\\nproblems. Others are more of a “black box” with limited capabilities for\\nexplaining the results. Building and optimizing these models in order to\\ndevelop useful, simple, and meaningful models can be time-consuming.\\nThere is a great deal of interplay between these three categories of\\ntasks. For example, it is important to summarize the data before building\\nmodels or finding hidden relationships. Understanding hidden relationships\\nbetween different items in the data can be of help in generating models.\\nTherefore, it is essential that data analysis or data mining experts work\\nclosely with the subject matter expertise in analyzing the data.\\n1.3.5 Deployment\\nIn the deployment step, analysis is translated into a benefit to the orga-\\nnization and hence this step should be carefully planned and executed.\\nThere are many ways to deploy the results of a data analysis or data min-\\ning project, as illustrated in Figure 1.5. One option is to write a report\\nfor management or the “customer” of the analysis describing the business\\nor scientific intelligence derived from the analysis. The report should be\\ndirected to those responsible for making decisions and focused on sig-\\nnificant and actionable items—conclusions that can be translated into a\\ndecision that can be used to make a difference. It is increasingly common\\nfor the report to be delivered through the corporate intranet.\\nFIGURE 1.5\\nSummary of deployment options.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 25}, page_content='12\\nINTRODUCTION\\nWhen the results of the project include the generation of predictive mod-\\nels to use on an ongoing basis, these models can be deployed as standalone\\napplications or integrated with other software such as spreadsheet applica-\\ntions or web services. The integration of the results into existing operational\\nsystems or databases is often one of the most cost-effective approaches\\nto delivering a solution. For example, when a sales team requires the\\nresults of a predictive model that ranks potential customers based on\\nthe likelihood that they will buy a particular product, the model may be\\nintegrated with the customer relationship management (CRM) system that\\nthey already use on a daily basis. This minimizes the need for training and\\nmakes the deployment of results easier. Prediction models or data mining\\nresults can also be integrated into systems accessible by your customers,\\nsuch as e-commerce websites. In the web pages of these sites, additional\\nproducts or services that may be of interest to the customer may have been\\nidentified using a mathematical model embedded in the web server.\\nModels may also be integrated into existing operational processes where\\na model needs to be constantly applied to operational data. For example,\\na solution may detect events leading to errors in a manufacturing system.\\nCatching these errors early may allow a technician to rectify the problem\\nwithout stopping the production system.\\nIt is important to determine if the findings or generated models are being\\nused to achieve the business objectives outlined at the start of the project.\\nSometimes the generated models may be functioning as expected but the\\nsolution is not being used by the target user community for one reason or\\nanother. To increase confidence in the output of the system, a controlled\\nexperiment (ideally double-blind) in the field may be undertaken to assess\\nthe quality of the results and the organizational impact. For example, the\\nintended users of a predictive model could be divided into two groups. One\\ngroup, made up of half of the users (randomly selected), uses the model\\nresults; the other group does not. The business impact resulting from the\\ntwo groups can then be measured. Where models are continually updated,\\nthe consistency of the results generated should also be monitored over\\ntime.\\nThere are a number of deployment issues that may need to be consid-\\nered during the implementation phase. A solution may involve changing\\nbusiness processes. For example, a solution that requires the development\\nof predictive models to be used by end users in the field may change the\\nwork practices of these individuals. The users may even resist this change.\\nA successful method for promoting acceptance is to involve the end users\\nin the definition of the solution, since they will be more inclined to use\\na system they have helped design. In addition, in order to understand'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 26}, page_content='OVERVIEW OF BOOK\\n13\\nand trust the results, the users may require that all results be appro-\\npriately explained and linked to the data from which the results were\\ngenerated.\\nAt the end of a project it is always a useful exercise to look back at what\\nworked and what did not work. This will provide insight for improving\\nfuture projects.\\n1.4 OVERVIEW OF BOOK\\nThis book outlines a series of introductory methods and approaches impor-\\ntant to many data analysis or data mining projects. It is organized into five\\ntechnical chapters that focus on describing data, preparing data tables,\\nunderstanding relationships, understanding groups, and building models,\\nwith a hands-on tutorial covered in the appendix.\\n1.4.1 Describing Data\\nThe type of data collected is one of the factors used in the selection of the\\ntype of analysis to be used. The information examined on the individual\\nattributes collected in a data set includes a categorization of the attributes’\\nscale in order to understand whether the field represents discrete elements\\nsuch as gender (i.e., male or female) or numeric properties such as age or\\ntemperature. For numeric properties, examining how the data is distributed\\nis important and includes an understanding of where the values of each\\nattribute are centered and how the data for that attribute is distributed\\naround the central values. Histograms, box plots, and descriptive statistics\\nare useful for understanding characteristics of the individual data attributes.\\nDifferent approaches to characterizing and summarizing elements of a data\\ntable are reviewed in Chapter 2, as well as methods that make statements\\nabout or summarize the individual attributes.\\n1.4.2 Preparing Data Tables\\nFor a given data collection, it is rarely the case that the data can be used\\ndirectly for analysis. The data may contain errors or may have been col-\\nlected or combined from multiple sources in an inconsistent manner. Many\\nof these errors will be obvious from an inspection of the summary graphs\\nand statistics as well as an inspection of the data. In addition to cleaning the\\ndata, it may be necessary to transform the data into a form more amenable'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 27}, page_content='14\\nINTRODUCTION\\nfor data analysis. Mapping the data onto new ranges, transforming cate-\\ngorical data (such as different colors) into a numeric form to be used in a\\nmathematical model, as well as other approaches to preparing tabular or\\nnonstructured data prior to analysis are reviewed in Chapter 3.\\n1.4.3 Understanding Relationships\\nUnderstanding the relationships between pairs of attributes across the items\\nin the data is the focus of Chapter 4. For example, based on a collection of\\nobservations about the population of different types of birds throughout the\\nyear as well as the weather conditions collected for a specific region, does\\nthe population of a specific bird increase or decrease as the temperature\\nincreases? Or, based on a double-blind clinical study, do patients taking\\na new medication have an improved outcome? Data visualization, such\\nas scatterplots, histograms, and summary tables play an important role in\\nseeing trends in the data. There are also properties that can be calculated to\\nquantify the different types of relationships. Chapter 4 outlines a number of\\ncommon approaches to understand the relationship between two attributes\\nin the data.\\n1.4.4 Understanding Groups\\nLooking at an entire data set can be overwhelming; however, exploring\\nmeaningful subsets of items may provide a more effective means of ana-\\nlyzing the data.\\nMethods for identifying, labeling, and summarizing collections of items\\nare reviewed in Chapter 5. These groups are often based upon the multiple\\nattributes that describe the members of the group and represent subpopu-\\nlations of interest. For example, a retail store may wish to group a data set\\ncontaining information about customers in order to understand the types\\nof customers that purchase items from their store. As another example,\\nan insurance company may want to group claims that are associated with\\nfraudulent or nonfraudulent insurance claims. Three methods of automati-\\ncally identifying such groups—clustering, association rules, and decision\\ntrees—are described in Chapter 5.\\n1.4.5 Building Models\\nIt is possible to encode trends and relationships across multiple attributes\\nas mathematical models. These models are helpful in understanding rela-\\ntionships in the data and are essential for tasks involving the prediction'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 28}, page_content='OVERVIEW OF BOOK\\n15\\nof items with unknown values. For example, a mathematical model could\\nbe built from historical data on the performance of windmills as well as\\ngeographical and meteorological data concerning their location, and used\\nto make predictions on potential new sites. Chapter 6 introduces important\\nconcepts in terms of selecting an approach to modeling, selecting attributes\\nto include in the models, optimization of the models, as well as methods\\nfor assessing the quality and usefulness of the models using data not used\\nto create the model. Various modeling approaches are outlined, including\\nlinear regression, logistic regression, classification and regression trees,\\nand k-nearest neighbors. These are described in Chapter 6.\\n1.4.6 Exercises\\nAt the conclusion of selected chapters, there are a series of exercises to help\\nin understanding the chapters’ material. It should be possible to answer\\nthese practical exercises by hand and the process of going through them\\nwill support learning the material covered. The answers to the exercises\\nare provided in the book’s appendix.\\n1.4.7 Tutorials\\nAccompanying the book is a piece of software called Traceis, which is\\nfreely available from the book’s website. In the appendix of the book, a\\nseries of data analysis and data mining tutorials are provided that provide\\npractical exercises to support learning the concepts in the book using a\\nseries of data sets that are available for download.\\nFIGURE 1.6\\nSummary of steps to consider in developing a data analysis or data\\nmining project.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 29}, page_content='16\\nINTRODUCTION\\n1.5 SUMMARY\\nThis chapter has described a simple four-step process to use in any data\\nanalysis or data mining projects. Figure 1.6 outlines the different stages as\\nwell as deliverables to consider when planning and implementing a project\\nto make sense of data.\\nFURTHER READING\\nThis chapter has reviewed some of the sources of data used in exploratory\\ndata analysis and data mining. The following books provide more infor-\\nmation on surveys and polls: Fowler (2009), Rea (2005), and Alreck &\\nSettle (2003). There are many additional resources describing experimental design,\\nincluding Montgomery (2012), Cochran & Cox (1999), Barrentine (1999), and\\nAntony (2003). Operational databases and data warehouses are summarized in the\\nfollowing books: Oppel (2011) and Kimball & Ross (2013). Oppel (2011) also\\nsummarizes access and manipulation of information in databases. The CRISP-DM\\nproject (CRoss Industry Standard Process for Data Mining) consortium has pub-\\nlished in Chapman et al. (2000) a data mining process covering data mining stages\\nand the relationships between the stages. SEMMA (Sample, Explore, Modify,\\nModel, Assess) describes a series of core tasks for model development in the SAS\\nEnterprise MinerTM software authored by Rohanizadeh & Moghadam (2009).\\nThis chapter has focused on issues relating to large and potentially complex data\\nanalysis and data mining projects. There are a number of publications that provide\\na more detailed treatment of general project management issues, including Berkun\\n(2005), Kerzner (2013), and the Project Management Institute (2013). The fol-\\nlowing references provide additional case studies: Guidici & Figini (2009), Rud\\n(2000), and Lindoff & Berry (2011).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 30}, page_content='CHAPTER 2\\nDESCRIBING DATA\\n2.1 OVERVIEW\\nThe starting point for data analysis is a data table (often referred to as a\\ndata set) which contains the measured or collected data values represented\\nas numbers or text. The data in these tables are called raw before they have\\nbeen transformed or modified. These data values can be measurements of\\na patient’s weight (such as 150 lb, 175 lb, and so on) or they can be differ-\\nent industrial sectors (such as the “telecommunications industry,” “energy\\nindustry,” and so on) used to categorize a company. A data table lists the\\ndifferent items over which the data has been collected or measured, such as\\ndifferent patients or specific companies. In these tables, information con-\\nsidered interesting is shown for different attributes. The individual items\\nare usually shown as rows in a data table and the different attributes shown\\nas columns. This chapter examines ways in which individual attributes\\ncan be described and summarized: the scales on which they are measured,\\nhow to describe their center as well as the variation using descriptive sta-\\ntistical approaches, and how to make statements about these attributes\\nusing inferential statistical methods, such as confidence intervals or\\nhypothesis tests.\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n17'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 31}, page_content='18\\nDESCRIBING DATA\\n2.2 OBSERVATIONS AND VARIABLES\\nAll disciplines collect data about items that are important to that field.\\nMedical researchers collect data on patients, the automotive industry on\\ncars, and retail companies on transactions. These items are organized into\\na table for data analysis where each row, referred to as an observation, con-\\ntains information about the specific item the row represents. For example,\\na data table about cars may contain many observations on different types\\nof cars. Data tables also contain information about the car, for example, the\\ncar’s weight, the number of cylinders, the fuel efficiency, and so on. When\\nan attribute is thought of as a set of values describing some aspect across\\nall observations, it is called a variable. An example of a table describing\\ndifferent attributes of cars is shown in Table 2.1 from Bache & Lichman\\n(2013). Each row of the table describes an observation (a specific car)\\nand each column describes a variable (a specific attribute of a car). In this\\nexample, there are five observations (“Chevrolet Chevelle Malibu,” “Buick\\nSkylark 320,” “Plymouth Satellite,” “AMC Rebel SST,” “Ford Torino”) and\\nthese observations are described using nine variables: Name, MPG, Cylin-\\nders, Displacement, Horsepower, Weight, Acceleration, Model year, and\\nOrigin. (It should be noted that throughout the book variable names in the\\ntext will be italicized.)\\nA generalized version of the data table is shown in Table 2.2, since\\na table can represent any number of observations described over multiple\\nvariables. This table describes a series of observations (from o1 to on) where\\neach observation is described using a series of variables (from x1 to xp). A\\nvalue is provided for each variable of each observation. For example, the\\nvalue of the first observation for the first variable is x11, the value for the\\nsecond observation’s first variable is x21, and so on. Throughout the book\\nwe will explore different mathematical operations that make use of this\\ngeneralized form of a data table.\\nThe most common way of looking at data is through a spreadsheet,\\nwhere the raw data is displayed as rows of observations and columns of\\nvariables. This type of visualization is helpful in reviewing the raw data;\\nhowever, the table can be overwhelming when it contains more than a\\nhandful of observations or variables. Sorting the table based on one or\\nmore variables is useful for organizing the data; however, it is difficult\\nto identify trends or relationships by looking at the raw data alone. An\\nexample of a spreadsheet of different cars is shown in Figure 2.1.\\nPrior to performing data analysis or data mining, it is essential to under-\\nstand the data table and an important first step is to understand in detail the\\nindividual variables. Many data analysis techniques have restrictions on'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 32}, page_content='TABLE 2.1\\nData Table Showing Five Car Records Described by Nine Variables\\nName\\nMPG\\nCylinders\\nDisplacement\\nHorsepower\\nWeight\\nAcceleration\\nModel Year\\nOrigin\\nChevrolet Chevelle Malibu\\n18\\n8\\n307\\n130\\n3504\\n12\\n70\\nAmerica\\nBuick Skylark 320\\n15\\n8\\n350\\n165\\n3693\\n11.5\\n70\\nAmerica\\nPlymouth Satellite\\n18\\n8\\n318\\n150\\n3436\\n11\\n70\\nAmerica\\nAMC Rebel SST\\n16\\n8\\n304\\n150\\n3433\\n12\\n70\\nAmerica\\nFord Torino\\n17\\n8\\n302\\n140\\n3449\\n10.5\\n70\\nAmerica\\n19'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 33}, page_content='20\\nDESCRIBING DATA\\nTABLE 2.2\\nGeneralized Form of a Data Table\\nVariables\\nObservations\\nx1\\nx2\\nx3\\n. . .\\nxp\\no1\\nx11\\nx12\\nx13\\n. . .\\nx1p\\no2\\nx21\\nx22\\nx23\\n. . .\\nx2p\\no3\\nx31\\nx32\\nx33\\n. . .\\nx3p\\n. . .\\n. . .\\n. . .\\n. . .\\n. . .\\n. . .\\non\\nxn1\\nxn2\\nxn3\\n. . .\\nxnp\\nthe types of variables that they are able to process. As a result, knowing the\\ntypes of variables allow these techniques to be eliminated from considera-\\ntion or the data must be transformed into a form appropriate for analysis. In\\naddition, certain characteristics of the variables have implications in terms\\nof how the results of the analysis will be interpreted.\\n2.3 TYPES OF VARIABLES\\nEach of the variables within a data table can be examined in different\\nways. A useful initial categorization is to define each variable based on\\nthe type of values the variable has. For example, does the variable contain\\na fixed number of distinct values (discrete variable) or could it take any\\nnumeric value (continuous variable)? Using the examples from Section 2.1,\\nan industrial sector variable whose values can be “telecommunication\\nindustry,” “retail industry,” and so on is an example of a discrete variable\\nsince there are a finite number of possible values. A patient’s weight is an\\nexample of a continuous variable since any measured value, such as 153.2\\nlb, 98.2 lb, is possible within its range. Continuous variables may have an\\ninfinite number of values.\\nFIGURE 2.1\\nSpreadsheet showing a sample of car observation.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 34}, page_content='TYPES OF VARIABLES\\n21\\nVariables may also be classified according to the scale on which they are\\nmeasured. Scales help us understand the precision of an individual variable\\nand are used to make choices about data visualizations as well as methods\\nof analysis.\\nA nominal scale describes a variable with a limited number of different\\nvalues that cannot be ordered. For example, a variable Industry would\\nbe nominal if it had categorical values such as “financial,” “engineering,”\\nor “retail.” Since the values merely assign an observation to a particular\\ncategory, the order of these values has no meaning.\\nAn ordinal scale describes a variable whose values can be ordered or\\nranked. As with the nominal scale, values are assigned to a fixed number of\\ncategories. For example, a scale where the only values are “low,” “medium,”\\nand “high” tells us that “high” is larger than “medium” and “medium” is\\nlarger than “low.” However, although the values are ordered, it is impossible\\nto determine the magnitude of the difference between the values. You\\ncannot compare the difference between “high” and “medium” with the\\ndifference between “medium” and “low.”\\nAn interval scale describes values where the interval between values can\\nbe compared. For example, when looking at three data values measured\\non the Fahrenheit scale—5◦F, 10◦F, 15◦F—the differences between the\\nvalues 5 and 10, and between 10 and 15 are both 5◦. Because the intervals\\nbetween values in the scale share the same unit of measurement, they can\\nbe meaningfully compared. However, because the scale lacks a meaningful\\nzero, the ratios of the values cannot be compared. Doubling a value does\\nnot imply a doubling of the actual measurement. For example, 10◦F is not\\ntwice as hot as 5◦F.\\nA ratio scale describes variables where both intervals between values\\nand ratios of values can be compared. An example of a ratio scale is a bank\\naccount balance whose possible values are $5, $10, and $15. The difference\\nbetween each pair is $5; and $10 is twice as much as $5. Scales for which\\nit is possible to take ratios of values are defined as having a natural zero.\\nA variable is referred to as dichotomous if it can contain only two\\nvalues. For example, the values of a variable Gender may only be “male”\\nor “female.” A binary variable is a widely used dichotomous variable with\\nvalues 0 or 1. For example, a variable Purchase may indicate whether a\\ncustomer bought a particular product using 0 to indicate that a customer did\\nnot buy and 1 to indicate that they did buy; or a variable Fuel Efficiency may\\nuse 0 to represent low efficiency vehicles and 1 to represent high efficiency\\nvehicles. Binary variables are often used in data analysis because they\\nprovide a convenient numeric representation for many different types of\\ndiscrete data and are discussed in detail throughout the book.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 35}, page_content='22\\nDESCRIBING DATA\\nCertain types of variables are not used directly in data analysis, but\\nmay be helpful for preparing data tables or interpreting the results of\\nthe analysis. Sometimes a variable is used to identify each observation\\nin a data table, and will have unique values across the observations. For\\nexample, a data table describing different cable television subscribers may\\ninclude a customer reference number variable for each customer. You\\nwould never use this variable in data analysis since the values are intended\\nonly to provide a link to the individual customers. The analysis of the\\ncable television subscription data may identify a subset of subscribers that\\nare responsible for a disproportionate amount of the company’s profit.\\nIncluding a unique identifier provides a reference to detailed customer\\ninformation not included in the data table used in the analysis. A variable\\nmay also have identical values across the observations. For example, a\\nvariable Calibration may define the value of an initial setting for a machine\\nused to generate a particular measurement and this value may be the same\\nfor all observations. This information, although not used directly in the\\nanalysis, is retained both to understand how the data was generated (i.e.,\\nwhat was the calibration setting) and to assess the data for accuracy when\\nit is merged from different sources. In merging data tables generated from\\ntwo sensors, if the data was generated using different calibration settings\\nthen either the two tables cannot be merged or the calibration setting needs\\nto be included to indicate the difference in how the data was measured.\\nAnnotations of variables are another level of detail to consider. They\\nprovide important additional information that give insight about the context\\nof the data: Is the variable a count or a fraction? A time or a date? A financial\\nterm? A value derived from a mathematical operation on other variables?\\nThe units of measurement are useful when presenting the results and are\\ncritical for interpreting the data and understanding how the units should\\nalign or which transformations apply when data tables are merged from\\ndifferent sources.\\nIn Chapter 6, we further categorize variables (independent variables\\nand response variables) by the roles they play in the mathematical models\\ngenerated from data tables.\\n2.4 CENTRAL TENDENCY\\n2.4.1 Overview\\nOf the various ways in which a variable can be summarized, one of the\\nmost important is the value used to characterize the center of the set of\\nvalues it contains. It is useful to quantify the middle or central location of\\na variable, such as its average, around which many of the observations’'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 36}, page_content='CENTRAL TENDENCY\\n23\\nvalues for that variable lie. There are several approaches to calculating\\nthis value and which is used can depend on the classification of the vari-\\nable. The following sections describe some common descriptive statistical\\napproaches for calculating the central location: the mode, the median, and\\nthe mean.\\n2.4.2 Mode\\nThe mode is the most commonly reported value for a particular variable.\\nThe mode calculation is illustrated using the following variable whose\\nvalues (after being ordered from low to high) are\\n3, 4, 5, 6, 7, 7, 7, 8, 8, 9\\nThe mode would be the value 7 since there are three occurrences of 7\\n(more than any other value). The mode is a useful indication of the central\\ntendency of a variable, since the most frequently occurring value is often\\ntoward the center of the variable’s range.\\nWhen there is more than one value with the same (and highest) number\\nof occurrences, either all values are reported or a midpoint is selected. For\\nexample, for the following values, both 7 and 8 are reported three times:\\n3, 4, 5, 6, 7, 7, 7, 8, 8, 8, 9\\nThe mode may be reported as {7, 8} or 7.5.\\nMode provides the only measure of central tendency for variables mea-\\nsured on a nominal scale; however, the mode can also be calculated for\\nvariables measured on the ordinal, interval, and ratio scales.\\n2.4.3 Median\\nThe median is the middle value of a variable, once it has been sorted from\\nlow to high. The following set of values for a variable will be used to\\nillustrate:\\n3, 4, 7, 2, 3, 7, 4, 2, 4, 7, 4\\nBefore identifying the median, the values must be sorted:\\n2, 2, 3, 3, 4, 4, 4, 4, 7, 7, 7\\nThere are 11 values and therefore the sixth value (five values above and\\nfive values below) is selected as the median value, which is 4:\\n2, 2, 3, 3, 4, 4, 4, 4, 7, 7, 7'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 37}, page_content='24\\nDESCRIBING DATA\\nFor variables with an even number of values, the average of the two\\nvalues closest to the middle is selected (sum the two values and divide\\nby 2).\\nThe median can be calculated for variables measured on the ordinal,\\ninterval, and ratio scales and is often the best indication of central tendency\\nfor variables measured on the ordinal scale. It is also a good indication of\\nthe central value for a variable measured on the interval or ratio scales\\nsince, unlike the mean, it will not be distorted by extreme values.\\n2.4.4 Mean\\nThe mean—commonly referred to as the average—is the most commonly\\nused summary of central tendency for variables measured on the interval\\nor ratio scales. It is defined as the sum of all the values divided by the\\nnumber of values. For example, for the following set of values:\\n3, 4, 5, 7, 7, 8, 9, 9, 9\\nThe sum of all nine values is (3 + 4 + 5 + 7 + 7 + 8 + 9 + 9 + 9) or 61.\\nThe sum divided by the number of values is 61 ÷ 9 or 6.78.\\nFor a variable representing a subset of all possible observations (x), the\\nmean is commonly referred to as ̄x. The formula for calculating a mean,\\nwhere n is the number of observations and xi is the individual values, is\\nusually written:\\n̄x =\\nn∑\\ni=1\\nxi\\nn\\nThe notation ∑n\\ni=1 is used to describe the operation of summing all\\nvalues of x from the first value (i = 1) to the last value (i = n), that is\\nx1 + x2 + ⋯+ xn.\\n2.5 DISTRIBUTION OF THE DATA\\n2.5.1 Overview\\nWhile the central location is a single value that characterizes an individual\\nvariable’s data values, it provides no insight into the variation of the data\\nor, in other words, how the different values are distributed around this'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 38}, page_content='DISTRIBUTION OF THE DATA\\n25\\nlocation. The frequency distribution, which is based on a simple count of\\nhow many times a value occurs, is often a starting point for the analysis\\nof variation. Understanding the frequency distribution is the focus of the\\nfollowing section and can be performed using simple data visualizations\\nand calculated metrics. As you will see later, the frequency distribution\\nalso plays a role in selecting which data analysis approaches to adopt.\\n2.5.2 Bar Charts and Frequency Histograms\\nVisualization is an aid to understanding the distribution of data: the range of\\nvalues, the shape created when the values are plotted, and the values called\\noutliers that are found by themselves at the extremes of the range of values.\\nA handful of charts can help to understand the frequency distribution of\\nan individual variable. For a variable measured on a nominal scale, a bar\\nchart can be used to display the relative frequencies for the different values.\\nTo illustrate, the Origin variable from the auto-MPG data table (partially\\nshown in Table 2.2) has three possible values: “America,” “Europe,” and\\n“Asia.” The first step is to count the number of observations in the data\\ntable corresponding to each of these values. Out of the 393 observations in\\nthe data table, there are 244 observations where the Origin is “America,”\\n79 where it is “Asia,” and 70 where it is “Europe.” In a bar chart, each\\nbar represents a value and the height of the bars is proportional to the\\nfrequency, as shown in Figure 2.2.\\nFor nominal variables, the ordering of the x-axis is arbitrary; however,\\nthey are often ordered alphabetically or based on the frequency value. The\\nFIGURE 2.2\\nBar chart for the Origin variable from the auto-MPG data table.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 39}, page_content='26\\nDESCRIBING DATA\\nFIGURE 2.3\\nBar charts for the Origin variables from the auto-MPG data table\\nshowing the proportion and percentage.\\ny-axis which measures frequency can also be replaced by values repre-\\nsenting the proportion or percentage of the overall number of observations\\n(replacing the frequency value), as shown in Figure 2.3.\\nFor variables measured on an ordinal scale containing a small number of\\nvalues, a bar chart can also be used to understand the relative frequencies\\nof the different values. Figure 2.4 shows a bar chart for the variable PLT\\n(number of mother’s previous premature labors) where there are four pos-\\nsible values: 1, 2, 3, and 4. The bar chart represents the number of values\\nfor each of these categories. In this example you can see that most of the\\nobservations fall into the “1” category with smaller numbers in the other\\ncategories. You can also see that the number of observations decreases as\\nthe values increase.\\nFIGURE 2.4\\nBar chart for a variable measured on an ordinal scale, PLT.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 40}, page_content='DISTRIBUTION OF THE DATA\\n27\\nFIGURE 2.5\\nFrequency histogram for the variable “Acceleration.”\\nThe frequency histogram is useful for variables with an ordered scale—\\nordinal, interval, or ratio—that contain a larger number of values. As\\nwith the bar chart, each variable is divided into a series of groups based\\non the data values and displayed as bars whose heights are proportional to\\nthe number of observations within each group. However, the criteria for\\ninclusion within a single bar is a specific range of values. To illustrate, a\\nfrequency histogram is shown in Figure 2.5 displaying a frequency distri-\\nbution for a variable Acceleration. The variable has been grouped into a\\nseries of ranges from 6 to 8, 8 to 10, 10 to 12, and so on. Since we will\\nneed to assign observations that fall on the range boundaries to only one\\ncategory, we will assign a value to a group where its value is greater than or\\nequal to the lower extreme and less than the upper extreme. For example,\\nan Acceleration value of 10 will be categorized into the range 10–12. The\\nnumber of observations that fall within each range is then determined. In\\nthis case, there are six observations that fall into the range 6–8, 22 observa-\\ntions that fall into the range 8–10, and so on. The ranges are ordered from\\nlow to high and plotted along the x-axis. The height of each histogram\\nbar corresponds to the number of observations for each of the ranges. The\\nhistogram in Figure 2.5 indicates that the majority of the observations are\\ngrouped in the middle of the distribution between 12 and 20 and there are\\nrelatively fewer observations at the extreme values. It is usual to display\\nbetween 5 and 10 groups in a frequency histogram using boundary values\\nthat are easy to interpret.\\nThe frequency histogram helps to understand the shape of the frequency\\ndistribution. Figure 2.6 illustrates a number of commonly encountered fre-\\nquency distributions. The first histogram illustrates a variable where, as\\nthe values increase, the number of observations in each group remains'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 41}, page_content='28\\nDESCRIBING DATA\\nFIGURE 2.6\\nExamples of frequency distributions.\\nconstant. The second histogram is of a distribution where most of the\\nobservations are centered around the mean value, with far fewer obser-\\nvations at the extremes, and with the distribution tapering off toward the\\nextremes. The symmetrical shape of this distribution is often identified as\\na bell shape and described as a normal distribution. It is very common for\\nvariables to have a normal distribution and many data analysis techniques\\nassume an approximate normal distribution. The third example depicts a\\nbimodal distribution where the values cluster in two locations, in this case\\nprimarily at both ends of the distribution. The final three histograms show\\nfrequency distributions that either increase or decrease linearly as the val-\\nues increase (fourth and fifth histogram) or have a nonlinear distribution\\nas in the case of the sixth histogram where the number of observations is\\nincreasing exponentially as the values increase.\\nA frequency histogram can also tell us if there is something unusual\\nabout the variables. In Figure 2.7, the first histogram appears to contain two\\napproximately normal distributions and leads us to question whether the\\ndata table contains two distinct types of observations, each with a separate\\nFIGURE 2.7\\nMore complex frequency distributions.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 42}, page_content='DISTRIBUTION OF THE DATA\\n29\\nfrequency distribution. In the second histogram, there appears to be a small\\nnumber of high values that do not follow the bell-shaped distribution that\\nthe majority of observations follow. In this case, it is possible that these\\nvalues are errors and need to be further investigated.\\n2.5.3 Range\\nThe range is a simple measure of the variation for a particular variable. It\\nis calculated as the difference between the highest and lowest values. The\\nfollowing variable will be used to illustrate:\\n2, 3, 4, 6, 7, 7, 8, 9\\nThe range is 7 calculated from the highest value (9) minus the lowest\\nvalue (2). Ranges can be used with variables measured on an ordinal,\\ninterval, or ratio scale.\\n2.5.4 Quartiles\\nQuartiles divide a continuous variable into four even segments based on\\nthe number of observations. The first quartile (Q1) is at the 25% mark,\\nthe second quartile (Q2) is at the 50% mark, and the third quartile (Q3) is\\nat the 75% mark. The calculation for Q2 is the same as the median value\\n(described earlier). The following list of values is used to illustrate how\\nquartiles are calculated:\\n3, 4, 7, 2, 3, 7, 4, 2, 4, 7, 4\\nThe values are initially sorted:\\n2, 2, 3, 3, 4, 4, 4, 4, 7, 7, 7\\nNext, the median or Q2 is located in the center:\\n2, 2, 3, 3, 4, 4, 4, 4, 7, 7, 7\\nWe now look for the center of the first half (shown underlined) or Q1:\\n2, 2, 3, 3, 4, 4, 4, 4, 7, 7, 7\\nThe value of Q1 is recorded as 3.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 43}, page_content='30\\nDESCRIBING DATA\\nFIGURE 2.8\\nOverview of elements of a box plot.\\nFinally, we look for the center of the second half (shown underlined)\\nor Q3:\\n2, 2, 3, 3, 4, 4, 4, 4, 7, 7, 7\\nThe value of Q3 is identified as 7.\\nWhen the boundaries of the quartiles do not fall on a specific value,\\nthe quartile value is calculated based on the two numbers adjacent to the\\nboundary. The interquartile range is defined as the range from Q1 to Q3.\\nIn this example it would be 7 −3 or 4.\\n2.5.5 Box Plots\\nBox plots provide a succinct summary of the overall frequency distribution\\nof a variable. Six values are usually displayed: the lowest value, the lower\\nquartile (Q1), the median (Q2), the upper quartile (Q3), the highest value,\\nand the mean. In the conventional box plot displayed in Figure 2.8, the box\\nin the middle of the plot represents where the central 50% of observations\\nlie. A vertical line shows the location of the median value and a dot\\nrepresents the location of the mean value. The horizontal line with a vertical\\nstroke between “lowest value” and “Q1” and “Q3” and “highest value” are\\nthe “tails”—the values in the first and fourth quartiles.\\nFigure 2.9 provides an example of a box plot for one variable (MPG).\\nThe plot visually displays the lower (9) and upper (46.6) bounds of the\\nvariable. Fifty percent of observations begin at the lower quartile (17.5)\\nFIGURE 2.9\\nBox plot for the variable MPG.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 44}, page_content='DISTRIBUTION OF THE DATA\\n31\\nFIGURE 2.10\\nComparison of frequency histogram and a box plot for the vari-\\nable MPG.\\nand end at the upper quartile (29). The median and the mean values are\\nclose, with the mean slightly higher (around 23.6) than the median (23).\\nFigure 2.10 shows a box plot and a histogram side-by-side to illustrate how\\nthe distribution of a variable is summarized using the box plot.\\n“Outliers,” the solitary data values close to the ends of the range of\\nvalues, are treated differently in various forms of the box plot. Some box\\nplots do not graphically separate them from the first and fourth quartile\\ndepicted by the horizontal lines that are to the left and the right of the\\nbox. In other forms of box plots, these extreme values are replaced with\\nthe highest and lowest values not considered an outlier and the outliers are\\nexplicitly drawn (using small circles) outside the main plot as shown in\\nFigure 2.11.\\nBox plots help in understanding the symmetry of a frequency distri-\\nbution. If both the mean and median have approximately the same value,\\nFIGURE 2.11\\nA box plot with extreme values explicitly shown as circles.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 45}, page_content='32\\nDESCRIBING DATA\\nthere will be about the same number of values above and below the mean\\nand the distribution will be roughly symmetric.\\n2.5.6 Variance\\nThe variance describes the spread of the data and measures how much\\nthe values of a variable differ from the mean. For variables that represent\\nonly a sample of some population and not the population as a whole, the\\nvariance formula is\\ns2 =\\nn∑\\ni=1\\n(xi −̄x)2\\nn −1\\nThe sample variance is referred to as s2. The actual value (xi) minus the\\nmean value (̄x) is squared and summed for all values of a variable. This\\nvalue is divided by the number of observations minus 1 (n −1).\\nThe following example illustrates the calculation of a variance for a\\nparticular variable:\\n3, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 8, 9\\nwhere the mean is\\n̄x = 3 + 4 + 4 + 5 + 5 + 5 + 6 + 6 + 6 + 7 + 7 + 8 + 9\\n13\\n̄x = 5.8\\nTable 2.3 is used to calculate the sum, using the mean value of 5.8.\\nTo calculate s2, we substitute the values from Table 2.3 into the variance\\nformula:\\ns2 =\\nn∑\\ni=1\\n(xi −̄x)2\\nn −1\\ns2 = 34.32\\n13 −1\\ns2 = 2.86\\nThe variance reflects the average squared deviation and can be calculated\\nfor variables measured on the interval or ratio scale.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 46}, page_content='DISTRIBUTION OF THE DATA\\n33\\nTABLE 2.3\\nVariance Intermediate Steps\\nx\\n̄x\\n(xi −̄x)\\n(xi −̄x)2\\n3\\n5.8\\n−2.8\\n7.84\\n4\\n5.8\\n−1.8\\n3.24\\n4\\n5.8\\n−1.8\\n3.24\\n5\\n5.8\\n−0.8\\n0.64\\n5\\n5.8\\n−0.8\\n0.64\\n5\\n5.8\\n−0.8\\n0.64\\n6\\n5.8\\n0.2\\n0.04\\n6\\n5.8\\n0.2\\n0.04\\n6\\n5.8\\n0.2\\n0.04\\n7\\n5.8\\n1.2\\n1.44\\n7\\n5.8\\n1.2\\n1.44\\n8\\n5.8\\n2.2\\n4.84\\n9\\n5.8\\n3.2\\n10.24\\nSum = 34.32\\n2.5.7 Standard Deviation\\nThe standard deviation is the square root of the variance. For a sample\\nfrom a population, the formula is\\ns =\\n√\\n√\\n√\\n√\\n√\\nn∑\\ni=1\\n(xi −̄x)2\\nn −1\\nwhere s is the sample standard deviation, xi is the actual data value, ̄x is the\\nmean for the variable, and n is the number of observations. For a calculated\\nvariance (e.g., 2.86) the standard deviation is calculated as\\n√\\n2.86 or 1.69.\\nThe standard deviation is the most widely used measure of the devia-\\ntion of a variable. The higher the value, the more widely distributed the\\nvariable’s data values are around the mean. Assuming the frequency distri-\\nbution is approximately normal (i.e., a bell-shaped curve), about 68% of all\\nobservations will fall within one standard deviation of the mean (34% less\\nthan and 34% greater than). For example, a variable has a mean value of\\n45 with a standard deviation value of 6. Approximately 68% of the obser-\\nvations should be in the range 39–51 (45 ± one standard deviation) and\\napproximately 95% of all observations fall within two standard deviations'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 47}, page_content='34\\nDESCRIBING DATA\\nof the mean (between 33 and 57). Standard deviations can be calculated\\nfor variables measured on the interval or ratio scales.\\nIt is possible to calculate a normalized value, called a z-score, for each\\ndata element that represents the number of standard deviations that ele-\\nment’s value is from the mean. The following formula is used to calculate\\nthe z-score:\\nz = xi −̄x\\ns\\nwhere z is the z-score, xi is the actual data value, ̄x is the mean for the\\nvariable, and s is the standard deviation. A z-score of 0 indicates that\\na data element’s value is the same as the mean, data elements with z-\\nscores greater than 0 have values greater than the mean, and elements with\\nz-scores less than 0 have values less than the mean. The magnitude of the\\nz-score reflects the number of standard deviations that value is from the\\nmean. This calculation can be useful for comparing variables measured on\\ndifferent scales.\\n2.5.8 Shape\\nPreviously in this chapter, we discussed ways to visualize the frequency dis-\\ntribution. In addition to these visualizations, there are methods for quanti-\\nfying the lack of symmetry or skewness in the distribution of a variable. For\\nasymmetric distributions, the bulk of the observations are either to the left\\nor the right of the mean. For example, in Figure 2.12 the frequency distribu-\\ntion is asymmetric and more of the observations are to the left of the mean\\nthan to the right; the right tail is longer than the left tail. This is an example\\nof a positive, or right skew. Similarly, a negative, or left skew would have\\nmore of the observations to the right of the mean value with a longer tail\\non the left.\\nIt is possible to calculate a value for skewness that describes whether the\\nvariable is positively or negatively skewed and the degree of skewness. One\\nformula for estimating skewness, where the variable is x with individual\\nvalues xi, and n data values is\\nskewness =\\n(√\\nn × (n −1)\\nn −2\\n)\\n×\\n1∕n × ∑n\\ni=1(xi −̄x)3\\n(1∕n × ∑n\\ni=1(xi −̄x)2\\n)3∕2'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 48}, page_content='DISTRIBUTION OF THE DATA\\n35\\nFIGURE 2.12\\nFrequency distribution showing a positive skew.\\nA skewness value of zero indicates a symmetric distribution. If the lower\\ntail is longer than the upper tail the value is positive; if the upper tail is\\nlonger than the lower tail, the skewness score is negative. Figure 2.13 shows\\nexamples of skewness values for two variables. The variable alkphos in the\\nplot on the left has a positive skewness value of 0.763, indicating that the\\nmajority of observations are to the left of the mean, whereas the negative\\nskewness value for the variable mcv in the plot on the right indicates that\\nthe majority are to the right of the mean. That the skewness value for mcv\\nis closer to zero than alkphos indicates that mcv is more symmetric than\\nalkphos.\\nIn addition to the symmetry of the distribution, the type of peak the\\ndistribution has should be considered and it can be characterized by a\\nmeasurement called kurtosis. The following formula can be used for\\n20\\n40\\n60\\n80\\nalkphos\\nmcv\\nSkewness = –0.394\\nSkewness = 0.763\\n100\\n120\\n140\\n60\\n0\\n40\\n80\\n120\\n160\\nFrequency\\n0\\n40\\n80\\n120\\n160\\nFrequency\\n65\\n70\\n75\\n80\\n85\\n90\\n95 100 105\\nFIGURE 2.13\\nSkewness estimates for two variables.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 49}, page_content='36\\nDESCRIBING DATA\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\nMPG\\nAcceleration\\nKurtosis = 0.485\\nKurtosis = –0.522\\n6\\n0\\n40\\n80\\n120\\n160\\nFrequency\\n0\\n40\\n80\\n120\\n160\\nFrequency\\n8\\n10 12 14 16 18 20 22 24 26\\nFIGURE 2.14\\nKurtosis estimates for two variables.\\ncalculating kurtosis for a variable x, where xi represents the individual\\nvalues, and n the number of data values:\\nkurtosis =\\nn −1\\n(n −2) × (n −3) ×\\n⎛\\n⎜\\n⎜\\n⎜⎝\\n(n + 1) ×\\n∑n\\ni=1\\n(\\nxi −̄x\\n)4/\\nn\\n(∑n\\ni=1(xi −x)2/\\nn\\n)2 −3\\n⎞\\n⎟\\n⎟\\n⎟⎠\\n+ 6\\nVariables with a pronounced peak near the mean have a high kurtosis\\nscore while variables with a flat peak have a low kurtosis score. Figure 2.14\\nillustrates kurtosis scores for two variables.\\nIt is important to understand whether a variable has a normal distri-\\nbution, since a number of data analysis approaches require variables to\\nhave this type of frequency distribution. Values for skewness and kurtosis\\nclose to zero indicate that the shape of a frequency distribution for a vari-\\nable approximates a normal distribution which is important for checking\\nassumptions in certain data analysis methods.\\n2.6 CONFIDENCE INTERVALS\\nUp to this point, we have been looking at ways of summarizing information\\non a set of randomly collected observations. This summary information is\\nusually referred to as statistics as they summarize only a collection of obser-\\nvations that is a subset of a larger population. However, information derived\\nfrom a sample of observations can only be an approximation of the entire\\npopulation. To make a definitive statement about an entire population,\\nevery member of that population would need to be measured. For example,\\nif we wanted to say for certain that the average weight of men in the United\\nStates is 194.7 lb, we would have to collect the weight measurements'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 50}, page_content='CONFIDENCE INTERVALS\\n37\\nfor every man living in the United States and derive a mean from these\\nobservations. This is not possible or practical in most situations.\\nIt is possible, however, to make estimates about a population by using\\nconfidence intervals. Confidence intervals are a measure of our uncertainty\\nabout the statistics we calculate from a single sample of observations. For\\nexample, the confidence interval might state that the average weight of\\nmen in the United States is between 191.2 lb and 198.2 lb to take into\\naccount the uncertainty of measuring only a sample of the total population.\\nOnly if the sample of observations is a truly random sample of the entire\\npopulation can these types of estimates be made.\\nTo understand how a statistic, such as the mean or mode, calculated\\nfrom a single sample can reliably be used to infer a corresponding value\\nof the population that cannot be measured and is therefore unknown, you\\nneed to understand something about sample distributions. Each statistic\\nhas a corresponding unknown value in the population called a parameter\\nthat can be estimated. In the example used in this section, we chose to\\ncalculate the statistic mean for the weight of US males. The mean value\\nfor the random sample selected is calculated to be 194.7 lb. If another\\nrandom sample with the same number of observations were collected,\\nthe mean could also be calculated and it is likely that the means of the\\ntwo samples would be close but not identical. If we take many random\\nsamples of equal size and calculate the mean value from each sample,\\nwe would begin to form a frequency distribution. If we were to take\\ninfinitely many samples of equal size and plot on a graph the value of\\nthe mean calculated from each sample, it would produce a normal fre-\\nquency distribution that reflects the distribution of the sample means for\\nthe population mean under consideration. The distribution of a statistic\\ncomputed for each of many random samples is called a sampling distri-\\nbution. In our example, we would call this the sampling distribution of\\nthe mean.\\nJust as the distributions for a statistical variable discussed in earlier\\nsections have a mean and a standard deviation, so also does the sampling\\ndistribution. However, to make clear when these measures are being used\\nto describe the distribution of a statistic rather than the distribution of a\\nvariable, distinct names are used. The mean of a sampling distribution\\nis called the expected value of the mean: it is the mean expected of the\\npopulation. The standard deviation of the sampling distribution is called\\nthe standard error: it measures how much error to expect from equally\\nsized random samples drawn from the same population. The standard error\\ninforms us of the average difference between the mean of a sample and the\\nexpected value.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 51}, page_content='38\\nDESCRIBING DATA\\nThe sample size is important. It is beyond the scope of this book to\\nexplain the details, but regardless of how the values of a variable for the\\npopulation are distributed, the sampling distribution of a statistic calculated\\non samples from that variable will have a normal form when the size chosen\\nfor the samples has at least 30 observations. This is known as the law of\\nlarge numbers, or more formally as the central limit theorem.\\nThe standard error plays a fundamental role in inferential statistics by\\nproviding a measurable level of confidence in how well a sample mean\\nestimates the mean of the population. The standard error can be calculated\\nfrom a sample using the following formula:\\nstandard error of the sampling distribution =\\ns\\n√\\nn\\nwhere s is the standard deviation of a sample and n is the number of\\nobservations in the sample. Because the size n is in the denominator and\\nthe standard deviation s is in the numerator, small samples with large\\nvariations increase the standard error, reducing the confidence that the\\nsample statistic is a close approximation of the population parameter we\\nare trying to estimate.\\nThe data analyst or the team calculating the confidence interval should\\ndecide what the desired level of confidence should be. Confidence intervals\\nare often based on a 95% confidence level, but sometimes a more stringent\\n99% confidence level or less stringent 90% level is used. Using a confidence\\ninterval of 95% to illustrate, one way to interpret this confidence level is\\nthat, on average, the correct population value will be found within the\\nderived confidence interval 95 times out of every 100 samples collected.\\nIn these 100 samples, there will be 5 occasions on average when this value\\ndoes not fall within the range. The confidence level is usually stated in\\nterms of 𝛼from the following equation:\\nconfidence interval = 100 × (1 −𝛼)\\nFor a 90% confidence level alpha is 0.1; for a 95% confidence level\\nalpha is 0.05; for a 99% confidence level 𝛼is 0.01; and so on. The value\\nused for this level of confidence will affect the size of the interval; that is,\\nthe higher the desired level of confidence the wider the confidence interval.\\nAlong with the value of 𝛼selected, the confidence interval is based on\\nthe standard error. The estimated range or confidence interval is calcu-\\nlated using this confidence level along with information on the number of'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 52}, page_content='CONFIDENCE INTERVALS\\n39\\nFIGURE 2.15\\nIllustration of the standard z-distribution to calculate z𝛼∕2.\\nobservations in the sample as well as the variation in the sample’s data. The\\nformula showing a confidence interval for a mean value is shown here:\\n̄x ± z𝛼∕2\\n(\\ns\\n√\\nn\\n)\\nwhere ̄x is the mean value, s is the standard deviation, and n is the number\\nof observations in the sample. The value for z𝛼∕2 is based on the area to\\nthe right of a standard z-distribution as illustrated in Figure 2.15 since the\\ntotal area under this curve is 1. This number can be derived from a standard\\nstatistical table or computer program.\\nTo illustrate, the fuel efficiency of 100 specific cars is measured and a\\nmean value of 30.35 MPG is calculated with a standard deviation of 2.01.\\nUsing an alpha value of 0.05 (which translates into a z𝛼∕2 value of 1.96),\\nthe confidence interval is calculated as\\n̄x ± z𝛼∕2\\n(\\ns\\n√\\nn\\n)\\n30.35 ± 1.96\\n(\\n2.01\\n√\\n100\\n)\\nHence, the confidence interval for the average fuel efficiency is 30.35 ±\\n0.393 or between 29.957 and 30.743.\\nFor calculation of a confidence interval where sigma is unknown and\\nthe number of observations is less than 30 observations, a t-distribution\\nshould be used (see Urdan (2010), Anderson et al. (2010), Witte & Witte\\n(2009), Kachigan (1991), Freedman et al. (2007), and Vickers (2010) for\\nmore details).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 53}, page_content='40\\nDESCRIBING DATA\\n2.7 HYPOTHESIS TESTS\\nHypothesis tests are used to support making decisions by helping to under-\\nstand whether data collected from a sample of all possible observations\\nsupports a particular hypothesis. For example, a company manufacturing\\nhair care products wishes to say that the average amount of shampoo within\\nthe bottles is 200 mL. To test this hypothesis, the company collects a ran-\\ndom sample of 100 shampoo bottles and precisely measures the contents\\nof the bottle. If it is inferred from the sample that the average amount of\\nshampoo in each bottle is not 200 mL then a decision may be made to stop\\nproduction and rectify the manufacturing problem.\\nThe first step is to formulate the hypothesis that will be tested. This\\nhypothesis is referred to as the null hypothesis (H0). The null hypothesis\\nis stated in terms of what would be expected if there were nothing unusual\\nabout the measured values of the observations in the data from the samples\\nwe collect—“null” implies the absence of effect. In the example above, if\\nwe expected each bottle of shampoo to contain 200 mL of shampoo, the\\nnull hypothesis would be: the average volume of shampoo in a bottle is\\n200 mL. Its corresponding alternative hypothesis (Ha) is that they differ\\nor, stated in a way that can be measured, that the average is not equal to\\n200 mL. For this example, the null hypothesis and alternative hypothesis\\nwould be shown as\\nH0 : 𝜇= 200\\nHa : 𝜇≠200\\nThis hypothesis will be tested using the sample data collected to deter-\\nmine whether the mean value is different enough to warrant rejecting the\\nnull hypothesis. Hence, the result of a hypothesis test is either to fail to\\nreject or reject the null hypothesis. Since we are only looking at a sample of\\nthe observations—we are not testing every bottle being manufactured—it\\nis impossible to make a statement about the average with total certainty.\\nConsequently, it is possible to reach an incorrect conclusion. There are two\\ncategories of errors that can be made. One is to reject the null hypothesis\\nwhen, in fact, the null hypothesis should stand (referred to as a type I\\nerror); the other is to accept the null hypothesis when it should be rejected\\n(or type II error). The threshold of probability used to determine a type I\\nerror should be decided before performing the test. This threshold, which\\nis also referred to as the level of significance or 𝛼, is often set to 0.05\\n(5% chance of a type I error); however, more stringent (such as 0.01 or 1%'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 54}, page_content='HYPOTHESIS TESTS\\n41\\nchance) or less stringent values (such as 0.1 or 10% chance) can be used\\ndepending on the consequences of an incorrect decision.\\nThe next step is to specify the standardized test statistic (T). We are inter-\\nested in determining whether the average of the sample data we collected is\\neither meaningfully or trivially different from the population average. Is it\\nlikely that we would find as great a difference from the population average\\nwere we to collect other random samples of the same size and compare\\ntheir mean values? Because the hypothesis involves the mean, we use the\\nfollowing formula to calculate the test statistic:\\nT = ̄x −𝜇0\\ns/√\\nn\\nwhere ̄x is the calculated mean value of the sample, 𝜇0 is the population\\nmean that is the subject of the hypothesis test, s is the standard deviation of\\nthe sample, and n is the number of observations. (Recall that the denom-\\ninator is the standard error of the sampling distribution.) In this example,\\nthe average shampoo bottle volume measured over the 100 samples (n) is\\n199.94 (̄x) and the standard deviation is 0.613 (s).\\nT = 199.94 −200\\n0.613/√\\n100\\n= −0.979\\nAssuming we are using a value for 𝛼of 0.05 as the significance level to\\nformulate a decision rule to either let the null hypothesis stand or reject it,\\nit is necessary to identify a range of values where 95% of all the sample\\nmeans would lie. As discussed in Section 2.6, the law of large numbers\\napplies to the sampling distribution of the statistic T: when there are at\\nleast 30 observations, the frequency distribution of the sample means is\\napproximately normal and we can use this distribution to estimate regions\\nto accept the null hypothesis. This region has two critical upper and lower\\nbound values C1 and C2. Ninety-five percent of all sample means lie\\nbetween these values and 5% lie outside these values (0.025 below C2 and\\n0.025 above C1) (see Figure 2.16). We reject the null hypothesis if the\\nvalue of T is outside this range (i.e., greater than C1 or less than C2) or let\\nthe null hypothesis stand if it is inside the range.\\nValues for C1 and C2 can be calculated using a standard z-distribution\\ntable lookup and would be C2 = −1.96 and C1 = +1.96. These z-values\\nwere selected where the combined area to the left of C2 and to the right of\\nC1 would equal 0.05.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 55}, page_content='42\\nDESCRIBING DATA\\nFIGURE 2.16\\nStandard z-distribution.\\nSince T is −0.979 and is greater than −1.96 and less than +1.96, we\\nlet the null hypothesis stand and conclude that the value is within the\\nacceptable range.\\nIn this example, the hypothesis test is referred to as a two-tailed test—\\nthat is, we tested the hypothesis for values above and below the critical\\nvalues; however, hypothesis tests can be structured such that they are testing\\nfor values only above or below a value.\\nIt is a standard practice to also calculate a p-value which corresponds\\nto the probability of obtaining a test statistic value at least as extreme as\\nthe observed value (assuming the null hypothesis is true). This p-value can\\nalso be used to assess the null hypothesis, where the null hypothesis is\\nrejected if it is less than the value of alpha. This value can be looked up\\nusing a standard z-distribution table as found by an online search or readily\\navailable software. In this example, the p-value would be 0.33. Since this\\nvalue is not less than 0.05 (as defined earlier) we again do not reject the\\nnull hypothesis.\\nEXERCISES\\nA set of 10 hypothetical patient records from a large database is presented\\nin Table 2.4. Patients with a diabetes value of 1 have type-II diabetes and\\npatients with a diabetes value of 0 do not have type-II diabetes.\\n1. For each of the following variables, assign them to one of the following\\nscales: nominal, ordinal, interval, or ratio:\\n(a) Name\\n(b) Age\\n(c) Gender\\n(d) Blood group\\n(e) Weight (kg)'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 56}, page_content='TABLE 2.4\\nTable of Patient Records\\nName\\nAge\\nGender\\nBlood\\nGroup\\nWeight\\n(kg)\\nHeight\\n(m)\\nSystolic Blood\\nPressure\\n(mmHg)\\nDiastolic Blood\\nPressure\\n(mmHg)\\nDiabetes\\nP. Lee\\n35\\nFemale\\nA Rh+\\n50\\n1.52\\n68\\n112\\n0\\nR. Jones\\n52\\nMale\\nO Rh−\\n115\\n1.77\\n110\\n154\\n1\\nJ. Smith\\n45\\nMale\\nO Rh+\\n96\\n1.83\\n88\\n136\\n0\\nA. Patel\\n70\\nFemale\\nO Rh−\\n41\\n1.55\\n76\\n125\\n0\\nM. Owen\\n24\\nMale\\nA Rh−\\n79\\n1.82\\n65\\n105\\n0\\nS. Green\\n43\\nMale\\nO Rh−\\n109\\n1.89\\n114\\n159\\n1\\nN. Cook\\n68\\nMale\\nA Rh+\\n73\\n1.76\\n108\\n136\\n0\\nW. Hands\\n77\\nFemale\\nO Rh−\\n104\\n1.71\\n107\\n145\\n1\\nP. Rice\\n45\\nFemale\\nO Rh+\\n64\\n1.74\\n101\\n132\\n0\\nF. Marsh\\n28\\nMale\\nO Rh+\\n136\\n1.78\\n121\\n165\\n1\\n43'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 57}, page_content='44\\nDESCRIBING DATA\\n(f) Height (m)\\n(g) Systolic blood pressure (mmHg)\\n(h) Diastolic blood pressure (mmHg)\\n(i) Diabetes\\nTABLE 2.5\\nTable with\\nVariables Name and Age\\nName\\nAge\\nP. Lee\\n35\\nR. Jones\\n52\\nJ. Smith\\n45\\nA. Patel\\n70\\nM. Owen\\n24\\nS. Green\\n43\\nN. Cook\\n68\\nW. Hands\\n77\\nP. Rice\\n45\\nF. Marsh\\n28\\nTABLE 2.6\\nRetail Transaction Data Set\\nCustomer\\nStore\\nProduct\\nCategory\\nProduct\\nDescription\\nSale\\nPrice\\n($)\\nProfit\\n($)\\nB. March\\nNew York, NY\\nLaptop\\nDR2984\\n950\\n190\\nB. March\\nNew York, NY\\nPrinter\\nFW288\\n350\\n105\\nB. March\\nNew York, NY\\nScanner\\nBW9338\\n400\\n100\\nJ. Bain\\nNew York, NY\\nScanner\\nBW9443\\n500\\n125\\nT. Goss\\nWashington, DC\\nPrinter\\nFW199\\n200\\n60\\nT. Goss\\nWashington, DC\\nScanner\\nBW39339\\n550\\n140\\nL. Nye\\nNew York, NY\\nDesktop\\nLR21\\n600\\n60\\nL. Nye\\nNew York, NY\\nPrinter\\nFW299\\n300\\n90\\nS. Cann\\nWashington, DC\\nDesktop\\nLR21\\n600\\n60\\nE. Sims\\nWashington, DC\\nLaptop\\nDR2983\\n700\\n140\\nP. Judd\\nNew York, NY\\nDesktop\\nLR22\\n700\\n70\\nP. Judd\\nNew York, NY\\nScanner\\nFJ3999\\n200\\n50\\nG. Hinton\\nWashington, DC\\nLaptop\\nDR2983\\n700\\n140\\nG. Hinton\\nWashington, DC\\nDesktop\\nLR21\\n600\\n60\\nG. Hinton\\nWashington, DC\\nPrinter\\nFW288\\n350\\n105\\nG. Hinton\\nWashington, DC\\nScanner\\nBW9443\\n500\\n125\\nH. Fu\\nNew York, NY\\nDesktop\\nZX88\\n450\\n45\\nH. Taylor\\nNew York, NY\\nScanner\\nBW9338\\n400\\n100'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 58}, page_content='FURTHER READING\\n45\\n2. Calculate the following statistics for the variable Age (from Table 2.5):\\n(a) Mode\\n(b) Median\\n(c) Mean\\n(d) Range\\n(e) Variance\\n(f) Standard deviation\\n3. Using the data in Table 2.6, create a histogram of Sale Price ($) using\\nthe following intervals: 0 to less than 250, 250 to less than 500, 500 to\\nless than 750, and 750 to less than 1000.\\nFURTHER READING\\nA number of books provide basic introductions to statistical methods including\\nDonnelly (2007) and Levine & Stephan (2010). Numerous books provide addi-\\ntional details on the descriptive and inferential statistics as, for example, Urdan\\n(2010), Anderson et al. (2010), Witte & Witte (2009), Kachigan (1991), Freed-\\nman et al. (2007), and Vickers (2010). The conceptual difference between standard\\nerror and standard deviation described in Sections 2.6 and 2.7 is often difficult to\\ngrasp. For further discussion, see the section on sampling distributions in Kachigan\\n(1991) and the chapter on standard error in Vickers (2010). For further reading on\\ncommunicating information, see Tufte (1990, 1997a, 1997b, 2001, 2006). These\\nworks describe a theory of data graphics and information visualization that are\\nillustrated by many examples.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 59}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 60}, page_content='CHAPTER 3\\nPREPARING DATA TABLES\\n3.1 OVERVIEW\\nPreparing the data is one of the most time-consuming parts of a data\\nanalysis/data mining project. This chapter outlines concepts and steps\\nnecessary to prepare a data set prior to beginning data analysis or data\\nmining. The way in which the data is collected and prepared is critical\\nto the confidence with which decisions can be made. The data needs\\nto be merged into a table and this may involve integration of the data\\nfrom multiple sources. Once the data is in a tabular format, it should be\\nfully characterized as discussed in the previous chapter. The data should\\nbe cleaned by resolving ambiguities and errors, removing redundant and\\nproblematic data, and eliminating columns of data irrelevant to the analysis.\\nNew columns of data may need to be calculated. Finally, the table should\\nbe divided, where appropriate, into subsets that either simplify the analysis\\nor allow specific questions to be answered more easily.\\nIn addition to the work done preparing the data, it is important to record\\nthe details about the steps that were taken and why they were done. This\\nnot only provides documentation of the activities performed so far, but it\\nalso provides a methodology to apply to similar data sets in the future. In\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n47'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 61}, page_content='48\\nPREPARING DATA TABLES\\naddition, when validating the results, these records will be important for\\nrecalling assumptions made about the data.\\nThe following chapter outlines the process of preparing data for analy-\\nsis. It includes methods for identifying and cleaning up errors, removing\\ncertain variables or observations, generating consistent scales across differ-\\nent observations, generating new frequency distributions, converting text\\nto numbers and vice versa, combining variables, generating groups, and\\npreparing unstructured data.\\n3.2 CLEANING THE DATA\\nFor variables measured on a nominal or ordinal scale (where there are a\\nfixed number of possible values) it is useful to inspect all possible values\\nto uncover mistakes, duplications and inconsistencies. Each value should\\nmap onto a unique term. For example, a variable Company may include\\na number of different spellings for the same company such as “General\\nElectric Company,” “General Elec. Co.,” “GE,” “Gen. Electric Company,”\\n“General electric company,” and “G.E. Company.” When these values refer\\nto the same company, the various terms should be consolidated into one.\\nSubject matter expertise may be needed to correct and harmonize these\\nvariables. For example, a company name may include one of the divisions\\nof the General Electric Company and for the purpose of this specific project\\nit should also be included as the “General Electric Company.”\\nA common problem with numeric variables is the inclusion of non-\\nnumeric terms. For example, a variable generally consisting of numbers\\nmay include a value such as “above 50” or “out of range.” Numeric analysis\\ncannot interpret a non-numeric value and hence, relying on subject matter\\nexpertise, these terms should be converted to a number or the observation\\nremoved.\\nAnother problem arises when observations for a particular variable are\\nmissing data values. Where there is a specific meaning for a missing data\\nvalue, the value may be replaced based on knowledge of how the data was\\ncollected.\\nIt can be more challenging to clean variables measured on an interval or\\nratio scale since they can take any possible value within a range. However,\\nit is useful to consider outliers in the data. Outliers are a single or a small\\nnumber of data values that differ greatly from the rest of the values. There\\nare many reasons for outliers. For example, an outlier may be an error\\nin the measurement or the result of measurements made using a different\\ncalibration. An outlier may also be a legitimate and valuable data point.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 62}, page_content='GENERATING CONSISTENT SCALES ACROSS VARIABLES\\n49\\nFIGURE 3.1\\nHistogram showing an outlier.\\nHistograms and box plots can be useful in identifying outliers as previously\\ndescribed. The histogram in Figure 3.1 displays a variable Height where\\none value is eight times higher than the average of all data points.\\nA particular variable may have been measured over different units. For\\nexample, a variable Weight may have been measured using both pounds and\\nkilograms for different observations or a variable Price may be measured\\nin different currencies. These should be standardized to a single scale so\\nthat they can be compared during analysis. In situations where data has\\nbeen collected over time, changes related to the passing of time may no\\nlonger be relevant to the analysis. For example, when looking at a variable\\nCost of production for which the data has been collected over many years,\\nthe rise in costs attributable to inflation may need to be considered for the\\nanalysis. When data is combined from multiple sources, an observation\\nis more likely to have been recorded more than once. Duplicate entries\\nshould be removed.\\n3.3 REMOVING OBSERVATIONS AND VARIABLES\\nAfter an initial categorization of the variables, it may be possible to remove\\nvariables from consideration. For example, constants and variables with\\ntoo many missing data values would be candidates for removal. Similarly,\\nit may be necessary to remove observations that have data missing for a\\nparticular variable. For more information on this process, see Section 3.10.\\n3.4 GENERATING CONSISTENT SCALES\\nACROSS VARIABLES\\nSometimes data analysis and data mining programs have difficulty pro-\\ncessing data in its raw form. For these cases, certain mathematical'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 63}, page_content='50\\nPREPARING DATA TABLES\\ntransformations can be applied to the data. Normalization uses a mathemat-\\nical function to transform numeric columns to a new range. Normalization\\nis important in preventing certain data analysis methods from giving some\\nvariables undue influence over others because of differences in the range\\nof their values. For example, when analyzing customer credit card data,\\nthe Credit limit value (whose values might range from $500 to $100,000)\\nshould not be given more weight in the analysis than the Customer’s age\\n(whose values might range from 18 to 100).\\nThe min–max transformation maps the values of a variable to a new\\nrange, such as from 0 to 1. The following formula is used:\\nx′\\ni =\\nxi −OriginalMin\\nOriginalMax −OriginalMin × (NewMax −NewMin) + NewMin\\nwhere x′\\ni is the new normalized value, xi is the original variable’s value,\\nOriginalMin is the minimum possible value in the original variable, Orig-\\ninalMax is the maximum original possible value, NewMin is the minimum\\nvalue for the normalized range, and NewMax is the maximum value for\\nthe normalized range. Since the minimum and maximum values for the\\noriginal variable are needed, if the original data does not contain the full\\nrange, either an estimate of the range is needed or the formula should be\\nrestricted to the range specified for future use.\\nThe z-score transformation normalizes the values around the mean of\\nthe set, with differences from the mean being recorded as standardized\\nunits, based on the frequency distribution of the variable, as discussed in\\nSection 2.5.7.\\nThe decimal scaling transformation moves the decimal point to ensure\\nthe range is between 1 and −1. The following formula is used:\\nx′\\ni = xi\\n10n\\nwhere n is the number of digits of the maximum absolute value. For\\nexample, if the largest number is 9948 then n would be 4. 9948 would\\nnormalize to 9948/104 or 9948/10,000 or 0.9948.\\nThe normalization process is illustrated using the data in Table 3.1.\\nAs an example, to calculate the normalized values using the min–max\\nequation for the variable Weight, first the minimum and maximum values\\nshould be identified: OriginalMin = 1613 and OriginalMax = 5140. The\\nnew normalized values will be between 0 and 1, hence NewMin = 0 and'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 64}, page_content='NEW FREQUENCY DISTRIBUTION\\n51\\nTABLE 3.1\\nNormalization of the Variable Weight Using the Min–Max,\\nz-score, and Decimal Scaling Transformations\\nMin–Max\\nz-score\\nDecimal Scaling\\nCar Name\\nWeight\\n(Weight)\\n(Weight)\\n(Weight)\\nDatsun 1200\\n1613\\n0\\n−1.59\\n0.161\\nHonda Civic Cvcc\\n1800\\n0.053\\n−1.37\\n0.18\\nVolkswagen Rabbit\\n1825\\n0.0601\\n−1.34\\n0.182\\nRenault 5 gtl\\n1825\\n0.0601\\n−1.34\\n0.182\\nVolkswagen Super Beetle\\n1950\\n0.0955\\n−1.19\\n0.195\\nMazda glc 4\\n1985\\n0.105\\n−1.15\\n0.198\\nFord Pinto\\n2046\\n0.123\\n−1.08\\n0.205\\nPlymouth Horizon\\n2200\\n0.166\\n−0.898\\n0.22\\nToyota Corolla\\n2265\\n0.185\\n−0.822\\n0.226\\nAMC Spirit dl\\n2670\\n0.3\\n−0.345\\n0.267\\nFord Maverick\\n3158\\n0.438\\n0.229\\n0.316\\nPlymouth Volare Premier v8\\n3940\\n0.66\\n1.15\\n0.394\\nDodge d200\\n4382\\n0.785\\n1.67\\n0.438\\nPontiac Safari (sw)\\n5140\\n1\\n2.56\\n0.514\\nNewMax = 1. To calculate the new min–max normalized value for the Ford\\nmaverick using the formula:\\nx′\\ni =\\nxi −OriginalMin\\nOriginalMax −OriginalMin × (NewMax −NewMin) + NewMin\\nx′\\ni = 3158 −1613\\n5140 −1613 × (1 −0) + 0\\nx′\\ni = 0.438\\nTable 3.1 shows some of the calculated normalized values for the min–\\nmax normalization, the z-score normalization, and the decimal scaling\\nnormalization.\\n3.5 NEW FREQUENCY DISTRIBUTION\\nA variable may not conform to a normal frequency distribution; however,\\ncertain data analysis methods may require that the data follow a normal\\ndistribution. Methods for visualizing and describing normal frequency\\ndistributions are described in the previous chapter. To transform the data'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 65}, page_content='52\\nPREPARING DATA TABLES\\n–3\\n1400\\n1200\\n1000\\n800\\n600\\nIC50\\nLog(IC50)\\n400\\n200\\n0\\nFrequency\\nFrequency\\n–2\\n0\\n0\\n15\\n30\\n45\\n60\\n5\\n10\\n15\\n20\\n–1\\n0\\n1\\n2\\n3\\n4\\nFIGURE 3.2\\nLog transformation converting a variable (IC50) to adjust the\\nfrequency distribution.\\nso that it more closely approximates a normal distribution, it may be\\nnecessary to take the log, exponential, or a Box–Cox transformation. The\\nformula for a Box–Cox transformation is\\nx′\\ni =\\nx𝜆\\ni −1\\n𝜆\\nwhere 𝜆is a value greater than 1.\\nFigure 3.2 shows how the distribution of the original variable IC50\\n(figure on the left) is transformed to a closer approximation of the normal\\ndistribution after the log transformation has been applied (figure on the\\nright).\\n3.6 CONVERTING TEXT TO NUMBERS\\nTo use variables that have been assigned as nominal or ordinal and\\ndescribed using text values within certain numerical analysis methods,\\nit is necessary to convert the variable’s values into numbers. For exam-\\nple, a variable with values “low,” “medium” and “high” may have “low”\\nreplaced by 0, “medium” replaced by 1, and “high” replaced by 2.\\nAnother way to handle nominal data is to convert each value into a\\nseparate column with values 1 (indicating the presence of the category)\\nand 0 (indicating the absence of the category). These new variables are\\noften referred to as dummy variables. In Table 3.2, for example, the variable\\nColor has now been divided into five separate columns, one for each color.\\nWhile we have shown a column for each color, in practice five dummy\\nvariables are not needed to encode the five colors. We could get by with\\nonly four variables (Color = “Black,” Color = “Blue,” Color = “Red,” and\\nColor = “Green”). To represent the five colors, the values for the “Black,”'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 66}, page_content='CONVERTING CONTINUOUS DATA TO CATEGORIES\\n53\\nTABLE 3.2\\nGenerating a Series of Dummy Variables from the Single\\nColor Variable\\nColor =\\nColor =\\nColor =\\nColor =\\nColor =\\nProduct ID\\nColor\\nBlack\\nBlue\\nRed\\nGreen\\nWhite\\n89893-22\\nBlack\\n1\\n0\\n0\\n0\\n0\\n849082-35\\nBlue\\n0\\n1\\n0\\n0\\n0\\n27037-84\\nRed\\n0\\n0\\n1\\n0\\n0\\n2067-09\\nGreen\\n0\\n0\\n0\\n1\\n0\\n44712-61\\nWhite\\n0\\n0\\n0\\n0\\n1\\n98382-34\\nBlue\\n0\\n1\\n0\\n0\\n0\\n72097-52\\nGreen\\n0\\n0\\n0\\n1\\n0\\n“Blue,” “Red,” and “Green” variables would be for Black: 1,0,0,0, for Blue:\\n0,1,0,0, for Red: 0,0,1,0, for Green: 0,0,0,1, and for White: 0,0,0,0.\\n3.7 CONVERTING CONTINUOUS DATA TO CATEGORIES\\nBy converting continuous data into discrete values, it might appear that\\nwe are reducing the information content of the variable. However, this\\nconversion is desirable in a number of situations. First, where a value\\nis defined on an interval or ratio scale but when knowledge about how\\nthe data was collected suggests the accuracy of the data does not warrant\\nthese scales, a variable may be a candidate for conversion to a categorical\\nvariable that reflects the true variation in the data. Second, because certain\\ntechniques can only process categorical data, converting continuous data\\ninto discrete values makes a numeric variable accessible to these methods.\\nFor example, a continuous variable credit score may be divided into four\\ncategories: poor, average, good, and excellent; or a variable Weight that\\nhas a range from 0 to 350 lb may be divided into five categories: less than\\n100 lb, 100–150 lb, 150–200 lb, 200–250 lb, and above 250 lb. All values\\nfor the variable Weight must now be assigned to a category and assigned\\nan appropriate value such as the mean of the assigned category. It is often\\nuseful to use the frequency distribution to understand appropriate range\\nboundaries.\\nThis process can also be applied to nominal variables, especially in\\nsituations where there are a large number of values for a given nominal\\nvariable. If the data set were to be summarized using each of the values,\\nthe number of observations for each value may be too small to reach any'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 67}, page_content='54\\nPREPARING DATA TABLES\\nmeaningful conclusions. However, a new variable could be created that\\ngeneralizes the values using a mapping of terms. For example, a data set\\nconcerning customer transactions may contain a variable Company that\\ndetails the individual customer’s company. There may only be a handful of\\nobservations for each company. However, this variable could be mapped\\nonto a new variable, Industries. The mapping of specific companies onto\\ngeneralized industries must be defined using a concept mapping (i.e., which\\ncompany maps onto which industry). Now, when the data set is summarized\\nusing the values for the Industries variable, meaningful trends may be\\nobserved.\\n3.8 COMBINING VARIABLES\\nThe variable that you are trying to use may not be present in the data set but\\nit may be derived from existing variables. Mathematical operations, such as\\naverage or sum, could be applied to one or more variables in order to create\\nan additional variable. For example, a project may be trying to understand\\nissues regarding a particular car’s fuel efficiency (Fuel Efficiency) using a\\ndata set of different journeys in which the fuel level at the start (Fuel Start)\\nand end (Fuel End) of a trip is measured along with the distance covered\\n(Distance). An additional column may be calculated using the following\\nformula:\\nFuel Efficiency = (Fuel End −Fuel Start)∕Distance\\nDifferent approaches to generating new variables to support model build-\\ning will be discussed in Chapter 6.\\n3.9 GENERATING GROUPS\\nGenerally, larger data sets take more computational time to analyze and\\ncreating subsets from the data can speed up the analysis. One approach\\nis to take a random subset which is effective where the data set closely\\nmatches the target population.\\nAnother reason for creating subsets is when a data set that has been\\nbuilt up over time for operational purposes, but is now to be used to\\nanswer an alternative business research question. It may be necessary to\\nselect a diverse set of observations that more closely matches the new\\ntarget population. For example, suppose a car safety organization has been'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 68}, page_content='PREPARING UNSTRUCTURED DATA\\n55\\nmeasuring the safety of individual cars based on specific requests from\\nthe government. Over time, the government may have requested car safety\\nstudies for certain types of vehicles. If the historical data set is to be used\\nto answer questions on the safety of all cars, this data set does not reflect\\nthe new target population. However, a subset of the car studies could be\\nselected to represent the more general questions now being asked of the\\ndata. The chapter on grouping will discuss how to create diverse data sets\\nwhen the data does not represent the target population.\\nA third reason is that when building predictive models from a data set, it\\nis important to keep the models as simple as possible. Breaking the data set\\ndown into subsets based on your knowledge of the data may allow you to\\ncreate several simpler models. For example, a project to model factors that\\ncontribute to the price of real estate may use a data set of nationwide house\\nprices and associated factors. However, your knowledge of the real estate\\nmarket suggests that factors contributing to housing prices are contingent\\nupon the area in which the house is located. Factors that contribute to\\nhouse prices in coastal locations are different from factors that contribute\\nto house prices in the mountains. It may make sense in this situation to\\ndivide the data set up into smaller sets based on location and model these\\nlocales separately. When doing this type of subsetting, it is important to\\nnote the criteria you are using to subset the data. The specific criteria is\\nneeded when data to be predicted are presented for modeling by assigning\\nthe data to one or more models. In situations where multiple predictions are\\ngenerated for the same unknown observation, a method for consolidating\\nthese predictions is required.\\n3.10 PREPARING UNSTRUCTURED DATA\\nIn many disciplines, the focus of a data analysis or data mining project\\nis not a simple data table of observations and variables. For example, in\\nthe life sciences, the focus of the analysis is genes, proteins, biological\\npathways, and chemical structures. In other disciplines, the focus of the\\nanalysis could be documents, web logs, device readouts, audio or video\\ninformation, and so on. In the analysis of these types of data, a preliminary\\nstep is often the computational generation of different attributes relevant\\nto the problem. For example, when analyzing a data set of chemicals, an\\ninitial step is to generate variables based on the composition of the chemical\\nsuch as its molecular weight or the presence or absence of molecular\\ncomponents.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 69}, page_content='TABLE 3.3\\nTable of Patient Records\\nSystolic Blood\\nDiastolic Blood\\nBlood\\nWeight\\nHeight\\nPressure\\nPressure\\nTemperature\\nName\\nAge\\nGender\\nGroup\\n(kg)\\n(m)\\n(mm Hg)\\n(mm Hg)\\n(◦F)\\nDiabetes\\nP. Lee\\n35\\nFemale\\nA Rh+\\n50\\n1.52\\n68\\n112\\n98.7\\n0\\nR. Jones\\n52\\nMale\\nO Rh−\\n115\\n1.77\\n110\\n154\\n98.5\\n1\\nJ. Smith\\n45\\nMale\\nO Rh+\\n96\\n1.83\\n88\\n136\\n98.8\\n0\\nA. Patel\\n70\\nFemale\\nO Rh−\\n41\\n1.55\\n76\\n125\\n98.6\\n0\\nM. Owen\\n24\\nMale\\nA Rh−\\n79\\n1.82\\n65\\n105\\n98.7\\n0\\nS. Green\\n43\\nMale\\nO Rh−\\n109\\n1.89\\n114\\n159\\n98.9\\n1\\nN. Cook\\n68\\nMale\\nA Rh+\\n73\\n1.76\\n108\\n136\\n99.0\\n0\\nW. Hands\\n77\\nFemale\\nO Rh−\\n104\\n1.71\\n107\\n145\\n98.3\\n1\\nP. Rice\\n45\\nFemale\\nO Rh+\\n64\\n1.74\\n101\\n132\\n98.6\\n0\\nF. Marsh\\n28\\nMale\\nO Rh+\\n136\\n1.78\\n121\\n165\\n98.7\\n1\\n56'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 70}, page_content='FURTHER READING\\n57\\nEXERCISES\\nA set of 10 hypothetical patient records from a large database is presented\\nin Table 3.3. Patients with a diabetes value of 1 have type-II diabetes and\\npatients with a diabetes value of 0 do not have type-II diabetes.\\n1. Create a new column by normalizing the Weight (kg) variable into\\nthe range 0–1 using the min–max normalization.\\n2. Create a new column by binning the Weight (kg) variable into three\\ncategories: low (less than 60 kg), medium (60–100 kg), and high\\n(greater than 100 kg).\\n3. Create an aggregated column, body mass index (BMI), which is\\ndefined by the formula:\\nBMI = Weight(kg)\\n(Height(m))2\\nFURTHER READING\\nFor additional data preparation approaches including the handling of missing data\\nsee Pearson (2005), Pyle (1999), and Dasu & Johnson (2003).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 71}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 72}, page_content='CHAPTER 4\\nUNDERSTANDING RELATIONSHIPS\\n4.1 OVERVIEW\\nA critical step in making sense of data is an understanding of the rela-\\ntionships between different variables. For example, is there a relation-\\nship between interest rates and inflation or education level and income?\\nThe existence of an association between variables does not imply that\\none variable causes another. These relationships or associations can be\\nestablished through an examination of different summary tables and data\\nvisualizations as well as calculations that measure the strength and con-\\nfidence in the relationship. The following sections examine a number\\nof ways to understand relationships between pairs of variables through\\ndata visualizations, tables that summarize the data, and specific calcu-\\nlated metrics. Each approach is driven by how the variables have been\\ncategorized such as the scale on which they are measured. The use\\nof data visualizations is important as it takes advantage of the human\\nvisual system’s ability to recognize complex patterns in what is seen\\ngraphically.\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n59'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 73}, page_content='60\\nUNDERSTANDING RELATIONSHIPS\\n4.2 VISUALIZING RELATIONSHIPS BETWEEN VARIABLES\\n4.2.1 Scatterplots\\nScatterplots can be used to identify whether a relationship exists between\\ntwo continuous variables measured on the ratio or interval scales. The\\ntwo variables are plotted on the x-and y-axis. Each point displayed on the\\nscatterplot is a single observation. The position of the point is determined\\nby the value of the two variables. The scatterplot in Figure 4.1 presents\\nhundreds of observations on a single chart.\\nScatterplots allow you to see the type of relationship that may exist\\nbetween two variables. A positive relationship results when higher values\\nin the first variable coincide with higher values in the second variable\\nand lower values in the first variable coincide with lower values in the\\nsecond variable (the points in the graph are trending upward from left to\\nright). Negative relationships result when higher values in the first variable\\ncoincide with lower values in the second variable and lower values in the\\nfirst variable coincide with higher values in the second variable (the points\\nare trending downward from left to right). For example, the scatterplot in\\nFigure 4.2 shows that the relationship between petal length (cm) and sepal\\nlength (cm) is positive.\\nThe nature of the relationships—linearity or nonlinearity—is also\\nimportant. A linear relationship exists when a second variable changes\\nproportionally in response to changes in the first variable. A nonlinear rela-\\ntionship is drawn as a curve indicating that as the first variable changes,\\nthe change in the second variable is not proportional. In Figure 4.2 the\\n5.5\\n5\\n4.5\\n4\\n3.5\\n3\\n2.5\\n2\\n1.5\\n1\\n0.5\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\nTotal phenols\\nFlavonoids\\nFIGURE 4.1\\nExample of a scatterplot where each point corresponds to an\\nobservation.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 74}, page_content='VISUALIZING RELATIONSHIPS BETWEEN VARIABLES\\n61\\n7\\n6.5\\n6\\n5.5\\n5\\n4.5\\n4\\n3.5\\n3\\n2.5\\n4.5\\n5\\n5.5\\n6.5\\n6\\n7\\n7.5\\n8\\nSepal length (cm)\\nPetal length (cm)\\nFIGURE 4.2\\nA scatterplot showing a positive relationship.\\nrelationship is primarily linear—as sepal length (cm) increases, petal length\\n(cm) increases proportionally. A scatterplot can also show if there are points\\n(e.g., X on Figure 4.3) that do not follow this linear relationship. These are\\nreferred to as outliers.\\nScatterplots may also indicate negative relationships. For example, it\\ncan be seen in Figure 4.4 that as the values for weight increase, the val-\\nues for MPG decrease. In situations where the relationship between the\\nvariables is more complex, there may be a combination of positive and\\nnegative relationships at various points. In Figure 4.4, the points follow a\\ncurve indicating that there is also a nonlinear relationship between the two\\nvariables—as weight increases MPG decreases, but the rate of decrease is\\nnot proportional.\\n5.5\\n5\\n4.5\\n4\\n3.5\\n3\\n2.5\\n2\\n1.5\\n1\\n0.5\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\nTotal phenols\\nFlavonoids\\nX\\nFIGURE 4.3\\nObservation (marked as X) that does not follow the relationship.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 75}, page_content='62\\nUNDERSTANDING RELATIONSHIPS\\n50\\n45\\n40\\n35\\n30\\n25\\n20\\n15\\n10\\n5\\n1500\\n2000\\n2500\\n3000\\n4000\\n3500\\n4500 5000\\n5500\\nWeight\\nMPG\\nFIGURE 4.4\\nScatterplot showing a negative nonlinear relationship.\\nScatterplots can also show the lack of any relationship. In Figure\\n4.5, the points scattered throughout the graph indicates that there is no\\nobvious relationship between Alcohol and Nonflavonoid phenols in this\\ndata set.\\n4.2.2 Summary Tables and Charts\\nA simple summary table is a common way of understanding the relationship\\nbetween two variables where at least one of the variables is discrete. For\\nexample, a national retail company may have collected information on the\\nsale of individual products for every store. To summarize the performance\\nof these stores, they may wish to generate a summary table to communicate\\nthe average sales per store.\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n11\\n11.5\\n12\\n12.5\\n13\\n13.5\\n14\\n14.5\\n15\\nAlcohol\\nNonflavonoid phenols\\nFIGURE 4.5\\nScatterplot showing no relationships.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 76}, page_content='VISUALIZING RELATIONSHIPS BETWEEN VARIABLES\\n63\\nFIGURE 4.6\\nExample of a summary table.\\nA single categorical variable (or a continuous variable converted into\\ncategories) is used to group the observations, and each row of the table\\nrepresents a single group. Summary tables will often show a count of\\nthe number of observations (or percentage) that has the particular value\\n(or range). Any number of other columns can be shown alongside to\\nsummarize the second variable. Since each row now refers to a set of\\nobservations, other columns of variables included in the table must also\\ncontain summary information. Descriptive statistics that summarize a set of\\nobservations can be used including mean, median, mode, sum, minimum,\\nmaximum, variance, and standard deviation.\\nIn Figure 4.6, the relationship between two variables class and petal\\nwidth (cm) is shown from Fisher (1936). The class variable is a discrete\\nvariable (nominal) that can take values “Iris-setosa,” “Iris-versicolor,” and\\n“Iris-virginica” and each of these values is shown in the first column.\\nThere are 50 observations that correspond to each of these values and each\\nrow of the table describes the corresponding 50 observations. Each row\\nis populated with summary information about the second variable (petal\\nwidth (cm)) for the set of 50 observations. In this example, the minimum\\nand maximum values are shown alongside the mean, median, and standard\\ndeviation. It can be seen that the class “Iris-setosa” is associated with the\\nsmallest petal width with a mean of 0.2. The set of 50 observations for the\\nclass “Iris-versicolor” has a mean of 1.33, and the class “Iris-virginica”\\nhas the highest mean of 2.03.\\nIt can also be helpful to view this information as a graph. For example in\\nFigure 4.7, a bar chart is drawn with the x-axis showing the three nominal\\nclass categories and the y-axis showing the mean value for petal width\\n(cm). The relative mean values of the three classes can be easily seen using\\nthis view.\\nMore details on the frequency distribution of the three separate sets\\ncan be seen by using box plots for each category as shown in Figure 4.8.\\nAgain, the x-axis represents the three classes; however, the y-axis is now\\nthe original values for the petal width of each observation. The box plot\\nillustrates for each individual class how the observations are distributed\\nrelative to the other class. For example, the 50 observations in the “Iris-\\nsetosa” class have no overlapping values with either of the other two'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 77}, page_content='64\\nUNDERSTANDING RELATIONSHIPS\\nFIGURE 4.7\\nExample of a summary graph.\\nclasses. In contrast, the frequency distribution of the “Iris-versicolor” and\\nthe “Iris-virginica” show an amount of overlap at the extreme values.\\nSummary tables can also be used to show the relationship between\\nordinal variables and another variable. In the example shown in Figure\\n4.9, three ordered categories are used to group the observations. Since\\nthe categories can be ordered, it is possible to see how the mean weight\\nchanges as the MPG category increases. It is clear from this table that as\\nMPG categories increases the mean weight decreases.\\nThe same information can be seen as a histogram and a series of box\\nplots. By ordering the categories on the x-axis and plotting the information,\\nthe trend can be seen more easily, as shown in Figure 4.10.\\nIn many situations, a binary variable is used to represent a variable\\nwith two possible values, with 0 representing one value and 1 the other.\\nFor example, 0 could represent the case where a patient has a specific\\ninfection and 1 the case where the patient does not. To illustrate, a new\\nblood test is being investigated to predict whether a patient has a specific\\nFIGURE 4.8\\nExample of a multiple box plot graph.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 78}, page_content='VISUALIZING RELATIONSHIPS BETWEEN VARIABLES\\n65\\nFIGURE 4.9\\nExample of summary table where the categorical variable is\\nordinal.\\ntype of infection. This test calculates a value between −5.0 and +1.0, with\\nhigher values indicating the patient has the infection and lower values\\nindicating the patient does not have the infection. The results of this test\\nare summarized in Figure 4.11, which shows the data categorized by the\\ntwo patient classes. There are 23 patients in the data that did not have the\\ninfection (infection = 0) and 32 that did have the infection (infection = 1).\\nIt can be seen that there is a difference between the blood test results for the\\ntwo groups. The mean value of the blood test in the group with the infection\\nis −3.05, whereas the mean value of the blood test over patients without\\nthe infection is −4.13. Although there is a difference, the overlap of the\\nblood test results between the two groups makes it difficult to interpret the\\nresults.\\nA different way to assess this data would be to use the blood test results\\nto group the summary table and then present descriptive statistics for the\\ninfection variable. Since the blood test values are continuous, the first step\\nis to organize the blood test results into ranges. Range boundaries were\\nset at −5, −4, −3, −2, −1, 0, +1, and +2 and groups generated as shown\\nin Figure 4.12. The mean of the binary variable infection is shown in the\\nFIGURE 4.10\\nGraph of summary data for an ordinal variable against a contin-\\nuous (weight).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 79}, page_content='66\\nUNDERSTANDING RELATIONSHIPS\\nFIGURE 4.11\\nSummary table and corresponding box plot chart to summarizing\\nthe results of a trial for a new blood test to predict an infection.\\nsummary table corresponding to each of the groups and also plotted using\\na histogram. If all observations in a group were 0, then the mean would\\nbe 0; and if all observations in a group were 1, then the mean would be\\n1. Hence the upper and lower limits on these mean values are 0 and 1.\\nThis table and chart more clearly shows the relationship between these\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\nFIGURE 4.12\\nSummary table and histogram using continuous data that has\\nbeen binned to generate the group summarized using the binary variable.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 80}, page_content='VISUALIZING RELATIONSHIPS BETWEEN VARIABLES\\n67\\nFIGURE 4.13\\nContingency table showing the relationship between two dichoto-\\nmous variables.\\ntwo variables. For lower values of the blood test (from −5 to −2) there are\\nfew cases of the infection—the test correctly predicts the majority of these\\ncases as negative. Similarly, for the range −1 to 2 most of these cases are\\npositive and the test would correctly predict most of these cases as positive.\\nHowever, for the range of blood test from −2 to −1, 55% are predicted\\npositive and 45% are predicted negative (based on the mean value for this\\nrange). For this range of values the blood test performs poorly. It would be\\nreasonable to decide that for a test result in this range, the results should\\nnot be trusted.\\n4.2.3 Cross-Classification Tables\\nCross-classification tables or contingency tables provide insight into the\\nrelationship between two categorical variables (or non-categorical vari-\\nables transformed to categorical variables). A variable is often dichoto-\\nmous; however, a contingency table can represent variables with more\\nthan two values. Figure 4.13 provides an example of a contingency table\\nfor two variables over a series of patients: Test Results and Infection Class.\\nThe variable Infection Class identifies whether a patient has the specific\\ninfection and can take two possible values (“Infection negative” and “Infec-\\ntion positive”). The corresponding Test Results identified whether the blood\\ntest results were positive (“Blood test positive”) or negative (“Blood test\\nnegative”). In this data set there were 55 observations (shown in the bottom-\\nright cell). Totals for the Test Results values are shown in the rightmost\\ncolumn labeled “Totals” and totals for Infection Class values are shown\\non the bottom row also labeled “Totals.” In this example, there are 27\\npatients where the values for Test Results are “Blood test negative” and\\n28 that are “Blood test positive” (shown in the right column). Similarly,\\nthere are 23 patients that are categorized as “Infection negative” and 32'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 81}, page_content='68\\nUNDERSTANDING RELATIONSHIPS\\nFIGURE 4.14\\nContingency table illustrating the number of females and males\\nin each age-group.\\nas “Infection positive.” The table cells in the center of the table show the\\nnumber of patients that correspond to pairs of values. For example, there\\nare 17 patients who had a negative blood test result and did not have the\\ninfection (“Infection negative”), 10 patients who had a negative blood test\\nresult but had an infection, 6 who had a positive blood test result and did\\nnot have an infection, and 22 who had a positive blood test and had an\\ninfection. Contingency tables provide a view of the relationship between\\ntwo categorical variables. It can be seen from this example that although\\nthe new blood test did not perfectly identify the presence or absence of\\nthe infection, it correctly classified the presence of the infection for 39\\npatients (22 + 17) and incorrectly classified the infection in 16 patients\\n(10 + 6).\\nContingency tables can be used to understand relationships between\\ncategorical (both nominal and ordinal) variables where there are more\\nthan two possible values. In Figure 4.14, the data set is summarized using\\ntwo variables: gender and age-group. The variable gender is dichoto-\\nmous and the two values (“female” and “male”) are shown in the table’s\\nheader on the x-axis. The other selected variable is age-group, which has\\nbeen broken down into nine categories: 10–19, 20–29, 30–39, etc. For\\neach level of each variable, a total is displayed. For example, there are\\n21,790 observations where gender is “male” and there are 1657 obser-\\nvations where age is between 10 and 19. The total number of observa-\\ntions summarized in the table is shown in the bottom right-hand corner\\n(32,561).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 82}, page_content='CALCULATING METRICS ABOUT RELATIONSHIPS\\n69\\n4.3 CALCULATING METRICS ABOUT RELATIONSHIPS\\n4.3.1 Overview\\nThere are many ways to measure the strength of the relationship between\\ntwo variables. These metrics are usually based on the types of variables\\nbeing considered, such as a comparison between categorical variables and\\ncontinuous variables. The following section describes common methods\\nfor quantifying the strength of relationships between variables.\\n4.3.2 Correlation Coefficients\\nFor pairs of variables measured on an interval or ratio scale, a correlation\\ncoefficient (r) can be calculated. This value quantifies the linear relation-\\nship between the variables by generating values from −1.0 to +1.0. If the\\noptimal straight line is drawn through the points on a scatterplot, the value\\nof r reflects how closely the points lie to this line. Positive numbers for r\\nindicate a positive correlation between the pair of variables, and negative\\nnumbers indicate a negative correlation. A value of r close to 0 indicates\\nlittle or no relationship between the variables.\\nFor example, the two scatterplots shown in Figure 4.15 illustrate dif-\\nferent values for r. The first graph illustrates a strong positive correlation\\nbecause the points lie relatively close to an imaginary line sloping upward\\nfrom left to right through the center of the points; the second graph illus-\\ntrates a weaker correlation.\\nThe formula used to calculate r is shown here:\\nr =\\nn\\n∑\\ni=1\\n(xi −̄x)(yi −̄y)\\n(n −1)sxsy\\n7\\n2.6\\n2.4\\n2.2\\n2\\n1.8\\n1.6\\n1.4\\n1.2\\n1\\n0.8\\n6.5\\n6\\n5.5\\n5\\n4.5\\n4\\n3.5\\n3\\n2.54.5\\n5\\n5.5\\n6\\n6.5\\nr = 0.83\\nr = 0.59\\n7\\n7.5\\n8\\n4.5\\n5\\n5.5\\n6\\n6.5\\n7\\n7.5\\n8\\nSepal length (cm)\\nSepal length (cm)\\nPetal length (cm)\\nPetal lwidth (cm)\\nFIGURE 4.15\\nScatterplots illustrate values for the correlation coefficient (r).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 83}, page_content='70\\nUNDERSTANDING RELATIONSHIPS\\nTABLE 4.1\\nTable of Data with Values for\\nthe x and y Variables\\nx\\ny\\n92\\n6.3\\n145\\n7.8\\n30\\n3.0\\n70\\n5.5\\n75\\n6.5\\n105\\n5.5\\n110\\n6.5\\n108\\n8.0\\n45\\n4.0\\n50\\n5.0\\n160\\n7.5\\n155\\n9.0\\n180\\n8.6\\n190\\n10.0\\n63\\n4.2\\n85\\n4.9\\n130\\n6\\n132\\n7\\nwhere x and y are variables, xi are the individual values of x, yi are the\\nindividual values of y, ̄x is the mean of the x variable, ̄y is the mean of\\nthe y variable, sx and sy are the standard deviations of the variables x\\nand y, respectively, and n is the number of observations. To illustrate the\\ncalculation, two variables (x and y) are used and shown in Table 4.1. The\\nscatterplot of the two variables indicates a positive correlation between\\nthem, as shown in Figure 4.16. The specific value of r is calculated using\\nTable 4.2:\\nr =\\nn\\n∑\\ni=1\\n(xi −̄x)(yi −̄y)\\n(n −1)sxsy\\nr =\\n1357.06\\n(18 −1)(47.28)(1.86)\\nr = 0.91'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 84}, page_content='CALCULATING METRICS ABOUT RELATIONSHIPS\\n71\\n11\\n10\\n9\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\n180\\n200\\nx\\ny\\nFIGURE 4.16\\nScatterplot to illustrate the relationship between the x and y\\nvariables.\\nTABLE 4.2\\nTable Showing the Calculation of the Correlation Coefficient\\nxi\\nyi\\n(xi −̄x)\\n(yi −̄y)\\n(xi −̄x)(yi −̄y)\\n92\\n6.3\\n−14.94\\n−0.11\\n1.64\\n145\\n7.8\\n38.06\\n1.39\\n52.90\\n30\\n3\\n−76.94\\n−3.41\\n262.37\\n70\\n5.5\\n−36.94\\n−0.91\\n33.62\\n75\\n6.5\\n−31.94\\n0.09\\n−2.87\\n105\\n5.5\\n−1.94\\n−0.91\\n1.77\\n110\\n6.5\\n3.06\\n0.09\\n0.28\\n108\\n8\\n1.06\\n1.59\\n1.69\\n45\\n4\\n−61.94\\n−2.41\\n149.28\\n50\\n5\\n−56.94\\n−1.41\\n80.04\\n160\\n7.5\\n53.06\\n1.09\\n58.07\\n155\\n9\\n48.06\\n2.59\\n124.68\\n180\\n8.6\\n73.06\\n2.19\\n160.00\\n190\\n10\\n83.06\\n3.59\\n298.19\\n63\\n4.2\\n−43.94\\n−2.21\\n97.11\\n85\\n4.9\\n−21.94\\n−1.51\\n33.13\\n130\\n6\\n23.06\\n−0.41\\n−9.45\\n132\\n7\\n25.06\\n0.59\\n14.79\\n̄x = 106.94\\n̄y = 6.41\\nSum = 1,357.06\\nsx = 47.28\\nsy = 1.86'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 85}, page_content='72\\nUNDERSTANDING RELATIONSHIPS\\n4.3.3 Kendall Tau\\nKendall Tau is another approach for measuring associations between pairs\\nof variables. It is based on a ranking of the observations for two variables.\\nThis ranking can be derived by ordering the values and then replacing the\\nactual values with a rank from 1 to n (where n is the number of observations\\nin the data set). The overall formula is based on counts of concordant and\\ndiscordant pairs of observations. Table 4.3 is used to illustrate Kendall Tau\\nusing two variables, Variable X and Variable Y, containing a ranking with\\n10 observations (A through J).\\nA pair of observations is concordant if the difference of one of the two\\nvariable’s values is in the same direction as the difference of the other\\nvariable’s values. For example, to determine if the observations A and B\\nare concordant, we compare the difference of the values for Variable X\\n(XB −XA or 2 −1 = 1) with the difference of the values for Variable Y\\n(YB −YA or 4 −2 = 2). Since these differences are in the same direction—\\n1 and 2 are both positive—the observations A and B are concordant. We\\nwould get the same result if we compared the difference of (XA −XB)\\nand (YA −YB). In this case the differences would both be negative, but\\nstill in the same direction. Calculated either way, the observations A and B\\nare concordant. A discordant pair occurs when the differences of the two\\nvariables’ values move in opposite directions. The pair of observations B\\nand C illustrates this. The difference of the values for Variable X (XC −XB\\nor 3 −2 = 1) compared with the difference of the values for Variable Y\\n(YC −YB or 1 −4 = −3) are in different directions: the first is positive and\\nthe second negative. The pair of observations B and C is discordant.\\nTABLE 4.3\\nData Table of Rankings for Two Variables\\nObservation name\\nVariable X\\nVariable Y\\nA\\n1\\n2\\nB\\n2\\n4\\nC\\n3\\n1\\nD\\n4\\n3\\nE\\n5\\n6\\nF\\n6\\n5\\nG\\n7\\n7\\nH\\n8\\n8\\nI\\n9\\n10\\nJ\\n10\\n9'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 86}, page_content='CALCULATING METRICS ABOUT RELATIONSHIPS\\n73\\nTABLE 4.4\\nCalculation of Concordant and Discordant Pairs\\nObservation\\nConcordant\\nDiscordant\\nName\\nVariable X\\nVariable Y\\nPairs\\nPairs\\nA\\n1\\n2\\n8\\n1\\nB\\n2\\n4\\n6\\n2\\nC\\n3\\n1\\n7\\n0\\nD\\n4\\n3\\n6\\n0\\nE\\n5\\n6\\n4\\n1\\nF\\n6\\n5\\n4\\n0\\nG\\n7\\n7\\n2\\n1\\nH\\n8\\n8\\n2\\n0\\nI\\n9\\n10\\n0\\n1\\nJ\\n10\\n9\\n0\\n0\\nSum:\\n39\\n6\\nIn Table 4.4, the observations A–J are ordered using Variable X, and\\neach unique pair of observations is compared. For example, observation\\nA is compared with all other observations (B, C, . . . , J) and the number\\nof concordant and discordant pairs are summed as shown in the last two\\ncolumns of Table 4.4. For observations A, there are eight concordant pairs\\n(A–B, A–D, A–E, A–F, A–G, A–H, A–I, A–J) and one discordant pair (A–\\nC). To make sure that each pair of observations is considered in this process,\\nthis is repeated for all other observations. Observation B is compared to\\nobservations C through J, observation C compared to D though J, and\\nso on. The number of concordant and discordant pairs is shown for each\\nobservation, and the sum of the concordant and discordant pairs is also\\ncomputed.\\nA final Kendall Tau measure is computed based on these computed sums.\\nKendall Tau measures associations between variables with 1 indicating a\\nperfect ranking and −1 a perfect disagreement of the rankings. A zero\\nvalue—assigned when the ranks are tied—indicates a lack of association,\\nor in other words, that the two variables are independent. The simplest form\\nof the Kendall Tau calculation is referred to as Tau A and has the formula:\\n𝜏A =\\nnc −nd\\nn (n −1)∕2\\nwhere nc and nd are the number of concordant and discordant pairs,\\nrespectively, and n is the number of observations. In this example, the Tau'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 87}, page_content='74\\nUNDERSTANDING RELATIONSHIPS\\nA (𝜏A) would be:\\n𝜏A = 39 −6\\n45\\n𝜏A = 0.73\\nIn most practical situations, there are ties based on either variable. In\\nthese situations, the formula Tau B is often used. It considers the ties in the\\nfirst variable (tx) and the ties in the second variable (ty) and is computed\\nusing the following formulas:\\n𝜏B =\\nnc −nd\\n√(nc + nd + tx)(nc + nd + ty)\\nIn most software applications the Kendall Tau B function is used.\\n4.3.4 t-Tests Comparing Two Groups\\nIn Chapter 2, we used a hypothesis test (t-test) to determine whether the\\nmean of a variable was sufficiently different from a specified value as to be\\nconsidered statistically significant, meaning that it would be unlikely that\\nthe difference between the value and the mean was due to normal variation\\nor chance. This concept can be extended to compare the mean values of two\\nsubsets. We can explore if the means of two groups are different enough to\\nsay the difference is significant, or conclude that a difference is simply due\\nto chance. This concept is similar to the description in Chapter 2; however,\\ndifferent formulas are utilized.\\nIn looking at the difference between two groups, we need to not only\\ntake into account the values for the mean values of the two groups, but also\\nthe deviation of the data for the two groups.\\nThe formula is used where it is assumed that the value being assessed\\nacross the two groups is both independently and normally distributed and\\nvariances between the two groups are either equal or similar. The for-\\nmula takes into account the difference between the two groups as well as\\ninformation concerning the distribution of the two groups:\\nT =\\n̄x1 −̄x2\\nsp\\n√\\n1\\nn1\\n+ 1\\nn2\\nwhere ̄x1 is the mean value of the first group, ̄x2 is the mean value of the\\nsecond group, and n1 and n2 are the number of observations in the first and'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 88}, page_content='CALCULATING METRICS ABOUT RELATIONSHIPS\\n75\\nsecond group respectively, and sp is an estimate of the standard deviation\\n(pooled estimate). The sp is calculated using the following formula:\\nsp =\\n√\\n(n1 −1)s2\\n1 + (n2 −1)s2\\n2\\nn1 + n2 −2\\nwhere n1 and n2 are again the number of observations in group 1 and group\\n2 respectively, and s2\\n1, s2\\n2 are the calculated variances for group 1 and group\\n2. This formula follows a t-distribution, with the number of degrees of\\nfreedom (df) calculated as\\ndf = n1 + n2 −2\\nwhere it cannot be assumed that the variances across the two groups are\\nequal, another formula is used:\\nT =\\n̄x1 −̄x2\\n√\\ns2\\n1\\nn1\\n+\\ns2\\n2\\nn1\\nwhere ̄x1and ̄x2 are the average values for the two groups (1 and 2), s2\\n1 and\\ns2\\n2 are the calculated variances for the two groups, and n1 and n2 are the\\nnumber of observations in the two groups.\\nAgain, it follows a t-distribution and the number of degrees of freedom\\n(df) is calculated using the following formula:\\ndf =\\n(\\ns2\\n1\\nn1\\n+\\ns2\\n1\\nn2\\n)2\\n1\\nn1 −1\\n(\\ns2\\n1\\nn1\\n)2\\n+\\n1\\nn2 −1\\n(\\ns2\\n2\\nn2\\n)2\\nThese t-values will be positive if the mean of group 1 is larger than\\nthe mean of group 2 and negative if the mean of group 2 is larger than\\nthe mean of group 1. In a similar manner as described in Chapter 2, these\\nt-values can be used in a hypothesis test where the null hypothesis states\\nthat the two means are equal and the alternative hypothesis states that the\\ntwo means are not equal. This t-value can be used to accept or reject the\\nnull hypothesis as well as calculate a p-value, either using a computer or a\\nstatistical table, in a manner similar to that described in Chapter 2.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 89}, page_content='76\\nUNDERSTANDING RELATIONSHIPS\\nTABLE 4.5\\nCalls Processed by Different Call Centers\\nCall Center A\\nCall Center B\\nCall Center C\\nCall Center D\\n136\\n124\\n142\\n149\\n145\\n131\\n145\\n157\\n139\\n128\\n139\\n154\\n132\\n130\\n145\\n155\\n141\\n129\\n143\\n151\\n143\\n135\\n141\\n156\\n138\\n132\\n138\\n139\\n146\\n4.3.5 ANOVA\\nThe following section reviews a technique called the completely random-\\nized one-way analysis of variance that compares the means of three or\\nmore different groups. The test determines whether there is a difference\\nbetween the groups. This method can be applied to cases where the groups\\nare independent and random, the distributions are normal and the pop-\\nulations have similar variances. For example, an online computer retail\\ncompany has call centers in four different locations. These call centers are\\napproximately the same size and handle a certain number of calls each day.\\nAn analysis of the different call centers based on the average number of\\ncalls processed each day is required to understand whether one or more of\\nthe call centers are under- or over-performing. Table 4.5 illustrates the calls\\nserviced daily.\\nAs with other hypothesis tests, it is necessary to state a null and alterna-\\ntive hypothesis. Generally, the hypothesis statement will take the standard\\nform:\\nH0: The means are equal.\\nHa: The means are not equal.\\nTo determine whether a difference exists between the means or whether\\nthe difference is due to random variation, we must perform a hypothesis\\ntest. This test will look at both the variation within the groups and the\\nvariation between the groups. The test performs the following steps:\\n1. Calculate group means and variance.\\n2. Determine the within-group variation.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 90}, page_content='CALCULATING METRICS ABOUT RELATIONSHIPS\\n77\\nTABLE 4.6\\nCalculating Means and Variances\\nCall\\nCall\\nCall\\nCall\\nGroups\\nCenter A\\nCenter B\\nCenter C\\nCenter D\\n(k = 4)\\n136\\n124\\n142\\n149\\n145\\n131\\n145\\n157\\n139\\n128\\n139\\n154\\n132\\n130\\n145\\n155\\n141\\n129\\n143\\n151\\n143\\n135\\n141\\n156\\n138\\n132\\n138\\n139\\n146\\nCount\\n8\\n7\\n8\\n6\\nTotal count\\nN = 29\\nMean\\n139.1\\n129.9\\n142.4\\n153.7\\nVariance\\n16.4\\n11.8\\n8.6\\n9.5\\n3. Determine the between-group variation.\\n4. Determine the F-statistic, which is based on the between-group and\\nwithin group ratio.\\n5. Test the significance of the F-statistic.\\nThe following sections describe these steps in detail:\\nCalculate group means and variances\\nIn Table 4.6, for each call center a count along with the mean and\\nvariance has been calculated. In addition, the total number of groups\\n(k = 4) and the total number of observations (N = 29) is listed. An average\\nof all values (x = 140.8) is calculated by summing all values and dividing\\nit by the number of observations:\\nx = 136 + 145 + … + 151 + 156\\n29\\n= 140.8\\nDetermine the within-group variation\\nThe variation within groups is defined as the within-group variance or\\nmean square within (MSW). To calculate this value, we use a weighted\\nsum of the variance for the individual groups. The weights are based on\\nthe number of observations in each group. This sum is divided by the'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 91}, page_content='78\\nUNDERSTANDING RELATIONSHIPS\\nnumber of degrees of freedom calculated by subtracting the number of\\ngroups (k) from the total number of observations (N):\\nMSW =\\nk∑\\ni=1\\n(ni −1)s2\\ni\\nN −k\\nIn this example:\\nMSW = (8 −1) × 16.4 + (7 −1) × 11.8 + (8 −1) × 8.6 + (6 −1) × 9.5\\n(29 −4)\\nMSW = 11.73\\nDetermine the between-group variation\\nNext, the between-group variation or mean square between (MSB) is\\ncalculated. The MSB is the variance between the group means. It is calcu-\\nlated using a weighted sum of the squared difference between the group\\nmean (̄xi) and the average of all observations (x). This sum is divided by\\nthe number of degrees of freedom. This is calculated by subtracting one\\nfrom the number of groups (k). The following formula is used to calculate\\nthe MSB:\\nMSB =\\nk∑\\ni=1\\nni(̄xi −x)2\\nk −1\\nwhere ni is the number for each group and ̄xi is the average for each group.\\nIn this example,\\nMSB =\\n(8 × (139.1 −140.8)2) + (7 × (129.9 −140.8)2)\\n+(8 × (142.4 −140.8)2) + (6 × (153.7 −140.8)2)\\n4 −1\\nMSB = 624.58\\nDetermine the F-statistic\\nThe F-statistic is the ratio of the MSB and the MSW:\\nF = MSB\\nMSW'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 92}, page_content='CALCULATING METRICS ABOUT RELATIONSHIPS\\n79\\nIn this example:\\nF = 624.58\\n11.73\\nF = 53.25\\nTest the significance of the F-statistic\\nBefore we can test the significance of this value, we must determine the\\ndegrees of freedom (df) for the two mean squares (within and between).\\nThe degrees of freedom for the MSW (dfwithin) is calculated using the\\nfollowing formula:\\ndf within = N −k\\nwhere N is the total number of observations in all groups and k is the\\nnumber of groups.\\nThe degrees of freedom for the MSB (df between) is calculated using the\\nfollowing formula:\\ndf between = k −1\\nwhere k is the number of groups.\\nIn this example,\\ndf between = 4 −1 = 3\\ndf within = 29 −4 = 25\\nWe already calculated the F-statistic to be 53.39. This number indicates\\nthat the mean variation between groups is much greater than the mean\\nvariation within groups due to errors. To test this, we look up the critical\\nF-statistic from an F-table (see the Further Readings section). To find this\\ncritical value we need 𝛼(confidence level), v1 (df between), and v2 (df within).\\nThe critical value for the F-statistic is 3.01 (when 𝛼is 0.05). Since the\\ncalculated F-statistic is greater than the critical value, we reject the null\\nhypothesis. The means for the different call centers are not equal.\\n4.3.6 Chi-Square\\nThe chi-square test for indepedence is a hypothesis test for use with vari-\\nables measured on a nominal or ordinal scale. It allows an analysis of\\nwhether there is a relationship between two categorical variables. As with'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 93}, page_content='80\\nUNDERSTANDING RELATIONSHIPS\\nTABLE 4.7\\nCalculation of Chi-Square\\nk\\nCategory\\nObserved (O)\\nExpected (E)\\n(O −E)2/E\\n1\\nr = Brand X, c = 43221\\n5,521\\n4,923\\n72.6\\n2\\nr = Brand Y, c = 43221\\n4,597\\n4,913\\n20.3\\n3\\nr = Brand Z, c = 43221\\n4,642\\n4,925\\n16.3\\n4\\nr = Brand X, c = 43026\\n4,522\\n4,764\\n12.3\\n5\\nr = Brand Y, c = 43026\\n4,716\\n4,754\\n0.3\\n6\\nr = Brand Z, c = 43026\\n5,047\\n4,766\\n16.6\\n7\\nr = Brand X, c = 43212\\n4,424\\n4,780\\n26.5\\n8\\nr = Brand Y, c = 43212\\n5,124\\n4,770\\n26.3\\n9\\nr = Brand Z, c = 43212\\n4,784\\n4,782\\n0.0008\\nSum = 191.2\\nother hypothesis tests, it is necessary to state a null and alternative hypoth-\\nesis. Generally, these hypothesis statements are as follows:\\nH0: There is no relationship.\\nHa: There is a relationship.\\nUsing Table 4.7, we will look at whether a relationship exists between\\nwhere a consumer lives (represented by a zip code) and the brand of\\nwashing powder they buy (brand X, brand Y, and brand Z). The “r” and\\n“c” refer to the row (r) and column (c) in a contingency table.\\nThe Chi-Square test compares the observed frequencies with the\\nexpected frequencies. The expected frequencies are calculated using the\\nfollowing formula:\\nEr,c = r × c\\nn\\nwhere Er,c is the expected frequency for a particular cell in a contingency\\ntable, r is the row count, c is the column count and n is the total number of\\nobservations in the sample.\\nFor example, to calculate the expected frequency for the table cell where\\nthe washing powder is brand X and the zip code is 43221 would be\\nEBrand X,43221 = 14,467 × 14,760\\n43,377\\nEBrand X,43221 = 4,923'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 94}, page_content='EXERCISES\\n81\\nThe Chi-Square test (𝜒2) is computed with the following equation:\\n𝜒2 =\\nk∑\\ni=1\\n(Oi −Ei)2\\nEi\\nwhere k is the number of all categories, Oi is the observed cell frequency\\nand Ei is the expected cell frequency. The test is usually performed when\\nall observed cell frequencies are greater than 10. Table 4.7 shows the\\ncomputed 𝜒2 for this example.\\nThere is a critical value at which the null hypothesis is rejected (𝜒2\\nc ) and\\nthis value is found using a standard Chi-Square table (see Further Reading\\nSection). The value is dependent on the degrees of freedom (df ), which is\\ncalculated:\\ndf = (r −1) × (c −1)\\nFor example, the number of degrees of freedom for the above example\\nis (3 −1) × (3 −1) which equals 4. Looking up the critical value for\\ndf = 4 and 𝛼= 0.05, the critical value is 9.488. Since 9.488 is less than\\nthe calculated chi-square value of 191.2, we reject the null hypothesis and\\nstate that there is a relationship between zip codes and brands of washing\\npowder.\\nEXERCISES\\nTable 4.8 shows a series of retail transactions monitored by the main office\\nof a computer store.\\n1. Generate a contingency table summarizing the variables Store and\\nProduct category.\\n2. Generate the following summary tables:\\na. Grouping by Customer with a count of the number of observations\\nand the sum of Sale price ($) for each row.\\nb. Grouping by Store with a count of the number of observations and\\nthe mean Sale price ($) for each row.\\nc. Grouping by Product category with a count of the number of\\nobservations and the sum of the Profit ($) for each row.\\n3. Create a scatterplot showing Sales price ($) against Profit ($).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 95}, page_content='82\\nUNDERSTANDING RELATIONSHIPS\\nTABLE 4.8\\nRetail Transaction Data Set\\nProduct\\nProduct\\nSale\\nProfit\\nCustomer\\nStore\\nCategory\\nDescription\\nPrice ($)\\n($)\\nB. March\\nNew York, NY\\nLaptop\\nDR2984\\n950\\n190\\nB. March\\nNew York, NY\\nPrinter\\nFW288\\n350\\n105\\nB. March\\nNew York, NY\\nScanner\\nBW9338\\n400\\n100\\nJ. Bain\\nNew York, NY\\nScanner\\nBW9443\\n500\\n125\\nT. Goss\\nWashington, DC\\nPrinter\\nFW199\\n200\\n60\\nT. Goss\\nWashington, DC\\nScanner\\nBW39339\\n550\\n140\\nL. Nye\\nNew York, NY\\nDesktop\\nLR21\\n600\\n60\\nL. Nye\\nNew York, NY\\nPrinter\\nFW299\\n300\\n90\\nS. Cann\\nWashington, DC\\nDesktop\\nLR21\\n600\\n60\\nE. Sims\\nWashington, DC\\nLaptop\\nDR2983\\n700\\n140\\nP. Judd\\nNew York, NY\\nDesktop\\nLR22\\n700\\n70\\nP. Judd\\nNew York, NY\\nScanner\\nFJ3999\\n200\\n50\\nG. Hinton\\nWashington, DC\\nLaptop\\nDR2983\\n700\\n140\\nG. Hinton\\nWashington, DC\\nDesktop\\nLR21\\n600\\n60\\nG. Hinton\\nWashington, DC\\nPrinter\\nFW288\\n350\\n105\\nG. Hinton\\nWashington, DC\\nScanner\\nBW9443\\n500\\n125\\nH. Fu\\nNew York, NY\\nDesktop\\nZX88\\n450\\n45\\nH. Taylor\\nNew York, NY\\nScanner\\nBW9338\\n400\\n100\\nFURTHER READING\\nFor more information on inferential statistics used to assess relationships see\\nUrdan (2010), Anderson et al. (2010), Witte & Witte (2009), and Vickers\\n(2010).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 96}, page_content='CHAPTER 5\\nIDENTIFYING AND UNDERSTANDING\\nGROUPS\\n5.1 OVERVIEW\\nIt is often useful to decompose a data set into simpler subsets to help make\\nsense of the entire collection of observations. These groups may reflect\\nthe types of observations found in a data set. For example, the groups\\nmight summarize the different types of customers who visit a particular\\nshop based on collected demographic information. Finding subgroups may\\nhelp to uncover relationships in the data such as groups of consumers who\\nbuy certain combinations of products. The process of grouping a data set\\nmay also help identify rules from the data, which can in turn be used to\\nsupport future decisions. For example, the process of grouping historical\\ndata can be used to understand which combinations of clinical treatments\\nlead to the best patient outcomes. These rules can then be used to select an\\noptimal treatment plan for new patients with the same symptoms. Finally,\\nthe process of grouping also helps discover observations dissimilar from\\nthose in the major identified groups. These outliers should be more closely\\nexamined as possible errors or anomalies.\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n83'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 97}, page_content='84\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.1\\nSimple summary table showing how the mean petal width changes\\nfor the different classes of flowers.\\nThe identification of interesting groups is not only a common deliv-\\nerable for a data analysis project, but can also support other data mining\\ntasks such as the development of a model to use in forecasting future\\nevents (as described in Chapter 6). This is because the process of grouping\\nand interpreting the groups of observations helps the analyst to thoroughly\\nunderstand the data set which, in turn, supports the model building\\nprocess. This grouping process may also help to identify specific subsets\\nthat lead to simpler and more accurate models than those built from the\\nentire set. For example, in developing models for predicting house prices,\\nthere may be groups of houses (perhaps based on locations) where a\\nsimple and clear relationship exists between a specific variable collected\\nand the house prices which allows for the construction of specific models\\nfor these subpopulations.\\nThe analysis we have described in Chapter 4 looks at the simple rela-\\ntionship between predefined groups of observations—those encoded using\\na single predefined variable—and one other variable. For example, in look-\\ning at a simple categorization such as types of flowers (“Iris-setosa,” “Iris-\\nversicolor,” and “Iris-virginica”), a question we might ask is how a single\\nvariable such as petal width varies among different species as illustrated\\nin Figure 5.1.\\nThis can be easily extended to help understand the relationships between\\ngroups and multiple variables, as illustrated in Figure 5.2 where three\\npredefined categories are used to group the observations. Summary infor-\\nmation on multiple variables is presented (using the mean value in this\\nFIGURE 5.2\\nThe use of a summary table to understand multiple variables for a\\nseries of groups.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 98}, page_content='OVERVIEW\\n85\\nexample). As described earlier, these tables can use summary statistics\\n(e.g., mean, mode, median, and so on) in addition to graphs such as box\\nplots that illustrate the subpopulations. The tables may also include other\\nmetrics that summarize associations in the data, as described in Chapter 4.\\nA variety of graphs (e.g., histograms, box plots, and so on) for each group\\ncan also be shown in a table or grid format known as small multiples that\\nallows comparison. For example, in Figure 5.3, a series of variables are\\nplotted (cylinder, displacement, horsepower, acceleration, and MPG) for\\nthree groups (“American cars,” “European cars,” and “Asian cars”), which\\nclearly illustrates changes in the frequency distribution for each of these\\nclasses of cars.\\nThrough an interactive technique known as brushing, a subset of obser-\\nvations can also be highlighted within a frequency distribution of the\\nwhole data set as illustrated using the automobile example in Figure 5.4.\\nThe shaded areas of the individual plots are observations where the car’s\\norigin is “American.” The chart helps visualize how this group of selected\\ncars is associated with lower fuel efficiency. As shown in the graph in the\\ntop left plot (MPG), the distribution of the group (dark gray) overlays the\\ndistribution of all the cars in the data set (light gray).\\nIn this chapter, we will explore different ways to visualize and\\ngroup observations by looking at multiple variables simultaneously. One\\napproach is based on similarities of the overall set of variables of interest,\\nas in the case of clustering. For example, observations that have high val-\\nues for certain variables may form groups different from those that have\\nlow values for the same variables. In this approach, the pattern of values of\\nvariables for observations within a group will be similar, even though the\\nindividual data values may differ. A second approach is to identify groups\\nbased on interesting combinations of predefined categories, as in the case\\nof association rules. This more directed approach identifies associations\\nor rules about groups that can be used to support decision making. For\\nexample, a rule might be that a group of customers who historically pur-\\nchased products A, B, and C also purchased product X. A third directed\\napproach, referred to as decision trees, groups observations based on a\\ncombination of ranges of continuous variables or of specific categories. As\\nan example, a data set of patients could be used to generate a classification\\nof cholesterol levels based on information such as age, genetic predisposi-\\ntion, lifestyle choices, and so on. This chapter describes how each of these\\napproaches calculates groups and explains techniques for optimizing the\\nresults. It also discusses the strengths and weaknesses of each approach\\nand provides worked examples to illustrate them.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 99}, page_content='100\\n75\\n50\\n25\\n0\\n5\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n10\\n15\\n20\\nMPG\\nAmerican cars\\nAsian cars\\nEuropean cars\\nMPG\\nCylinders\\nDisplacement\\nHorsepower\\nHorsepower\\nHorsepower\\nAcceleration\\nAcceleration\\nAcceleration\\nDisplacement\\nDisplacement\\nCylinders\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n3\\n4\\n6\\n11 12\\n12\\n6\\n8 10 12 14 16 18 20 22 24\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\n13 14 15 16 17 18 19 20 21 22\\nMPG\\n25\\n30\\n35\\n40\\n40\\n30\\n20\\n10\\n0\\n40\\n30\\n20\\n10\\n0\\n0\\n20\\n40\\n60\\n80\\nFrequency\\n4\\n6\\n8\\n60\\n80 100 120 140 160 180 200\\n0\\n30\\n60\\n90\\n120\\n60\\n80\\n60\\n40\\n20\\n0\\n80\\n60\\n40\\n20\\n0\\n45\\n30\\n15\\n0\\n40\\n30\\n20\\n10\\n0\\n40\\n30\\n20\\n10\\n0\\n40\\n30\\n20\\n10\\n0\\nFrequency\\nFrequency\\nFrequency\\nFrequency\\nFrequency\\n40\\n30\\n20\\n10\\n0\\nFrequency\\nFrequency\\nFrequency\\n40\\n20\\n15\\n10\\n5\\n0\\n30\\n20\\n10\\n0\\nFrequency\\nFrequency\\nCylinders\\n4\\n5\\n6\\n0\\n20\\n40\\n60\\n80\\nFrequency\\nFrequency\\nFrequency\\nFrequency\\nFIGURE 5.3\\nMatrix showing the frequency distribution for a common set of variables for three groups of cars—American,\\nEuropean, and Asian.\\n86'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 100}, page_content='140\\n105\\n70\\n30\\n0\\n0\\n30\\n60\\n90\\n120\\n6\\n3\\n4\\n5\\n6\\n8\\n50 100 150 200 250 300 350 400 450 500\\n5\\n10 15 20 25 30 35 40 45 50\\n40 60 80 100120140160180200220240\\n8 10 12 14 16\\nAcceleration\\nCylinders\\nDisplacement\\nHorsepower\\nMPG\\nFrequency\\nFrequency\\n100\\n200\\n150\\n100\\n50\\n0\\n75\\n50\\n25\\n0\\nFrequency\\n100\\n75\\n50\\n25\\n0\\nFrequency\\nFrequency\\n18 20 22 24 26\\nFIGURE 5.4\\nHighlighting a group of observations using shading.\\n87'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 101}, page_content='88\\nIDENTIFYING AND UNDERSTANDING GROUPS\\n5.2 CLUSTERING\\n5.2.1 Overview\\nFor a given data set it is not necessarily known beforehand what groups of\\nobservations the entire data set is composed of. For example, in examining\\ncustomer data collected from a particular store, it is possible to identify and\\nsummarize classes of customers directly from the data to answer questions\\nsuch as “What types of customers visit the store?” Clustering groups data\\ninto sets of related observations or clusters, so that observations within\\neach group are more similar to other observations within the group than to\\nobservations within other groups. Here, we use the concept of similarity\\nabstractly but define it more precisely in Section 5.2.2.\\nClustering is an unsupervised method for grouping. By unsupervised,\\nwe mean that the groups are not known in advance and a goal—a specific\\nvariable—is not used to direct how the grouping is generated. Instead, all\\nvariables are considered in the analysis. The clustering method chosen to\\nsubdivide the data into groups applies an automated procedure to discover\\nthe groups based on some criteria and its solution is extracted from patterns\\nor structure existing in the data. There are many clustering methods, and\\nit is important to know that each will group the data differently based on\\nthe criteria it uses, regardless of whether meaningful groups exist or not.\\nFor clustering, there is no way to measure accuracy and the solution is\\njudged by its “usefulness.” For that reason, clustering is used as an open-\\nended way to explore, understand, and formulate questions about the data\\nin exploratory data analysis.\\nTo illustrate the process of clustering, a set of observations are shown\\non the scatterplot in Figure 5.5. These observations are plotted using two\\nhypothetical dimensions and the similarity between the observations is\\nproportional to the physical distance between the observations. There are\\ntwo clear regions that can be considered as clusters: Cluster A and Cluster\\nB, since many of the observations are contained within these two regions\\non the scatterplot.\\nClustering is a flexible approach for grouping. For example, based on the\\ncriteria for clustering the observations, observation X was not determined\\nto be a member of cluster A. However, if a more relaxed criterion was\\nused, X may have been included in cluster A. Clustering not only assists\\nin identifying groups of related observations, it can also locate outliers—\\nobservations that are not similar to others—since they fall into groups of\\ntheir own. In Figure 5.5, there are six observations that do not fall within\\ncluster A or B.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 102}, page_content='CLUSTERING\\n89\\nFIGURE 5.5\\nIllustration of clusters and outliers.\\nThere are two major categories of clustering. Some clustering methods\\norganize data sets hierarchically, which may provide additional insight into\\nthe problem under investigation. For example, when clustering genomics\\ndata sets, hierarchical clustering may provide insight into the biological\\nfunctional processes associated with collections of related genes. Other\\nclustering methods partition the data into lists of clusters based on a pre-\\ndefined number of groups. For these methods, the speed of computation\\noutweighs the challenge of determining in advance the number of groups\\nthat should be used.\\nThere are other factors to consider in choosing and fine-tuning the clus-\\ntering of a data set. Adjusting the criteria clustering methods use includes\\noptions for calculating the similarity between observations and for select-\\ning cluster size. Different problems require different clustering options and\\nmay require repeated examination of the results as the options are adjusted\\nto make sense of a particular cluster. Finally, it is important to know the\\nlimits of the algorithms. Some clustering methods are time-consuming\\nand, especially for large data sets, may be too computationally expensive\\nto consider, while other methods may have limitations on the number of\\nobservations they can process.\\nTo illustrate how clustering works, two clustering techniques will\\nbe described in this section: hierarchical agglomerative clustering and\\nk-means clustering. References to additional clustering methods will be\\nprovided in the Further Reading section of this chapter. All approaches to'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 103}, page_content='90\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nTABLE 5.1\\nTable Showing Two Observations A and B\\nObservation ID\\nVariable 1\\nVariable 2\\nA\\n2\\n3\\nB\\n7\\n8\\nclustering require a formal approach to defining how similar two observa-\\ntions are to each other as measured by the distance between them, and this\\nis described in Section 5.2.2.\\n5.2.2 Distances\\nA method of clustering needs a way to measure how similar observations\\nare to each other. To calculate similarity, we need to compute the distance\\nbetween observations. To illustrate the concept of distance we will use a\\nsimple example with two observations and two variables (see Table 5.1).\\nWe can see the physical distance between the two observations by plotting\\nthem on the following scatterplot (Figure 5.6). In this example, the distance\\nbetween the two observations is calculated using simple trigonometry:\\nx = 7 −2 = 5\\ny = 8 −3 = 5\\nd =\\n√\\nx2 + y2 =\\n√\\n25 + 25 = 7.07\\nFIGURE 5.6\\nDistance between two observations A and B.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 104}, page_content='CLUSTERING\\n91\\nTABLE 5.2\\nThree Observations with Values for Five Variables\\nID\\nVariable 1\\nVariable 2\\nVariable 3\\nVariable 4\\nVariable 5\\nA\\n0.7\\n0.8\\n0.4\\n0.5\\n0.2\\nB\\n0.6\\n0.8\\n0.5\\n0.4\\n0.2\\nC\\n0.8\\n0.9\\n0.7\\n0.8\\n0.9\\nIt is possible to calculate distances between observations with more\\nthan two variables by extending this approach. This calculation is called\\nthe Euclidean distance (d) and its general formula is\\nd =\\n√\\n√\\n√\\n√\\nn\\n∑\\ni=1\\n(pi −qi)2\\nThis formula calculates the distance between two observations p and\\nq where each observation has n variables. To illustrate the Euclidean\\ndistance calculation for observations with more than two variables, we\\nwill use Table 5.2.\\nThe Euclidean distance between A and B is\\ndA−B =\\n√\\n(0.7 −0.6)2 + (0.8 −0.8)2 + (0.4 −0.5)2 + (0.5 −0.4)2 + (0.2 −0.2)2\\ndA−B = 0.17\\nThe Euclidean distances between A and C is\\ndA−C =\\n√\\n(0.7 −0.8)2 + (0.8 −0.9)2 + (0.4 −0.7)2 + (0.5 −0.8)2 + (0.2 −0.9)2\\ndA−C = 0.83\\nThe Euclidean distance between B and C is\\ndB−C =\\n√\\n(0.6 −0.8)2 + (0.8 −0.9)2 + (0.5 −0.7)2 + (0.4 −0.8)2 + (0.2 −0.9)2\\ndB−C = 0.86\\nThe distance between A and B is 0.17, whereas the distance between\\nA and C is 0.83, which indicates that there is more similarity between\\nobservations A and B than A and C. C is not closely related to either A or\\nB. This can be seen in Figure 5.7 where the values for each variable are\\nplotted along the horizontal axis and the height of the bar measured against\\nthe vertical axis represents the data value. The shape of histograms A and\\nB are similar, whereas the shape of histogram C is not similar to A or B.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 105}, page_content='92\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.7\\nDistances between three observations: A–B, B–C, and A–C.\\nSometimes a distance metric can only be used for a particular type of\\nvariable. The Euclidean distance metric can be used only for numerical\\nvariables. Other metrics are needed for binary variables and one of which\\nis the Jaccard distance. This approach is based on the number of common\\nor different 0/1 values between corresponding variables across each pair\\nof observations using the following counts:\\nr Count11: Count of all variables that are 1 in “Observation 1” and 1 in\\n“Observation 2.”\\nr Count10: Count of all variables that are 1 in “Observation 1” and 0 in\\n“Observation 2.”\\nr Count01: Count of all variables that are 0 in “Observation 1” and 1 in\\n“Observation 2.”\\nr Count00: Count of all variables that are 0 in “Observation 1” and 0 in\\n“Observation 2.”'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 106}, page_content='CLUSTERING\\n93\\nTABLE 5.3\\nThree Observations Measured over Five Binary Variables\\nVariable 1\\nVariable 2\\nVariable 3\\nVariable 4\\nVariable 5\\nA\\n1\\n1\\n0\\n0\\n1\\nB\\n1\\n1\\n0\\n0\\n0\\nC\\n0\\n0\\n1\\n1\\n1\\nThe following formula is used to calculate the Jaccard distance:\\nd =\\nCount10 + Count01\\nCount11 + Count10 + Count01\\nThe Jaccard distance is illustrated using Table 5.3.\\nBetween observations A and B are two variables where both values are 1\\n(Variable 1 and Variable 2), two values where both variables are 0 (Variable\\n3 and Variable 4), one value where the value is 1 in observation A but 0 in\\nobservation B (Variable 5) and no values where a value is 0 in observation A\\nand 1 in observation B. Therefore, the Jaccard distance between A and B is\\ndA−B = (1 + 0)∕(2 + 1 + 0) = 0.33\\nThe Jaccard distance between A and C is\\ndA−C = (2 + 2)∕(1 + 2 + 2) = 0.8\\nThe Jaccard distance between B and C is\\ndB−C = (2 + 3)∕(0 + 2 + 3) = 1.0\\nThe Euclidean and Jaccard distance metrics are two examples of\\ntechniques for determining the distance between observations. Other tech-\\nniques include Mahalanobis, City Block, Minkowski, Cosine, Spearman,\\nHamming, and Chebyshev (see the Further Reading section for references\\non these and other methods).\\n5.2.3 Agglomerative Hierarchical Clustering\\nAgglomerative hierarchical clustering is an example of a hierarchical\\nmethod for grouping observations. It uses a “bottom-up” approach to clus-\\ntering as it starts with each observation and progressively creates clusters'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 107}, page_content='94\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nTABLE 5.4\\nTable of Observations to Cluster\\nName\\nVariable 1\\nVariable 2\\nVariable 3\\nVariable 4\\nVariable 5\\nA\\n7.9\\n8.6\\n4.4\\n5.0\\n2.5\\nB\\n6.8\\n8.2\\n5.2\\n4.2\\n2.2\\nC\\n8.7\\n9.6\\n7.5\\n8.9\\n9.8\\nD\\n6.1\\n7.3\\n7.9\\n7.3\\n8.3\\nE\\n1.5\\n2.0\\n5.1\\n3.6\\n4.2\\nF\\n3.7\\n4.3\\n5.4\\n3.3\\n5.8\\nG\\n7.2\\n8.5\\n8.6\\n6.7\\n6.1\\nH\\n8.5\\n9.7\\n6.3\\n5.2\\n5.0\\nI\\n2.0\\n3.4\\n5.8\\n6.1\\n5.6\\nJ\\n1.3\\n2.6\\n4.2\\n4.5\\n2.1\\nK\\n3.4\\n2.9\\n6.5\\n5.9\\n7.4\\nL\\n2.3\\n5.3\\n6.2\\n8.3\\n9.9\\nM\\n3.8\\n5.5\\n4.6\\n6.7\\n3.3\\nN\\n3.2\\n5.9\\n5.2\\n6.2\\n3.7\\nby merging observations together until all are a member of a final single\\ncluster. The major limitation of agglomerative hierarchical clustering is\\nthat it is normally limited to data sets with fewer than 10,000 observations\\nbecause the computational cost to generate the hierarchical tree can be\\nhigh, especially for larger numbers of observations.\\nTo illustrate the process of agglomerative hierarchical clustering, the\\ndata set of 14 observations measured over 5 variables as shown in Table 5.4\\nwill be used. In this example, the variables are all measured on the same\\nscale; however, where variables are measured on different scales they\\nshould be normalized to a comparable range (e.g., 0–1) prior to clustering.\\nThis prevents one or more variables from having a disproportionate weight\\nand creating a bias in the analysis.\\nFirst, the distance between all pairs of observations is calculated. The\\nmethod for calculating the distance along with the variables to include in\\nthe calculation should be set prior to clustering. In this example, we\\nwill use the Euclidean distance across all continuous variables shown in\\nTable 5.4. The distances between all combinations of observations are sum-\\nmarized in a distance matrix, as illustrated in Table 5.5. In this example,\\nthe distances between four observations are shown (A, B, C, D) and each\\nvalue in the table shows the distance between two indexed observations.\\nThe diagonal values are excluded, since these pairs are of the same obser-\\nvation. It should be noted that a distance matrix is usually symmetrical'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 108}, page_content='CLUSTERING\\n95\\nTABLE 5.5\\nDistance Matrix Format\\nA\\nB\\nC\\nD\\n. . .\\nA\\ndA,B\\ndA,C\\ndA,D\\n. . .\\nB\\ndB,A\\ndB,C\\ndB,D\\n. . .\\nC\\ndC,A\\ndC,B\\ndC,D\\n. . .\\nD\\ndD,A\\ndD,B\\ndD,C\\n. . .\\n. . .\\n. . .\\n. . .\\n. . .\\n. . .\\nabout the diagonal as the distance between, for example, A and B is the\\nsame as the distance between B and A.\\nFor the 14 observations in Table 5.4, the complete initial distance matrix\\nis shown in Table 5.6. This table is symmetrical about the diagonal since, as\\ndescribed previously, the ordering of the pairs is irrelevant when using the\\nEuclidean distance. The two closest observations are identified (M and N in\\nthis example) and are merged into a single cluster. These two observations\\nfrom now on will be considered a single group.\\nNext, all observations (minus the two that have been merged into a\\ncluster) along with the newly created cluster are compared to see which\\nobservation or cluster should be joined into the next cluster. Since we are\\nnow analyzing individual observations and clusters, a joining or linkage\\nrule is needed to determine the distance between an observation and a\\ncluster of observations. This joining/linkage rule should be set prior to\\nclustering. In Figure 5.8, two clusters have already been identified: Cluster\\nA and Cluster B. We now wish to determine the distance between observa-\\ntion X and the cluster A or B (to determine whether or not to merge X with\\none of the two clusters). There are a number of ways to calculate the dis-\\ntance between an observation and an already established cluster including\\naverage linkage, single linkage, and complete linkage. These alternatives\\nare illustrated in Figure 5.9.\\nr Average linkage: the distance between all members of the cluster\\n(e.g., a, b, and c) and the observation under consideration (e.g., x) are\\ncalculated and the average is used for the overall distance.\\nr Single linkage: the distance between all members of the cluster (e.g.,\\na, b, and c) and the observation under consideration (e.g., x) are\\ncalculated and the smallest is selected.\\nr Complete linkage: the distance between all members of the cluster\\n(e.g., a, b, and c) and the observation under consideration (e.g., x) are\\ncalculated and the highest is selected.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 109}, page_content='TABLE 5.6\\nCalculated Distances Between All Pairs of Observations\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nA\\n0.282\\n1.373\\n1.2\\n1.272\\n0.978\\n1.106\\n0.563\\n1.178\\n1.189\\n1.251\\n1.473\\n0.757\\n0.793\\nB\\n0.282\\n1.423\\n1.147\\n1.113\\n0.82\\n1.025\\n0.56\\n1.064\\n1.065\\n1.144\\n1.44\\n0.724\\n0.7\\nC\\n1.373\\n1.423\\n0.582\\n1.905\\n1.555\\n0.709\\n0.943\\n1.468\\n1.995\\n1.305\\n1.076\\n1.416\\n1.378\\nD\\n1.2\\n1.147\\n0.582\\n1.406\\n1.092\\n0.403\\n0.808\\n0.978\\n1.543\\n0.797\\n0.744\\n1.065\\n0.974\\nE\\n1.272\\n1.113\\n1.905\\n1.406\\n0.476\\n1.518\\n1.435\\n0.542\\n0.383\\n0.719\\n1.223\\n0.797\\n0.727\\nF\\n0.978\\n0.82\\n1.555\\n1.092\\n0.476\\n1.191\\n1.039\\n0.57\\n0.706\\n0.595\\n1.076\\n0.727\\n0.624\\nG\\n1.106\\n1.025\\n0.709\\n0.403\\n1.518\\n1.191\\n0.648\\n1.163\\n1.624\\n1.033\\n1.108\\n1.148\\n1.051\\nH\\n0.563\\n0.56\\n0.943\\n0.808\\n1.435\\n1.039\\n0.648\\n1.218\\n1.475\\n1.169\\n1.315\\n0.984\\n0.937\\nI\\n1.178\\n1.064\\n1.468\\n0.978\\n0.542\\n0.57\\n1.163\\n1.218\\n0.659\\n0.346\\n0.727\\n0.553\\n0.458\\nJ\\n1.189\\n1.065\\n1.995\\n1.543\\n0.383\\n0.706\\n1.624\\n1.475\\n0.659\\n0.937\\n1.344\\n0.665\\n0.659\\nK\\n1.251\\n1.144\\n1.305\\n0.797\\n0.719\\n0.595\\n1.033\\n1.169\\n0.346\\n0.937\\n0.64\\n0.774\\n0.683\\nL\\n1.473\\n1.44\\n1.076\\n0.744\\n1.223\\n1.076\\n1.108\\n1.315\\n0.727\\n1.344\\n0.64\\n0.985\\n0.919\\nM\\n0.757\\n0.724\\n1.416\\n1.065\\n0.797\\n0.727\\n1.148\\n0.984\\n0.553\\n0.665\\n0.774\\n0.985\\n0.196\\nN\\n0.793\\n0.7\\n1.378\\n0.974\\n0.727\\n0.624\\n1.051\\n0.937\\n0.458\\n0.659\\n0.683\\n0.919\\n0.196\\n96'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 110}, page_content='CLUSTERING\\n97\\nFIGURE 5.8\\nComparing observation X with two clusters A and B.\\nDistances between all combinations of groups and observations are\\nconsidered and the smallest distance is selected. Since we now may need\\nto consider the distance between two clusters, the linkage/joining concept\\nis extended to the joining of two clusters, as illustrated in Figure 5.10. The\\nprocess of assessing all pairs of observations/clusters, then combining the\\npair with the smallest distance is repeated until there are no more clusters\\nor observations to join together since only a single cluster remains.\\nFigure 5.11 illustrates this process for some steps based on the obser-\\nvations shown in Table 5.6. In step 1, it is determined that observations\\nM and N are the closest and they are combined into a cluster, as shown.\\nThe horizontal length of the lines joining M and N reflects the distance at\\nwhich the cluster was formed (0.196). From now on M and N will not be\\nconsidered individually, but only as a cluster. In step 2, distances between\\nall observations (except M and N) as well as the cluster containing M and\\nN are calculated. To determine the distance between the individual obser-\\nvations and the cluster containing M and N, the average linkage rule was\\nused. It is now determined that A and B should be joined as shown. Once\\nagain, all distances between the remaining ungrouped observations and the\\nnewly created clusters are calculated and the smallest distance selected.\\nFIGURE 5.9\\nDifferent linkage rules for considering an observation and a cluster.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 111}, page_content='98\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.10\\nDifferent linkage rules for considering two clusters.\\nSteps 3 to 5 follow the same process. In step 6, the shortest distance is\\nbetween observation I and the cluster containing M and N and in step 8,\\nthe shortest distance is between the cluster {M,N} and the cluster {F,I,K}.\\nThis process continues until only one cluster containing all the observa-\\ntions remains. Figure 5.12 shows the completed hierarchical clustering for\\nall 14 observations.\\nWhen clustering completes, a tree called a dendrogram is generated\\nshowing the similarity between observations and clusters as shown in\\nFigure 5.12. To divide a data set into a series of distinct clusters, we must\\nselect a distance at which the clusters are to be created. Where this distance\\nintersects with a horizontal line on the tree, a cluster is formed, as illustrated\\nFIGURE 5.11\\nSteps 1 through 8 of the clustering process.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 112}, page_content='CLUSTERING\\n99\\nFIGURE 5.12\\nCompleted hierarchical clustering for the 14 observations.\\nin Figure 5.13. In this example, three different distances (i, j, k) are used to\\ndivide the tree into clusters. Where this vertical line intersects with the tree\\n(shown by the circles) at distance i, two clusters are formed: {L,C,D,G}\\nand {H,A,B,E,J,M,N,F,I,K}; at distance j, four clusters are formed: {L},\\n{C,D,G}, {H,A,B}, and {E,J,M,N,F,I,K}; and at distance k, nine clusters\\nare formed: {L}, {C}, {D,G}, {H}, {A,B}, {E,J}, {M,N}, {F}, and {I,K}.\\nAs illustrated in Figure 5.13, adjusting the cut-off distance will change the\\nnumber of clusters created. Distance cut-offs toward the left will result in\\nfewer clusters with more diverse observations within each cluster. Cut-offs\\ntoward the right will result in a greater number of clusters with more similar\\nobservations within each cluster.\\nDifferent joining/linkage rules change how the final hierarchical cluster-\\ning is presented. Figure 5.14 shows the hierarchical clustering of the same\\nset of observations using the average linkage, single linkage, and complete\\nlinkage rules. Since the barrier for merging observations and clusters is\\nlowest with the single linkage approach, the clustering dendrogram may\\ncontain chains of clusters as well as clusters that are spread out. The barrier\\nto joining clusters is highest with complete linkage; however, it is possible\\nthat an observation is closer to observations in other clusters than the clus-\\nter to which it has been assigned. The average linkage approach moderates\\nthe tendencies of the single or complete linkage approaches.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 113}, page_content='100\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.13\\nCluster generation using three distance cut-offs.\\nThe following example uses a data set of 392 cars that will be explored\\nusing hierarchical agglomerative clustering. A portion of the data table is\\nshown in Table 5.7.\\nThis data set was clustered using the Euclidean distance method and the\\ncomplete linkage rule. The following variables were used in the clustering:\\nDisplacement, Horsepower, Acceleration, and MPG (miles per gallon).\\nFIGURE 5.14\\nDifferent results using three different methods for joining\\nclusters.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 114}, page_content='TABLE 5.7\\nData Table Containing Automobile Observations\\nCar Name\\nMPG\\nCylinders\\nDisplacement\\nHorsepower\\nWeight\\nAcceleration\\nModel Year\\nOrigin\\nChevrolet Chevelle Malibu\\n18\\n8\\n307\\n130\\n3,504\\n12\\n70\\nAmerican\\nBuick Skylark 320\\n15\\n8\\n350\\n165\\n3,693\\n11.5\\n70\\nAmerican\\nPlymouth Satellite\\n18\\n8\\n318\\n150\\n3,436\\n11\\n70\\nAmerican\\nAmc rebel sst\\n16\\n8\\n304\\n150\\n3,433\\n12\\n70\\nAmerican\\nFord Torino\\n17\\n8\\n302\\n140\\n3,449\\n10.5\\n70\\nAmerican\\nFord Galaxie 500\\n15\\n8\\n429\\n198\\n4,341\\n10\\n70\\nAmerican\\n101'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 115}, page_content='102\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.15\\nHierarchical agglomerative clustering dendrogram generated for\\nthe automobile data set.\\nThe dendrogram in Figure 5.15 of the generated clusters shows the rela-\\ntionships between observations based on the similarity of the four selected\\nvariables. Each horizontal line at the right represents a single automobile\\nand the order of the observations is related to how similar each car is to its\\nneighbors.\\nIn Figure 5.16 a distance cut-off has been set such that the data is divided\\ninto three clusters. In addition to showing the dendrogram, three small\\nmultiple charts illustrate the composition of each cluster – the highlighted\\nregion – for each of the four variables. Cluster 1 is a set of 97 observations\\nwith low fuel efficiency and low acceleration values, and generally higher\\nvalues for horsepower and displacement. Cluster 2 contains 85 observa-\\ntions with generally good fuel efficiency and acceleration as well as low\\nhorsepower and displacement. Cluster 3 contains 210 observations, the\\nmajority of which have average fuel efficiency and acceleration as well as\\nfew high values for displacement or horsepower.\\nTo explore the data set further we can adjust the distance cut-off to\\ngenerate different numbers of clusters. Figure 5.17 displays the case in\\nwhich the distance was set to create nine clusters. Cluster 1 (from Figure\\n5.16) is now divided into three clusters of sizes 36, 4, and 57. The new\\ncluster 1 is a set of 36 observations with high horsepower and displacement\\nvalues, as well as low fuel efficiency and acceleration; cluster 2 represents a\\nset of only 4 cars with the worst fuel efficiency and improved acceleration;\\nand cluster 3 is a set of 57 cars with lower horsepower than cluster 1 or 2\\nand improved MPG values. Similarly, cluster 2 (from Figure 5.16) is now\\ndivided into three clusters of sizes 33, 4, and 48 and cluster 3 (from Figure\\n5.16) is now divided into three clusters of sizes 73, 35, and 102.\\nSince the ordering of the observations provides insight into the data set’s\\norganization, a clustering dendrogram is often accompanied by a colored'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 116}, page_content='FIGURE 5.16\\nAutomobile data set with three clusters identified.\\n103'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 117}, page_content='104\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.17\\nAutomobile data set cluster and split into nine groups.\\nheatmap that uses different colors or shades to represent the different\\nobservation values across variables of interest. For example, in Figure 5.18\\nthe 14 observations from Table 5.6 have been clustered using agglomerative\\nhierarchical cluster, based on the Euclidean distance over the 5 continuous\\nvariables and the average linkage joining method. A heatmap, shown to\\nthe right of the dendrogram, is used to represent the data values for the 14\\nobservations. Different shades of gray are used to represent the data values\\n(binned as shown in the legend). It is possible to see patterns in the data set\\nusing this approach because observations with similar patterns have been\\ngrouped close together.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 118}, page_content='CLUSTERING\\n105\\nFIGURE 5.18\\nClustering dendrogram coupled with a colored heatmap.\\n5.2.4 k-Means Clustering\\nk-Means clustering is an example of a nonhierarchical method for grouping\\na data set. It groups data using a “top-down” approach since it starts with\\na predefined number of clusters and assigns all observations to each of\\nthem. There are no overlaps in the groups; each observation is assigned\\nonly to a single group. This approach is computationally faster and can\\nhandle greater numbers of observations than agglomerative hierarchical\\nclustering. However, there are several disadvantages to using this method.\\nThe most significant is that the number of groups must be specified before\\ncreating the clusters and this number is not guaranteed to provide the best\\npartitioning of the observations. Another disadvantage is that when a data\\nset contains many outliers, k-means may not create an optimal grouping\\n(discussed later in this section). Finally, no hierarchical organization is\\ngenerated using k-means clustering and hence there is no ordering of the\\nindividual observations.\\nThe process of generating clusters starts by defining the value k, which\\nis the number of groups to create. The method then initially allocates an\\nobservation – usually selected randomly – to each of these groups. Next,\\nall other observations are compared to each of the allocated observations\\nand placed in the group to which they are most similar. The center point for\\neach of these groups is then calculated. The grouping process continues by'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 119}, page_content='106\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.19\\nIllustrating conceptually k-means clustering.\\ndetermining the distance from all observations to these new group centers.\\nIf an observation is closer to the center of another group, it is moved to the\\ngroup that it is closest to. The centers of its old and new groups are then\\nrecalculated. The process of comparing and moving observations where\\nappropriate is repeated until no observations are moved after a recalculation\\nof the group’s center.\\nTo illustrate the process of clustering using k-means, a set of 12 hypothet-\\nical observations are used: a, b, c, d, e, f, g, h, i, j, k, and l. These observations\\nare shown as colored circles in Figure 5.19. It was determined at the start\\nthat three groups should be generated. Initially, an observation is randomly\\nassigned to each of the three groups as shown in step 1: f to cluster 1, d to\\ncluster 2, and e to cluster 3. Next, all remaining observations are assigned\\nto the cluster to which they are closest using one of the distance functions\\ndescribed earlier. For example, observation c is assigned to cluster 1 since\\nit is closer to f than to d or e. Once all observations have been assigned to\\nan initial cluster, the point at the center of each cluster is then calculated.\\nNext, distances from each observation to the center of each cluster are\\ncalculated. It is determined in step 3 that observation c is closer to the\\ncenter of cluster 2 than the other two clusters, so c is moved to cluster 1.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 120}, page_content='CLUSTERING\\n107\\nTABLE 5.8\\nData Table Used in the k-Mean Clustering Example\\nName\\nVariable 1\\nVariable 2\\nVariable 3\\nVariable 4\\nVariable 5\\nA\\n7.9\\n8.6\\n4.4\\n5.0\\n2.5\\nB\\n6.8\\n8.2\\n5.2\\n4.2\\n2.2\\nC\\n8.7\\n9.6\\n7.5\\n8.9\\n9.8\\nD\\n6.1\\n7.3\\n7.9\\n7.3\\n8.3\\nE\\n1.5\\n2.0\\n5.1\\n3.6\\n4.2\\nF\\n3.7\\n4.3\\n5.4\\n3.3\\n5.8\\nG\\n7.2\\n8.5\\n8.6\\n6.7\\n6.1\\nH\\n8.5\\n9.7\\n6.3\\n5.2\\n5.0\\nI\\n2.0\\n3.4\\n5.8\\n6.1\\n5.6\\nJ\\n1.3\\n2.6\\n4.2\\n4.5\\n2.1\\nK\\n3.4\\n2.9\\n6.5\\n5.9\\n7.4\\nL\\n2.3\\n5.3\\n6.2\\n8.3\\n9.9\\nM\\n3.8\\n5.5\\n4.6\\n6.7\\n3.3\\nN\\n3.2\\n5.9\\n5.2\\n6.2\\n3.7\\nIt is also determined that e and k are now closer to the center of cluster 1\\nso these observations are moved to cluster 1. Since the contents of all three\\nclusters have changed, the centers for all clusters are recalculated. This\\nprocess continues until no more observations are moved between clusters,\\nas shown in step n on the diagram.\\nA disadvantage of k-means clustering is that when a data set contains\\nmany outliers, k-means may not create an optimal grouping. This is because\\nthe reassignment of observations is based on closeness to the center of\\nthe cluster and outliers pull the centers of the clusters in their direction,\\nresulting in assignment of the remaining observations to other groups.\\nThe following example will illustrate the process of calculating the cen-\\nter of a cluster. The observations in Table 5.8 are grouped into three clusters\\nusing the Euclidean distance to determine the distance between observa-\\ntions. A single observation is randomly assigned to the three clusters as\\nshown in Figure 5.20: I to cluster 1, G to cluster 2, D to cluster 3. All other\\nFIGURE 5.20\\nSingle observation randomly assigned to each cluster.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 121}, page_content='108\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nTABLE 5.9\\nEuclidean Distances and Cluster Assignments\\nName\\nCluster 1\\nCluster 2\\nCluster 3\\nCluster Assignment\\nA\\n1.178\\n1.106\\n1.2\\n2\\nB\\n1.064\\n0.025\\n1.147\\n2\\nC\\n1.468\\n0.709\\n0.582\\n3\\nE\\n0.542\\n1.518\\n1.406\\n1\\nF\\n0.57\\n1.191\\n1.092\\n1\\nH\\n1.218\\n0.648\\n0.808\\n2\\nJ\\n0.659\\n1.624\\n1.543\\n1\\nK\\n0.346\\n1.033\\n0.797\\n1\\nL\\n0.727\\n1.108\\n0.744\\n1\\nM\\n0.553\\n1.148\\n1.065\\n1\\nN\\n0.458\\n1.051\\n0.974\\n1\\nobservations are compared to the three clusters by calculating the distance\\nbetween the observations and I, G, and D. Table 5.9 shows the Euclidean\\ndistance to I, G, and D from every other observation, along with the cluster\\nit is initially assigned to. All observations are now assigned to one of the\\nthree clusters (Figure 5.21).\\nNext, the center of each cluster is calculated by taking the average value\\nfor each variable in the group, as shown in Table 5.10. For example, the\\ncenter of cluster 1 is now\\n{Variable 1 = 2.65; Variable 2 = 3.99; Variable 3 = 5.38;\\nVariable 4 = 5.58; Variable 5 = 5.25}\\nEach observation is now compared to the centers of each cluster and\\nthe process of examining the observations and moving them as appropriate\\nis repeated until no further moves are needed. In this example, the final\\nassignment is shown in Figure 5.22.\\nFIGURE 5.21\\nInitial assignment of other observations to each cluster.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 122}, page_content='CLUSTERING\\n109\\nTABLE 5.10\\nCalculating the Center of Each Cluster\\nCluster 1\\nName\\nVariable 1\\nVariable 2\\nVariable 3\\nVariable 4\\nVariable 5\\nE\\n1.5\\n2\\n5.1\\n3.6\\n4.2\\nF\\n3.7\\n4.3\\n5.4\\n3.3\\n5.8\\nI\\n2\\n3.4\\n5.8\\n6.1\\n5.6\\nJ\\n1.3\\n2.6\\n4.2\\n4.5\\n2.1\\nK\\n3.4\\n2.9\\n6.5\\n5.9\\n7.4\\nL\\n2.3\\n5.3\\n6.2\\n8.3\\n9.9\\nM\\n3.8\\n5.5\\n4.6\\n6.7\\n3.3\\nN\\n3.2\\n5.9\\n5.2\\n6.2\\n3.7\\nAverage (Center)\\n2.65\\n3.99\\n5.38\\n5.58\\n5.25\\nCluster 2\\nName\\nVariable 1\\nVariable 2\\nVariable 3\\nVariable 4\\nVariable 5\\nA\\n7.9\\n8.6\\n4.4\\n5\\n2.5\\nB\\n6.8\\n8.2\\n5.2\\n4.2\\n2.2\\nG\\n7.2\\n8.5\\n8.6\\n6.7\\n6.1\\nH\\n8.5\\n9.7\\n6.3\\n5.2\\n5\\nAverage (Center)\\n7.60\\n8.75\\n6.13\\n5.28\\n3.95\\nCluster 3\\nName\\nVariable 1\\nVariable 2\\nVariable 3\\nVariable 4\\nVariable 5\\nC\\n8.7\\n9.6\\n7.5\\n8.9\\n9.8\\nD\\n6.1\\n7.3\\n7.9\\n7.3\\n8.3\\nAverage (Center)\\n7.40\\n8.45\\n7.70\\n8.10\\n9.05\\nA data set of 392 cars is grouped using k-means clustering. This is the\\nsame data set used in the agglomerative hierarchical clustering example.\\nThe Euclidean distance was used and the number of clusters was set to\\n3. The same set of variables was used as in the agglomerative hierarchi-\\ncal clustering example. Although both methods produce similar results,\\nthey are not identical. k-means cluster 3 (shown in Figure 5.23) is almost\\nFIGURE 5.22\\nFinal assignment of observations using k-means clustering.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 123}, page_content='FIGURE 5.23\\nThree clusters generated using k-means clustering.\\n110'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 124}, page_content='ASSOCIATION RULES\\n111\\nidentical to the agglomerative hierarchical cluster 1 (shown in Figure 5.16).\\nThere is some similarity between cluster 2 (k-means) and cluster 3 (agglom-\\nerative hierarchical) as well as cluster 1 (k-means) and cluster 3 (agglom-\\nerative hierarchical). Figure 5.24 shows the results of a k-means method\\ngenerating nine groups.\\n5.3 ASSOCIATION RULES\\n5.3.1 Overview\\nThe association rules method groups observations and attempts to discover\\nlinks or associations between different attributes of the group. Associative\\nrules can be applied in many situations such as data mining retail transac-\\ntions. This method generates rules from the groups as, for example,\\nIF the customer is age 18 AND\\nthe customer buys paper AND\\nthe customer buys a hole punch\\nTHEN the customer buys a binder\\nThe rule states that 18-year-old customers who purchase paper and a\\nhole punch often buy a binder at the same time. Using this approach, the\\nrule would be generated directly from a data set and using this information\\nthe retailer may decide, for example, to create a package of products for\\ncollege students.\\nThe association rules method is an example of an unsupervised group-\\ning method. (Recall from Section 5.2 that unsupervised methods are undi-\\nrected, i.e., no specific variable is selected to guide the process.) The\\nadvantages of this method include the generation of rules that are easy to\\nunderstand, the ability to perform an action based on the rule as in the pre-\\nvious example which allowed the retailer to apply the rule to make changes\\nto the marketing strategy, and the possibility of using this technique with\\nlarge numbers of observations.\\nThere are, however, limitations. This method forces you to either restrict\\nyour analysis to variables that are categorical or convert continuous vari-\\nables to categorical variables. Generating the rules can be computationally\\nexpensive, especially where a data set has many variables or many possi-\\nble values per variable, or both. There are ways to make the analysis run\\nfaster but they often compromise the final results. Finally, this method can\\ngenerate large numbers of rules that must be prioritized and interpreted.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 125}, page_content='FIGURE 5.24\\nNine clusters generated using k-means clustering.\\n112'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 126}, page_content='ASSOCIATION RULES\\n113\\nTABLE 5.11\\nData Table of Three Sample Examples\\nObservations with Three Variables\\nCustomer ID\\nGender\\nPurchase\\n932085\\nMale\\nTelevision\\n596720\\nFemale\\nCamera\\n267375\\nFemale\\nTelevision\\nIn this method, creating useful rules from the data is done by grouping\\nit, extracting rules from the groups, and then prioritizing the rules. The\\nfollowing sections describe the process of generating association rules.\\n5.3.2 Grouping by Combinations of Values\\nLet us first consider a simple situation concerning a shop that sells only\\ncameras and televisions. A data set of 31,612 sales transactions is used\\nwhich contains three variables: Customer ID, Gender, and Purchase. The\\nvariable Gender identifies whether the buyer is “male” or “female”. The\\nvariable Purchase refers to the item purchased and can have only two\\nvalues: “camera” and “television.” Table 5.11 shows three rows from this\\ntable. Each row of the table contains an entry where values are assigned\\nfor each variable (i.e., there are no missing values). By grouping this set\\nof 31,612 observations, based on specific values for the variables Gender\\nand Purchase, the groups shown in Table 5.12 are generated. There are\\neight ways of grouping this trivial example based on the values for the\\ndifferent categories of Gender and Purchase. For example, there are 7,889\\nobservations where Gender is “male” and Purchase is “camera.”\\nTABLE 5.12\\nGrouping by Different Value Combinations\\nGroup Number\\nCount\\nGender\\nPurchase\\nGroup 1\\n16,099\\nMale\\nCamera or Television\\nGroup 2\\n15,513\\nFemale\\nCamera or Television\\nGroup 3\\n16,106\\nMale or Female\\nCamera\\nGroup 4\\n15,506\\nMale or Female\\nTelevision\\nGroup 5\\n7,889\\nMale\\nCamera\\nGroup 6\\n8,210\\nMale\\nTelevision\\nGroup 7\\n8,217\\nFemale\\nCamera\\nGroup 8\\n7,296\\nFemale\\nTelevision'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 127}, page_content='114\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nTABLE 5.13\\nTable Showing Groups by Different Value Combinations\\nGroup\\nNumber\\nCount\\nGender\\nPurchase\\nIncome\\nGroup 1\\n16,099\\nMale\\nCamera or Television\\nBelow $50K or\\nAbove $50K\\nGroup 2\\n15,513\\nFemale\\nCamera or Television\\nBelow $50K or\\nAbove $50K\\nGroup 3\\n16,106\\nMale or Female\\nCamera\\nBelow $50K or\\nAbove $50K\\nGroup 4\\n15,506\\nMale or Female\\nTelevision\\nBelow $50K or\\nAbove $50K\\nGroup 5\\n15,854\\nMale or Female\\nCamera or Television\\nBelow $50K\\nGroup 6\\n15,758\\nMale or Female\\nCamera or Television\\nAbove $50K\\nGroup 7\\n7,889\\nMale\\nCamera\\nBelow $50K or\\nAbove $50K\\nGroup 8\\n8,210\\nMale\\nTelevision\\nBelow $50K or\\nAbove $50K\\nGroup 9\\n8,549\\nMale\\nCamera or Television\\nBelow $50K\\nGroup 10\\n7,550\\nMale\\nCamera or Television\\nAbove $50K\\nGroup 11\\n8,217\\nFemale\\nCamera\\nBelow $50K or\\nAbove $50K\\nGroup 12\\n7,296\\nFemale\\nTelevision\\nBelow $50K or\\nAbove $50K\\nGroup 13\\n7,305\\nFemale\\nCamera or Television\\nBelow $50K\\nGroup 14\\n8,208\\nFemale\\nCamera or Television\\nAbove $50K\\nGroup 15\\n8,534\\nMale or Female\\nCamera\\nBelow $50K\\nGroup 16\\n7,572\\nMale or Female\\nCamera\\nAbove $50K\\nGroup 17\\n7,320\\nMale or Female\\nTelevision\\nBelow $50K\\nGroup 18\\n8,186\\nMale or Female\\nTelevision\\nAbove $50K\\nGroup 19\\n4,371\\nMale\\nCamera\\nBelow $50K\\nGroup 20\\n3,518\\nMale\\nCamera\\nAbove $50K\\nGroup 21\\n4,178\\nMale\\nTelevision\\nBelow $50K\\nGroup 22\\n4,032\\nMale\\nTelevision\\nAbove $50K\\nGroup 23\\n4,163\\nFemale\\nCamera\\nBelow $50K\\nGroup 24\\n4,054\\nFemale\\nCamera\\nAbove $50K\\nGroup 25\\n3,142\\nFemale\\nTelevision\\nBelow $50K\\nGroup 26\\n4,154\\nFemale\\nTelevision\\nAbove $50K\\nIf an additional variable is added to this data set, the number of possible\\ngroups will increase as, for example, if another variable Income which has\\ntwo values – above $50K and below $50K – is added to the table, the\\nnumber of groups would increase to 26 as shown in Table 5.13.\\nIncreasing the number of variables or the number of possible values for\\neach variable or both increases the number of groups. When the number of'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 128}, page_content='ASSOCIATION RULES\\n115\\nFIGURE 5.25\\nTwenty-six observations characterized by shape, color, and bor-\\nder attributes.\\ngroups becomes too large, it becomes impractical to generate all combina-\\ntions. However, most data sets contain many combinations of values with\\nonly a few or no observations. Techniques for generating the groups take\\nadvantage of this by requiring that groups reach a certain size before they\\nare generated. This results in fewer groups and shortens the time required\\nto compute the results. However, care should be taken in setting this cut-off\\nvalue since rules can only be created from groups that are generated. For\\nexample, if this number is set to 10, then no rules will be generated from\\ngroups containing fewer than 10 observations. Subject matter knowledge\\nand information generated from the data characterization phase will help\\nin setting the cut-off value. There is a trade-off between speed of compu-\\ntation and how fine-grained the rules need to be (i.e., rules based on a few\\nobservations).\\n5.3.3 Extracting and Assessing Rules\\nSo far a data set has been grouped according to specific values for each\\nof the variables. In Figure 5.25, 26 observations (A through Z) are char-\\nacterized by three variables: Shape, Color, and Border. Observation A has\\nShape = square, Color = white and Border = thick and observation W has\\nShape = circle, Color = gray and Border = thin.\\nAs described in the previous section, the observations are grouped. An\\nexample of such a grouping is shown in Figure 5.26 where Shape = circle,\\nColor = gray and Border = thick.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 129}, page_content='116\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.26\\nGroup of six observations (gray circles with thick borders).\\nThe next step is to extract a rule from the group. There are three possible\\nrules (containing all three variables) that could be derived from this group\\n(Figure 5.26):\\nRule 1\\nIF Color = gray AND\\nShape = circle\\nTHEN Border = thick\\nRule 2\\nIF Border = thick AND\\nColor = gray\\nTHEN Shape = circle\\nRule 3\\nIF Border = thick AND\\nShape = circle\\nTHEN Color = gray\\nWe now examine each rule in detail and make a comparison to the\\nwhole data set in order to prioritize the rules. Three values are calculated\\nto support this assessment: support, confidence, and lift.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 130}, page_content='ASSOCIATION RULES\\n117\\nSupport\\nThe support is a measure of the number of observations a rule\\nmaps on to. Its value is the proportion of the observations a rule selects\\nout of all observations in the data set. In this example, the data set has 26\\nobservations and the group of gray circles with a thick border is 6, then the\\ngroup has a support value of 6 out of 26 or 0.23 (23%).\\nConfidence\\nEach rule is divided into two parts: antecedent and conse-\\nquence. The IF-part or antecedent refers to a list of statements linked with\\nAND in the first part of the rule. For example,\\nIF Color = gray AND\\nShape = circle\\nTHEN Border = thick\\nThe IF-part is the list of statements Color = gray AND Shape = circle.\\nThe THEN-part of the rule or consequence refers to statements after the\\nTHEN (Border = thick in this example).\\nThe confidence score is a measure for how predictable a rule is. The con-\\nfidence or predictability value is calculated using the support for the entire\\ngroup divided by the support for all observations satisfied by the IF-part\\nof the rule:\\nConfidence = group support∕IF-part support\\nFor example, the confidence value for Rule 1 is calculated using the\\nsupport value for the group and the support value for the IF-part of the rule\\n(see Figure 5.27).\\nRule 1\\nIF Color = gray AND\\nShape = circle\\nTHEN Border = thick\\nThe support value for the group (gray circles with a thick border) is 0.23\\nand the support value for the IF-part of the rule (gray circles) is 7 out of 26\\nor 0.27. To calculate the confidence, we divide the support for the group\\nby the support for the IF-part:\\nConfidence = 0.23∕0.27 = 0.85'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 131}, page_content='118\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.27\\nSeven observations for gray circles.\\nConfidence values range from no confidence (0) to high confidence (1).\\nSince a value of 0.85 is close to 1, we have a high degree of confidence in\\nthis rule.\\nLift\\nThe confidence value does not indicate the strength of the association\\nbetween gray circles (IF-part) and thick borders (THEN-part). The lift score\\ntakes this into account. The lift is often described as the importance of the\\nrule as it describes the association between the IF-part of the rule and the\\nTHEN-part of the rule. It is calculated by dividing the confidence value by\\nthe support value across all observations of the THEN-part:\\nLift = confidence∕THEN-part support\\nFor example, the lift for Rule 1:\\nRule 1\\nIF Color = gray AND\\nShape = circle\\nTHEN Border = thick\\nis calculated using the confidence and the support for the THEN-part of the\\nrule (see Figure 5.28). The confidence for Rule 1 is calculated as 0.85 and\\nthe support for the THEN-part of the rule (thick borders) is 14 out of 26 or'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 132}, page_content='ASSOCIATION RULES\\n119\\nFIGURE 5.28\\nFourteen observations for thick border objects.\\n0.54. To calculate the lift value, the confidence is divided by the support\\nvalue for the THEN-part of the rule:\\nLift = 0.85∕0.54 = 1.57\\nLift values greater than 1 indicate a positive association.\\nFigure 5.29 is used to determine the confidence and support for all three\\npotential rules:\\nThe following shows the calculations for support, confidence, and lift\\nfor the three rules:\\nRule 1\\nSupport = 6/26 = 0.23\\nConfidence = 0.23/(7/26) = 0.85\\nLift = 0.85/(14/26) = 1.58\\nRule 2\\nSupport = 6/26 = 0.23\\nConfidence = 0.23/(6/26) = 1\\nLift = 1/(9/26) = 2.89'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 133}, page_content='120\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.29\\nSeparating objects for each rule calculation.\\nRule 3\\nSupport = 6/26 = 0.23\\nConfidence = 0.23/(7/26) = 0.85\\nLift = 0.85/(11/26) = 2.01\\nThe values are summarized in Table 5.14.\\nRule 2 would be considered the most interesting because it has a confi-\\ndence score of 1 and a high positive lift score indicating that gray shapes\\nwith a thick border are likely to be circles.\\nTABLE 5.14\\nSummary of Support, Confidence, and\\nLift for the Three Rules\\nRule 1\\nRule 2\\nRule 3\\nSupport\\n0.23\\n0.23\\n0.23\\nConfidence\\n0.85\\n1.0\\n0.85\\nLift\\n1.58\\n2.89\\n2.01'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 134}, page_content='ASSOCIATION RULES\\n121\\n5.3.4 Example\\nIn this example, we will compare two rules generated from the Adult data\\nset available from Bache and Lichman (2013), a set of income data that\\nincludes the following variables along with all possible values shown in\\nparenthesis:\\nr Class of work (Private, Self-emp-not-inc, Self-emp-inc, Federal-gov,\\nLocal-gov, State-gov, Without-pay, Never-worked)\\nr Education (Bachelors, Some-college, 11th, HS-grad, Prof-school,\\nAssoc-acdm, Assoc-voc, 9th, 7th–8th, 12th, Masters, 1st–4th, 10th,\\nDoctorate, 5th–6th, Preschool)\\nr Income (>50K, ≤50K)\\nThere are 32,561 observations. Using the associative rule method, many\\nrules were identified. For example,\\nRule 1\\nIF Class of work is Private AND\\nEducation is Doctorate\\nTHEN Income is <=50K\\nRule 2\\nIF Class of work is Private AND\\nEducation is Doctorate\\nTHEN Income is >50K\\nHere is a summary of the counts:\\nClass of work is Private: 22,696 observations\\nEducation is Doctorate: 413 observations\\nClass of work is private and Education is Doctorate: 181 observations\\nIncome is <=50K: 24,720 observations\\nIncome is >50K: 7841 observations\\nTable 5.15 shows the information calculated for the rules. Of the 181\\nobservations where Class of work is Private and Education is Doctor-\\nate, 132 (73%) of those observations also had Income >50K. This is\\nreflected in the much higher confidence score for Rule 2 (0.73) compared\\nto Rule 1 (0.27). Over the entire data set of 32,561 observations there\\nare about three times the number of observations where income ≤50K as'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 135}, page_content='122\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nTABLE 5.15\\nSummary of Scores for Two Rules\\nRule 1\\nRule 2\\nCount\\n49\\n132\\nSupport\\n0.0015\\n0.0041\\nConfidence\\n0.27\\n0.73\\nLift\\n0.36\\n3.03\\ncompared to observations where the income is >50K. The lift term takes\\ninto consideration the relative frequency of the THEN-part of the rule.\\nHence, the lift value for Rule 2 is considerably higher (3.03) than the lift\\nvalue for Rule 1. Rule 2 has good confidence and lift values, making it an\\ninteresting rule. Rule 1 has poor confidence and lift values. The following\\nillustrates examples of other generated rules:\\nRule 3\\nIF Class of work is State-gov AND\\nEducation is 9th\\nTHEN Income is <=50K\\n(Count: 6; Support: 0.00018; Confidence: 1; Lift: 1.32)\\nRule 4\\nIF Class of work is Self-emp-inc AND\\nEducation is Prof-school\\nTHEN Income is >50K\\n(Count: 78; Support: 0.0024 Confidence: 0.96; Lift: 4)\\nRule 5\\nIF Class of work is Local-gov AND\\nEducation is 12th\\nTHEN Income is <=50K\\n(Count: 17; Support: 0.00052; Confidence: 0.89; Lift: 1.18)\\n5.4 LEARNING DECISION TREES FROM DATA\\n5.4.1 Overview\\nIt is often necessary to ask a series of questions before coming to a decision.\\nThe answers to one question may lead to other questions or may lead to'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 136}, page_content='LEARNING DECISION TREES FROM DATA\\n123\\nFIGURE 5.30\\nDecision tree for the diagnosis of colds and flu.\\na decision. For example, you may visit a doctor and your doctor may ask\\nyou to describe your symptoms. You respond by saying you have a stuffy\\nnose. In trying to diagnose your condition the doctor may ask you further\\nquestions such as whether you are suffering from extreme exhaustion.\\nAnswering yes may suggest you have the flu, whereas answering no might\\nsuggest that you have a cold. This line of questioning is common to many\\ndecision-making processes and can be shown visually as a decision tree,\\nas shown in Figure 5.30.\\nDecision trees are often generated by hand to precisely and consistently\\ndefine a decision-making process; however, they can also be generated\\nautomatically from the data. They consist of a series of decision points\\nbased on certain selected variables. Figure 5.31 illustrates a simple decision\\ntree. This decision tree generated was based on a data set of cars that\\nincluded variables for the number of cylinders (Cylinders) and the car’s\\nfuel efficiency (MPG). The decision tree uses the number of cylinders\\n(Cylinders) to attempt to achieve the goal of classifying the observations\\naccording to their fuel efficiency. At the top of the tree is a node representing\\nthe entire data set of 392 observations (Size = 392). The data set is initially\\ndivided into two subsets: to the left is a set of 203 cars (i.e., Size = 203)\\nwhere the number of cylinders is fewer than 5 and to the right are the\\nremaining observations (number of cylinders 5 or greater). We describe in\\na later section how this division was determined. Cars with fewer than five\\ncylinders are grouped together as they generally have good fuel efficiency.\\nIn this case the average of MPG is 29.11. The remaining 189 cars are further\\nclassified into a set of 86 cars where the number of cylinders is fewer than\\n7. This set does not include the cars with fewer than five cylinders because\\nthose cars were moved to a separate group in an earlier step. The set\\nof 86 cars are grouped together as they generally have reasonable fuel\\nefficiency—the average of MPG is 20.23—as compared with the poor fuel'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 137}, page_content='124\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.31\\nDecision tree generated from a data set of cars.\\nefficiency of the remaining group. The remaining group is a set of 103 cars\\nwhere the number of cylinders in each car is greater than 7 and the average\\nof MPG is 14.96.\\nIn contrast with clustering or association rules, decision trees are an\\nexample of a supervised method. Supervised methods, as opposed to unsu-\\npervised methods, are an attempt to place (classify) each observation into\\ninteresting groups (based on a selected variable). These methods iterate\\nover a training set of observations and adjust parameters as the classifier\\ncorrectly or incorrectly classifies each observation. In this example, the\\ndata set was classified into groups using the variable MPG to guide how\\nthe tree was constructed. Figure 5.32 illustrates how the tree, guided by the\\ndata, was put together. A histogram of the MPG data is shown alongside\\nthe nodes used to classify the vehicles. The overall shape of the histograms\\ndepicts the frequency distribution for the MPG variable. The highlighted\\nfrequency distribution is the subset within the node. The frequency distri-\\nbution for the node containing 203 observations shows a set biased toward\\ngood fuel efficiency, whereas for the node of 103 observations it illustrates\\na set biased toward poor fuel efficiency. The MPG variable has not been\\nused in any of the decision points, only the number of cylinders. This is'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 138}, page_content='LEARNING DECISION TREES FROM DATA\\n125\\nCylinders < 7\\nSize = 203\\nAvg. MPG = 29.11\\nSize = 392\\nAvg. MPG = 23.45\\nSize = 189\\nAvg. MPG = 23.45\\nSize = 86\\nAvg. MPG = 20.23\\nSize = 103\\nAvg. MPG = 14.96\\nCylinders < 5\\n100\\n75\\n50\\n25\\n0\\n5\\n10\\nFrequency\\n15 20 25\\nMPG\\n30 35 40 45 50\\n100\\n75\\n50\\n25\\n0\\n5\\n10\\nFrequency\\n15 20 25\\nMPG\\n30 35 40 45 50\\n100\\n75\\n50\\n25\\n0\\n5\\n10\\nFrequency\\n15 20 25\\nMPG\\n30 35 40 45 50\\nCylinders ≥ 7\\nCylinders ≥ 5\\nFIGURE 5.32\\nDecision tree illustrating the use of a response variable MPG to\\nguide the tree generation.\\na trivial example, but it shows how a data set can be divided into regions\\nusing decision trees.\\nThere are two primary reasons to use decision trees. First, they are\\neasy to understand and use in explaining how decisions are reached based\\non multiple criteria. Second, they can handle categorical and continuous\\nvariables since they partition a data set into distinct regions based on ranges\\nor specific values. However there are disadvantages, as building decision\\ntrees can be computationally expensive, particularly when analyzing a large\\ndata set with many continuous variables. In addition, generating a useful\\ndecision tree automatically can be challenging, since large and complex\\ntrees can be easily generated; trees that are too small may not capture\\nenough information; and generating the “best” tree through optimization\\nis difficult. At the end of this chapter, there are a series of references to\\nmethods for optimizing decision trees further.\\n5.4.2 Splitting\\nA tree is made up of a series of decision points, where the split of the\\nentire set of observations or a subset of the observations is based on some'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 139}, page_content='126\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.33\\nRelationship between parent and child nodes.\\ncriteria. Each point in the tree represents a set of observations called a node.\\nThe relationship between two connected nodes is defined as a parent–child\\nrelationship. The larger set that will be divided into two or more smaller\\nsets is the parent node. The nodes resulting from the division of the parent\\nare child nodes as shown in Figure 5.33. A child node with no children is\\na leaf node as shown in Figure 5.34.\\nA table of data is used to generate a decision tree where certain variables\\nare used as potential decision points (splitting variables) and one variable is\\nused to guide the construction of the tree (response variable). The response\\nvariable will be used to guide which splitting variables are selected and\\nat what value the split is made. A decision tree splits the data set into\\nincreasingly smaller, nonoverlapping subsets. The topmost node, or root of\\nthe tree, contains all observations. Based on some criteria, the observations\\nare usually split into two new nodes, where each node represents a subset of\\nobservations as shown in Figure 5.35. Node N1 represents all observations.\\nBy analyzing all splitting variables and examining many splitting points\\nfor each variable, an initial split is made (C1). The data set represented\\nat node N1 is now divided into a subset N2 that meets criteria C1, and a\\nsubset represented by node N3 that does not satisfy the criteria.\\nThe process of examining the variables to determine a criterion for\\nsplitting is repeated for all subsequent nodes. Additionally, a condition is\\nneeded to end the process. For example, the process can stop when the size\\nof the subset is less than a predetermined value. In Figure 5.36, each of the\\ntwo newly created subsets (N2 and N3) is examined in turn to determine\\nif they should be further split or whether the splitting should stop.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 140}, page_content='LEARNING DECISION TREES FROM DATA\\n127\\nFIGURE 5.34\\nIllustration of leaf nodes.\\nFIGURE 5.35\\nNode N1 split into two based on the criteria C1.\\nFIGURE 5.36\\nEvaluation of whether to continue to grow the tree.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 141}, page_content='128\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFIGURE 5.37\\nTree further divided.\\nIn Figure 5.37, the subset at node N2 is examined to determine if\\nthe splitting should stop. In this case, because the condition for stopping\\nsplitting is not met, the process continues. All the variables assigned as\\nsplitting variables are considered along with alternative values. The best\\ncriterion is selected and the set at node N2 is again divided into two subsets,\\nrepresented by N4 and N5. Node N4 represents a set of observations\\nthat satisfies the splitting criteria (C2) and node N5 the remaining set of\\nobservations. Next, node N3 is examined. In this case, the condition to stop\\nsplitting is met and the process is halted.\\n5.4.3 Splitting Criteria\\nDividing Observations\\nIt is common for the split at each level to be a\\ntwo-way split. Although there are methods that split more than two ways,\\ncare should be taken when using these methods because making too many\\nsplits early in the construction of the tree may result in missing interesting\\nrelationships that become exposed as tree construction continues. This\\nresults from dividing the set into small groups based on a single criterion.\\nFigure 5.38 illustrates the two alternatives.\\nAny variable type can be split using a two-way split (as shown in Figure\\n5.39):\\nr Dichotomous: Variables with two values are the most straightforward\\nto split since each branch represents a specific value. For example,\\na variable Temperature may have only two values: “hot” and “cold.”'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 142}, page_content='LEARNING DECISION TREES FROM DATA\\n129\\nFIGURE 5.38\\nAlternative splitting of nodes.\\nObservations will be split to separate those with “hot” and those with\\n“cold” temperature values.\\nr Nominal: Since nominal values are discrete values with no order, a\\ntwo-way split is accomplished by one subset being composed of a set\\nof observations that equal a certain value and the other being those\\nobservations that do not equal that value. For example, a variable Color\\nthat can take the values “red,” “green,” “blue,” and “black” may be\\nsplit two-ways. Observations, for example, which have Color equaling\\n“red” generate one subset and those not equaling “red” creating the\\nother subset, that is, “green,” “blue,” and “black.”\\nr Ordinal: In the case where a variable’s discrete values are ordered,\\nthe resulting subsets may be made up of more than one value, as\\nFIGURE 5.39\\nSplitting examples based on variable type.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 143}, page_content='130\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nlong as the ordering is retained. For example, a variable Quality with\\npossible values “low,” “medium,” “high,” and “excellent” may be split\\nfour ways. For example, observations with Quality equaling “low”\\nor “medium” may be in one subset and observations with Quality\\nequaling “high” and “excellent” in another. Another possibility is that\\n“low” values of Quality are in one set and “medium,” “high,” and\\n“excellent” values are in the other set.\\nr Continuous: For variables with continuous values to be split two-\\nways, a specific cut-off value needs to be determined so that obser-\\nvations with values less than the cut-off are in the subset on the left\\nand those with values greater than or equal to are in the subset on\\nthe right. For example, a variable Weight which can take any value\\nbetween 0 and 1,000 with a selected cut-off of 200. The left subset\\nwould be those observations where the Weight is below 200 and the\\nright subset those where the Weight is greater than or equal to 200.\\nA splitting criterion usually has two components: (1) the variable on\\nwhich to split and (2) the values of that variable to use for the split. To\\ndetermine the best split, a ranking is made of all possible splits of all\\nvariables using a score calculated for each split. There are many ways\\nto rank the split. The following describes two approaches for prioritizing\\nsplits, based on whether the response is categorical or continuous.\\nScoring Splits for Categorical Response Variables\\nTo illustrate\\nhow to score splits when the response is a categorical variable, three splits\\n(Split a, Split b, Split c) for a set of observations are shown in Figure\\n5.40. The objective for an optimal split is to create subsets which result\\nin observations with a single response value. In this example, there are 20\\nobservations prior to splitting. The response variable (Temperature) has\\nFIGURE 5.40\\nEvaluating splits based on categorical response data.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 144}, page_content='LEARNING DECISION TREES FROM DATA\\n131\\ntwo possible values: “hot” and “cold.” Prior to the split, the response has\\nan even distribution: the number of observations where the Temperature\\nequals “hot” is 10 and the number of observations where the Temperature\\nequals “cold” is also 10.\\nDifferent criteria are considered for splitting these observations, which\\nresults in different distributions of the response variables for each subset\\n(N2 and N3):\\nr Split a: Each subset contains 10 observations. All 10 observations in\\nN2 have “hot” temperature values and all 10 observations in node N3\\nare “cold.”\\nr Split b: Again each subset (N2 and N3) contain 10 observations.\\nHowever, in this case there is an even distribution of “hot” and “cold”\\nvalues in each subset.\\nr Split c: In this case the splitting criterion results in two subsets where\\nnode N2 has nine observations (1 “hot” and 8 “cold”) and node N3\\nhas 11 observations (9 “hot” and 2 “cold”).\\nSplit a is the best split since each node contains observations where\\nthe response for each node is all of the same category. Split b results in\\nthe same even split of “hot” and “cold” values (50% “hot,” 50% “cold”)\\nin each of the resulting nodes (N2 and N3) and would not be considered\\na good split. Split c is a good split even though the split is not as clean\\nas Split a, since both subsets have a mixture of “hot” and “cold” values.\\nThe proportion of “hot” and “cold” values in node N2 is biased toward\\ncold values and in node N3 toward hot values. The “goodness” of the\\nsplitting criteria is determined by how clean each split is: it is based on\\nthe proportion of the different categories of the response variable, which\\nis a measurement known as impurity. As the tree is being generated, it is\\ndesirable to decrease the level of impurity until ideally there is only one\\ncategory at a terminal node (a node with no children).\\nThere are three primary methods for calculating impurity: misclassifica-\\ntion, Gini, and entropy. In the following examples the entropy calculation\\nwill be used; however, the other methods give similar results. To illustrate\\nthe use of the entropy calculation, a set of 10 observations with two possi-\\nble response values (“hot” and “cold”) are used (Table 5.16). All possible\\nscenarios for splitting this set of 10 observations are shown: Scenario 1\\nthrough 11. In scenario 1, all 10 observations have value “cold” whereas\\nin scenario 2, one observation has value “hot” and nine observations have\\nvalue “cold.” For each scenario, an entropy score is calculated. Cleaner\\nsplits result in lower scores. In scenario 1 and scenario 11 the split cleanly'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 145}, page_content='132\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nTABLE 5.16\\nEntropy Scores According to Different\\nSplitting Criteria\\nResponse Values\\nScenario\\nHot\\nCold\\nEntropy\\nScenario 1\\n0\\n10\\n0\\nScenario 2\\n1\\n9\\n0.469\\nScenario 3\\n2\\n8\\n0.722\\nScenario 4\\n3\\n7\\n0.881\\nScenario 5\\n4\\n6\\n0.971\\nScenario 6\\n5\\n5\\n1\\nScenario 7\\n6\\n4\\n0.971\\nScenario 8\\n7\\n3\\n0.881\\nScenario 9\\n8\\n2\\n0.722\\nScenario 10\\n9\\n1\\n0.469\\nScenario 11\\n10\\n0\\n0\\nbreaks the set into observations with only one value. The score for these\\nscenarios is 0. In scenario 6, the observations are split evenly across the\\ntwo values and this is reflected in a score of 1. In other cases, the score\\nreflects how cleanly the two values are split.\\nThe formula for entropy is\\nEntropy(S) = −\\nc∑\\ni=1\\npi log2 pi\\nThe entropy calculation is performed on a set of observations S. pi refers\\nto the fraction of the observations that belong to a particular value and c\\nis the number of different possible values of the response variable. For\\nexample, for a set of 100 observations where the Temperature response\\nvariable had 60 observations with “hot” values and 40 with “cold” values,\\nthe phot would be 0.6 and the pcold would be 0.4. When pi = 0, then the\\nvalue for 0 log2 (0) = 0.\\nWe illustrate this with the example shown in Figure 5.40. Values for\\nentropy are calculated for each of the three splits:\\nSplit a\\nEntropy (N1) = −(10/20) log2 (10/20) −(10/20) log2 (10/20) = 1\\nEntropy (N2) = −(10/10) log2 (10/10) −(0/10) log2 (0/10) = 0\\nEntropy (N3) = −(0/10) log2 (0/10) −(10/10) log2 (10/10) = 0'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 146}, page_content='LEARNING DECISION TREES FROM DATA\\n133\\nSplit b\\nEntropy (N1) = −(10/20) log2 (10/20) −(10/20) log2 (10/20) = 1\\nEntropy (N2) = −(5/10) log2 (5/10) −(5/10) log2 (5/10) = 1\\nEntropy (N3) = −(5/10) log2 (5/10) −(5/10) log2 (5/10) = 1\\nSplit c\\nEntropy (N1) = −(10/20) log2 (10/20) −(10/20) log2 (10/20) = 1\\nEntropy (N2) = −(1/9) log2 (1/9) −(8/9) log2 (8/9) = 0.503\\nEntropy (N3) = −(9/11) log2 (9/11) −(2/11) log2 (2/11) = 0.684\\nIn order to determine the best split, we now need to calculate a ranking\\nbased on how cleanly each split separates the response data. This is calcu-\\nlated based on the impurity before and after the split. The formula for this\\ncalculation is\\nGain = Entropy(parent) −\\nk∑\\nj=1\\nN(vj)\\nN\\nEntropy(vj)\\nwhere N is the number of observations in the parent node, k is the number\\nof possible resulting nodes, N(vj) is the number of observations for each of\\nthe j child nodes, and vj is the set of observations for the jth node. It should\\nbe noted that the Gain formula can be used with other impurity metrics by\\nreplacing the entropy calculation.\\nIn the example described throughout this section, the gain values are\\ncalculated and shown in Figure 5.41.\\nFIGURE 5.41\\nCalculation of gain for each split.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 147}, page_content='134\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nGain(Splita) = 1 −(((10∕20) 0) + ((10∕20) 0)) = 1\\nGain(Splitb) = 1 −(((10∕20) 1) + ((10∕20) 1)) = 0\\nGain(Split c) = 1 −(((9∕20) 0.503) + ((11∕20) 0.684)) = 0.397\\nThe criterion used in Split a is selected as the best splitting criteria.\\nDuring the tree generation process, the method examines all possible\\nsplitting values for all splitting variables, calculates a gain function, and\\nselects the best splitting criterion.\\nScoring Splits for Continuous Response Variables\\nWhen the\\nresponse variable is continuous, one popular method for ranking the splits\\nuses the sum of the squares of error (SSE). The resulting split should ide-\\nally result in sets where the response values are close to the mean of the\\ngroup. The lower a group’s SSE value is, the closer that group’s values are\\nto the mean of the set. For each potential split, a SSE value is calculated\\nfor each resulting node. A score for the split is calculated by summing\\nthe SSE values of each resulting node. Once all splits for all variables are\\ncomputed, then the split with the lowest score is selected.\\nThe formula for SSE is\\nSSE =\\nn\\n∑\\ni=1\\n(yi −̄y)2\\nFor a subset of n observations, the SSE value is computed where yi is the\\nindividual value for the response and ̄y is the average value for the subset.\\nTo illustrate, the data in Table 5.17 is processed to identify the best split.\\nThe variable Weight is assigned as a splitting variable and MPG will be\\nTABLE 5.17\\nTable of Eight Observations\\nwith Values for Two Variables\\nObservations\\nWeight\\nMPG\\nA\\n1,835\\n26\\nB\\n1,773\\n31\\nC\\n1,613\\n35\\nD\\n1,834\\n27\\nE\\n4,615\\n10\\nF\\n4,732\\n9\\nG\\n4,955\\n12\\nH\\n4,741\\n13'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 148}, page_content='LEARNING DECISION TREES FROM DATA\\n135\\nFIGURE 5.42\\nIllustration of splitting values.\\nused as the response variable. A series of values is used to split the variable\\nWeight: 1,693, 1,805, 1,835, 3,225, 4,674, 4,737, and 4,955. These values\\nare the midpoint between each pair of values (after sorting) and were\\nselected because they divided the data set into all possible two-ways splits,\\nas illustrated in Figure 5.42. In this example, we will only calculate a score\\nfor splits which result in three or more observations, that is, Split 3, Split\\n4, and Split 5. The MPG response variable is used to calculate the score.\\nSplit 3\\nFor the subset where Weight is less than 1835 (C, B, D):\\nAverage = (35 + 31 + 27)/3 = 31\\nSSE = (35 −31)2 + (31 −31)2 + (27 −31)2 = 32'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 149}, page_content='136\\nIDENTIFYING AND UNDERSTANDING GROUPS\\nFor the subset where Weight is greater than or equal to 1835 (A, E, F,\\nH, G):\\nAverage = (26 + 10 + 9 + 13 + 12)/5 = 14\\nSSE = (26 −14)2 + (10 −14)2 + (9 −14)2 + (13 −14)2\\n+ (12 −14)2 = 190\\nSplit score = 32 + 190 = 222\\nSplit 4\\nFor the subset where Weight is less than 3225 (C, B, D, A):\\nAverage = (35 + 31 + 27 + 26)/4 = 29.75\\nSSE = (35 – 29.75)2 + (31 – 29.75)2 + (27 – 29.75)2\\n+ (26 – 29.75)2 = 50.75\\nFor the subset where Weight is greater than or equal to 3225 (E, F, H,\\nG):\\nAverage = (10 + 9 + 13 + 12)/4 = 11\\nSSE = (10 −11)2 + (9 −11)2 + (13 −11)2 + (12 −11)2 = 10\\nSplit score = 50.75 + 10 = 60.75\\nSplit 5\\nFor the subset where Weight is less than 4674 (C, B, D, A, E):\\nAverage = (35 + 31 + 27 + 26 + 10)/5 = 25.8\\nSSE = (35 – 25.8)2 + (31 – 25.8)2 + (27 – 25.8)2 + (26 – 25.8)2\\n+ (10 – 25.8)2 = 362.8\\nFor the subset where Weight is greater than or equal to 4674 (F, H, G):\\nAverage = (9 + 13 + 12)/3 = 11.33\\nSSE = (9 – 11.33)2 + (13 – 11.33)2 + (12 – 11.33)2 = 8.67\\nSplit score = 362.8 + 8.67 = 371.47\\nIn this example, Split 4 has the lowest score and would be selected as\\nthe best split.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 150}, page_content='EXERCISES\\n137\\n5.4.4 Example\\nIn the following example, a set of 392 cars is analyzed using a decision tree.\\nTwo variables were used to split nodes in the tree: Horsepower, Weight.\\nMPG (miles per gallon) was used to guide the generation of the tree. A\\ndecision tree (Figure 5.43) was automatically generated using a 40 node\\nminimum as the terminating criterion.\\nThe leaf nodes of the tree can be interpreted using a series of rules.\\nThe decision points that are traversed in getting to the node are the rule\\nconditions. The average MPG value for the leaf nodes will be interpreted\\nhere as low (less than 22), medium (22–26), and high (greater than 26).\\nThe following two example rules can be extracted from the tree:\\nNode A\\nIF Horsepower <106 AND\\nWeight < 2067.5\\nTHEN MPG is high\\nNode B\\nIF Horsepower <106 AND\\nWeight 2067.5 – 2221.5\\nTHEN MPG is high\\nIn addition to grouping data sets, decision trees can also be used in\\nmaking predictions and this will be reviewed in Chapter 6.\\nEXERCISES\\nPatient data was collected concerning the diagnosis of cold or flu (Table\\n5.18).\\n1. Calculate the Jaccard distance (replacing None with 0, Mild with\\n1, and Severe with 2) using the variables: Fever, Headaches, Gen-\\neral aches, Weakness, Exhaustion, Stuffy nose, Sneezing, Sore throat,\\nChest discomfort, for the following pairs of patient observations:\\n(a) 1326 and 398\\n(b) 1326 and 1234\\n(c) 6377 and 2662'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 151}, page_content='FIGURE 5.43\\nDecision tree generated using Horsepower and Weight as splitting values and guided by MPG.\\n138'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 152}, page_content='TABLE 5.18\\nTable of Patient Records\\nGeneral\\nStuffy\\nSore\\nChest\\nPatient ID\\nFever\\nHeadaches\\nAches\\nWeakness\\nExhaustion\\nNose\\nSneezing\\nThroat\\nDiscomfort\\nDiagnosis\\n1326\\nNone\\nMild\\nNone\\nNone\\nNone\\nMild\\nSevere\\nSevere\\nMild\\nCold\\n398\\nSevere\\nSevere\\nSevere\\nSevere\\nSevere\\nNone\\nNone\\nSevere\\nSevere\\nFlu\\n6377\\nSevere\\nSevere\\nMild\\nSevere\\nSevere\\nSevere\\nNone\\nSevere\\nSevere\\nFlu\\n1234\\nNone\\nNone\\nNone\\nMild\\nNone\\nSevere\\nNone\\nMild\\nMild\\nCold\\n2662\\nSevere\\nSevere\\nMild\\nSevere\\nSevere\\nSevere\\nNone\\nSevere\\nSevere\\nFlu\\n9477\\nNone\\nNone\\nNone\\nMild\\nNone\\nSevere\\nSevere\\nSevere\\nNone\\nCold\\n7286\\nSevere\\nSevere\\nSevere\\nSevere\\nSevere\\nNone\\nNone\\nNone\\nSevere\\nFlu\\n1732\\nNone\\nNone\\nNone\\nNone\\nNone\\nSevere\\nSevere\\nNone\\nMild\\nCold\\n1082\\nNone\\nMild\\nMild\\nNone\\nNone\\nSevere\\nSevere\\nSevere\\nSevere\\nCold\\n1429\\nSevere\\nSevere\\nSevere\\nMild\\nMild\\nNone\\nSevere\\nNone\\nSevere\\nFlu\\n14455\\nNone\\nNone\\nNone\\nMild\\nNone\\nSevere\\nMild\\nSevere\\nNone\\nCold\\n524\\nSevere\\nMild\\nSevere\\nMild\\nSevere\\nNone\\nSevere\\nNone\\nMild\\nFlu\\n1542\\nNone\\nNone\\nMild\\nMild\\nNone\\nSevere\\nSevere\\nSevere\\nNone\\nCold\\n8775\\nSevere\\nSevere\\nSevere\\nSevere\\nMild\\nNone\\nSevere\\nSevere\\nSevere\\nFlu\\n1615\\nMild\\nNone\\nNone\\nMild\\nNone\\nSevere\\nNone\\nSevere\\nMild\\nCold\\n1132\\nNone\\nNone\\nNone\\nNone\\nNone\\nSevere\\nSevere\\nSevere\\nSevere\\nCold\\n4522\\nSevere\\nMild\\nSevere\\nMild\\nMild\\nNone\\nNone\\nNone\\nSevere\\nFlu\\n139'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 153}, page_content='140\\nIDENTIFYING AND UNDERSTANDING GROUPS\\n2. The patient observations described in Table 5.18 are being clus-\\ntered using agglomerative hierarchical clustering. The Euclidean dis-\\ntance is used to calculate the distance between observations using\\nthe following variables: Fever, Headaches, General aches, Weak-\\nness, Exhaustion, Stuffy nose, Sneezing, Sore throat, Chest discom-\\nfort (replacing None with 0, Mild with 1, and Severe with 2). The\\naverage linkage joining rule is being used to create the hierarchical\\nclusters. During the clustering process observations 6377 and 2662\\nare already grouped together. Calculate the distance from observation\\n398 to this group.\\n3. A candidate rule has been extracted using the associative rule method:\\nIf Exhaustion = None AND\\nStuffy nose = Severe\\nTHEN Diagnosis = cold\\nCalculate the support, confidence, and lift for this rule.\\n4. Table 5.18 is to be used to build a decision tree to classify whether a\\npatient has a cold or flu. As part of this process the Fever column is\\nbeing considered as a splitting point. Two potential splitting values\\nare being considered:\\n(a) Where the data is divided into two sets when (1) Fever is none\\nand (2) Fever is mild and severe.\\n(b) Where the data is divided into two sets when (1) Fever is severe\\nand (2) Fever is none and mild.\\nCalculate the gain for each of these splits using the entropy impurity\\ncalculation.\\nFURTHER READING\\nFor additional information on general data mining approaches to grouping and\\noutlier detection, see Han et al. (2012) and Hand et al. (2001). Everitt et al. (2011)\\nand Myatt & Johnson (2009) provide further details about similarity methods\\nand approaches to clustering, with Fielding (2007) focusing on clustering and\\nclassification methods and their application to bioinformatics and the biological\\nsciences. In addition, Hastie et al. (2009) covers in detail additional grouping\\napproaches.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 154}, page_content='CHAPTER 6\\nBUILDING MODELS FROM DATA\\n6.1 OVERVIEW\\nIn Chapter 4, we looked at different ways to understand and quantify rela-\\ntionships between variables. Is there a relationship between age and choles-\\nterol levels? Do patients in a clinical trial taking a drug have improved out-\\ncomes versus patients taking a placebo? Formal ways to describe, encode,\\nand test if and how one or more variables relate to others is to build and eval-\\nuate models from the data. These models describe important relationships\\nin the data, including the strength and direction—positive or negative—of\\nthe relation. The models can encode linear and nonlinear relationships in\\nthe data. They can also be used to confirm a hypothesis about relationships.\\nAll these uses help to summarize and understand the data. However, one\\nof the most widely used applications of a model is for making predic-\\ntions. For example, a data set of historical purchases along with customer\\ngeographical and demographic data (such as the customer’s age, location,\\nsalary, and so on) could be collected and used to generate a model that\\nencodes what type of products clients purchase. Once the model is built, it\\ncould be used to identify from a list of potential clients those most likely\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n141'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 155}, page_content='142\\nBUILDING MODELS FROM DATA\\ny  =  f(xi)\\nFIGURE 6.1\\nIllustration of response versus independent variables.\\nto make a purchase, and customers on this prioritized list could be targeted\\nwith marketing material or other promotions.\\nIn this chapter, we will review how models can be built from data sets.\\nA model is usually built to predict values for a specific variable. For exam-\\nple, were a data set composed of historical data containing attributes of\\npharmaceuticals and their observed side effects to be collected, a model\\ncan be generated from this data to predict the side effects from the phar-\\nmaceuticals’ attributes.\\nA variable that a model is to predict is often referred to as a y-variable\\nor response variable. The variables that will be encoded in the model and\\nused in predicting this response are referred to as the x-variables or the\\nindependent variables. In Figure 6.1, a data table composed of cars is\\nused to generate a model. Because we want the model to predict the car’s\\nfuel efficiency, we have chosen the response variable to be miles per gallon\\n(MPG). Other variables will be used as independent variables (x-variables).\\nIn this case, these will be Cylinders (x1), Displacement (x2), Horsepower\\n(x3), Weight (x4), and Acceleration (x5). A generalized format for the model\\nis shown where some function of the independent variables (xi) is used to\\npredict the response (y), which in this case is MPG.\\nModels built to predict categorical variables (such as a binary variable or\\na nominal variable) are referred to as classification models, whereas models'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 156}, page_content='OVERVIEW\\n143\\nthat predict continuous variables are called regression models. There are\\nmany ways to generate classification and regression models. For example,\\na classification tree is a method for building a classification model while\\na multiple linear regression is a method for building a regression model.\\nSpecific approaches may have restrictions relating to the types of variables\\nthat can be used in the model as, for example, a model that requires contin-\\nuous variables to have a normal frequency distribution. For certain types of\\nmodels it is possible to fine-tune the performance of the model by varying\\ndifferent parameters. In building a model, it will be important to understand\\nthe restrictions placed on the types of independent or response variables or\\nboth, as well as how to optimize the performance of the model by varying\\nthe values of the parameters. Another way in which approaches to mod-\\neling differ is in the ease of access to the internal calculations, otherwise\\nknown as the transparency of the model, in order to explain the results: is\\nit possible to understand how the model calculated a prediction or is the\\nmodel a “black box” that only calculates a prediction result with no corre-\\nsponding explanation? Issues related to transparency may be important in\\nexplaining the results when the model is deployed in certain situations.\\nAlthough the response variable is known, when building models it is not\\nalways apparent beforehand which variables should be used as independent\\nvariables. Therefore, the selection of the independent variables is an impor-\\ntant step in building a model. A good model will make reliable predictions,\\nbe plausible, and use as few independent variables as possible. In Chapter\\n4, we reviewed different visualizations and metrics to use in understanding\\nrelationships in the data, such as scatterplots, contingency tables, t-tests,\\nChi-Square tests, and so on. These approaches can be used to prioritize can-\\ndidate independent variables to use in building a model, especially where\\nthere are many variables to consider. However, care should be taken when\\nusing statistical tests to prioritize large numbers of potential independent\\nvariables as a correction may need to be used (see the Further Reading\\nsection of this chapter for more information). For example, a matrix of\\nscatterplots could be used to visually identify which variables have the\\nstrongest relationship to the response variable. In addition, knowledge of\\nthe problem can also guide the choice of variables to use in the models.\\nAlternatively, we could build multiple models with different combinations\\nof independent variables and select the best fitting model.\\nAnother issue to consider when selecting independent variables is the\\nrelationship between the independent variables. Combinations of variables\\nthat have strong relationships to each other should be avoided since they\\nwill be essentially encoding the same relationship to the response. Includ-\\ning all the variables from each group of strongly related variables produces'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 157}, page_content='144\\nBUILDING MODELS FROM DATA\\noverly complex models (violating the “as simple as possible rule”) and\\nwith some approaches to modeling can produce results that are difficult to\\ninterpret.\\nIn developing a model, it may also be necessary to use derived vari-\\nables, that is, a new variable that is a function of one or more variables. For\\nexample, if the model expects the variables of a data set to have a normal\\nfrequency distribution and some variables have an exponential frequency\\ndistribution, it may be necessary to create new variables using a log trans-\\nformation. As another example, because most modeling methods require\\nnumeric data, if a data set has nominal variables that will be used in the\\nmodel, the values of these variables must be transformed into numbers.\\nFor example, if color is an important variable with values “Blue,” “Green,”\\n“Red,” and “Yellow,” color could be transformed into a series of binary\\ndummy variables as described in Chapter 3.\\nIn this chapter, we discuss how to generate models from data sets. The\\ndata set used to build a model is referred to as the training set. To objectively\\ntest the performance of a generated model, a test set with observations\\ndifferent from those in the training set is used to test how well the model\\nperforms. The model uses the values of each observation in the test set to\\npredict a value for the response variable. From these predictions, a variety\\nof metrics, such as the number of correct versus incorrect predictions made,\\nare used to assess the accuracy of the model. The use of training and test\\nsets is illustrated in Figure 6.2.\\nA good way to build and test a model would be to use all the observations\\nin the original data set as the training set to build the model and to use new,\\nindependent observations as the test set to measure accuracy. However,\\nbecause the number of available test sets is often small, a common way to\\ntest the performance of a model is to use a category of methods called cross-\\nvalidation. In the k-fold partitioning method, the original data set is divided\\nFIGURE 6.2\\nUse of training sets to build models and test sets to assess their\\nperformance.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 158}, page_content='OVERVIEW\\n145\\ninto k equally sized partitions. The model is measured k times. In the first\\niteration, one of the partitions is selected as the test set and the remaining\\npartitions comprise the training set. The model is tested and an accuracy\\nscore is generated. In each subsequent iteration, a partition different from\\nany already used as a test set is selected as the test set and the remaining\\npartitions become the training set. Another score is calculated. At the end\\nof this process, the accuracy of the model is based on the average of the\\nk scores. For example, suppose we partition a data set into 10 partitions\\nwhere each partition consists of observations randomly selected from the\\ndata set. In each of the 10 iterations, we designate one partition (10% of\\nthe data set) as the test set and the other 9 partitions (90% of the data set)\\nas the training set. At the end of the 10 iterations, an average of the 10\\nscores is used to assess the model’s accuracy. Taking k-fold partitioning to\\nan extreme would result in the case where k is the number of observations\\nin the data set and each partition contains a single observation. This is a\\ncross-validation method known as leave-one-out.\\nIn cross-validation, each partition will have been used as a test set or, in\\nother words, every observation in the data set will have been tested once.\\nThis ensures that a prediction will be calculated for every observation in\\nthe data set and avoids introducing bias into a model. Bias is a measure\\nof the model’s accuracy and indicates how close the predictions of the\\nresponse value made by the model are to the actual response value of new\\nobservations. It can be introduced when models become overly complex\\nby optimizing the model for just the training set used to build the model.\\nWhen the performance is tested for these overtrained models against either\\na separate test set or through cross-validation, the performance will be\\npoorer. In cross-validation methods, bias can be introduced when training\\nsets overlap (some observations are used more than once) or the combined\\ntraining sets do not cover the data set (some observations are never used).\\nFor classification models, one way to assess the performance of a model\\nis to look at the results of applying the models (such as the results from a test\\nset or the cross-validation results) and determine how many observations\\nare correctly or incorrectly classified. The accuracy or concordance of the\\nmodel is based on the proportion or percentage of correctly predicted\\nobservations in comparison to the whole set. For example, if the test\\nset contained 100 observations and the model predicted 78 correctly (22\\nincorrectly), then the concordance would be 78/100 or 78%.\\nA common type of classification model is a model to predict a binary\\nresponse, where a true response is coded as 1 and a false response is coded\\nas 0. For example, a model could be built to predict, based on geologi-\\ncal data, whether there is evidence of an oil deposit, with a true response'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 159}, page_content='146\\nBUILDING MODELS FROM DATA\\nTABLE 6.1\\nContingency Table Summarizing the Correct and Incorrect\\nPredictions from a Binary Classification Model\\nActual\\nTrue (1)\\nFalse (0)\\nPrediction\\nTrue (1)\\nTrue positives\\n(TP)\\nFalse positive\\n(FP)\\nNumber of\\nobservations\\npredicted true (1)\\nFalse (0)\\nFalse negatives\\n(FN)\\nTrue negatives\\n(TN)\\nNumber of\\nobservations\\npredicted false (0)\\nNumber of\\nactual true (1)\\nvalues\\nNumber of\\nactual false\\n(0) values\\nTotal\\nobservations\\nencoded as a value of 1 when there is evidence for an oil deposit and a\\nfalse response as a value of 0 if there is not. A good way to evaluate a\\nclassification model’s performance is through a contingency table sum-\\nmarizing the number of correct and incorrect classifications. The number\\nof correctly predicted positive observations (true positives or TP) and the\\nnumber of correctly predicted negative observations (true negatives or TN)\\nis shown in Table 6.1. In addition, the number of positive predictions that\\nare incorrect is referred to as false positives (or FP) and the number of\\nnegative predictions that are incorrect is referred to as false negatives (or\\nFN) are also summarized in Table 6.1.\\nTo illustrate the difference in how well models are able to predict\\npositive and negative values, the results from three models are presented\\nin Figure 6.3. Model 1 correctly predicts 75% (36 out of 48 positives and\\n39 out of 52 negatives). In Model 1, the number of false positives (12) and\\nthe number of false negatives (13) are quite similar. Model 2 has an overall\\nconcordance of 80% with few false negatives (only 3) but a larger number\\nof false positives (17). In Model 3, the balance of the false positives and\\nfalse negatives is more biased toward false negatives (22) than to false\\npositives (9). However, the overall concordance values do not reflect\\nbiases in the model’s ability to predict true or false values. To better assess\\nthe overall performance of a binary classification model, it is necessary\\nto calculate additional metrics. Two commonly used calculations are\\nsensitivity and specificity:\\nSensitivity = TP∕(TP + FN)\\nSpecificity = TN∕(TN + FP)'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 160}, page_content='FIGURE 6.3\\nContingency tables and performance metrics for three models.\\n147'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 161}, page_content='148\\nBUILDING MODELS FROM DATA\\n2620\\n16\\n14\\n12\\n10\\n8\\n6\\n4\\n2\\n0\\n0\\n2\\n4\\nActual versus predicted for a well-fitting model\\nActual versus predicted for a poorly fitting model\\n6\\nCrew\\n8\\n10\\n12\\n14\\n2600\\n2580\\n2560\\n2540\\n1000 1500 2000 2500\\nCurb-weight\\nPrediction (Curb-weight)\\nPrediction (Crew)\\n3000 3500 4000 4500\\n2520\\n2500\\nFIGURE 6.4\\nScatterplot for a well- and poorly fitting regression model.\\nSensitivity generally describes how well a model predicts positives,\\nwhereas specificity generally describes how well a model predicts nega-\\ntives. The values for sensitivity and specificity in Model 1 are similar since\\nthe number of false positives and false negatives are similar. In Model 2,\\nthe sensitivity value is high (91%) which reflects the low number of false\\nnegatives, whereas the specificity in Model 3 is high reflecting the lower\\nnumber of false positives.\\nIn assessing the performance of a regression model, a scatterplot with\\naxes showing the actual values and the predicted values is a useful way to\\nstart to understand the performance of the model. Models that accurately\\npredict a response variable have points close to and evenly distributed\\nabout a straight line, as shown in the left scatterplot in Figure 6.4, whereas\\npoor performing models have points scattered as illustrated in the right\\nscatterplot in Figure 6.4. Scatterplots can also help to understand if there\\nare observations that will be poorly predicted for a given model. These\\nobservations appear as points that do not fall close to the best fit line. If the\\nscatterplot trend has a nonlinear shape, then the model is not capturing the\\nnonlinear relationships and one or more of the variables included in the\\nmodel may require a data transformation or a nonlinear modeling approach\\nselected. The error or residual is the difference between the predicted value\\nand the actual value. An overall score based on these residual values can\\nbe helpful in assessing the relative performance of different models. Since\\na residual can contain positive and negative values, it is usual to calculate\\nan overall assessment of the residuals, such as the sum of the absolute\\nresidual or the square of the residual.\\nIn using a model in practice, it is not advisable to apply a model to data\\nsets that are not similar to those used in building the model (extrapolation).\\nIt is usual to place some restriction on the variables of the data sets that will\\nbe input to the model, such as the requirement that values not be outside\\nthe range of the training set variables.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 162}, page_content='LINEAR REGRESSION\\n149\\nThe following sections outline a number of common and diverse\\napproaches to building models: linear regression, logistic regression, k-\\nnearest neighbors, and classification and regression trees (CART). The\\nchapter ends with a review of other approaches to building models and\\nadditional information on resources for these topics.\\n6.2 LINEAR REGRESSION\\n6.2.1 Overview\\nThe following section discusses how to generate linear models to describe\\na relationship between one or more independent variables and a single\\nresponse variable. For example, we could build a linear regression model\\nto predict cholesterol levels using data about a patient’s age. This model\\nwill likely be a poor predictor of cholesterol levels; however, incorporating\\nmore information, such as body mass index (BMI) may result in a model\\nthat provides a better prediction of cholesterol levels. Using a single inde-\\npendent variable is referred to as simple linear regression, whereas using\\nmore than one independent variable is referred to as multiple linear regres-\\nsion. Although these models do not make causal inferences, they are useful\\nfor understanding how a set of independent variables is associated with a\\nresponse variable. The following sections describe how to generate and\\nassess linear regression models and test the assumptions about the model.\\n6.2.2 Fitting a Simple Linear Regression Model\\nA simple linear regression model can be generated where there is a linear\\nrelationship between two variables. For example, Figure 6.5 shows the\\nrelationship between the independent variable Age and the response vari-\\nable Blood fat content. The diagram shows a high degree of correlation\\nbetween the two variables. As variable Age increases, response variable\\nBlood fat content increases proportionally. A straight line, representing a\\nlinear model, can be drawn through the center of the points.\\nThis straight line can be described using the formula\\ny = b0 + b1x\\nwhere b0 is the point of intersection with the y-axis and b1 is the slope\\nof the line, which is shown graphically in Figure 6.6. The simple linear\\nregression model is usually shown with an error term; however, it is not\\nincluded here to simplify the example.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 163}, page_content='150\\nBUILDING MODELS FROM DATA\\nFIGURE 6.5\\nA straight line drawn through the relationship between variables\\nAge and Blood fat content.\\nThe point at which the line intercepts with the y-axis is noted (approx-\\nimately 100) and the slope of the line is calculated (approximately 5.3).\\nFor this data set, an approximate formula for the relationship between Age\\nand Blood fat content is\\nBlood fat content = 100 + 5.3 × Age\\nParameters b0 and b1 can be derived manually by drawing a line\\nthrough the points in the scatterplot and then visually inspecting where\\nthe line crosses the y-axis (b0) and measuring the slope (b1), as previously\\ndescribed. The least-squares method is able to calculate these parameters\\nautomatically. The formula for calculating a slope (b1) is\\nb1 =\\nn\\n∑\\ni=1\\n(xi −̄x)(yi −̄y)\\nn\\n∑\\ni=1\\n(xi −̄x)2'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 164}, page_content='LINEAR REGRESSION\\n151\\nFIGURE 6.6\\nDeriving the straight line formula from the graph.\\nwhere xi and yi are the individual values for the independent variable (x)\\nand the response variable (y), and where ̄x is the mean of x and ̄y is the\\nmean of y.\\nThe formula for calculating the intercept with the y-axis is\\nb0 = ̄y −b1̄x\\nThe slope and intercept are calculated using the data from Table 6.2.\\nThe mean of x is 39.12 and the mean of y is 310.72.\\nSlope (b1) = 19,157.84∕3,600.64\\nSlope (b1) = 5.32\\nIntercept (b0) = 310.72 −(5.32 × 39.12)\\nIntercept (b0) = 102.6\\nHence the equation is\\nBlood fat content = 102.6 + 5.32 × Age'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 165}, page_content='152\\nBUILDING MODELS FROM DATA\\nTABLE 6.2\\nCalculation of Linear Regression with Least Square Method\\nX\\nY\\n(xi −̄x)\\n(yi −̄y)\\n(xi −̄x)(yi −̄y)\\n(xi −̄x)2\\n46\\n354\\n6.88\\n43.28\\n297.7664\\n47.3344\\n20\\n190\\n−19.12\\n−120.72\\n2,308.1664\\n365.5744\\n52\\n405\\n12.88\\n94.28\\n1,214.3264\\n165.8944\\n30\\n263\\n−9.12\\n−47.72\\n435.2064\\n83.1744\\n57\\n451\\n17.88\\n140.28\\n2,508.2064\\n319.6944\\n25\\n302\\n−14.12\\n−8.72\\n123.1264\\n199.3744\\n28\\n288\\n−11.12\\n−22.72\\n252.6464\\n123.6544\\n36\\n385\\n−3.12\\n74.28\\n−231.7536\\n9.7344\\n57\\n402\\n17.88\\n91.28\\n1,632.0864\\n319.6944\\n44\\n365\\n4.88\\n54.28\\n264.8864\\n23.8144\\n24\\n209\\n−15.12\\n−101.72\\n1,538.0064\\n228.6144\\n31\\n290\\n−8.12\\n−20.72\\n168.2464\\n65.9344\\n52\\n346\\n12.88\\n35.28\\n454.4064\\n165.8944\\n23\\n254\\n−16.12\\n−56.72\\n914.3264\\n259.8544\\n60\\n395\\n20.88\\n84.28\\n1,759.7664\\n435.9744\\n48\\n434\\n8.88\\n123.28\\n1,094.7264\\n78.8544\\n34\\n220\\n−5.12\\n−90.72\\n464.4864\\n26.2144\\n51\\n374\\n11.88\\n63.28\\n751.7664\\n141.1344\\n50\\n308\\n10.88\\n−2.72\\n−29.5936\\n118.3744\\n34\\n220\\n−5.12\\n−90.72\\n464.4864\\n26.2144\\n46\\n311\\n6.88\\n0.28\\n1.9264\\n47.3344\\n23\\n181\\n−16.12\\n−129.72\\n2,091.0864\\n259.8544\\n37\\n274\\n−2.12\\n−36.72\\n77.8464\\n4.4944\\n40\\n303\\n0.88\\n−7.72\\n−6.7936\\n0.7744\\n30\\n244\\n−9.12\\n−66.72\\n608.4864\\n83.1744\\nSum\\n19,157.84\\n3,600.64\\nThese coefficient values are close to the values calculated using the\\nmanual approach.\\nFor each value of the x-variable, the corresponding y-variable value\\n(taken from the straight line) represents the expected mean y value. The\\nactual values will fall above and below the straight line since the line\\nrepresents the mean.\\nOnce a formula for the straight line has been established, predicting\\nvalues for the y response variable based on the x independent variable can\\nbe easily calculated. However, the formula should only be used for values\\nof the x variable within the range in which the formula was derived. In\\nthis example, Age values should only be between 20 and 60. A prediction'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 166}, page_content='LINEAR REGRESSION\\n153\\nfor Blood fat content based on Age can be calculated, for example, for an\\nindividual whose age is 33, the Blood fat content would be predicted as\\nBlood fat content = 102.6 + 5.32 × 33 = 278.16\\nThe slope can be interpreted as the average amount that the Blood fat\\ncontent changes for each unit (year) change in Age. The intercept represents\\nthe average value of the Blood fat content when Age is zero; however, in\\nthis case an Age of zero is out of the range of meaningful interpretation.\\n6.2.3 Fitting a Multiple Linear Regression Model\\nIn most practical situations, a simple linear regression is not sufficient\\nbecause the models will need more than one independent variable. The\\ngeneral form for a multiple linear regression equation is a linear function\\nof the independent variables:\\ny = b0 + b1x1i + b2x2i + ⋯+ bkxpi + ei\\nwhere the response variable (y) is shown with p independent variables\\n(x-variables), b0 is a constant value, k is the number of coefficients of\\nthe independent variables, and ei refers to an error term measuring the\\nunexplained variation or noise in the linear relationship.\\nThe set of coefficients are calculated as part of the model building\\nprocess to minimize the overall differences between the observed and the\\npredicted response values. Since the mathematics for computing all but the\\nsimplest models make it impossible to compute by hand, software tools\\nare typically used to perform the computation. This results in an equation\\nwhere the coefficients are estimated:\\n̂y = ̂b0 + ̂b1x1 + ̂b2x2 + ⋯+ ̂bpxk\\nIn this equation, the coefficients are shown with a “hat” to represent\\nthat they are estimated. This form was not presented for the simple linear\\nregression example to simplify the example. For example, a data set of\\ncruise ships is used to build a model to predict the number of crew required\\nin hundreds (Crew), with the first five rows (out of a total of 154) shown in\\nTable 6.3 from Winner (2013). A multiple linear regression model is built\\nusing the variable Cabins (number of cabin on the ship in hundreds) and\\nPassenger density (the passenger to space ratio). The model equation is\\nCrew = −0.423 + 0.75 × Cabins + 0.0377 × Passenger density'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 167}, page_content='154\\nBUILDING MODELS FROM DATA\\nTABLE 6.3\\nSample from the Data Set of Cruise Ships\\nOrder\\nShip Name\\nCruise Line\\nCabins\\nPassenger Density\\nCrew\\n1\\nJourney\\nAzamara\\n3.55\\n42.64\\n3.55\\n2\\nQuest\\nAzamara\\n3.55\\n42.64\\n3.55\\n3\\nCelebration\\nCarnival\\n7.43\\n31.8\\n6.7\\n4\\nDestiny\\nCarnival\\n13.21\\n38.36\\n10\\n5\\nEcstasy\\nCarnival\\n10.2\\n34.29\\n9.2\\nIt is now possible to estimate the number of crew that would be required\\nfor a cruise ship. For example, a cruise ship with 11.1 cabins (1,100 actual\\ncabins) and passenger density of 42.7 would require a crew of 9.51 or 951\\n(since the Crew variable is based on hundreds):\\nCrew = −0.423 + 0.75 × 11.1 + 0.0377 × 42.7 = 9.51\\nThe individual coefficients, similar to the simple linear regression situa-\\ntion, can be interpreted as slopes of the independent variables. In assessing\\nthe slope (coefficient) for a particular variable, the slope represents the\\naverage amount of increase (for positive slope values) or decrease (for\\nnegative slope values) of the response per one unit increase/decrease in\\nthe variable under consideration (keeping the other variables constant).\\nFor example, if Passenger density is held constant then an increase in the\\nCabins variable of 1.0 would mean an increase in the Crew variable of 0.75\\n(the coefficient or slope value for Cabins).\\n6.2.4 Assessing the Model Fit\\nAs part of the process of generating the linear regression model, or in\\nother words estimating the model coefficients, a set of statistics are usu-\\nally generated that help to understand the overall accuracy of the model.\\nThe residual (̂e) is an error term representing the difference between the\\nobserved value (y) and the predicted value (̂y):\\n̂e = y −̂y\\nFor example, Table 6.4 shows Table 6.3 with two additional columns.\\nColumn (̂y) was added to show the predicted value for each row. Since\\nthe actual and predicted values differ, another column was added to show\\nthe residual (̂e). For the ship “Journey,” the actual value for Crew is 3.55,\\nwhereas the predicted value is slightly higher (3.85) resulting in a negative\\nresidual (−0.3). For the ship “Celebration,” the actual value is 6.7, whereas\\nthe predicted value is 6.35 and so the residual is positive (0.35).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 168}, page_content='LINEAR REGRESSION\\n155\\nTABLE 6.4\\nCruise Ship Data Annotated with Predicted Values and\\nCalculated Residuals\\nCruise\\nPassenger\\nPredicted Residual\\nOrder Ship Name\\nLine\\nCabins\\nDensity\\nCrew\\n(̂y)\\n(̂e)\\n1\\nJourney\\nAzamara\\n3.55\\n42.64\\n3.55\\n3.85\\n−0.3\\n2\\nQuest\\nAzamara\\n3.55\\n42.64\\n3.55\\n3.85\\n−0.3\\n3\\nCelebration Carnival\\n7.43\\n31.8\\n6.7\\n6.35\\n0.35\\n4\\nDestiny\\nCarnival\\n13.21\\n38.36\\n10\\n10.9\\n−0.9\\n5\\nEcstasy\\nCarnival\\n10.2\\n34.29\\n9.2\\n8.52\\n0.68\\nLooking at the residual—the difference between the prediction and the\\nactual value—helps to better understand how well the model is performing.\\nThe sum of squares total (SST) is a measure of the variation of the\\ny-values about their mean:\\nSST =\\nn\\n∑\\ni=1\\n(yi −̄y)2\\nOf this total variation, part of the variation is explained and attributable\\nto the relationship between the x-variables and the y-variable or sum of\\nsquares due to regression (SSR). This formula looks at the differences\\nbetween the predicted values (calculated from the regression equation)\\nand the average y-values:\\nSSR =\\nn\\n∑\\ni=1\\n(̂yi −̄y)2\\nThe other part of the total variation is unexplained by the model and\\nhence attributable to the error or sum of squares of error (SSE) and looks at\\nthe differences between the actual y-values (yi) and the predicted y-values\\n(̂yi):\\nSSE =\\nn\\n∑\\ni=1\\n(yi −̂yi)2\\nSince the total sum of squares (SST) is composed of the explained (SSR)\\nand the unexplained (SSE) variations, it follows that the value of SST can\\nbe derived from SSR and SSE:\\nSST = SSR + SSE\\nIn the example of the cruise ship linear model SSR is 1,484, SSE is\\n88.3 and SST is 1,573. The coefficient of determination (R2) represents the'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 169}, page_content='156\\nBUILDING MODELS FROM DATA\\nproportion of the variation that is explained by the set of x-variables in the\\nmodel and is the ratio of SSR to SST:\\nR2 = SSR\\nSST\\nUsing the values from the cruise ship example, R2 would be\\nR2 = 1,484\\n1,573 = 0.94\\n94% of the variable Crew can be explained by the variability in the inde-\\npendent variables (Cabins and Passenger density) with 5.7% attributable to\\nsomething else. R2 values vary between 0 and 1. The closer the values are\\nto 1, the more accurate are the predictions of the model; we say these mod-\\nels have a closer fit. With multiple linear regression, an adjusted R2 value\\n(R2\\nadj) is usually considered to better account for the multiple independent\\nvariables used in the analysis as well as the sample size. Its formula is\\nR2\\nadj = 1 −\\n[(1 −R2)(n −1)\\nn −k −1\\n]\\nwhere n is the number of observations and k is the number of independent\\nvariables. In the cruise ship example, the value of R2\\nadj is\\nR2\\nadj = 1 −\\n[(1 −0.94)(154 −1)\\n154 −2 −1\\n]\\n= 0.94\\nIt is also a typical practice to calculate the standard error of the esti-\\nmate (sy.x), which is a measure of the variation of the y-values about the\\nregression line. This value is interpreted in a similar manner to standard\\ndeviation and has the formula\\nsy.x =\\n√\\nSSE\\nn −2\\nIn the cruise ship example, s would be\\nsy.x =\\n√\\n88.3\\n154 −2 = 0.76\\nThe value indicates the model’s accuracy: the larger the value for the\\nstandard error of estimate, the lower the precision.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 170}, page_content='LINEAR REGRESSION\\n157\\nAs long as the linear regression assumptions (described in Section 6.2.5)\\nare not seriously violated, inferences can be made. A t-test is used to deter-\\nmine whether there is a significant linear relationship between a specific\\nindependent variable and the response. As described earlier, the null and\\nalternative hypothesis should be defined where the null hypothesis is that\\nthere is no linear relationship and the alternative hypothesis states that\\nthere is. If the null hypothesis can be rejected, then there is evidence of\\na linear relationship. The following formula for calculating the t-value\\nis used:\\nt =\\n̂bi −bi\\nsbi\\nwhere\\nsbi =\\nsx.y\\n√∑(xi −̄x)2\\nFor the Cabins variable, the t-value is calculated as follows using 0 for\\nbi to represent that there is no relationship:\\nt = 0.75 −0\\n0.0151 = 49.7\\nFor the Passenger density variable, the t-value is calculated as\\nt = 0.0377 −0\\n0.00734\\n= 5.14\\nUsing a value for ∝of 0.05, the critical t-value is ±1.96. Since the\\nt-values for both Cabins and Passenger density are greater than the critical\\nvalue, we reject the null hypothesis and conclude that each of the two\\nindividual coefficients exhibit a significant relationship with the response\\nvariable. It is usual to calculate a p-value, which would be less than 0.0001\\nfor both the Cabins and the Passenger density coefficient.\\nThe F-test is used to test whether there is a statistically significant\\nrelationship between the x-variables and the y-response. Again, a null\\nhypothesis (there is no linear relationship) and an alternative hypothesis (a\\nlinear relationship exists between at least one of the independent variables)'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 171}, page_content='158\\nBUILDING MODELS FROM DATA\\nis stated. The F-test makes use of the formulas for mean square regression\\n(MSR) and mean square error (MSE):\\nMSR = SSR\\nk\\nMSE =\\nSSE\\nn −k −1\\nF = MSR\\nMSE\\nwhere n is the size of the data set and k is the degrees of freedom (as\\ndiscussed in Section 4.3.5). The null hypothesis is rejected where the\\nF-value is greater than the critical value of F based on k degrees of freedom\\n(regression) and n −k −1 degrees of freedom (error).\\nIn the cruise ship example,\\nMSR = 1,484\\n2\\n= 742\\nMSE =\\n88.3\\n154 −2 −1 = 0.585\\nF = 742\\n0.585 = 1,268\\nSince this value is greater than the critical value of F (identified from a\\nstandard F-distribution table), we reject the null hypothesis and conclude\\nthat there is at least one significant relationship.\\nThe full results of a linear regression are often presented as shown in\\nFigure 6.7.\\n6.2.5 Testing Assumptions\\nLinear regression models are based on a series of assumptions. If a data set\\ndoes not conform to these assumptions then either the model needs to be\\nadjusted—such as applying a mathematical transformation to the data—or\\nmultiple linear regression may not be suitable for modeling the data set.\\nThe first assumption is that of linearity: the relationship between the\\nindependent variables and the response variable should be linear. A scat-\\nterplot displaying the actual response values plotted against the predicted\\nvalues is one approach to checking this assumption. The points on the\\nscatterplot should be evenly distributed on both sides of the regression\\nline. Another approach is to look at the residual values plotted against the'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 172}, page_content='LINEAR REGRESSION\\n159\\nFIGURE 6.7\\nSummary of the multiple linear regression results.\\npredicted values. There should be no discernable trend in the data. Fig-\\nure 6.8 is a scatterplot of the residual values plotted on the y-axis and the\\npredicted values plotted on the x-axis showing no readily observable trend.\\nIf a nonlinear trend is observed, a mathematical transformation, such as a\\nlog transformation or the introduction of an additional x2 value (to obtain\\nan equation in the form y = b0 + b1x + b2x2), should be considered.\\nThe second assumption is the normality of the error distribution. The\\nerror about the line of regression should be approximately normally dis-\\ntributed for each value of x. This assumption can be tested using either a\\nfrequency histogram, statistical measures of skewness/kurtosis, or a normal\\nprobability plot. If the residuals frequency distribution does not approx-\\nimate a normal distribution, then transformations of the variables should\\nbe considered to map them more closely onto a normal distribution. The\\npresence of outliers in the data may also affect the distribution and these'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 173}, page_content='160\\nBUILDING MODELS FROM DATA\\n2.5\\n2\\n1.5\\n1\\n0.5\\n0\\n–0.5\\n–1\\n–1.5\\n–2\\n–2.5\\n0\\n2\\n4\\n6\\nPrediction\\nResidual\\n8\\n10\\n12\\n14\\n16\\nFIGURE 6.8\\nScatterplot of the residuals against the predicted values.\\nshould be checked in case there are errors or anomalies that would warrant\\nremoving them from the data set.\\nA third assumption is homoscedasticity of the errors. The variation of\\nthe error or residual across each of the independent variables should remain\\nconstant either as a function of time (in time-series data sets) or a function\\nof the predicted value. For example, the errors in models generated from\\nstock market data could be affected by seasonal changes or by an increase\\nin the rate of inflation over time. There should be no discerning trend when\\nthe residuals are plotted on the y-axis against (1) the order in which the\\nvalues were measured, (2) the predicted values, and (3) the independent\\nvariables. Trends such as the variation in the error getting larger or smaller\\nas the values along the x-axis increase or decrease would suggest a violation\\nof the homoscedasticity assumption.\\nThe final assumption is the independence of errors. There should be\\nno trend in the residuals based on the order in which the observations\\nwere collected. Again, this can be tested using a scatterplot of the residual\\nvalues versus the order in which they were collected. There should be no\\ndiscernable trend in the data—the observations should spread out evenly.\\nMethods examining this assumption in more detail include the use of the\\nDurbin–Watson statistic.\\n6.2.6 Selecting and Assessing Independent Variables\\nAn important part of generating linear regression models is the selection of\\nindependent variables. It is important to start with a plausible combination\\nof variables, which is a set that a domain expert would identify as having'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 174}, page_content='LOGISTIC REGRESSION\\n161\\na relationship to the response. It is also important to generate the simplest\\npossible model that contains only those independent variables considered\\nnecessary. A rule of thumb is to keep the number of independent variables\\nto a relatively small set and to include at least 10 observations in the training\\nset for every independent variable included in the model.\\nAs described in Chapter 3, it may also be necessary to perform transfor-\\nmations on the pool of potential independent variables. Several techniques\\nare available for doing this. Dummy variables can ge generated where there\\nare nominal variables that need to be included. Continuous variables that\\nneed to be transformed into a categorical variable can use a function that\\nincorporates a series of cutoffs identified by one or more points at which\\nthe response changes dramatically. If the relationship between a potential\\nindependent variable and the response variable needs to be converted from\\nnonlinear to linear, it can be done by the application of transformations\\nsuch as a log or exponential function. Finally, it may be necessary to intro-\\nduce a new independent variable that is a function of two or more variables,\\nsuch as a multiplication or a ratio.\\nPrior to building a prediction model, it is helpful to use exploratory\\ndata analysis methods to inspect the relationships between the variables.\\nThis includes the relationship between each independent variable under\\nconsideration and the response. It is also important to understand the\\nrelationships between each pair of independent variables because including\\nvariables strongly related to each other adds little new information to the\\nmodel and makes the final model difficult to interpret.\\nMultiple combinations of different independent variables can be used to\\nbuild a set of models from which the best performing, most plausible, and\\nsimplest model is selected. In addition, there are ways of automatically\\nselecting the “best” combinations of independent variables (discussed in\\nthe Further Reading section) using methods such as stepwise regression.\\nOnce the equation parameters have been selected and a model has\\nbeen built, tools that generate linear regression models produce a series\\nof statistics about the coefficients. In addition to the value of the constants\\nused in the equations, the standard error, t-stat, and p-value is calculated,\\nas discussed earlier, which can be used to help in the selection of the\\nindependent variables.\\n6.3 LOGISTIC REGRESSION\\n6.3.1 Overview\\nAs discussed in the previous section, the multiple linear regression\\napproach can only be used to make predictions when the response variable'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 175}, page_content='162\\nBUILDING MODELS FROM DATA\\nis continuous. It cannot be used when the response variable is categori-\\ncal. Logistic regression is a popular approach to building models where\\nthe response variable is usually binary (dichotomous). For example, the\\nresponse variable could indicate whether a consumer purchases a product\\n(1 if they purchase and 0 if they do not) or whether a candidate drug is\\npotent (1 if the candidate drug is potent and 0 if it is not). Logistic regres-\\nsion provides a flexible and easy-to-interpret method for building models\\nfrom binary data. The following section outlines how to build, use, and\\nassess logistic regression models.\\n6.3.2 Fitting a Simple Logistic Regression Model\\nA data set related to the presence of gold deposits (five rows of which\\nare shown in Table 6.5) will be used to illustrate how a logistic regression\\nmodel operates from Sahoo and Pandalai (1999). The data set includes\\nobservations showing measured Sb levels (log transformed) and whether\\nthere is a gold deposit within 0.5 km (1 indicates there is a gold deposit\\nand 0 indicates there is none). The average log(Sb level) where Gold\\ndeposit proximity is 1 is 0.445 and the average value when Gold deposit\\nproximity is 0 is −0.444 indicating that there is a difference between the\\ntwo values; however, it does not describe the type of relationship well. To\\nbetter understand this relationship, we will make the values of the log(Sb\\nlevel) discrete and plot these values, as shown in Table 6.6 and Figure 6.9.\\nThe relationship between the average value of the Gold deposit proximity\\nvariable and the log(Sb level) can be seen in Figure 6.10. As the values for\\nlog(Sb level) increase, the mean values for the Gold deposit proximity also\\nrise; however, the relationship is not linear. It follows an S-shaped curve,\\nstarting at a mean value of 0 (all values are 0) and ending at a mean value\\nof 1 (all values are 1), with a more rapid transition from the low to high\\nmean values toward the center of the graph. The graph can never go below\\n0 or above 1 and the mean values at the low end of the log(Sb level) range\\nas well as at the high log(Sb level) range are flat.\\nTABLE 6.5\\nFive Rows from the Gold Data Set\\nLog(Sb level)\\nGold Deposit Proximity\\n−0.251811973\\n0\\n−0.22184875\\n0\\n−0.15490196\\n1\\n−0.096910013\\n0\\n−0.040958608\\n0'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 176}, page_content='LOGISTIC REGRESSION\\n163\\nTABLE 6.6\\nThe Mean Value for the Variable Gold Deposit\\nProximity for Different Ranges of Log(Sb Level)\\nLog(Sb Level) Ranges\\nMean Gold Deposit Proximity\\n−1.1 to −0.7\\n0\\n−0.7 to −0.3\\n0.04\\n−0.3 to 0.1\\n0.43\\n0.1 to 0.5\\n0.89\\n0.5 to 0.9\\n0.9\\n0.9 to 1.3\\n1\\nTo generate a model that describes the relationship between the inde-\\npendent variable or x-variable (log(Sb level)) and the response or y-variable\\n(Gold deposit proximity), we will need to understand the relationship\\nbetween the mean or expected value (E) of the response (Y) given a specific\\nvalue for the x variable (E(Y|x)). In Figure 6.10 we are showing the mean\\nresponse value on the y-axis. In order to map the x values onto the mean\\ny-values we need to model this S-shaped curve which can be accomplished\\nthrough a logistic formula:\\nE(Y|x) =\\ne𝛽0+𝛽1x\\n1 + e𝛽0+𝛽ix\\nFIGURE 6.9\\nGraph showing how the mean values for variable Gold deposit\\nproximity increase as values for the log-transformed Sb level increase.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 177}, page_content='164\\nBUILDING MODELS FROM DATA\\nFIGURE 6.10\\nShape of the graph showing how the mean values for variable\\nGold deposit proximity increase as values for the log-transformed Sb level\\nincrease.\\nIn this equation, the expected value of y given x (E(Y|x)) is calculated\\nwhere e is the exponential function and 𝛽0, 𝛽1 are constant values. This\\nformula will calculate values for the E(Y|x) along the “S”-shaped curve.\\nIt also ensures that values do not exceed 1 or go below 0, as shown in\\nFigure 6.10.\\nThe error for logistic regression has different characteristics to the error\\ndiscussed in the section on linear regression. Since the values of Y can be\\nonly 1 or 0, the error is either 1 −E(Y|x) or 0 −E(Y|x), hence it follows\\na binomial distribution (rather than an approximate normal distribution as\\nin the case of linear regression).\\nFor a given data set, the beta coefficients are estimated using a maximum\\nlikelihood method (see Hosmer et al. (2013) for details). This process is\\ninvariably performed using computer software. For the data set illustrated\\nin Table 6.5, the following formula is generated:\\nE(Gold deposit proximity|log(Sb level)) =\\ne−0.0728+5.82×log(Sb level)\\n1 + e−0.0728+5.85×log(Sb level)\\nTo calculate a value for the expected value (mean) for the Gold\\ndeposit proximity (E(Gold deposit proximity|log(Sb level)) we can sub-\\nstitute the original value with its log-transformed value in the formula.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 178}, page_content='LOGISTIC REGRESSION\\n165\\nIf log(Sb level) was 0.4, then\\nE(Gold deposit proximity|log(Sb level) = 0.4) =\\ne−0.0728+5.82×0.4\\n1 + e−0.0728+5.85×0.4\\n= 0.905\\nSince the expected value is close to 1, we could conclude that it is likely\\nthere will be a gold deposit within 0.5 km.\\n6.3.3 Fitting and Interpreting Multiple Logistic\\nRegression Models\\nIn almost all practical situations, multiple independent variables will be\\nused to build a logistic regression model. The formula can be extended to\\naccommodate p independent variables:\\nE(Y|x) =\\ne𝛽0+𝛽1x1+𝛽2x2+⋯+𝛽pxp\\n1 + e𝛽0+𝛽1x1+𝛽2x2+⋯+𝛽pxp\\nFor example, two measurements, As levels and Sb levels, could be col-\\nlected and used to predict whether a gold deposit will be identified within\\n0.5 km (Gold deposit proximity). Again the log transformation of the origi-\\nnal As levels and Sb level measurements will be used: log(As levels), log(Sb\\nlevels). By loading the data set into a computational package, it is possible\\nto derive a formula to predict the expected value for Gold deposit proximity:\\nE(Y|x) =\\ne−1.84+5.2 log(As levels)+3.5 log(Sb levels)\\n1 + e−1.84+5.2 log(As levels)+3.5 log(Sb levels)\\nThe predicted value based on actual As levels and Sb levels would be\\nE(Y|x) =\\ne−1.84+5.2×0.1+3.5×0.05\\n1 + e−1.84+5.2×0.1+3.5×0.05 = 0.24\\nwhere a log(As level) of 0.1 and a log(Sb level) of 0.05 has been used. Since\\n0.24 is closer to 0 we might conclude that it is unlikely that there will be a\\ngold deposit within 0.5 km.\\n6.3.4 Significance of Model and Coefficients\\nWhen the coefficients of a logistic regression model are calculated, the\\nmethod for identifying these coefficients attempts to maximize the log'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 179}, page_content='166\\nBUILDING MODELS FROM DATA\\nlikelihood (L), which takes into account the difference between the actual\\nvalue of y (yi) and the predicted value 𝜋i:\\nL =\\nn\\n∑\\ni=1\\n(yi ln(𝜋i) + (1 −yi) ln(1 −ln(𝜋i)))\\nIn the Gold example, the log likelihood function is calculated to be\\n−9.02. To better understand the significance of this model, this value is\\ncompared to the log likelihood for the constant only model (in this example\\n−43.7). The model’s log likelihood is clearly higher; however, we can\\nperform a test using the likelihood ratio statistic (LR). This statistic takes\\ninto account both the full model (based on the two independent variables\\nin this example) as well as the reduced model (in this case the constant\\nonly model). It is calculated using the following formula:\\nLR = −2 [L (reduced) −L (full)]\\nIn this example this translates to\\nLR = −2 [(−43.7) −(−9.02)] = 69\\nThe LR statistic follows a chi-square distribution and we can determine\\na p-value by looking up the value in a chi-square distribution table where\\nthe degrees of freedom are the difference in the number of independent\\nvariables between the two models being assessed. In this case, p < 0.0001\\nand hence it would be considered significant. The LR statistic can be also\\nused to understand the difference between two models containing different\\nsets of independent variables.\\nWe can also look at the significance of the individual coefficients in\\na manner similar to the linear regression coefficients. This assessment\\nmakes use of the Wald test (see Hosmer et al. (2013) for details). In this\\nexample, the coefficients are summarized in Table 6.7. It can be seen that\\nTABLE 6.7\\nSummary of the Logistic Regression Coefficients for the Gold\\nModel\\nIndependent\\nBeta\\nStandard\\nVariable\\nCoefficients\\nError\\nz-Values\\np-Values\\nConstant\\n−1.84\\n0.95\\n−1.93\\n0.052\\nLog(As Level)\\n5.2\\n1.91\\n2.72\\n0.0063\\nLog(Sb Level)\\n3.5\\n1.82\\n1.92\\n0.054'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 180}, page_content='k-NEAREST NEIGHBORS\\n167\\nTABLE 6.8\\nAccuracy, Sensitivity, and Specificity Based on Three Cut-Off\\nValues\\nCut-Off\\nAccuracy (%)\\nSensitivity (%)\\nSpecificity (%)\\n0.2\\n89.10\\n96.40\\n83.30\\n0.5\\n92.20\\n92.90\\n91.70\\n0.8\\n84.40\\n71.40\\n94.40\\nlog(As level) has a low p-value below 0.05 and is clearly an important\\nvariable in the model; whereas log(Sb level) is around 0.05.\\n6.3.5 Classification\\nUp to this point we have been calculating a value for the response E(Y|x);\\nhowever, we may wish to have the predicted value in the same form as the\\ny-variable, which is either 0 or 1. To map a value to either 0 or 1, we need to\\ndetermine a cut-off value where values greater than or equal to the cut-off\\nare assigned the value 1 and those less than the cut-off are assigned the\\nvalue 0. Once we have the prediction in this form (0 or 1), we can assess\\nthe overall accuracy of the model in terms of the percentage of correct\\npredictions as well as the sensitivity and specificity of the model.\\nOne approach to determining the cut-off is to select a value by hand.\\nTo illustrate, we will use three possible cut-off values: 0.2, 0.5, and\\n0.8. By applying these values to the gold deposit example, the overall\\nclassification accuracy as well as sensitivity and specificity will change, as\\nshown in Table 6.8. It is possible to automatically identify a cut-off value\\nthat is a good balance of sensitivity and specificity, and in this example it\\nwould be 0.532.\\n6.4 k-NEAREST NEIGHBORS\\n6.4.1 Overview\\nThe k-nearest neighbors (kNN) method provides a simple approach to cal-\\nculating predictions for unknown observations. This method calculates a\\nprediction by looking at similar observations in the training set and uses\\nsome function of their response values, such as an average, to calculate\\nthe prediction. Like all prediction methods, it starts with a training set. It\\ndiffers from other methods by determining the optimal number of simi-\\nlar observations to use in making the prediction rather than producing a\\nmathematical model.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 181}, page_content='168\\nBUILDING MODELS FROM DATA\\n26\\n24\\n22\\n20\\n18\\n16\\n14\\n12\\n10\\nAcceleration\\n1500\\n2000\\n2500\\n3000\\n3500\\nWeight\\n4000\\n4500\\n5000\\n5500\\n8\\n6\\nGood fuel efficiency\\nPoor fuel efficiency\\nFIGURE 6.11\\nScatterplot showing fuel efficiency classifications.\\nThe scatterplot in Figure 6.11 is based on a data set of cars and will be\\nused to illustrate how kNN operates. Two variables that will be used as\\nindependent variables are plotted on the x- and y-axis (Weight and Acceler-\\nation). The response variable is a dichotomous variable (Fuel Efficiency)\\nwhich has two values: good and poor fuel efficiency. The darker shaded\\nobservations have good fuel efficiency and the lighter shaded observations\\nhave poor fuel efficiency.\\nDuring the learning phase, the “best” number of similar observations\\nis chosen (k). The selection of k is described in Section 6.4.2. Once a\\nvalue for k has been determined it is possible to make a prediction for a car\\nwith unknown fuel efficiency. To illustrate, cars A and B with unknown fuel\\nefficiency are presented to the kNN model in Figure 6.12. The Acceleration\\nand Weight of these observations are known and the two observations are\\nplotted alongside the training set. Based on the optimal value for k, the\\nk-nearest neighbors (most similar observations) to A and B are identified\\nin Figure 6.12. For example, if k was calculated to be 10, then the 10 most\\nsimilar observations from the training set would be selected. A prediction\\nis made for A and B based on the response of the nearest neighbors (see\\nFigure 6.13). In this case, observation A would be predicted to have good\\nfuel efficiency since its neighbors primarily have good fuel efficiency.\\nObservation B would be predicted to have poor fuel efficiency since its\\nneighbors all have poor fuel efficiency.\\nkNN is relatively insensitive to errors or outliers in the data and can be\\nused with large training sets; however, it can be computationally slow when\\nit is applied to a new data set since a similarity score must be generated\\nbetween the observations presented to the model and every member of the\\ntraining set.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 182}, page_content='k-NEAREST NEIGHBORS\\n169\\n26\\n24\\n22\\n20\\n18\\n16\\n14\\n12\\n10\\nAcceleration\\n1500\\n2000\\n2500\\n3000\\n3500\\nWeight\\n4000\\n4500\\n5000\\n5500\\nB\\nA\\n8\\n6\\nGood fuel efficiency\\nPoor fuel efficiency\\nFIGURE 6.12\\nTwo test set observations (A and B).\\n26\\nA\\nB\\n24\\n22\\n20\\n18\\n16\\n14\\n12\\n10\\nAcceleration\\n1500\\n2000\\n2500\\n3000\\n3500\\nWeight\\n4000\\n4500\\n5000\\n5500\\nGood fuel efficiency\\nPoor fuel efficiency\\n8\\n6\\nFIGURE 6.13\\nLooking at similar observation to support making predictions for\\nA and B.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 183}, page_content='170\\nBUILDING MODELS FROM DATA\\n6.4.2 Training\\nA kNN model uses the k most similar neighbors to the observation to\\ncalculate a prediction. Where a response variable is continuous, the pre-\\ndiction is the mean of these nearest neighbors. Where a response variable\\nis categorical, the prediction can be presented as a mean or a particular\\nclassification scheme that selects the most common classification term.\\nIn the learning phase, three items should be considered and optimized:\\n(1) the similarity method (2) the value of k, and (3) the combination of\\nindependent variables to use. As described in Chapter 5, there are many\\nmethods for determining whether two observations are similar including,\\nfor example, the Euclidean or the Jaccard distance. As with clustering, it\\nis important to normalize the values of the variables so that no variables\\nare considered to be more important based solely on the range of values\\nover which they were measured. The number of similar observations that\\nproduces the best predictions or k must be determined. If this value is too\\nhigh, the kNN model will overgeneralize; if the value is too small, it will\\nlead to a large variation in the prediction.\\nThe selection of k is performed by evaluating different values of k within\\na range and selecting the value that gives the “best” prediction. To ensure\\nthat models generated using different values of k are not over-fitting, a\\nseparate training and test set should be used, such as a 10% cross-validation.\\nTo assess the different values for k, the SSE evaluation criteria will be\\nused:\\nSSE =\\nn\\n∑\\ni=1\\n(yi −̂yi)2\\nSmaller SSE values indicate that the predictions are closer to the actual\\nvalues.\\nTo illustrate, a data set of cars will be used and a model built to test the\\ncar’s fuel efficiency (MPG). The following variables will be used as inde-\\npendent variables within the model: Cylinders, Weight, and Acceleration.\\nThe Euclidean distance calculation was selected to represent the distance\\nbetween observations. To calculate an optimal value for k, different values\\nof k were selected between 1 and 20. The SSE evaluation criterion was\\nused to assess the quality of each model. In this example, the value of k\\nwith the lowest SSE value is 8 and this value is selected for use with the\\nkNN model (see Table 6.9).\\n6.4.3 Predicting\\nOnce a value for k has been set in the training phase, the model can now be\\nused to make predictions. For example, an observation x has values for the'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 184}, page_content='k-NEAREST NEIGHBORS\\n171\\nTABLE 6.9\\nValues for SSE\\nfor Different Values of K\\nk\\nSSE\\n1\\n12,003\\n2\\n8,358\\n3\\n7,525\\n4\\n7,246\\n5\\n6,870\\n6\\n6,906\\n7\\n6,628\\n8\\n6,504\\n9\\n6,533\\n10\\n6,658\\n11\\n6,621\\n12\\n6,612\\n13\\n6,648\\n14\\n6,773\\n15\\n6,811\\n16\\n6,943\\n17\\n6,965\\n18\\n7,015\\n19\\n6,963\\n20\\n6,996\\nindependent variables but not for the response. Using the same technique\\nfor determining similarity as used in the model building phase, observa-\\ntion x is compared against all observations in the training set. A distance is\\ncomputed between x and each training set observation. The closest k obser-\\nvations are selected and a prediction is made, for example, the average of\\nthe k-nearest neighbors is calculated to determine a prediction.\\nThe observation (Chevrolet Chevelle Malibu) in Table 6.10 was pre-\\nsented to the kNN model built to predict a car’s fuel efficiency (MPG).\\nThe Chevrolet Chevelle Malibu observation was compared to all obser-\\nvations in the training set and a Euclidean distance was computed. The\\neight observations with the smallest distance scores are selected, as shown\\nTABLE 6.10\\nkNN Test Observations\\nCar Name\\nCylinders\\nWeight\\nAcceleration\\nChevrolet Chevelle Malibu\\n6\\n3897\\n18.5'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 185}, page_content='172\\nBUILDING MODELS FROM DATA\\nTABLE 6.11\\nCalculating the kNN Prediction Based on the Eight Nearest\\nNeighbors\\nCalculated\\nCar Name\\nDistance\\nCylinders Weight Acceleration MPG\\nDodge Aspen\\n0.0794\\n6\\n3620\\n18.7\\n18.6\\nAmc Matador\\n0.0808\\n6\\n3632\\n18\\n16\\nDodge Aspen Se\\n0.0844\\n6\\n3651\\n17.7\\n20\\nPlymouth Volare Custom\\n0.0894\\n6\\n3630\\n17.7\\n19\\nChevrolet Chevelle\\nMalibu Classic\\n0.0951\\n6\\n3781\\n17\\n16\\nMercedes-Benz 280s\\n0.1093\\n6\\n3820\\n16.7\\n16.5\\nFord Granada\\n0.1095\\n6\\n3525\\n19\\n18.5\\nPontiac Phoenix Lj\\n0.1108\\n6\\n3535\\n19.2\\n19.2\\nAverage\\n18\\nin Table 6.11. The prediction is 18, which is the average of these eight\\nobservations.\\nThe training set of observations can be used to explain how the prediction\\nwas reached and to assess the confidence in this prediction. For example,\\nif the response values for all these observations are close, it increases the\\nconfidence in the prediction.\\n6.5 CLASSIFICATION AND REGRESSION TREES\\n6.5.1 Overview\\nIn Chapter 5, decision trees were described as a way of grouping obser-\\nvations based on specific values or ranges of independent variables. For\\nexample, the tree in Figure 6.14 organizes a set of observations based on\\nthe car’s number of cylinders (Cylinders). The tree was constructed using\\nthe variable MPG as the response variable. This variable was used to guide\\nhow the tree was constructed, resulting in groupings that characterize a\\ncar’s fuel efficiency. The terminal nodes of the tree (A, B, and C) show a\\npartitioning of cars into sets with good (node A), moderate (node B), and\\npoor (node C) fuel efficiency.\\nEach terminal node is a mutually exclusive set of observations, that is,\\nthere is no overlap in observations between nodes A, B, or C. The criteria\\nfor inclusion in each of these nodes are defined by the set of branch points\\nused to partition the data. For example, terminal node B is defined as\\nobservations where Cylinders ≥5 and Cylinders < 7.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 186}, page_content='CLASSIFICATION AND REGRESSION TREES\\n173\\nFIGURE 6.14\\nDecision tree built from an automobile data set.\\nDecision trees can be used as both classification and regression predic-\\ntion models. Decision trees that are built to predict a continuous response\\nvariable are called regression trees and decision trees built to predict a cat-\\negorical response are called classification trees. During the learning phase,\\na decision tree is constructed using the training set. Predictions in decision\\ntrees are made using the criteria associated with the trees terminal nodes.\\nA new observation is assigned to a terminal node in the tree using these\\nsplitting criteria. The prediction for the new observation is either the node’s\\nclassification (in the case of a classification tree) or the average value (in\\nthe case of a regression tree). As with other approaches to predtive model-\\ning, the quality of the prediction can be assessed using a separate training\\nset.\\n6.5.2 Predicting\\nIn Figure 6.15, a set of cars are shown on a scatterplot. The cars are defined\\nas having good or poor fuel efficiency. Those with good fuel efficiency\\nare shaded darker than those with poor fuel efficiency. Values for the\\nAcceleration and Weight variables are shown on the two axes.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 187}, page_content='174\\nBUILDING MODELS FROM DATA\\n26\\n24\\n22\\n20\\n18\\n16\\n14\\n12\\n10\\nAcceleration\\n1500\\n2000\\n2500\\n3000\\n3500\\nWeight\\n4000\\n4500\\n5000\\n5500\\n8\\n6\\nGood fuel efficiency\\nPoor fuel efficiency\\nFIGURE 6.15\\nScatterplot of cars show those with good and poor fuel efficiency.\\nA decision tree is generated using the car’s fuel efficiency as the response\\nvariable. This results in a decision tree where the terminal nodes partition\\nthe set of observations according to ranges in the independent variables.\\nOne potential partition of the data is shown in Figure 6.16. The prediction\\nis then made based on the observations used to train the model that are\\nwithin the specific region, such as the most popular class or the average\\nvalue (see Figure 6.17).\\nWhen an observation with unknown fuel efficiency is presented to the\\ndecision tree model it is placed within one of the regions. The placement\\nis based on the observation’s values for the independent variables. Two\\n26\\n24\\n22\\n20\\n18\\n16\\n14\\n12\\n10\\nAcceleration\\n1500\\n2000\\n2500\\n3000\\n3500\\nWeight\\n4000\\n4500\\n5000\\n5500\\n8\\n6\\nGood fuel efficiency\\nPoor fuel efficiency\\nFIGURE 6.16\\nRegion of interest generated from the decision tree.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 188}, page_content='CLASSIFICATION AND REGRESSION TREES\\n175\\n26\\n24\\n22\\n20\\n18\\n16\\n14\\n12\\n10\\nAcceleration\\n1500\\n2000\\n2500\\n3000\\n3500\\nWeight\\n4000\\n4500\\n5000\\n5500\\n8\\n6\\nGood fuel efficiency\\nPoor fuel efficiency\\nRegion predicted to have\\ngood fuel efficiency\\nRegion predicted to have\\npoor fuel efficiency\\nFIGURE 6.17\\nScatterplot illustrating the classification response per region iden-\\ntified by the decision tree.\\nobservations (A and B) with values for Acceleration and Weight but\\nno value for whether the cars have a good or poor fuel efficiency are\\npresented to the model. These observations are shown on the scatterplot in\\nFigure 6.18 showing how the ranges of the variables used as independent\\nvariables partition the data. Observation A will be predicted to have good\\n26\\n24\\n22\\n20\\n18\\n16\\n14\\n12\\n10\\nAcceleration\\n1500\\n2000\\n2500\\n3000\\n3500\\nWeight\\n4000\\n4500\\n5000\\n5500\\nB\\nA\\n8\\n6\\nGood fuel efficiency\\nPoor fuel efficiency\\nRegion predicted to have\\ngood fuel efficiency\\nRegion predicted to have\\npoor fuel efficiency\\nFIGURE 6.18\\nClassification of two automobiles with unknown MPG values.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 189}, page_content='FIGURE 6.19\\nDecision tree generated from the automobile training set.\\n176'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 190}, page_content='CLASSIFICATION AND REGRESSION TREES\\n177\\nfuel efficiency whereas observation B will be predicted to have poor fuel\\nefficiency.\\nDecision trees are useful for prediction since the results are easy to\\nexplain. Unfortunately these types of models can be quite sensitive to large\\nvariation in the training set that cannot be explained.\\nThe same parameters used to build the tree (described in Section 5.5)\\ncan be set to build a decision tree model, that is, different input variable\\ncombinations and different stopping criteria for the tree.\\n6.5.3 Example\\nThe decision tree in Figure 6.19 was built from a data set of 382 cars\\nusing the continuous variable MPG to split the observations. The average\\nvalue shown in the diagram is the MPG value for the set of observations.\\nThe nodes are not split further if there are less than 40 observations in the\\nterminal node.\\nIn Table 6.12, three observations not used in building the tree are shown\\nwith both an actual and a predicted value. The second to last column (Rule)\\nindicates the node in the tree that was used to calculate the prediction. For\\nexample, the “AMC Matador” with a weight of 3,730, six cylinders, and\\nan acceleration of 19 will fit into a region defined by node F in the tree.\\nNode F has an average MPG value of 16.9 and hence this is the predicted\\nMPG value. The table also indicates the actual MPG values for the\\ncars tested.\\nThe examples used in this section were simple in order to describe how\\npredictions can be made using decision trees. It is standard practice to use\\nlarger numbers of independent variables. Also, building a series of models\\nby changing the terminating criteria can also be a useful way to optimize\\nthe decision tree models.\\nTABLE 6.12\\nTest Set of Three Automobiles Not Used in Building the\\nModel\\nCar Name\\nCylinders Weight Acceleration MPG Rule Prediction\\nOldsmobile Cutlass\\nSalon Brougham\\n8\\n3420\\n22.2\\n23.9\\nE\\n20.48\\nAmc Matador\\n6\\n3730\\n19\\n15\\nF\\n16.9\\nDodge D200\\n8\\n4382\\n13.5\\n11\\nG\\n13.98'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 191}, page_content='178\\nBUILDING MODELS FROM DATA\\nThe terminal nodes in the decision trees can be described as rules (as\\nshown in Section 5.4.4) and these rules can be useful in explaining how\\na prediction was made. In addition, looking at the data on which each\\nrule is based allows you to understand the degree of confidence for each\\nprediction. For example, the number of observations and the distribution\\nof the response variable can help to understand how much confidence you\\nshould have in the prediction.\\n6.6 OTHER APPROACHES\\n6.6.1 Neural Networks\\nLike all prediction models, the neural network approach uses a training set\\nof examples to generate the model. This training set is used to generalize the\\nrelationships between the “input” independent variables and the “output”\\nresponse variables. As part of this process, a series of interconnected nodes\\nare organized between the input nodes (each input node corresponds to\\nan independent variable) and the output response variables (represented as\\nnodes). These nodes can be organized into multiple layers of interconnected\\nnodes. As part of the process of learning, weights on the connections\\nbetween the nodes in the neural network are refined to generate a prediction\\nmodel. Once a neural network has been created, it can be used to make\\npredictions. A prediction is made by presenting a test observation to the\\ninput nodes of the network and, based on local calculations, allowing\\nthe values to propagate through the network to eventually generate the\\nscores for the output variables. These scores are the final prediction. Neural\\nnetworks are a flexible way to generate models from the data and are\\ncapable of modeling complex linear and nonlinear relationships between\\nthe input variables and one or more response variables. They are, however,\\nconsidered a “black box” since it is difficult to get a useful explanation of\\nhow the predictions were derived.\\n6.6.2 Support Vector Machines\\nSupport vector machines are used primarily for classification problems.\\nThey attempt to identify a hyper-plane that separates the different classes\\nbeing modeled. The observations on one side of the plane represent one\\nof the classes being predicted, whereas the observations on the other side\\nrepresent the other class. Despite their general usefulness and ability to\\nhandle complex classification problems, they can be difficult to interpret.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 192}, page_content='EXERCISES\\n179\\n6.6.3 Discriminant Analysis\\nDiscriminant analysis is an example of a classification approach. It classi-\\nfies two or more values by constructing a linear combination of variables\\nthat characterize the different classes. One necessary assumption is that\\nthe independent variables are normally distributed.\\n6.6.4 Na¨ıve Bayes\\nNa¨ıve Bayes is a classification approach to building a predictive model.\\nIt makes use of the Bayesian theorem to compute probabilities of class\\nmembership. It provides a simple approach to modeling and can be easily\\nused on large data sets. In addition, it can also be used to rank observations\\nusing a computed probability.\\n6.6.5 Random Forests\\nRandom forests make use of multiple decision trees, with each tree using\\na different set of independent variables. The final prediction is calculated\\nfrom the collection of decision tree results. As with Classification and\\nRegression Trees or CART, this approach can be used for both classification\\nand regression problems.\\nEXERCISES\\n1. A classification prediction model was built using a training set of exam-\\nples. A separate test set of 20 examples is used to test the model.\\nTable 6.13 shows the results of applying this test set.\\nCalculate the model’s\\n(a) concordance\\n(b) sensitivity\\n(c) specificity\\n2. A regression prediction model was built using a training set of examples.\\nA separate test set was applied to the model and the results are shown\\nin Table 6.14.\\n(a) Calculate the residual for each observation.\\n(b) Determine the sum of the square of the residual.\\n3. Table 6.15 shows the relationship between the amount of fertilizer used\\nand the height of a plant.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 193}, page_content='180\\nBUILDING MODELS FROM DATA\\nTABLE 6.13\\nTable of Actual Versus\\nPredicted Values (Categorical Response)\\nObservation\\nActual\\nPredicted\\n1\\n0\\n0\\n2\\n1\\n1\\n3\\n1\\n1\\n4\\n0\\n0\\n5\\n0\\n0\\n6\\n1\\n0\\n7\\n0\\n0\\n8\\n0\\n0\\n9\\n1\\n1\\n10\\n1\\n1\\n11\\n1\\n1\\n12\\n0\\n1\\n13\\n0\\n0\\n14\\n1\\n1\\n15\\n0\\n0\\n16\\n1\\n1\\n17\\n0\\n0\\n18\\n1\\n1\\n19\\n0\\n1\\n20\\n0\\n0\\n(a) Calculate a simple linear regression equation using Fertilizer as the\\nindependent variable and Height as the response.\\n(b) Predict the height when fertilizer is 12.3.\\n4. A kNN model is being used to predict house prices. A training set was\\nused to generate a kNN model and k is determined to be 5. The unseen\\nobservation in Table 6.16 is presented to the model. The kNN model\\ndetermines the five observations in Table 6.17 from the training set to\\nbe the most similar. What would be the predicted house price value?\\n5. A classification tree model is being used to predict which brand of printer\\na customer would purchase with a computer. The tree in Figure 6.20\\nwas built from a training set of examples. For a customer whose Age =\\n32 and Income = $35,000, which brand of printer would the tree predict\\nthey would buy?'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 194}, page_content='EXERCISES\\n181\\nTABLE 6.14\\nTable of Actual Versus\\nPredicted (Continuous Response)\\nObservation\\nActual\\nPredicted\\n1\\n13.7\\n12.4\\n2\\n17.5\\n16.1\\n3\\n8.4\\n6.7\\n4\\n16.2\\n15.7\\n5\\n5.6\\n8.4\\n6\\n20.4\\n15.6\\n7\\n12.7\\n13.5\\n8\\n5.9\\n6.4\\n9\\n18.5\\n15.4\\n10\\n17.2\\n14.5\\n11\\n5.9\\n5.1\\n12\\n9.4\\n10.2\\n13\\n14.8\\n12.5\\n14\\n5.8\\n5.4\\n15\\n12.5\\n13.6\\n16\\n10.4\\n11.8\\n17\\n8.9\\n7.2\\n18\\n12.5\\n11.2\\n19\\n18.5\\n17.4\\n20\\n11.7\\n12.5\\nTABLE 6.15\\nTable of Plant Experiment\\nFertilizer\\nHeight\\n10\\n0.7\\n5\\n0.4\\n12\\n0.8\\n18\\n1.4\\n14\\n1.1\\n7\\n0.6\\n15\\n1.3\\n13\\n1.1\\n6\\n0.6\\n8\\n0.7\\n9\\n0.7\\n11\\n0.9\\n16\\n1.3\\n20\\n1.5\\n17\\n1.3'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 195}, page_content='182\\nBUILDING MODELS FROM DATA\\nTABLE 6.16\\nHouse with Unknown Price\\nBedroom\\nNumber of Bathrooms\\nSquare Feet\\nGarage\\nHouse Price\\n2\\n2\\n1810\\n0\\nTABLE 6.17\\nTable of Similar Observations\\nBedroom\\nNumber of Bathrooms\\nSquare Feet\\nGarage\\nHouse Price\\n2\\n2\\n1504\\n0\\n355,000\\n2\\n2\\n1690\\n0\\n352,000\\n2\\n3\\n1945\\n0\\n349,000\\n3\\n2\\n2146\\n0\\n356,000\\n3\\n2\\n1942\\n0\\n351,000\\nFIGURE 6.20\\nA classification tree model used to predict which brand of printer\\na customer would purchase.\\nFURTHER READING\\nWhen using statistical approaches (such as a t-tests) to identify independent vari-\\nables from a large number of potential variables, it is important to use correction\\nfactors such as those discussed in Westfall et al. (1999) and Hsu (1996). Principal\\ncomponent analysis is an approach to dimension reduction and outlined in Jol-\\nliffe (2002) and Jackson (2003). See Kleinbaum et al. (2013) for a more detailed\\ntreatment of linear regression including a discussion of confidence intervals for\\ncoefficients and prediction; the use of the Durbin–Watson and autocorrelation\\nmethods for testing normality assumptions; and the use of automated approaches,'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 196}, page_content='FURTHER READING\\n183\\nincluding stepwise linear regression, to automatically identify combination of\\nindependent variables to use in the model. For a more detailed discussion on\\nlogistic regression, including the use of the Score Test, the use of the odds ratios\\nto help interpret the models and the use of stepwise logistic regression see Hosmer\\net al. (2013). Agresti (2013) covers logistic regression as well as other methods\\nto handle categorical data. For a more comprehensive treatment of advanced data\\nmining approaches see Fausett (1993), Cristianini & Shawe-Taylor (2000), and\\nHastie et al. (2009).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 197}, page_content=''),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 198}, page_content='APPENDIX A\\nANSWERS TO EXERCISES\\nChapter 2\\n1a. Nominal\\n1b. Ratio\\n1e. Ratio\\n1f. Ratio\\n1g. Ratio\\n1h. Ratio\\n1i. Nominal\\n2a. 45\\n2b. 45\\n2c. 48.7\\n2d. 53\\n2e. 324.9\\n2f. 18.02\\n3. See Figure A.1\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n185'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 199}, page_content='186\\nANSWERS TO EXERCISES\\n0\\n0–250\\n250–500\\n500–750\\n750–1000\\n5\\n10\\nFrequency\\n15\\n20\\nFIGURE A.1\\nFrequency distribution for Exercise 3 from Chapter 2.\\nChapter 3\\n1. See Table A.1\\n2. See Table A.2\\n3. See Table A.3\\nChapter 4\\nSee Table A.4\\n2a. See Table A.5\\n2b. See Table A.6\\n2c. See Table A.7\\nSee Figure A.2\\nTABLE A.1\\nChapter 3, Question 1 Answer\\nName\\nWeight (kg)\\nWeight (kg)—Normalize to 0–1\\nP. Lee\\n50\\n0.095\\nR. Jones\\n115\\n0.779\\nJ. Smith\\n96\\n0.579\\nA. Patel\\n41\\n0\\nM. Owen\\n79\\n0.4\\nS. Green\\n109\\n0.716\\nN. Cook\\n73\\n0.337\\nW. Hands\\n104\\n0.663\\nP. Rice\\n64\\n0.242\\nF. Marsh\\n136\\n1'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 200}, page_content='ANSWERS TO EXERCISES\\n187\\nTABLE A.2\\nChapter 3, Question 2 Answer\\nWeight (kg)—Categorized\\nName\\nWeight (kg)\\n(Low, Medium, High)\\nP. Lee\\n50\\nLow\\nR. Jones\\n115\\nHigh\\nJ. Smith\\n96\\nMedium\\nA. Patel\\n41\\nLow\\nM. Owen\\n79\\nMedium\\nS. Green\\n109\\nHigh\\nN. Cook\\n73\\nMedium\\nW. Hands\\n104\\nHigh\\nP. Rice\\n64\\nLow\\nF. Marsh\\n136\\nHigh\\nTABLE A.3\\nChapter 3, Question 3 Answer\\nName\\nWeight (kg)\\nHeight (m)\\nBMI\\nP. Lee\\n50\\n1.52\\n21.6\\nR. Jones\\n115\\n1.77\\n36.7\\nJ. Smith\\n96\\n1.83\\n28.7\\nA. Patel\\n41\\n1.55\\n17.1\\nM. Owen\\n79\\n1.82\\n23.8\\nS. Green\\n109\\n1.89\\n30.5\\nN. Cook\\n73\\n1.76\\n23.6\\nW. Hands\\n104\\n1.71\\n35.6\\nP. Rice\\n64\\n1.74\\n21.1\\nF. Marsh\\n136\\n1.78\\n42.9\\nTABLE A.4\\nChapter 4, Question 1 Answer\\nStore\\nNew York, NY\\nWashington, DC\\nTotals\\nProduct Category\\nLaptop\\n1\\n2\\n3\\nPrinter\\n2\\n2\\n4\\nScanner\\n4\\n2\\n6\\nDesktop\\n3\\n2\\n5\\nTotals\\n10\\n8\\n18'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 201}, page_content='188\\nANSWERS TO EXERCISES\\nTABLE A.5\\nChapter 4, Question 2a Answer\\nCustomer\\nNumber of Observations\\nSum of Sales Price ($)\\nB. March\\n3\\n1700\\nJ. Bain\\n1\\n500\\nT. Goss\\n2\\n750\\nL. Nye\\n2\\n900\\nS. Cann\\n1\\n600\\nE. Sims\\n1\\n700\\nP. Judd\\n2\\n900\\nG. Hinton\\n4\\n2150\\nH. Fu\\n1\\n450\\nH. Taylor\\n1\\n400\\nTABLE A.6\\nChapter 4, Question 2b Answer\\nStore\\nNumber of Observations\\nMean Sale Price ($)\\nNew York, NY\\n10\\n485\\nWashington, DC\\n8\\n525\\nTABLE A.7\\nChapter 4, Question 2c Answer\\nProduct Category\\nNumber of Observations\\nSum of Profit ($)\\nLaptop\\n3\\n470\\nPrinter\\n4\\n360\\nScanner\\n6\\n640\\nDesktop\\n5\\n295\\n40\\n60\\n80\\n100\\n120\\nProfit ($)\\nSales price ($)\\n140\\n160\\n180\\n200\\n200\\n100\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\n1000\\nFIGURE A.2\\nScatterplot, Chapter 4 question 3.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 202}, page_content='ANSWERS TO EXERCISES\\n189\\nChapter 5\\n1a. 4.8\\n1b. 2.8\\n1c. 0\\n2. 2.24\\n3. Support = 0.47, Confidence = 1, Lift = 1.89\\n4a. 0.83\\n4b. 0.998\\nChapter 6\\n1a. 0.85\\n1b. 0.89\\n1c. 0.82\\n2a. 0.87\\nTABLE A.8\\nChapter 6, Question 2b Answer\\nObservation\\nActual\\nPredicted\\nResidual\\n1\\n13.7\\n12.4\\n1.3\\n2\\n17.5\\n16.1\\n1.4\\n3\\n8.4\\n6.7\\n1.7\\n4\\n16.2\\n15.7\\n0.5\\n5\\n5.6\\n8.4\\n−2.8\\n6\\n20.4\\n15.6\\n4.8\\n7\\n12.7\\n13.5\\n−0.8\\n8\\n5.9\\n6.4\\n−0.5\\n9\\n18.5\\n15.4\\n3.1\\n10\\n17.2\\n14.5\\n2.7\\n11\\n5.9\\n5.1\\n0.8\\n12\\n9.4\\n10.2\\n−0.8\\n13\\n14.8\\n12.5\\n2.3\\n14\\n5.8\\n5.4\\n0.4\\n15\\n12.5\\n13.6\\n−1.1\\n16\\n10.4\\n11.8\\n−1.4\\n17\\n8.9\\n7.2\\n1.7\\n18\\n12.5\\n11.2\\n1.3\\n19\\n18.5\\n17.4\\n1.1\\n20\\n11.7\\n12.5\\n−0.8'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 203}, page_content='190\\nANSWERS TO EXERCISES\\n2b See Table A.8\\n3a. Height = −9.8 + 0.9 Fertilizer\\n3b. 1.3\\n4. $352,600\\n5. Brand B'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 204}, page_content='APPENDIX B\\nHANDS-ON TUTORIALS\\nB.1 TUTORIAL OVERVIEW\\nTraceis 2014 is a software tool for exploratory data analysis and data min-\\ning, designed to be used alongside this book to provide practical experience\\nof the methods described. It includes a number of tools for preparing and\\nsummarizing data, as well as methods for grouping, exploring patterns\\nand trends, and building models. The following sections describe how to\\ninstall and use the Traceis 2014 software and provide a series of hands-on\\nexercises making use of sample data sets.\\nB.2 ACCESS AND INSTALLATION\\nThe\\nTraceis\\n2014\\nsoftware\\ncan\\nbe\\naccessed\\nfrom\\nthe\\nwebsite\\nhttp://www.makingsenseofdata.com/. The software is contained in a zipped\\nfile. Once downloaded, it can be unzipped into a folder on a computer. In\\naddition to downloading the zipped file, a license key to use the software\\ncan be obtained by sending an email to software@makingsenseofdata.com.\\nAn email will be sent to you containing the key, which is simply a number.\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n191'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 205}, page_content='192\\nHANDS-ON TUTORIALS\\nThe associated website (http://www.makingsenseofdata.com/) contains the\\ncurrent minimum requirements for running the software, which can be used\\non a computer with the Java Virtual Machine (JVM) installed. The JVM\\nusually comes installed on most computers; however, it can be downloaded\\nfrom the Java website (http://www.java.com/), if necessary.\\nTo run the software, either double click on the Traceis.jar file or open\\nthe Traceis.jar file from the Java software platform software. The first\\ntime the software runs, you will be asked to enter the license key number\\nmentioned in the email sent. Also contained in the folder with the software\\nis a subfolder called “Tutorial data sets,” which contains sample data sets\\nto use with the software, along with descriptions of the data sets.\\nB.3 SOFTWARE OVERVIEW\\nThe Traceis 2014 software contains a series of tools, such as multiple linear\\nregression or clustering. These tools are available throughout the Traceis\\n2014 user interface. The user interface is divided into five areas, as shown\\nin Figure B.1: “Categories,” “Tools,” “Options,” “Results,” and “Selected\\nobservations.”\\nFIGURE B.1\\nOrganization of the Traceis 2014 user interface.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 206}, page_content='READING IN DATA\\n193\\nTABLE B.1\\nTools Available in the Traceis 2014 Software\\nCategory\\nTools\\nPreparation\\nLoading the data (open), searching the data set (search),\\ncharacterizing variables (characterize), removing\\nobservations and variables (remove), cleaning the data\\n(clean), transforming variables (transform), segmenting the\\ndata set (segment), and principal component analysis (PCA)\\nTables and\\ngraphs\\nContingency table, summary table, graphs, and graph matrices\\nStatistics\\nDescriptive statistics, confidence intervals, t-tests, chi-square\\ntest, ANOVA, and comparative statistics\\nGrouping\\nClustering, association rules, decision trees\\nModels\\nLinear regression, discriminant analysis, logistic regression,\\nNa¨ıve Bayes, k-nearest neighbors (kNN), classification and\\nregression trees (CART), and neural networks\\nThe different types of tools are organized within the four-step pro-\\ncess outlined in Chapter 1 of this book: (1) definition, (2) preparation,\\n(3) analysis, and (4) deployment. The “Categories” options that can be\\nselected are “Prepare,” “Tables and graphs,” “Statistics,” “Grouping,”\\n“Models,” and “Apply models.” In Figure B.1 the “Tables and graphs” cat-\\negory has been selected and the tools available are presented accordingly,\\nas shown in the tabs “Contingency table,” “Summary table,” “Graphs,”\\nand “Graph matrices.” In this example, the “Graphs” tool was selected.\\nThe tools option area of the screen shows the different parameters and\\nsettings for performing or displaying an analysis of the data. For example,\\ndifferent types of graphs along with the graph’s axes have been selected in\\nFigure B.1. The “Results” area of the screen contains the results of a visu-\\nalization or analysis. In Figure B.1 the selected graphs that are displayed\\nare interactive. All cars with four cylinders were selected (by clicking on\\nthe four-cylinder bar in the top left chart) and the selected observations are\\nhighlighted on the other graphs in the display and shown in the selected\\nobservations area of the user interface. Table B.1 shows the tools that can\\nbe selected.\\nB.4 READING IN DATA\\nThe first step is to load data into the system. The data set should be in a\\ntext file and should contain all observations in the data set with informa-\\ntion on all variables. The format follows the conventions of comma- or'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 207}, page_content='194\\nHANDS-ON TUTORIALS\\ntab-separated tabular data files. Each observation should be on a sepa-\\nrate line and the values of the observed properties should be recorded\\nconsistently. The variable names should be on the first line in the file. A\\nspecific separator or delimiter should separate each individual value, such\\nas a comma, tab, or semicolon. The following provides an example of the\\ncontent of a text file containing a data table:\\nShip Name;Cruise Line;Age;Tonnage;Passengers\\nJourney;Azamara;6;30.277;6.94\\nQuest;Azamara;6;30.277;6.94\\nCelebration;Carnival;26;47.262;14.86\\nThe first row contains the column headings (variable names), each sub-\\nsequent row contains the individual observations, and the values are con-\\nsistently separated with semicolons.\\nSelecting the “Open” button, as shown in Figure B.1, initiates the process\\nof loading a data set. Once the file has been located, you will be asked to\\nreview the data table to make sure it is formatted into the correct rows\\nand columns, as shown in Figure B.2. If the data does not look correct,\\nFIGURE B.2\\nPreview of the data to be loaded.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 208}, page_content='PREPARATION TOOLS\\n195\\nyou may override the default assumptions made by the software about\\nthe data format. If no header is contained in the text file, the software will\\nautomatically assign a header to each column (“Variable(1),” “Variable(2),”\\nand so on). Once the data is correctly assigned as rows and columns,\\nclicking on the “OK” button will load the data into the Traceis 2014\\nsoftware.\\nB.5 PREPARATION TOOLS\\nB.5.1 Searching the Data\\nOnce a data set has been loaded, one or more defined queries can be\\nused to search the data set from the “Search” tab. These queries will\\nsearch over specific variables. The queries may include different operators\\n(=, <, >, and ≠), as well as specific values. You may also generate a\\nnew dummy variable from the search results, where a value of one is\\nassigned if an observation satisfied the criteria of the search and zero is\\nassigned otherwise. By checking the box “Generate variable from results”\\nand entering a name, the new variable is generated after the search you\\ninitiate is completed.\\nB.5.2 Variable Characterization\\nOnce loaded, the variables are analyzed and automatically assigned to\\nvarious categories. This assignment can be reviewed by clicking on the\\n“Characterization” tab.\\nB.5.3 Removing Observations and Variables\\nThe “Remove” tab provides ways to remove observations or variables from\\nthe data table. Observations can be selected from most pages and may be\\nremoved by either clicking on the “Delete” button at the top of the user\\ninterface or from the “Remove” tab. Constants or specific variables can\\nalso be removed.\\nB.5.4 Cleaning the Data\\nFor variables containing missing data or non-numeric data, a series of\\noptions are available from the “Clean” tab to “clean” the data. Once a\\nvariable is selected, the following summaries are provided: a count of the'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 209}, page_content='196\\nHANDS-ON TUTORIALS\\nTABLE B.2\\nAvailable Options for Transforming One or More Variables\\nTool\\nOptions\\nNormalization (new range)\\nMin-max, z-score, decimal scaling\\nNormalization (new distribution)\\nlog, −log, Box-Cox\\nNormalization (text-to-numbers)\\nValues for selected variable\\nValue mapping (dummy variables)\\nVariables\\nDiscretization (using ranges)\\nSelect ranges for selected variable\\nDiscretization (using values)\\nSelect new values for existing values, for\\nthe selected variable\\nGeneral transformations\\nSingle variable: x × c, x + c, x ÷ c, c ÷ x,\\nx −c, c −x, x2, x3, and\\n√\\nx\\nTwo variables: mean, minimum,\\nmaximum, x + y, x ÷ y, y ÷ x, x −y, and\\ny −x\\nMore than two variables: mean, minimum,\\nmaximum, sum, and product\\nnumeric observations, a count of non-numeric observations, and a count\\nof observations with missing data. Non-numeric observations in the data\\nmay either be removed or replaced by a numeric value. A similar set of\\noptions are available for handling missing data. Once the variable has been\\nupdated, the changes will be reflected in the results area of the updated\\ntable.\\nB.5.5 Transforming the Data\\nThe “Transform” tab provides a number of options for transforming one or\\nmore variables into a new variable. These tools are summarized in Table B.2\\nand can be selected from the drop-down “Select type of transformation.”\\nIt should be noted that all transformation options generate a new variable\\nand do not replace the original variable(s).\\nThe “Normalization (new range)” option provides three alternatives for\\ntransforming a single variable to a new range: min-max, z-score, and deci-\\nmal scaling. Certain analysis options require that the frequency distribution\\nreflect a normal or bell-shaped curve. The “Normalization (new distribu-\\ntion)” option provides a number of transformations that generates a new\\nfrequency distribution for a variable. The following transformations are\\navailable: log (log base 10 transformation), −log (a negative log base 10\\ntransformation), and Box-Cox.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 210}, page_content='PREPARATION TOOLS\\n197\\nCertain ordinal variables contain text values, and before these variables\\ncan be used within numeric analyses a conversion from the text values to\\nnumeric values must be performed. The “Value mapping (text-to-number)”\\noption provides tools to change each value of the selected variable into a\\nspecific number. To use nominal variables within numeric analyses, the\\nvariables are usually converted into a series of dummy variables. Each\\ndummy variable corresponds to specific values for the nominal variable,\\nwhere one indicates the presence of the value while a zero indicates its\\nabsence. The “Value mapping (dummy variables)” tool can be applied to a\\nvariable containing text values, and it will automatically generate a series\\nof variables.\\nThe “Discretization (using ranges)” option provides tools for convert-\\ning a continuous numeric variable into a series of discrete values. Having\\nselected a single variable, you can set the number of ranges and the lower\\nand upper bounds for each range. Once the ranges are set, this tool substi-\\ntutes the old, continuous numeric variable with the new value associated\\nwith the range in which that variable falls (greater than or equal to the\\nlower bound and less than the upper bound). Additionally, categorical\\nvariables can be transformed to a series with fewer discrete values using\\n“Discretization (using values).” Instead of grouping the values into ranges,\\nthis technique involves grouping the values of selected variables into a\\nlarger set, and assigning all of the observations within that larger set to the\\nnew value. The individual values can either be typed in or selected from a\\ndrop-down containing the alternatives already entered.\\nA series of “General transformations” can be selected in order to per-\\nform mathematical operations on one or more variables. Having selected\\n“General transformation,” one or more variables can be selected. To select\\na single variable, click once on the desired variable name; to select multiple\\nvariables use the <ctrl> + click to add individual items to the selection\\nand <shift> + click to add all items between the current and the initial\\nselection. When a single variable has been selected, the following com-\\nmon mathematical operations are available, where x refers to the selected\\nvariable and c is a specified constant: x × c, x + c, x ÷ c, c ÷ x, x −c, c −x,\\nx2, x3, and\\n√\\nx. When two variables x and y have been selected, in addition\\nto the mean, minimum, and maximum functions, the following mathemat-\\nical transformations are also available: x + y, x ÷ y, y ÷ x, x −y, and y −x.\\nWhen more than two variables are selected, the following operations can\\nbe applied to the values: mean, minimum, maximum, sum, and product.\\nBy combining the results of these transformations, more complex formu-\\nlas can be generated. Figure B.3 shows the generation of a specific new\\nvariable.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 211}, page_content='198\\nHANDS-ON TUTORIALS\\nFIGURE B.3\\nA general transformation applied to a variable.\\nB.5.6 Segmentation\\nIn some situations, to conduct an efficient analysis of the data it may be\\nnecessary to generate a smaller subset of observations. The desired number\\nof observations to be included in the subset should be set from the “Seg-\\nment” tab. The options “Random” and “Diverse” are available for selecting\\na subset of observations. The “Random” option will select the specified\\nnumber of observations randomly and each observation in the original set\\nhas an equal chance of being included in the new set. The “Diverse” option\\nwill identify the desired number of observations that are representative of\\nthe original set. The selection of the diverse set of observations is done\\nby clustering and then choosing from each cluster a representative for the\\ncluster. The clustering is performed using k-means clustering, where k is\\nthe target number of observations in the subset, with the Euclidean distance\\nmetric measuring the distance between pairs of observations; the represen-\\ntative chosen from the cluster is the one closest to the center of the cluster.\\nThere are two options available from the “Select how to create subset”\\ndrop-down to describe what to do with the identified subset: (1) generate\\na new data set containing only the subset (“Remove observations”), or (2)\\ngenerate a new dummy variable where one (1) represents the inclusion\\nof the observation in the new subset, and zero (0) indicates its absence'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 212}, page_content='TABLES AND GRAPH TOOLS\\n199\\n(“Generate dummy variable”). If option (2) is selected, the name of this\\nnew dummy variable should also be entered.\\nB.6 TABLES AND GRAPH TOOLS\\nB.6.1 Contingency Tables\\nA contingency table can be generated from the “Contingency table” option.\\nThe x- and y-axes on the table are both categorical variables, and they can\\nbe selected from the options. Having selected the x- and y-axes, clicking\\non the “Display” button will generate a contingency table in the results\\nwindow. The table shows counts corresponding to pairs of values from\\nthe selected categorical variables. In addition, totals are presented for each\\nrow and column in the table. Each of the cells in the table can be selected,\\nand the observations included in these counts will be displayed in the\\nselected observations panel and highlighted in other views involving those\\nobservations. An example is shown in Figure B.4.\\nFIGURE B.4\\nContingency table generated from a data set.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 213}, page_content='200\\nHANDS-ON TUTORIALS\\nB.6.2 Summary Tables\\nA summary table groups observations using a single categorical variable\\nand provides summarized information about other variables for each of\\nthese groups. A summary table can be generated from the “Summary\\ntable” tab. First, a categorical variable for grouping the observations is\\nselected. An optional count of the number of observations in each group\\ncan also be selected. A number of additional columns can be added to the\\ntable, and this number is set with the “Number of columns” counter. There\\nare seven options for summarizing each selected variable: (1) mean, (2)\\nmode, (3) minimum, (4) maximum, (5) sum, (6) variance, and (7) standard\\ndeviation. Clicking on the “Display” button will generate a summary table\\ncorresponding to the options selected. Individual rows can be selected, and\\nthe resulting observations will be shown in the selected observations panel\\nas well as in other views, as illustrated in Figure B.5.\\nB.6.3 Graphs\\nOne or more graphs can be shown on a single screen to summarize the\\ndata set and these specific graphs are selected from the “Graphs” tab.\\nFIGURE B.5\\nGeneration of a summary table from a data set.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 214}, page_content='TABLES AND GRAPH TOOLS\\n201\\nFIGURE B.6\\nDifferent data graphs generated from the data set.\\nAfter identifying the desired number of graphs to display, a series of\\noptions for each graph is provided. There are four types of graphs: (1)\\nhistogram, (2) scatterplot, (3) box plot, and (4) frequency polygram. In\\naddition, the variable or pair of variables to display should be selected.\\nOnce the collection of graphs to display has been determined, clicking on\\nthe “Display” button will show these graphs in the results area. In each\\nof the graphs, selected observations will be highlighted on all graphs with\\ndarker shading. The histograms, frequency polygrams, and scatterplots\\nare all interactive. For instance, observations can be selected by clicking\\non a histogram bar or a point in the scatterplot or frequency polygram.\\nIn addition, a lasso can be drawn around multiple bars or points. Any\\nselection will be updated on the other graphs in the results area, as well as\\nbeing made available in other analysis options throughout the program, as\\nillustrated in Figure B.6.\\nB.6.4 Graph Matrices\\nThe “Graph matrices” tab generates a matrix of graphs in one view. The\\ntools provide options to display a histogram, scatterplot, or a box plot\\nmatrix for the selected variables. Multiple variables can be selected using\\n<ctrl> + click for non-contiguous variables, <shift> + click for continuous'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 215}, page_content='202\\nHANDS-ON TUTORIALS\\nvariables, and <ctrl> + A for all variables. Clicking on the “Display”\\nbutton will show the matrix in the results area. The histogram and box plot\\nmatrices present the graphs only for the selected variables. In contrast, the\\nscatterplot matrix shows scatterplots for all combinations of the variables\\nselected. The names of the scatterplot axes are shown in the boxes where\\nno graphs are drawn.\\nB.7 STATISTICS TOOLS\\nB.7.1 Descriptive Statistics\\nThe tools available in the “Descriptive” tab will generate a variety of\\ndescriptive statistics for a single variable. For the selected variable, descrip-\\ntive statistics can be generated for (1) all observations, (2) the selected\\nobservations, and (3) observations not selected. Clicking on the “Display”\\nbutton presents the selected descriptive statistics in the results area. For\\neach of the sets of observations selected, a number of descriptive statistics\\nare calculated. They are organized into the following categories: number\\nof observations, central tendency (mode, medium, and mean), variation\\n(minimum value, maximum value, the three quartiles—Q1, Q2, and Q3,\\nvariance, and standard deviation), and shape (estimates of skewness and\\nkurtosis).\\nB.7.2 Confidence Intervals\\nThe “Confidence intervals” analysis calculates an interval estimate for a\\nselected variable that is based on a specific confidence level. In addition,\\nconfidence intervals for groups of observations—defined using a single\\ncategorical variable—can also be displayed. The confidence intervals for\\nthe variables, as well as for any selected groups, can be seen in the results\\narea.\\nB.7.3 t-test\\nThe “t-test” tool will perform a hypothesis test on a single variable. When a\\ncategorical variable is selected, the hypothesis test takes into consideration\\nthe proportion of the selected variable with a specified value. This value\\nmust be set with the “Where x is:” drop-down option, where x is the selected\\ncategorical variable. When a continuous variable is selected, the hypothesis\\ntest uses the mean. The test can take into consideration one or two groups'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 216}, page_content='STATISTICS TOOLS\\n203\\nof observations. When the “single group” option is selected, the members\\nof this group should be defined. The four alternatives for membership\\nare: (1) all observations in the data table, (2) selected observations, (3)\\nobservations not selected, or (4) observations corresponding to a specific\\nvalue of a categorical variable. The confidence level, or alpha, should be\\nselected and set to one of the following: 0.1, 0.05, or 0.01. The hypothesis\\ntest should then be described for the selected observations. The value for\\nthe null hypothesis should be set along with information about whether the\\nvalue for the alternative hypothesis should be greater than, less than, or not\\nequal to value for the null hypothesis.\\nThere are two options when looking at two groups: “Two groups (equal\\nvariance)” and “Two groups (unequal variance).” After the two groups are\\nselected, the members of each group should then be defined. The three\\nalternatives are: (1) selected observations, (2) observations not selected,\\nand (3) observations corresponding to a specific value of a categorical\\nvariable. As before, the confidence level, or alpha, should be selected from\\none of the following: 0.1, 0.05, or 0.01. The specific hypothesis test should\\nbe defined for the selected observations. The choice for null hypothesis will\\nbe either that the two means are equal or the proportions of the two selected\\nvariables are the same. Again, the value of the alternative hypothesis should\\nbe set to either “less than,” “greater than,” or “not equal to” the value of the\\nnull hypothesis. The results of the hypothesis test are presented in the results\\narea. These results include details of the variable and the observations\\nassessed, including the mean value or values, the actual hypothesis test\\nwith confidence level, as well as the hypothesis or z-score, the critical\\nz-score, the p-value, and whether to accept or reject the null hypothesis.\\nB.7.4 Chi-Square Test\\nThe “Chi-Square” option allows for comparison between two categorical\\nvariables that are selected from the two drop-down menus. The results of\\nthe analysis are shown in two contingency tables in the results area. One\\nof the contingency tables includes the actual data, and the second contains\\nexpected results. A Chi-Square assessment is also calculated along with\\nits attained significance level.\\nB.7.5 ANOVA\\nThe “ANOVA” tool assesses the relationship for a particular variable\\nbetween different groups of observations within that variable. The vari-\\nable to assess and the categorical variable used to group the observations'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 217}, page_content='204\\nHANDS-ON TUTORIALS\\nFIGURE B.7\\nComparing variables within a data set.\\nshould be selected and a confidence level, or alpha, should be assigned. The\\nresulting analysis is presented in the results area, which shows the groups\\nidentified using the selected categorical variable, the number of observa-\\ntions in each group, the mean value for each group, and the variance for\\neach group. The mean square within and between is calculated, along with\\nthe F-statistic and p-value.\\nB.7.6 Comparative Statistics\\nThere are a number of ways to calculate metrics showing the strength of the\\nrelationships between combinations of variables. There are four options\\nfor displaying the association between variables: the correlation coefficient\\n(r), the squared correlational coefficient (r2), Kendall Tau, and Spearman\\nRho. For the selected variables the tool will present in one or more tables\\nthe select coefficients for all pairs of variables, as illustrated in Figure B.7.\\nB.8 GROUPING TOOLS\\nB.8.1 Clustering\\nA number of methods for clustering observations are available includ-\\ning agglomerative hierarchical clustering and partitioned (k-means) clus-\\ntering. These clustering methods require the selection of one or more'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 218}, page_content='GROUPING TOOLS\\n205\\nvariables, as well as the selection of a distance measure used to assess\\nthe degree of similarity between observations. For numeric variables\\n(not binary), a number of distance calculations are available including\\nEuclidean; for binary variables, a different set of distance methods are\\nprovided including Jaccard. It should be noted that it is not necessary to\\nnormalize the data to a standard range, as the software will perform this\\nstep automatically.\\nIf the “Agglomerative hierarchical clustering” option is selected, a link-\\nage method must be chosen from the list: average linkage, complete link-\\nage, or single linkage. The assignment of the observations to specific\\nclusters can be stored as a separate column in the table by selecting the\\n“Generate clusters as column(s)” option where a single variable will be\\ngenerated. Having specified the type of clustering required, clicking on\\nthe “Cluster” button will generate the clusters and the results will be sum-\\nmarized in the results area. When agglomerative hierarchical clustering is\\nselected, the results are displayed as a dendrogram showing the hierarchi-\\ncal organization of the data. A vertical line dissects the dendrogram, thus\\ncreating clusters of observations to the right of the vertical line. A rectangle\\nis placed around each cluster and, space permitting, a number indicating\\nthe size of the cluster is annotated on the right. When the data set has a\\nlabel variable, clusters with only a single observation are replaced by the\\nlabel’s value. The cut-off is interactive; it can be moved by clicking on\\nthe square toward the bottom of the cut-off line and moving it to the left\\nor right. Moving the line changes the distance at which the clusters are\\ngenerated, and hence the number of clusters will change as the cut-off line\\nmoves. If the “Generate clusters as column(s)” option is selected, the col-\\numn in the data table describing the cluster membership will also change.\\nAgglomerative hierarchical clustering is illustrated in Figure B.8.\\nIf the “Partitioned (k-means)” clustering option is selected, the number\\nof clusters needs to be specified and the results are presented in a table\\nwhere each row represents a single cluster. The centroid values for each\\ncluster are presented next to the number of observations in the cluster.\\nThis “number of observations” cell in the table can be selected, and those\\nselected observations are displayed in the selected observations panel, as\\nwell as throughout the program.\\nB.8.2 Association Rules\\nThe “Association rules” option will group observations into overlapping\\ngroups that are characterized by “If . . . then . . . ” rules. A set of cat-\\negorical variables can be selected, and specific values corresponding to'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 219}, page_content='206\\nHANDS-ON TUTORIALS\\nFIGURE B.8\\nIllustration of agglomerative hierarchical clustering.\\nthese variables will be used in the generated rules. The software includes\\na “Restrict rules on the THEN-part” option, which will only result in rules\\nwhere the THEN-part incorporates the selected variable. Also, the “Restrict\\nrules by specific value” function allows for the selection of an appropriate\\nvalue from the drop-down list. This option is particularly useful when the\\nrules are being generated from a series of dummy variables, and only rules\\nwith values of “one” (1) contain useful information. Generating rules with\\na minimum value for support, consequence, and lift can also be set. The\\nresulting rules are shown as a table in the results area, where the “IF-part”\\nof the rule is shown in the first column, and the “THEN-part” of the rule is\\nshown in the second column. Other columns display a count of the number\\nof observations from which the rule is derived. The table also displays\\nvalues for support, consequence, and lift. The table can be sorted by any\\nof these columns. Selecting a single row will display the observations in\\nthe selected observations panel (as illustrated in Figure B.9), and those\\nobservations will be highlighted throughout the program.\\nB.8.3 Decision Trees\\nA decision tree can be built from a data table using the “Decision tree”\\noption. Any number of variables can be used as independent variables and a\\nsingle variable should be assigned as the response. In addition, a minimum'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 220}, page_content='MODELS TOOLS\\n207\\nFIGURE B.9\\nIllustration of the association rules results.\\ntree node size should be set which prevents the software from generating\\na tree with fewer nodes than this specified value. Once a decision tree\\nhas been built, the results will be shown in the results area. The nodes\\nof the decision tree represent sets of observations, which are summarized\\nwith a count, along with the average value for the response variable (if the\\nresponse variable is continuous). For categorical response variables, the\\nnumber of observations in the set is shown, along with the most common\\nvalue qualified by the number of occurrences of that value compared to the\\ntotal number of nodes. In both trees, the criteria used to split the trees are\\nindicated just above the node. Oval nodes represent non-terminal nodes,\\nwhereas rectangular nodes represent terminal nodes. The decision trees are\\ninteractive; each node can be selected, and the selected observations can be\\nseen below the tree as well as in other views, as illustrated in Figure B.10.\\nB.9 MODELS TOOLS\\nB.9.1 Linear Regression\\nTools for building multiple linear regression models are found under the\\n“Linear regression” tab. Any number of independent variables can be\\nselected using <ctrl> + click to select non-contiguous individual variables\\nand <shift> + click to select a continuous range of variables. A single'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 221}, page_content='208\\nHANDS-ON TUTORIALS\\nFIGURE B.10\\nIllustration of view with decision tree results.\\ncontinuous response variable should also be selected. The cross-validation\\npercentage should be set to indicate the proportion of observations to set\\naside for testing. To further analyze the results, a series of new variables can\\nbe generated: Prediction, CV Prediction, and Order. A final multiple linear\\nregression model will be built from the entire data set, and the Prediction\\nvariable will have a prediction of all observations from this model. CV\\nPrediction represents the cross-validated prediction, where the predicted\\nvalues are calculated using a model built from other observations. Order\\ncontains a number for each observation reflecting the order in which the\\nobservation appears in the data set.\\nOnce a model is built, the results are displayed in the results area.\\nThe independent variables and the response variable are initially summa-\\nrized, including the model coefficients and the significance of the coeffi-\\ncients. The software provides a regression analysis summarizing the model\\naccuracy, including R-squared, adjusted R-squared, and standard error. An\\nANOVA analysis is generated showing the degrees of freedom (df) of the\\nregression and the residual, along with the mean square (MS), the sum of\\nsquares (SS), the F-statistic, and the p-value (as shown in Figure B.11). To\\nevaluate the model in more detail, a residual variable can be generated using\\nthe “Transform” tab option under “Preparation.” To create a residual vari-\\nable, first select a “General transformation” and select the actual response\\nand the prediction, and then select “Actual–prediction.” This data can be\\nplotted in the “Graphs” tab to analyze the model further. Once a model is'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 222}, page_content='MODELS TOOLS\\n209\\nFIGURE B.11\\nIllustration of the results of running a linear regression model.\\nbuilt and evaluated, it can be saved permanently. The model is saved by\\nclicking on the “Save model” button. A file name should be provided and\\nthe model will be saved to a file for future use with other data sets.\\nB.9.2 Logistic Regression\\nThe “Logistic regression” option enables the generation of a logistic regres-\\nsion model which can only be built for binary response variables. Numeric\\nvariables can be used as independent variables. A logistic regression model\\ngenerates an expected value of the y-variable or probability, and the clas-\\nsification prediction is generated from this probability using a specified\\nthreshold value or by automatically generating a cut-off which is a good bal-\\nance of sensitivity and specificity. Observations above this threshold value\\nwill be assigned a prediction of one, and those below will be assigned\\na prediction of zero. In addition to the cross-validated percentage to set\\naside, a number of predicted values can be optionally generated by select-\\ning the “Generate results” option: “Prediction,” “Prediction prob.,” “CV\\nPrediction,” and “CV Prediction prob.” The “Prediction” is the 0 or 1\\nclassification using the final model with the “Prediction prob.” being the\\nprobability calculated. “CV Prediction” and “CV Prediction prob.” report\\nthe same information as part of the cross-validation. Once a model has\\nbeen built, the results are displayed in the results area. The independent\\nvariables and the response variable are initially summarized, along with'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 223}, page_content='210\\nHANDS-ON TUTORIALS\\nFIGURE B.12\\nIllustration of the results from a logistic regression.\\nthe model coefficients and their significance. A summary of the cross-\\nvalidated results is also presented. Figure B.12 illustrates the results from\\na logistic regression model. Generated models can be saved for use with\\nother data sets using the “Save model.”\\nB.9.3 k-Nearest Neighbor\\nThe “kNN” analysis tab lets you build k-nearest neighbor models. Models\\ncan be built for any type of response variable, and any type of numeric\\nvariable can be selected as an independent variable. It is not necessary\\nto normalize the data to a standard range as the software will do this\\nautomatically. The distance metric should be selected from the drop-down\\nmenu. A value for k can be set manually. Alternatively, a range can be\\nspecified which instructs the Traceis software to build all models between\\nthe lower and upper bound and from these select the model with the smallest\\nerror. In addition to the cross-validated percentage to set aside, a number\\nof predicted values can be optionally generated by selecting the “Generate\\nresults” option: “kNN-Pred” and “kNN-Pred(CV),” which are the predicted\\nvalues for the final model, along with the predicted values from the cross-\\nvalidated models. Once a model has been built, the results are displayed in'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 224}, page_content='EXERCISES\\n211\\nthe results area. The independent variables and the response variable are\\nshown, as well as the value for the k-nearest neighbor parameter that was\\neither set manually or automatically derived. A summary of the standard\\ncross-validated results is presented. Models that are generated can be saved\\nfor future use with other data sets, using the “Save model” button.\\nB.9.4 CART\\nThe “CART” analysis tab is used to build models based on either a regres-\\nsion tree or a classification tree. Models can be built for any type of response\\nvariable or independent variables. Decision trees are generated for the mod-\\nels. If the “Minimum node size” is set, the process that generates decision\\ntree will prune from the tree nodes that are smaller than the specified size.\\nOther predicted values can be optionally generated by selecting the “Gen-\\nerate results” option: “CART-Pred” and “CART-Pred(CV).” Once a model\\nhas been built, the results are displayed in the results area. The indepen-\\ndent variables along with the response variable are shown, as well as the\\nvalues used for minimum node size, where size is a count of the number\\nof observations at each node. A summary of the standard cross-validated\\nresults is presented. Models that are generated can be saved for later use\\nwith other data sets, using the “Save model” button.\\nB.10 APPLY MODEL\\nModels that are built and saved can be used with a new data set by selecting\\nthe “Apply model” option under the “4. Deployment” step. When the model\\nand the new data set are opened, a summary of the model and the data is\\nshown. Selecting the “Apply” button will generate a prediction for the\\nobservations in the new data set. The column headings must match those\\nused to build the model. In addition, the ranges of the variables must be\\nwithin the variable ranges used to build the model, otherwise a prediction\\ncannot be generated.\\nB.11 EXERCISES\\nB.11.1 Overview\\nThe following four exercises are available to use with the Traceis software\\nor other available software. The data set for use with this hands-on tutorial\\nare available from the tutorials folder. In addition, a series of other data\\nsets is available to use with the Traceis 2014 software.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 225}, page_content='212\\nHANDS-ON TUTORIALS\\nB.11.2 Exercise 1: Analysis of Recycling Data\\nThe data set contains 31 observations, where each observation represents\\na Scottish local authority from Baird et al. (2013). The file contains five\\nvariables (1) Local authority, (2) Recycling capacity (in liters/week), (3)\\nResidual Capacity (in liters/week), (4) Extended materials (number of\\nextended materials collected), and (5) Yield of extended materials (in\\nkg/hh/wk). The data set is available in the tutorials folder under the name\\nscottish_recycle.txt.\\nThe objective of this exercise is to generate a regression model that\\npredicts the Yield of extended materials.\\nStep 1: Load the data set into Traceis 2014 software by clicking on the\\n“Open” button and find the file scottish_recycle.txt in the tutorials folder.\\nSelect “OK” from the preview window and you should be able to see the\\ndata as in Figure B.13.\\nStep 2: Characterize the variables based on the scales over which they\\nwere measured.\\nStep 3: Plot the frequency histogram (from the “Graphs” panel) and\\ngenerate summary statistics (from the “Descriptive” statistics panel) for\\neach numeric variable, generating the information shown in Figure B.14.\\nFIGURE B.13\\nData loaded into the Traceis software.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 226}, page_content='20\\n15\\n10\\nFrequency\\n5\\n0\\n20\\n15\\n10\\nFrequency\\n5\\n0\\n20\\n15\\n10\\nFrequency\\n5\\n0\\n20\\n40\\n60\\n80\\nRecycling capacity\\n100 120 140 160\\n90\\n120\\nResidual capacity\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nExtended materials\\n20\\n15\\n10\\nFrequency\\n5\\n00.5 1\\n1.5\\n2 2.5 3 3.5 4 4.5\\nYield of extended materials\\n180\\n240\\nFIGURE B.14\\nFrequency histogram and descriptive statistics for all four variables.\\n213'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 227}, page_content='214\\nHANDS-ON TUTORIALS\\nRecycling capacity\\nResidual capacity\\nExtended materials\\nYield of extended\\nmat.\\nFIGURE B.15\\nScatterplot matrix and correlation coefficients.\\nStep 4: Look at the relationship between the variables using the scatter-\\nplot matrix (“Graph matrices”) and the correlation statistics (r) (“Compar-\\native statistics”), as shown in Figure B.15.\\nStep 5: Build a series of linear regression models (from models “Linear\\nRegression”) using all combinations of independent variables (Recycling\\ncapacity, Residual capacity, and Extended materials) to predict Yield of\\nextended materials and select the best performing, simplest model (as seen\\nin Figure B.16). Check the “Generate results” option and click on “Save”\\nto save this model.\\nStep 6: Calculate a residual value (from “Transform,” which is an option\\nunder the “Prepare” category). Choose “General transformations,” select\\nthe Prediction variable as well as the actual response variable (Yield of\\nextended materials) using <ctrl> + click for non-contiguous variables\\nselection and select the specific transformation of “Yield of extended\\nmaterial—Prediction.” Name the new variable “Residual” as shown in\\nFigure B.17. Look at the different graphs (from the “Graphs” tab), as\\nshown in Figure B.18, to test the model assumptions.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 228}, page_content='EXERCISES\\n215\\nFIGURE B.16\\nLinear regression model built.\\nFIGURE B.17\\nCalculating a residual value.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 229}, page_content='216\\nHANDS-ON TUTORIALS\\n1\\n–1\\n–0.8\\n–0.6\\n–0.4\\n–0.2\\n0\\n0.2\\nResidual\\n0.4\\n0.6\\n0.8\\n1\\n1.5\\n2\\n2.5\\nPrediction\\n3\\n3.5\\n4\\n4.5\\n1\\n5\\n10\\n15\\nOrder\\n20\\n25\\n30\\n35\\n–1\\n–0.8\\n–0.6\\n–0.4\\n–0.2\\n0\\n0.2\\nResidual\\n0.4\\n0.6\\n0.8\\n1\\nFIGURE B.18\\nTesting linear regression assumptions.\\nStep 7: Apply the saved model as shown in Figure B.19, using either\\na hypothetical test set with the same column names or the original\\ndata set.\\nB.11.3 Exercise 2: Analysis of Gold Deposit Data\\nThis data set contains 64 observations concerning whether a gold deposit\\nwas identified within 0.5 km (Gold deposit proximity) (Sahoo & Pandalai,\\n1999). For this variable, the values are 1 if a gold deposit was identified\\nand 0 if not. Other variables were measured that will be used to predict'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 230}, page_content='EXERCISES\\n217\\nFIGURE B.19\\nApplying a saved model.\\nwhether a gold deposit is within 0.5 km: As level, Sb level, and Lineament\\nproximity.\\nThe objective of this exercise is to develop a classification model to\\npredict Gold deposit proximity from any combination of the collected data\\n(As level, Sb level, Lineament proximity).\\nStep 1: Load the data into the Traceis software by selecting the\\ngold_target1.txt from the tutorials folder.\\nStep 2: Explore and determine the scales over which the variables are\\nmeasured.\\nStep 3: Look at the frequency distribution and the descriptive statistics\\nfor the variables in the data, as shown in Figure B.20, using the “Graphs”\\nand “Descriptive” statistics tools.\\nStep 4: Since the As level and Sb level variables do not follow a normal\\ndistribution, perform a log transformation on the values of these two vari-\\nables (from the “Transform” tool using “Normalization (new distribution)”\\noption) and then re-examine the new frequency distributions (as illustrated\\nin Figure B.21).\\nStep 5: Look at the relationships between Gold deposit proximity and\\nLineaments proximity using the “Contingency table” tool. Next, explore\\nthe relationship between Gold deposit proximity and As level and Sb\\nlevel by first creating a discretized version of the log (As level) and log'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 231}, page_content='60\\n45\\nFrequency\\n30\\n15\\n0\\n0\\n60\\n45\\nFrequency\\n30\\n15\\n0\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n5\\n10\\n15\\n20\\nAs level\\nSb level\\n40\\n30\\nFrequency\\n20\\n10\\n0\\n0\\n1\\nLineament proximity\\n40\\n30\\nFrequency\\n20\\n10\\n0\\n0\\n1\\nGold deposit proximity\\n25\\n30\\n35\\n40\\n45\\nFIGURE B.20\\nFrequency distribution and descriptive statistics for four variables.\\n218'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 232}, page_content='EXERCISES\\n219\\n60\\n45\\n30\\nFrequency\\n15\\n0\\n0\\n5\\n10\\n15\\n20\\nAs level\\n20\\n15\\n10\\nFrequency\\n5\\n0\\n–1.5\\n–1\\n–0.5\\n0\\n0.5\\n1\\n1.5\\n2\\nlog(As level)\\n25\\n30\\n35\\n40\\n45\\nFIGURE B.21\\nFrequency distribution for As level and Sb level as well as log\\ntransformed As level and Sb level.\\n(Sb level) variables using the “Transform” tool with the “Discretization\\n(using ranges)” options, and then using the “Summary table” tool to create\\na summary of how the mean Gold deposit proximity values change as the\\nAs and Sb levels increase. Figure B.22 illustrates the three graphs.\\nStep 6: Build a series of logistic regression models (from the “Logistic\\nregression” tool) to predict Gold deposit proximity from all combinations\\nof: Lineament proximity, log (As level), and log (Sb level), as illustrated\\nin Figure B.23. Select the simplest, best performing model and save the\\nmodel.\\nB.11.4 Exercise 3: Analysis of Morphologic Difference across\\nthe Iris Plant Species\\nThis data set contains 150 observations concerning morphologic differ-\\nences across three species of the Iris flower (class): “Iris setosa,” “Iris\\nvirginica,” and “Iris versicolor” from Fisher (1936). In biology, morphol-\\nogy refers to the study of living organisms through their form and structure.\\nThe objective of this analysis is to understand how the three species\\nare characterized by the morphologic variations: sepal width (cm), sepal\\nlength (cm), petal width (cm), and petal length (cm). These will be your\\nmorphological variables.\\nStep 1: Open the IRIS.txt file located in the tutorials directory.\\nFIGURE B.22\\nExploring the relationships to gold deposit proximity.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 233}, page_content='220\\nHANDS-ON TUTORIALS\\nFIGURE B.23\\nLogistic regression model built to predict gold deposit proximity.\\nStep 2: Characterize the scales over which the variables are measured\\nand calculate descriptive statistics and frequency distributions for all mea-\\nsured variables (as shown in Figure B.24).\\nStep 3: Generate three scatterplot matrices using the measured morpho-\\nlogic variables where each matrix is highlighted with a different species\\nof the Iris flower, as shown in Figure B.25. The histogram of the Iris\\nflower species can be displayed from the “Graphs” tool and highlighted by\\nclicking on the histogram bar. The “Graph matrices” tool can be used to\\ngenerate the scatterplot matrix where any selected set of observations will\\nalso be highlighted.\\nStep 4: Cluster the data set with agglomerative hierarchical clustering\\n(using Euclidean distance to measure similarity between observations and\\naverage linkage to determine how clusters are formed) and set the cut-off\\nvalue such that there are three clusters generated. Figure B.26 shows how\\nthe scatterplot matrices and the species histogram are highlighted when\\nyou select each of the three clusters (as shown in the first row of graphs in\\nFigure B.26).\\nStep 5: Build a classification tree (with the “Decision tree” tool) using\\nthe four measured properties and the response (class) to guide how the tree\\nwas generated, using a minimum of 30 observations per node (as seen in\\nFigure B.27).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 234}, page_content='40\\n30\\n20\\nFrequency\\n10\\n0\\n4\\n4.5\\n5\\n5.5\\n6\\nSepal length (cm)\\n60\\n45\\n30\\nFrequency\\n15\\n0\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nPetal length (cm)\\n40\\n30\\n20\\nFrequency\\n10\\n0\\n2 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6\\n4\\n3.8\\n4.2 4.4\\nSepal width (cm)\\n40\\n30\\n20\\nFrequency\\n10\\n0\\n2 0.20.40.60.8 1 1.2 1.4 1.6 1.8 2 2.22.42.6\\nPetal width (cm)\\n6.5\\n7\\n7.5\\n8\\nFIGURE B.24\\nFrequency histogram and descriptive statistics for all four measured variables from the Iris data set.\\n221'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 235}, page_content='60\\n45\\n30\\nFrequency\\n15\\nIris\\nsetosa\\nIris\\nversicolor\\nClass\\nIris\\nvirginica\\nIris\\nversicolor\\nIris\\nvirginica\\n0\\nsepal length\\n(cm)\\nsepal width\\n(cm)\\npetal length\\n(cm)\\npetal width\\n(cm)\\nsepal length\\n(cm)\\nsepal width\\n(cm)\\npetal length\\n(cm)\\npetal width\\n(cm)\\nsepal length\\n(cm)\\nsepal width\\n(cm)\\npetal length\\n(cm)\\npetal width\\n(cm)\\n60\\n45\\n30\\nFrequency\\n15\\nIris\\nsetosa\\nClass\\nIris\\nversicolor\\nIris\\nvirginica\\nClass\\n0\\n60\\n45\\n30\\nFrequency\\n15\\nIris\\nsetosa\\n0\\nFIGURE B.25\\nSpecies of the Iris flower highlighted on the scatterplot matrix.\\n222'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 236}, page_content='60\\n45\\n30\\nFrequency\\n15\\nIris\\nsetosa\\nIris\\nversicolor\\nClass\\nIris\\nvirginica\\n0\\n60\\n45\\n30\\nFrequency\\n15\\nIris\\nsetosa\\nIris\\nversicolor\\nClass\\nIris\\nvirginica\\n0\\n60\\n45\\n30\\nFrequency\\n15\\nIris\\nsetosa\\nIris\\nversicolor\\nClass\\nIris\\nvirginica\\n0\\nsepal length\\n(cm)\\nsepal width\\n(cm)\\n34\\n66\\n50\\n34\\n66\\n50\\n34\\n66\\n50\\npetal length\\n(cm)\\npetal width\\n(cm)\\nsepal length\\n(cm)\\nsepal width\\n(cm)\\npetal length\\n(cm)\\npetal width\\n(cm)\\nsepal length\\n(cm)\\nsepal width\\n(cm)\\npetal length\\n(cm)\\npetal width\\n(cm)\\nFIGURE B.26\\nVisualization of the clustering results.\\n223'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 237}, page_content='224\\nHANDS-ON TUTORIALS\\nFIGURE B.27\\nDecision tree generated to classify the Iris plant species.\\nB.11.5 Exercise 4: Analysis of Census Data\\nThis data set was collected from the 1994 census data and includes obser-\\nvations on individuals making either less than or greater than $50K per\\nyear. These measurements are captured in the variable salary from Kohavi\\n& Becker (1994). There are 32,561 records with a set of variables contain-\\ning measurements that include the following: age, workclass, education,\\neducation-num, and occupation.\\nThe objective of this exercise is to characterize the differences between\\nindividuals making less than $50K and those making greater than $50K.\\nStep 1: Load the adult data set file Adult.txt located in the tutorials\\ndirectory.\\nStep 2: Calculate new values for both the education-num and age vari-\\nables that contain categories based on ranges, as shown in Figures B.28\\nand B.29.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 238}, page_content='EXERCISES\\n225\\nFIGURE B.28\\nGeneration of a new variable with discretized values for the\\neducation-num.\\nFIGURE B.29\\nGeneration of a new variable with discretized values for the age.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 239}, page_content='226\\nHANDS-ON TUTORIALS\\nFIGURE B.30\\nAssociation rules generated from the adult data set.\\nStep 3: Generate association rules by selecting the “Association rules”\\ntool, and selecting the following variables: workclass, occupation, salary,\\nage (discretized), and education-num (discretized), with “Restrict rule on\\nTHEN-part” set to salary, “Minimum support” set to 15, “Minimum con-\\nfidence” to 0.8, and “Minimum lift” to 1.0 (as shown in Figure B.30).\\nStep 4: Click on the individual rules to view the underlying data (as\\nshown in Figure B.30).'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 240}, page_content='BIBLIOGRAPHY\\nAgresti A (2013). Categorical Data Analysis, 3rd edn. John Wiley & Sons, Inc.,\\nHoboken, NJ.\\nAlreck PL, Settle RB (2003). The Survey Research Handbook, 3rd edn. McGraw-\\nHill/Irwin.\\nAnderson DR, Sweeney DJ, Williams TA (2010). Statistics for Business and\\nEconomics. South-Western College Publishing.\\nAntony J (2003). Design of Experiments for Engineers and Scientists. Butterworth-\\nHeinemann, Oxford.\\nBache K, Lichman M (2013). UCI Machine Learning Repository, http://archive.\\nics.uci.edu/ml (accessed December 10, 2013). School of Information and Com-\\nputer Science, University of California, Irvine, CA.\\nBaird J, Curry R, Reid T (2013). Development and application of a multiple linear\\nregression model to consider the impact of weekly waste container capacity on\\nthe yield from kerbside recycling programmes in Scotland. Waste Management\\n& Research, Vol. 31, pp. 306–314.\\nBarrentine LB (1999). An Introduction to Design of Experiments: A Simplified\\nApproach. ASQ Quality Press, Milwaukee, WI.\\nBerkun S (2005) The Art of Project Management. O’Reily Media Inc., Sebastopol,\\nCA.\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n227'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 241}, page_content='228\\nBIBLIOGRAPHY\\nChapman P, Clinton J, Kerber R, Khabaza T, Reinartz T, Shearer C, Wirth R\\n(2000).\\nftp://ftp.software.ibm.com/software/analytics/spss/support/Modeler/\\nDocumentation/14/UserManual/CRISP-DM.pdf\\n(accessed\\nDecember\\n10,\\n2013).\\nCochran WG, Cox GM (1999). Experimental Designs, 2nd edn. John Wiley &\\nSons, Inc.\\nCristianini N, Shawe-Taylor J (2000). An Introduction to Support Vector Machines\\nand Other Kernel-Based Learning Methods. Cambridge University Press.\\nDasu T, Johnson T (2003). Exploratory Data Mining and Data Cleaning. John\\nWiley & Sons, Inc., Hoboken, NJ.\\nDonnelly RA (2007). Complete Idiot’s Guide to Statistics, 2nd edn. Alpha Books,\\nNew York.\\nEveritt BS, Landau S, Leese M, Stahl D (2011). Cluster Analysis, 5th edn. John\\nWiley & Sons, Ltd.\\nFausett L (1993). Fundamentals of Neural Networks: Architecture, Algorithms,\\nand Applications. Pearson.\\nFielding AH. (2007). Cluster and Classification Techniques for the Biosciences.\\nCambridge University Press.\\nFisher RA (1936). The use of multiple measurements in taxonomic problems.\\nAnnals of Eugenics, Vol. 7, No. 2, pp. 179–188.\\nFowler FJ (2009). Survey Research Methods (Applied Social Research Methods),\\n4th edn. Sage Publications, Inc., Thousand Oaks, CA.\\nFreedman D, Pisani R, Purves R (2007). Statistics, 4th edn. W.W. Norton,\\nNew York.\\nGold target data (1999). http://www.stat.ufl.edu/∼winner/data/gold target1.dat\\n(accessed December 10, 2013).\\nGuidici P, Figini S (2009). Applied Data Mining for Business and Industry\\n(Statistics in Practice). John Wiley & Sons, Ltd.\\nHan J, Kamber M, Pei J (2012). Data Mining: Concepts and Techniques, 3rd edn.\\nMorgan Kaufmann Publishers.\\nHand DJ, Mannila H, Smyth P (2001). Principles of Data Mining. MIT Press.\\nHastie T, Tibshirani R, Friedman JH (2009). The Elements of Statistical Learning:\\nData Mining, Inference, and Prediction. Springer, New York.\\nHosmer DW, Lemeshow S, Sturdivant RX (2013). Applied Logistic Regression,\\n3rd edn. John Wiley & Sons.\\nHsu J (1996). Multiple Comparisons: Theory and Methods. Chapman & Hall/CRC.\\nIRIS Flower data (1936). http://archive.ics.uci.edu/ml/datasets/Iris (accessed\\nDecember 10, 2013).\\nJackson JE (2003). A User’s Guide to Principal Components. John Wiley & Sons,\\nInc., Hoboken, NJ.\\nJolliffe IT (2002). Principal Component Analysis, 2nd edn. Springer-Verlag,\\nNew York.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 242}, page_content='BIBLIOGRAPHY\\n229\\nKachigan SK (1991). Multivariate Statistical Analysis: A Conceptual Introduction,\\n2nd edn. Radius Press, New York.\\nKerzner H (2013). Project Management: A Systems Approach to Planning,\\nScheduling and Controlling, 9th edn. John Wiley & Sons.\\nKimball R, Ross M (2013). The Data Warehouse Toolkit: The Complete Guide to\\nDimensional Modeling, 2nd edn. Wiley Publishing Inc., Indianapolis, IN.\\nKleinbaum DG, Kupper LL, Nizam A, Rosenberg ES (2013). Applied Regression\\nAnalysis and Other Multivariate Methods, 5th edn. Cengage Learning.\\nKohavi R, Becker B (1994). http://archive.ics.uci.edu/ml/datasets/Adult (accessed\\nDecember 10, 2013).\\nLevine DM, Stephan DF (2010). Even You Can Learn Statistics: A Guide for\\nEveryone Who Has Ever Been Afraid of Statistics, 2nd edn. Pearson Education,\\nInc.\\nLindoff G, Berry MJA (2011). Data Mining Techniques for Marketing, Sales and\\nCustomer Support, 2nd edn. Wiley Publishing, Inc., Indianapolis, IN.\\nMontgomery DC (2012). Design and Analysis of Experiments, 8th edn. John Wiley\\n& Sons, Inc.\\nMyatt GJ, Johnson WP (2009). Making Sense of Data II: A Practical Guide to\\nData Visualization, Advanced Data Mining Methods, and Applications. John\\nWiley & Sons.\\nOppel A (2011). Databases DeMYSTiFieD, 2nd edn. McGraw-Hill/Osborne.\\nPearson RK (2005). Mining Imperfect Data: Dealing With Contamination and\\nIncomplete Records. Society of Industrial and Applier Mathematics.\\nProject Management Institute (2013). A Guide to the Project Management Body\\nof Knowledge (PMBOK Guides), 5th edn. Project Management Institute.\\nPyle D (1999). Data Preparation for Data Mining. Morgan Kaufmann.\\nRea LM (2005). Designing and Conducting Survey Research: A Comprehensive\\nGuide. Jossey-Bass.\\nRohanizadeh SS, Moghadam, MB (2009). A proposed data mining methodology\\nand its application to industrial procedures. Journal of Industrial Engineering,\\nVol. 4, pp. 37–50.\\nRud OP (2000). Data Mining Cookbook: Modeling Data for Marketing, Risk, and\\nCustomer Relationship Management. Wiley.\\nSahoo NR, Pandalai HS (1999). Integration of sparse geologic information in\\ngold targeting using logistic regression analysis in the Hutti-Maski Schist Belt,\\nRaichur, Karnataka, India—a case study. Natural Resources Research, Vol. 8,\\nNo. 3, pp. 233–250.\\nScottish\\nrecycle\\ndata\\n(2013).\\nhttp://www.stat.ufl.edu/∼winner/data/scottish\\nrecycle.dat (accessed December 10, 2013).\\nTufte ER (1990). Envisioning Information. Graphics Press.\\nTufte ER (1997a). Visual Explanations: Images and Quantities, Evidence and\\nNarrative. Graphics Press.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 243}, page_content='230\\nBIBLIOGRAPHY\\nTufte ER (1997b). Visual and Statistical Thinking: Displays of Evidence for Mak-\\ning Decisions. Graphics Press.\\nTufte ER (2001). The Visual Display of Quantitative Information, 2nd edn. Graph-\\nics Press.\\nTufte ER (2006). Beautiful Evidence. Graphics Press.\\nUrdan TC (2010). Statistics in Plain English. Routledge, Taylor & Francis Group.\\nVickers A (2010). What Is a p-Value Anyway? 34 Stories to Help You Actually\\nUnderstand Statistics. Addison-Wesley.\\nWestfall P, Hochberg Y, Rom D, Wolfinger R, Tobias R (1999). Multiple Com-\\nparisons and Multiple Tests Using the SAS System. SAS Institute.\\nWinner L (2013). http://www.stat.ufl.edu/∼winner/data/cruise ship.dat (accessed\\nDecember 10, 2013).\\nWitte RS, Witte JS (2009). Statistics, 9th edn. Wiley.'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 244}, page_content='INDEX\\nAccuracy, 145\\nAgglomerative hierarchical clustering,\\n93–105, 204–205\\nAlternative hypothesis, 40. See also\\nHypothesis testing\\nAnalysis of data, see Data analysis\\nAnalysis of variance, see One-way\\nanalysis of variance\\nAntecedent in decision tree rule, 117. See\\nalso Association rules\\nArtificial neural network, see Neural\\nnetwork\\nAssociation rules, 85, 111–122, 205–206\\nantecedent part of, 117\\nconfidence measure of, 116–120\\nconsequence part of, 117\\nextraction of, 115–116\\ngrouping of, 113–115\\nlift measure of, 116–120\\nsupport measure of, 116–120\\nAssumptions, linear regression, 158–160\\nhomoscedasticity of errors, 160\\nindependence, 160\\nlinearity, 158\\nnormality of error distribution, 159\\nAverage, see Mean\\nBar chart, 25–26. See also Graphs\\nBimodal distribution, 28. See also\\nDistributions\\nBinary, see Variables, binary\\nBlack-box, see Model, transparency\\nBox-and-whisker plots, see Box plots\\nBox plots, 30–32. See also Graphs\\nBrushing, 85\\nCentral limit theorem, 38\\nCentral tendency, 22–24\\nmean, 24\\nmedian, 23\\nmode, 23\\nCharts, see Graphs\\nChi-square, 79–81, 203. See also\\nHypothesis tests\\nMaking Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining,\\nSecond Edition. Glenn J. Myatt and Wayne P. Johnson.\\n© 2014 John Wiley & Sons, Inc. Published 2014 by John Wiley & Sons, Inc.\\n231'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 245}, page_content='232\\nINDEX\\nClassification, 142\\nmodels, 142–143. See also Model\\nand regression trees (CART), see\\nDecision trees\\ntrees, 172–178, 211\\nCleaning data, 48–49, 195–196\\nClustering, 85, 88–111, 204–205. See also\\nGrouping\\nagglomerative hierarchical, 93–105,\\n205\\nk-means, 105–111, 205\\nCoefficient of determination, 155–156\\nConcordance, 145\\nConfidence, 116–120. See also\\nAssociation rules\\nConfidence interval, 36–39, 202\\nConsequence, 117. See also Association\\nrules\\nConstant, 22, 49, 195, 197\\nContingency tables\\nin Traceis software, 199\\nfor validating models, 146\\nfor visualizing relationships, 67–68\\nCorrelation coefficient (r), 69–71, 204\\nCRISP-DM, 16\\nCross-classification tables, 67–68, 199.\\nSee also Contingency tables\\nCross-disciplinary teams, 6–7\\nCross-validation, 144–145\\nk-fold, 144–145\\nleave-one-out, 145\\nCustomer relationship management\\n(CRM), 3\\nData\\ncleaning, see Data preparation\\ndescribing, 17\\nsegmentation of, see Data preparation\\ntransformation of, see Data preparation\\nuse of, 1\\nData analysis, 3–13\\nproblems in, 3–8\\nprocesses of, 3–13\\nData matrix, see Data table\\nData mining process, 3–13\\nData preparation, 4, 8–9, 13–14, 47–57\\ncleaning, 48–49, 195–196\\ncombining variables, 54\\nconverting continuous data to\\ncategories, 53\\nconverting text to numbers, 52–53\\ndata transformation, 49–52\\nremoving variables, 49\\nsegmentation, 54–55\\nData sets, see Data tables\\nData sources, 2–3\\nData tables, 18–20\\nData transformation\\nBox–Cox, 52\\nexponential, 52\\nlog, 52\\nData warehouse, 3\\nDecision trees, 85, 122–138, 206–207\\nchild node, 126\\nparent nodes, 126\\nscoring spilts for categorical response,\\n128–131\\nscoring splits for continuous response,\\n130, 134–136\\nsplitting criteria, 128–136\\nDendrogram, 97–105. See also\\nAgglomerative hierarchical\\nclustering\\nDeployment of project, 4, 11–13\\nDescriptive statistics, 17, 202\\nDiscriminant analysis, 179. See also\\nModel\\nDistance matrix, 94–96\\nDistance metrics, see Similarity measures\\nDistributions, 24–36\\nbimodal, 28\\nfrequency, 25–29, 34–36\\nnormal, 28\\nsampling, 37\\nstandard z-, 39\\nDiverse set, 198\\nEnterprise Resource Planning (ERP), 3\\nEntropy, 132–133\\nErrors\\nin classification models, 146\\nfalse negatives, 146\\nfalse positives, 146\\nin linear regression models, 154\\ntype I, 40\\ntype II, 40'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 246}, page_content='INDEX\\n233\\nEuclidean distance, 91–92. See also\\nDistance metrics\\nExperimental data, use of, 2–3\\nExtracting rules, 115–116. See also\\nAssociation rules\\nFalse negatives, 146. See also Errors\\nFalse positives, 146. See also Errors\\nFrequency distribution, 24–29, 34–36. See\\nalso Distribution\\nFrequency histogram, see Histogram\\nF-test, 157–158. See also Hypothesis tests\\nGain, 133. See also Decision trees\\nGini, 131. See also Decision trees\\nGraphs\\nbar charts, 25–26\\nbox plots, 30–32, 49\\ndecision tree, 123–125\\ndendrogram, 98\\nfrequency histogram, 27–29\\nheatmap, 104–105\\nscatterplot matrix, 201–202\\nscatteplots, 60–62\\nsmall multiples, 85–87\\nGrouping, 14, 83–140\\nin association rules, 85\\nin clustering, 85, 88–111, 204–205\\nin decision trees, 85\\nsupervised methods, 124\\nunsupervised methods, 88\\nHistogram, 27–29. See also Graphs\\nHypothesis test, 40–42\\nalpha(𝛼), 40\\nalternative hypothesis, 40\\nnull hypothesis, 40\\np-value, 42\\nHypothesis tests, 40\\nchi-square, 79–81, 203\\nF-test, 77, 158\\nt-test, 41, 74\\nImpurity in decision trees, 131\\nIntercept in linear regression, 151\\nInterquartile range, 30\\nJaccard distance, 92–93. See also Distance\\nmetrics\\nKendall Tau, 72–74, 204\\nk-Means clustering, 89, 105–111, 205. See\\nalso Grouping\\nk-Nearest neighbors (kNN), 167–172,\\n210–211\\nlearning, 170\\nprediction, 170–172\\nKurtosis, 35–36. See also Shape\\nLaw of large numbers, 38\\nLeast squares method, 150. See also\\nModel\\nLift, 116–120. See also Association rules\\nLinear regression, 149–161, 207–209. See\\nalso Model\\nassumptions, 158–160\\nmodeling, 149–161\\nLinear relationship, 60, 69\\nLinearity assumption, 158–159\\nLinkage rules, 95–100. See also\\nAgglomerative hierarchical\\nclustering\\naverage linkage, 95–100\\ncomplete linkage, 95–100\\nsingle linkage, 95–100\\nLocation, see Central tendency\\nLog transformation, 52\\nLogistic regression, 161–167, 209–210.\\nSee also Model\\nclassification, 167\\ncoefficients, 165–166\\nfitting, 162–165\\ninterpreting, 165\\nlikelihood ratio test, 166\\nWald test, 166\\nLower quartile, 30. See also Box plots\\nMean, 24. See also Central tendency\\nMedian, 23. See also Central tendency\\nMode, 23. See also Central tendency\\nModel\\nassessing fit of, 145–148\\nclassification, see Classification\\nmodels\\ndiscriminant analysis, see\\nDiscriminant analysis\\nk-nearest neighbors, see\\nk-Nearest-neighbors'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 247}, page_content='234\\nINDEX\\nModel (Continued)\\nlinear regression, see Linear regression\\nlogistic regression, see Logistic\\nregression\\nmultiple linear regression, see Multiple\\nlinear regression\\nmultiple logistic regression, see\\nMultiple logistic regression\\nna¨ıve Bayes, see Na¨ıve Bayes\\nclassifiers\\nneural networks, see Neural networks\\nrandom forest, see Random forest\\nregression tree, see Regression tree\\nselection of variables in, 143\\nsignificance of coefficients, 157,\\n165–168\\nsupport vector machines, see Support\\nvector machines\\ntransparency, 143\\nMultiple linear regression, 153–161. See\\nalso Model\\nMultiple logistic regression, 165\\nNa¨ıve Bayes classifiers, 179. See also\\nModel\\nNegative relationship between variables,\\n60\\nNeural networks, 178. See also Model\\nNonlinear relationships between variables,\\n60–62\\nNormal distribution, 28. See also\\nDistribution\\nNormalization, 8, 50–51\\nNull hypothesis, 40\\nObservations, 18\\nOne-way analysis of variance, 76–79,\\n203–204\\nOperational databases, 1, 3\\nOutliers, 61\\nParameters, 37\\nPolls, 2\\nPopulations, 36\\nPositive relationship between variables, 60\\nPredictive models, 14–15, 141–183. See\\nalso Model\\nPreparation, see Data preparation\\nPrincipal component analysis, 182\\nProcess of analysis, 3–13\\nProject management, 16\\np-Value, 42\\nQuartiles, 29–30. See also Box plot\\nRandom forest, 179. See also Model\\nRandom subset, 198\\nRange, 29\\nRegression model, 143. See also Model\\nRegression tree, 172–178, 211\\nRelationships\\nin data, 14, 59–82\\nvisualizing, 59–82\\nResiduals, 148, 154\\nRoles of individuals involved in\\nanalysis, 7\\nRules, see Association rules\\nSample standard deviation, 33–34\\nSample variance, 32\\nSamples, 36\\nSampling distribution, 37. See also\\nDistribution\\nScale, 21\\ninterval, 21\\nnominal, 21\\nordinal, 21\\nratio, 21\\nScaling, decimal, 50\\nScatterplot matrix, 201–202. See also\\nGraphs\\nScatterplots, 60–62. See also Graphs\\nSegmentation, doing, 198–199\\nSEMMA, 16\\nSensitivity, 146–148. See also Errors\\nShape, 34–36\\nkurtosis, 34–36\\nskewness, 35–36\\nSimilarity measures, 91–93\\nEuclidean, 91\\nJaccard, 92\\nSimple linear regression, 149–153. See\\nalso Linear regression; Model\\nSkewness, 34–35'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 248}, page_content='INDEX\\n235\\nSlope, 149–151\\nSmall multiples, 85–87. See also Graphs\\nSpearman Rho, 204\\nSpecificity, 146–148. See also Errors\\nSplitting decision trees, see Decision trees\\nStandard deviation, 33–34\\nStandard error, 37–38\\nStepwise regression, 161\\nSubsets, see Segmentation\\nSuccess criteria, 5\\nSummary charts, 63–67\\nSummary tables, 62–63, 84–85, 200\\nSupport, 116–120. See also Association\\nrules\\nSupport vector machines, 178\\nSurveys, 2\\nTables, 18–20\\ncross-classification, 67–68\\nsummary, 62–63\\nTest set, 144\\nTraceis, 191–226\\nTraining set, 144\\nTransformation, 49–54, 196–198\\nBox–Cox, 52, 196\\ndecimal scaling, 50\\nexponential, 52\\nmin–max, 50\\nz-score, 34\\nTransparency of model, see Model,\\ntransparency\\nTrue negatives, 146. See also Errors, in\\nclassification models\\nTrue positives, 146. See also Errors, in\\nclassification models\\nt-Test, see Hypothesis tests\\nTwo-way cross-classification table, see\\nContingency tables\\nType I error, 40. See also Errors\\nType II error, 40. See also Errors\\nUnstructured data, 55\\nUpper extreme, 30. See also Box plots\\nUpper quartile, 31. See also Box plots\\nVariables, 18, 20–22\\nbinary, 21\\nconstant, 22\\ncontinuous, 20\\ndichotomous, 21\\ndiscrete, 20\\ndummy, 52–53\\nindependent, 142\\nlabels for, 22\\nresponse, 142\\nVariables, measuring relationships\\nbetween, 69–81\\nchi-square, 79–81\\ncorrelation coefficient, 69\\nKendall Tau, 72–74\\nt-test, 74–75\\nF-test, 77\\nVariance, 32–33\\nX variables, see Variables, independent\\ny-intercept in linear regression, 151\\nX variables, see Variables, response\\nz-scores, 34\\nz-distribution, 39'),\n",
              " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 249}, page_content='WILEY END USER LICENSE\\nAGREEMENT\\nGo to www.wiley.com/go/eula to access Wiley’s ebook\\nEULA.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 0}, page_content=''),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 1}, page_content='Introduction to Probability and\\nStatistics for Engineers and Scientists'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 2}, page_content=''),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 3}, page_content='Introduction to Probability\\nand Statistics for\\nEngineers and Scientists\\nSixth Edition\\nSheldon M. Ross\\nUniversity of Southern California\\nLos Angeles, CA, United States'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 4}, page_content='Academic Press is an imprint of Elsevier\\n125 London Wall, London EC2Y 5AS, United Kingdom\\n525 B Street, Suite 1650, San Diego, CA 92101, United States\\n50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States\\nThe Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, United Kingdom\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\nNo part of this publication may be reproduced or transmitted in any form or by any means, electronic\\nor mechanical, including photocopying, recording, or any information storage and retrieval system,\\nwithout permission in writing from the publisher. Details on how to seek permission, further\\ninformation about the Publisher’s permissions policies and our arrangements with organizations such\\nas the Copyright Clearance Center and the Copyright Licensing Agency, can be found at our website:\\nwww.elsevier.com/permissions.\\nThis book and the individual contributions contained in it are protected under copyright by the\\nPublisher (other than as may be noted herein).\\nNotices\\nKnowledge and best practice in this ﬁeld are constantly changing. As new research and experience\\nbroaden our understanding, changes in research methods, professional practices, or medical treatment\\nmay become necessary.\\nPractitioners and researchers must always rely on their own experience and knowledge in evaluating\\nand using any information, methods, compounds, or experiments described herein. In using such\\ninformation or methods they should be mindful of their own safety and the safety of others, including\\nparties for whom they have a professional responsibility.\\nTo the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume\\nany liability for any injury and/or damage to persons or property as a matter of products liability,\\nnegligence or otherwise, or from any use or operation of any methods, products, instructions, or ideas\\ncontained in the material herein.\\nLibrary of Congress Cataloging-in-Publication Data\\nA catalog record for this book is available from the Library of Congress\\nBritish Library Cataloguing-in-Publication Data\\nA catalogue record for this book is available from the British Library\\nISBN: 978-0-12-824346-6\\nFor information on all Academic Press publications\\nvisit our website at https://www.elsevier.com/books-and-journals\\nPublisher: Katey Birtcher\\nEditorial Project Manager: Sara Valentino\\nProduction Project Manager: Rukmani Krishnan\\nDesigner: Patrick Ferguson\\nTypeset by VTeX'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 5}, page_content='For\\nElise'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 6}, page_content='Contents\\nPREFACE ...........................................................................................................xiii\\nCHAPTER 1\\nIntroduction to statistics............................................................ 1\\n1.1\\nIntroduction....................................................................... 1\\n1.2\\nData collection and descriptive statistics........................ 1\\n1.3\\nInferential statistics and probability models................... 2\\n1.4\\nPopulations and samples................................................. 3\\n1.5\\nA brief history of statistics................................................ 4\\nProblems ........................................................................... 7\\nCHAPTER 2\\nDescriptive statistics................................................................ 11\\n2.1\\nIntroduction..................................................................... 11\\n2.2\\nDescribing data sets....................................................... 12\\n2.2.1\\nFrequency tables and graphs ............................. 12\\n2.2.2\\nRelative frequency tables and graphs................ 14\\n2.2.3\\nGrouped data, histograms, ogives, and\\nstem and leaf plots.............................................. 16\\n2.3\\nSummarizing data sets................................................... 19\\n2.3.1\\nSample mean, sample median, and sample\\nmode..................................................................... 19\\n2.3.2\\nSample variance and sample standard\\ndeviation............................................................... 24\\n2.3.3\\nSample percentiles and box plots ...................... 26\\n2.4\\nChebyshev’s inequality ................................................... 29\\n2.5\\nNormal data sets ............................................................ 33\\n2.6\\nPaired data sets and the sample\\ncorrelation coefﬁcient..................................................... 36\\n2.7\\nThe Lorenz curve and Gini index.................................... 43\\n2.8\\nUsing R ............................................................................ 48\\nProblems ......................................................................... 52\\nCHAPTER 3\\nElements of probability............................................................ 63\\n3.1\\nIntroduction..................................................................... 63\\n3.2\\nSample space and events............................................... 64\\nvii'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 7}, page_content='viii\\nContents\\n3.3\\nVenn diagrams and the algebra of events..................... 66\\n3.4\\nAxioms of probability ...................................................... 67\\n3.5\\nSample spaces having equally likely outcomes............ 70\\n3.6\\nConditional probability.................................................... 75\\n3.7\\nBayes’ formula ................................................................ 79\\n3.8\\nIndependent events ........................................................ 86\\nProblems ......................................................................... 89\\nCHAPTER 4\\nRandom variables and expectation......................................... 99\\n4.1\\nRandom variables ........................................................... 99\\n4.2\\nTypes of random variables ........................................... 102\\n4.3\\nJointly distributed random variables........................... 105\\n4.3.1\\nIndependent random variables......................... 111\\n4.3.2\\nConditional distributions................................... 114\\n4.4\\nExpectation.................................................................... 117\\n4.5\\nProperties of the expected value ................................. 121\\n4.5.1\\nExpected value of sums of random variables.. 124\\n4.6\\nVariance......................................................................... 128\\n4.7\\nCovariance and variance of sums of\\nrandom variables.......................................................... 132\\n4.8\\nMoment generating functions...................................... 138\\n4.9\\nChebyshev’s inequality and the weak law of large\\nnumbers ........................................................................ 139\\nProblems ....................................................................... 142\\nCHAPTER 5\\nSpecial random variables...................................................... 151\\n5.1\\nThe Bernoulli and binomial random variables ........... 151\\n5.1.1\\nUsing R to calculate binomial probabilities..... 157\\n5.2\\nThe Poisson random variable....................................... 158\\n5.2.1\\nUsing R to calculate Poisson probabilities ...... 166\\n5.3\\nThe hypergeometric random variable ......................... 167\\n5.4\\nThe uniform random variable ...................................... 171\\n5.5\\nNormal random variables ............................................ 179\\n5.6\\nExponential random variables ..................................... 190\\n5.6.1\\nThe Poisson process ......................................... 193\\n5.6.2\\nThe Pareto distribution ..................................... 196\\n5.7\\nThe gamma distribution ............................................... 199\\n5.8\\nDistributions arising from the normal ........................ 201\\n5.8.1\\nThe chi-square distribution .............................. 201\\n5.8.2\\nThe t-distribution .............................................. 206\\n5.8.3\\nThe F-distribution ............................................. 208\\n5.9\\nThe logistics distribution.............................................. 209\\n5.10 Distributions in R .......................................................... 210\\nProblems ....................................................................... 212'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 8}, page_content='Contents\\nix\\nCHAPTER 6\\nDistributions of sampling statistics...................................... 221\\n6.1\\nIntroduction................................................................... 221\\n6.2\\nThe sample mean ......................................................... 222\\n6.3\\nThe central limit theorem ............................................ 224\\n6.3.1\\nApproximate distribution of the sample mean 227\\n6.3.2\\nHow large a sample is needed?........................ 230\\n6.4\\nThe sample variance..................................................... 230\\n6.5\\nSampling distributions from a normal population ..... 231\\n6.5.1\\nDistribution of the sample mean...................... 232\\n6.5.2\\nJoint distribution of X and S2 ........................... 232\\n6.6\\nSampling from a ﬁnite population ............................... 234\\nProblems ....................................................................... 238\\nCHAPTER 7\\nParameter estimation............................................................ 245\\n7.1\\nIntroduction................................................................... 245\\n7.2\\nMaximum likelihood estimators .................................. 246\\n7.2.1\\nEstimating life distributions ............................. 255\\n7.3\\nInterval estimates......................................................... 257\\n7.3.1\\nConﬁdence interval for a normal mean when\\nthe variance is unknown ................................... 262\\n7.3.2\\nPrediction intervals ........................................... 268\\n7.3.3\\nConﬁdence intervals for the variance of a\\nnormal distribution ........................................... 269\\n7.4\\nEstimating the difference in means of two normal\\npopulations.................................................................... 270\\n7.5\\nApproximate conﬁdence interval for the mean of a\\nBernoulli random variable ........................................... 275\\n7.6\\nConﬁdence interval of the mean of the exponential\\ndistribution ................................................................... 280\\n7.7\\nEvaluating a point estimator ........................................ 281\\n7.8\\nThe Bayes estimator..................................................... 287\\nProblems ....................................................................... 292\\nCHAPTER 8\\nHypothesis testing ................................................................. 305\\n8.1\\nIntroduction................................................................... 305\\n8.2\\nSigniﬁcance levels ........................................................ 306\\n8.3\\nTests concerning the mean of a normal population... 307\\n8.3.1\\nCase of known variance .................................... 307\\n8.3.2\\nCase of unknown variance: the t-test.............. 319\\n8.4\\nTesting the equality of means of two normal\\npopulations.................................................................... 326\\n8.4.1\\nCase of known variances .................................. 326\\n8.4.2\\nCase of unknown variances.............................. 328\\n8.4.3\\nCase of unknown and unequal variances ........ 333\\n8.4.4\\nThe paired t-test................................................ 333\\n8.5\\nHypothesis tests concerning the variance of a\\nnormal population ........................................................ 336'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 9}, page_content='x Contents\\n8.5.1\\nTesting for the equality of variances of two\\nnormal populations........................................... 337\\n8.6\\nHypothesis tests in Bernoulli populations .................. 339\\n8.6.1\\nTesting the equality of parameters in two\\nBernoulli populations........................................ 342\\n8.7\\nTests concerning the mean of a Poisson distribution 345\\n8.7.1\\nTesting the relationship between two\\nPoisson parameters.......................................... 346\\nProblems ....................................................................... 348\\nCHAPTER 9\\nRegression.............................................................................. 365\\n9.1\\nIntroduction................................................................... 365\\n9.2\\nLeast squares estimators of the\\nregression parameters................................................. 367\\n9.3\\nDistribution of the estimators...................................... 371\\n9.4\\nStatistical inferences about the\\nregression parameters................................................. 377\\n9.4.1\\nInferences concerning β ................................... 377\\n9.4.2\\nInferences concerning α ................................... 386\\n9.4.3\\nInferences concerning the mean response\\nα + βx0 ............................................................... 386\\n9.4.4\\nPrediction interval of a future response .......... 389\\n9.4.5\\nSummary of distributional results ................... 392\\n9.5\\nThe coefﬁcient of determination and the sample\\ncorrelation coefﬁcient................................................... 392\\n9.6\\nAnalysis of residuals: assessing the model................ 395\\n9.7\\nTransforming to linearity.............................................. 396\\n9.8\\nWeighted least squares ................................................ 400\\n9.9\\nPolynomial regression.................................................. 406\\n9.10 Multiple linear regression............................................ 410\\n9.10.1 Predicting future responses ............................. 420\\n9.10.2 Dummy variables for categorical data............. 424\\n9.11 Logistic regression models for binary output data..... 425\\nProblems ....................................................................... 429\\nCHAPTER 10 Analysis of variance ............................................................... 453\\n10.1 Introduction................................................................... 453\\n10.2 An overview ................................................................... 454\\n10.3 One-way analysis of variance....................................... 456\\n10.3.1 Using R to do the computations ....................... 463\\n10.3.2 Multiple comparisons of sample means.......... 466\\n10.3.3 One-way analysis of variance with unequal\\nsample sizes ...................................................... 468\\n10.4 Two-factor analysis of variance: introduction and\\nparameter estimation................................................... 470\\n10.5 Two-factor analysis of variance: testing hypotheses.. 474'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 10}, page_content='Contents\\nxi\\n10.6 Two-way analysis of variance with interaction ........... 479\\nProblems ....................................................................... 487\\nCHAPTER 11 Goodness of ﬁt tests and categorical data analysis............. 499\\n11.1 Introduction................................................................... 499\\n11.2 Goodness of ﬁt tests when all parameters are\\nspeciﬁed ........................................................................ 500\\n11.2.1 Determining the critical region by simulation. 506\\n11.3 Goodness of ﬁt tests when some parameters are\\nunspeciﬁed .................................................................... 508\\n11.4 Tests of independence in contingency tables.............. 510\\n11.5 Tests of independence in contingency tables having\\nﬁxed marginal totals..................................................... 514\\n11.6 The Kolmogorov–Smirnov goodness of ﬁt test for\\ncontinuous data............................................................. 517\\nProblems ....................................................................... 522\\nCHAPTER 12 Nonparametric hypothesis tests........................................... 529\\n12.1 Introduction................................................................... 529\\n12.2 The sign test.................................................................. 529\\n12.3 The signed rank test..................................................... 533\\n12.4 The two-sample problem ............................................. 538\\n12.4.1 Testing the equality of multiple probability\\ndistributions....................................................... 541\\n12.5 The runs test for randomness ..................................... 544\\nProblems ....................................................................... 547\\nCHAPTER 13 Quality control ........................................................................ 555\\n13.1 Introduction................................................................... 555\\n13.2 Control charts for average values: the x control\\nchart............................................................................... 556\\n13.2.1 Case of unknown μ and σ ................................. 559\\n13.3 S-control charts ............................................................ 564\\n13.4 Control charts for the fraction defective ..................... 567\\n13.5 Control charts for number of defects.......................... 569\\n13.6 Other control charts for detecting changes in the\\npopulation mean ........................................................... 573\\n13.6.1 Moving-average control charts ........................ 573\\n13.6.2 Exponentially weighted moving-average\\ncontrol charts .................................................... 576\\n13.6.3 Cumulative sum control charts........................ 581\\nProblems ....................................................................... 583\\nCHAPTER 14 Life testing∗............................................................................ 591\\n14.1 Introduction................................................................... 591\\n14.2 Hazard rate functions ................................................... 591\\n∗Optional chapter.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 11}, page_content='xii Contents\\n14.3 The exponential distribution in life testing.................. 594\\n14.3.1 Simultaneous testing — stopping at the rth\\nfailure ................................................................. 594\\n14.3.2 Sequential testing ............................................. 599\\n14.3.3 Simultaneous testing — stopping by a ﬁxed\\ntime .................................................................... 603\\n14.3.4 The Bayesian approach..................................... 606\\n14.4 A two-sample problem ................................................. 607\\n14.5 The Weibull distribution in life testing......................... 609\\n14.5.1 Parameter estimation by least squares........... 611\\nProblems ....................................................................... 613\\nCHAPTER 15 Simulation, bootstrap statistical methods, and\\npermutation tests................................................................... 619\\n15.1 Introduction................................................................... 619\\n15.2 Random numbers ......................................................... 619\\n15.2.1 The Monte Carlo simulation approach............. 622\\n15.3 The bootstrap method .................................................. 623\\n15.4 Permutation tests ......................................................... 631\\n15.4.1 Normal approximations in permutation tests. 634\\n15.4.2 Two-sample permutation tests ........................ 637\\n15.5 Generating discrete random variables........................ 639\\n15.6 Generating continuous random variables................... 641\\n15.6.1 Generating a normal random variable............. 643\\n15.7 Determining the number of simulation runs\\nin a Monte Carlo study.................................................. 644\\nProblems ....................................................................... 645\\nCHAPTER 16 Machine learning and big data.............................................. 649\\n16.1 Introduction................................................................... 649\\n16.2 Late ﬂight probabilities ................................................ 650\\n16.3 The naive Bayes approach............................................ 651\\n16.3.1 A variation of naive Bayes approach ................ 654\\n16.4 Distance-based estimators. The k-nearest\\nneighbors rule............................................................... 657\\n16.4.1 A distance-weighted method............................ 658\\n16.4.2 Component-weighted distances....................... 659\\n16.5 Assessing the approaches............................................ 660\\n16.6 When characterizing vectors are quantitative ............ 662\\n16.6.1 Nearest neighbor rules..................................... 662\\n16.6.2 Logistics regression.......................................... 663\\n16.7 Choosing the best probability: a bandit problem........ 664\\nProblems ....................................................................... 666\\nAPPENDIX OF TABLES ................................................................................... 669\\nINDEX .............................................................................................................. 673'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 12}, page_content='Preface\\nThe sixth edition of this book continues to demonstrate how to apply probabil-\\nity theory to gain insight into real, everyday statistical problems and situations.\\nAs in the previous editions, carefully developed coverage of probability mo-\\ntivates probabilistic models of real phenomena and the statistical procedures\\nthat follow. This approach ultimately results in an intuitive understanding of\\nstatistical procedures and strategies most often used by practicing engineers\\nand scientists.\\nThis book has been written for an introductory course in statistics or in proba-\\nbility and statistics for students in engineering, computer science, mathematics,\\nstatistics, and the natural sciences. As such it assumes knowledge of elementary\\ncalculus.\\nOrganization and coverage\\nChapter 1 presents a brief introduction to statistics, presenting its two branches\\nof descriptive and inferential statistics, and a short history of the subject and\\nsome of the people whose early work provided a foundation for work done\\ntoday.\\nThe subject matter of descriptive statistics is then considered in Chapter 2.\\nGraphs and tables that describe a data set are presented in this chapter, as are\\nquantities that are used to summarize certain of the key properties of the data\\nset.\\nTo be able to draw conclusions from data, it is necessary to have an under-\\nstanding of the data’s origination. For instance, it is often assumed that the\\ndata constitute a “random sample” from some population. To understand ex-\\nactly what this means and what its consequences are for relating properties of\\nthe sample data to properties of the entire population, it is necessary to have\\nsome understanding of probability, and that is the subject of Chapter 3. This\\nchapter introduces the idea of a probability experiment, explains the concept\\nof the probability of an event, and presents the axioms of probability.\\nxiii'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 13}, page_content='xiv Preface\\nOur study of probability is continued in Chapter 4, which deals with the\\nimportant concepts of random variables and expectation, and in Chapter 5,\\nwhich considers some special types of random variables that often occur in ap-\\nplications. Such random variables as the binomial, Poisson, hypergeometric,\\nnormal, uniform, gamma, chi-square, t, and F are presented.\\nIn Chapter 6, we study the probability distribution of such sampling statistics\\nas the sample mean and the sample variance. We show how to use a remark-\\nable theoretical result of probability, known as the central limit theorem, to\\napproximate the probability distribution of the sample mean. In addition, we\\npresent the joint probability distribution of the sample mean and the sample\\nvariance in the important special case in which the underlying data come from\\na normally distributed population.\\nChapter 7 shows how to use data to estimate parameters of interest. For in-\\nstance, a scientist might be interested in determining the proportion of Mid-\\nwestern lakes that are afﬂicted by acid rain. Two types of estimators are studied.\\nThe ﬁrst of these estimates the quantity of interest with a single number (for\\ninstance, it might estimate that 47 percent of Midwestern lakes suffer from acid\\nrain), whereas the second provides an estimate in the form of an interval of\\nvalues (for instance, it might estimate that between 45 and 49 percent of lakes\\nsuffer from acid rain). These latter estimators also tell us the “level of con-\\nﬁdence” we can have in their validity. Thus, for instance, whereas we can be\\npretty certain that the exact percentage of afﬂicted lakes is not 47, it might very\\nwell be that we can be, say, 95 percent conﬁdent that the actual percentage is\\nbetween 45 and 49.\\nChapter 8 introduces the important topic of statistical hypothesis testing,\\nwhich is concerned with using data to test the plausibility of a speciﬁed hy-\\npothesis. For instance, such a test might reject the hypothesis that fewer than\\n44 percent of Midwestern lakes are afﬂicted by acid rain. The concept of the\\np-value, which measures the degree of plausibility of the hypothesis after the\\ndata have been observed, is introduced. A variety of hypothesis tests concerning\\nthe parameters of both one and two normal populations are considered. Hy-\\npothesis tests concerning Bernoulli and Poisson parameters are also presented.\\nChapter 9 deals with the important topic of regression. Both simple linear\\nregression — including such subtopics as regression to the mean, residual anal-\\nysis, and weighted least squares — and multiple linear regression are consid-\\nered.\\nChapter 10 introduces the analysis of variance. Both one-way and two-way\\n(with and without the possibility of interaction) problems are considered.\\nChapter 11 is concerned with goodness of ﬁt tests, which can be used to test\\nwhether a proposed model is consistent with data. In it we present the classical\\nchi-square goodness of ﬁt test and apply it to test for independence in con-\\ntingency tables. The ﬁnal section of this chapter introduces the Kolmogorov–'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 14}, page_content='Preface\\nxv\\nSmirnov procedure for testing whether data come from a speciﬁed continuous\\nprobability distribution.\\nChapter 12 deals with nonparametric hypothesis tests, which can be used\\nwhen one is unable to suppose that the underlying distribution has some spec-\\niﬁed parametric form (such as normal).\\nChapter 13 considers the subject matter of quality control, a key statistical tech-\\nnique in manufacturing and production processes. A variety of control charts,\\nincluding not only the Shewhart control charts but also more sophisticated\\nones based on moving averages and cumulative sums, are considered.\\nChapter 14 deals with problems related to life testing. In this chapter, the ex-\\nponential, rather than the normal, distribution plays the key role.\\nIn Chapter 15, we consider the statistical inference techniques of bootstrap sta-\\ntistical methods and permutation tests. We ﬁrst show how probabilities can be\\nobtained by simulation and then how to utilize simulation in these statistical\\ninference approaches.\\nChapter 16, new to this edition, introduces machine learning and big data\\ntechniques. These are methods that are applicable in situations where one has\\na large amount of data that can be used to estimate probabilities without as-\\nsuming any particular probability model. For instance, we consider situations\\nwhere one wants to estimate the probability that an experiment, characterized\\nby a vector (x1,...,xn), will be a success. When the characterizing vectors are\\nqualitative in nature, such techniques as the naive Bayes approach, and nearest\\nneighbor rules are studied. In cases where the components of the characteristic\\nvector are quantitative, we also study logistic regression models.\\nAside from the newly added Chapter 16, the most important change in this\\nedition is the use of the statistical software R. No previous experience with R is\\nnecessary, and we incorporate its use throught the text. Aside from additional\\nsubsections devoted to R. we also have the newly added Section 2.7, dealing\\nwith Lorenz curves and the Gini index. There are also many new examples and\\nproblems in this edition. In addition, the sixth edition contains a multitude of\\nsmall changes designed to even further increase the clarity of the text’s presen-\\ntations and arguments.\\nSupplemental materials\\nSolutions manual for instructors is available at: https://textbooks.elsevier.com/\\nweb/Manuals.aspx?isbn=9780128243466.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 15}, page_content='xvi Preface\\nAcknowledgments\\nWe thank the following people for their helpful comments on material of the\\nsixth edition:\\n■Gideon Weiss, University of Haifa\\n■N. Balakrishnan, McMaster University\\n■Mark Brown, Columbia University\\n■Rohitha Goonatilake, Texas A and M University\\n■Steve From, University of Nebraska at Omaha\\n■Subhash Kochar, Portland State University\\n■Sumona Mondal, Mathematics, Clarkson University\\n■Kamel Belbahri, Mathematics and Statistics, Université de Montréal\\n■Anil Aswani, Industrial Engineering and Operations Research, University\\nof California, Berkeley\\nas well as all those reviewers who asked to remain anonymous.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 16}, page_content='CHAPTER 1\\nIntroduction to statistics\\n1.1\\nIntroduction\\nIt has become accepted in today’s world that in order to learn about something,\\nyou must ﬁrst collect data. Statistics is the art of learning from data. It is con-\\ncerned with the collection of data, its subsequent description, and its analysis,\\nwhich often leads to the drawing of conclusions.\\n1.2\\nData collection and descriptive statistics\\nSometimes a statistical analysis begins with a given set of data: For instance, the\\ngovernment regularly collects and publicizes data concerning yearly precipita-\\ntion totals, earthquake occurrences, the unemployment rate, the gross domestic\\nproduct, and the rate of inﬂation. Statistics can be used to describe, summarize,\\nand analyze these data.\\nIn other situations, data are not yet available; in such cases statistical theory can\\nbe used to design an appropriate experiment to generate data. The experiment\\nchosen should depend on the use that one wants to make of the data. For\\ninstance, suppose that an instructor is interested in determining which of two\\ndifferent methods for teaching computer programming to beginners is most\\neffective. To study this question, the instructor might divide the students into\\ntwo groups, and use a different teaching method for each group. At the end\\nof the class the students can be tested and the scores of the members of the\\ndifferent groups compared. If the data, consisting of the test scores of members\\nof each group, are signiﬁcantly higher in one of the groups, then it might seem\\nreasonable to suppose that the teaching method used for that group is superior.\\nIt is important to note, however, that in order to be able to draw a valid con-\\nclusion from the data, it is essential that the students were divided into groups\\nin such a manner that neither group was more likely to have the students with\\ngreater natural aptitude for programming. For instance, the instructor should\\nnot have let the male class members be one group and the females the other.\\nFor if so, then even if the women scored signiﬁcantly higher than the men, it\\nwould not be clear whether this was due to the method used to teach them,\\nor to the fact that women may be inherently better than men at learning pro-\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00010-7\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 17}, page_content='2 CHAPTER 1: Introduction to statistics\\ngramming skills. The accepted way of avoiding this pitfall is to divide the class\\nmembers into the two groups “at random.” This term means that the division\\nis done in such a manner that all possible choices of the members of a group\\nare equally likely.\\nAt the end of the experiment, the data should be described. For instance, the\\nscores of the two groups should be presented. In addition, summary mea-\\nsures such as the average score of members of each of the groups should be\\npresented. This part of statistics, concerned with the description and summa-\\nrization of data, is called descriptive statistics.\\n1.3\\nInferential statistics and probability models\\nAfter the preceding experiment is completed and the data are described and\\nsummarized, we hope to be able to draw a conclusion about which teaching\\nmethod is superior. This part of statistics, concerned with the drawing of con-\\nclusions, is called inferential statistics.\\nTo be able to draw a conclusion from the data, we must take into account the\\npossibility of chance. For instance, suppose that the average score of members\\nof the ﬁrst group is quite a bit higher than that of the second. Can we conclude\\nthat this increase is due to the teaching method used? Or is it possible that the\\nteaching method was not responsible for the increased scores but rather that\\nthe higher scores of the ﬁrst group were just a chance occurrence? For instance,\\nthe fact that a coin comes up heads 7 times in 10 ﬂips does not necessarily\\nmean that the coin is more likely to come up heads than tails in future ﬂips.\\nIndeed, it could be a perfectly ordinary coin that, by chance, just happened to\\nland heads 7 times out of the total of 10 ﬂips. (On the other hand, if the coin\\nhad landed heads 47 times out of 50 ﬂips, then we would be quite certain that\\nit was not an ordinary coin.)\\nTo be able to draw logical conclusions from data, we usually make some as-\\nsumptions about the chances (or probabilities) of obtaining the different data\\nvalues. The totality of these assumptions is referred to as a probability model for\\nthe data.\\nSometimes the nature of the data suggests the form of the probability model\\nthat is assumed. For instance, suppose that an engineer wants to ﬁnd out what\\nproportion of computer chips, produced by a new method, will be defective.\\nThe engineer might select a group of these chips, with the resulting data being\\nthe number of defective chips in this group. Provided that the chips selected\\nwere “randomly” chosen, it is reasonable to suppose that each one of them is\\ndefective with probability p, where p is the unknown proportion of all the chips\\nproduced by the new method that will be defective. The resulting data can then\\nbe used to make inferences about p.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 18}, page_content='1.4 Populations and samples\\n3\\nIn other situations, the appropriate probability model for a given data set will\\nnot be readily apparent. However, careful description and presentation of the\\ndata sometimes enable us to infer a reasonable model, which we can then try\\nto verify with the use of additional data.\\nBecause the basis of statistical inference is the formulation of a probability\\nmodel to describe the data, an understanding of statistical inference requires\\nsome knowledge of the theory of probability. In other words, statistical infer-\\nence starts with the assumption that important aspects of the phenomenon\\nunder study can be described in terms of probabilities; it then draws conclu-\\nsions by using data to make inferences about these probabilities.\\n1.4\\nPopulations and samples\\nIn statistics, we are interested in obtaining information about a total collection\\nof elements, which we will refer to as the population. The population is often too\\nlarge for us to examine each of its members. For instance, we might have all the\\nresidents of a given state, or all the television sets produced in the last year by\\na particular manufacturer, or all the households in a given community. In such\\ncases, we try to learn about the population by choosing and then examining a\\nsubgroup of its elements. This subgroup of a population is called a sample.\\nIf the sample is to be informative about the total population, it must be, in\\nsome sense, representative of that population. For instance, suppose that we are\\ninterested in learning about the age distribution of people residing in a given\\ncity, and we obtain the ages of the ﬁrst 100 people to enter the town library. If\\nthe average age of these 100 people is 46.2 years, are we justiﬁed in concluding\\nthat this is approximately the average age of the entire population? Probably\\nnot, for we could certainly argue that the sample chosen in this case is probably\\nnot representative of the total population because usually more young students\\nand senior citizens use the library than do working-age citizens.\\nIn certain situations, such as the library illustration, we are presented with a\\nsample and must then decide whether this sample is reasonably representative\\nof the entire population. In practice, a given sample generally cannot be as-\\nsumed to be representative of a population unless that sample has been chosen\\nin a random manner. This is because any speciﬁc nonrandom rule for selecting\\na sample often results in one that is inherently biased toward some data values\\nas opposed to others.\\nThus, although it may seem paradoxical, we are most likely to obtain a repre-\\nsentative sample by choosing its members in a totally random fashion without\\nany prior considerations of the elements that will be chosen. In other words, we\\nneed not attempt to deliberately choose the sample so that it contains, for in-\\nstance, the same gender percentage and the same percentage of people in each\\nprofession as found in the general population. Rather, we should just leave it'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 19}, page_content='4 CHAPTER 1: Introduction to statistics\\nup to “chance” to obtain roughly the correct percentages. Once a random sam-\\nple is chosen, we can use statistical inference to draw conclusions about the\\nentire population by studying the elements of the sample.\\n1.5\\nA brief history of statistics\\nA systematic collection of data on the population and the economy was begun\\nin the Italian city-states of Venice and Florence during the Renaissance. The\\nterm statistics, derived from the word state, was used to refer to a collection of\\nfacts of interest to the state. The idea of collecting data spread from Italy to the\\nother countries of Western Europe. Indeed, by the ﬁrst half of the 16th cen-\\ntury it was common for European governments to require parishes to register\\nbirths, marriages, and deaths. Because of poor public health conditions this\\nlast statistic was of particular interest.\\nThe high mortality rate in Europe before the 19th century was due mainly to\\nepidemic diseases, wars, and famines. Among epidemics, the worst were the\\nplagues. Starting with the Black Plague in 1348, plagues recurred frequently\\nfor nearly 400 years. In 1562, as a way to alert the King’s court to consider\\nmoving to the countryside, the City of London began to publish weekly bills of\\nmortality. Initially these mortality bills listed the places of death and whether\\na death had resulted from plague. Beginning in 1625 the bills were expanded\\nto include all causes of death.\\nIn 1662 the English tradesman John Graunt published a book entitled Natural\\nand Political Observations Made upon the Bills of Mortality. Table 1.1, which notes\\nthe total number of deaths in England and the number due to the plague for\\nﬁve different plague years, is taken from this book.\\nTable 1.1 Total Deaths in England.\\nYear\\nBurials\\nPlague Deaths\\n1592\\n25,886\\n11,503\\n1593\\n17,844\\n10,662\\n1603\\n37,294\\n30,561\\n1625\\n51,758\\n35,417\\n1636\\n23,359\\n10,400\\nSource: John Graunt, Observations Made upon the Bills of Mortal-\\nity. 3rd ed. London: John Martyn and James Allestry (1st ed. 1662).\\nGraunt used London bills of mortality to estimate the city’s population. For in-\\nstance, to estimate the population of London in 1660, Graunt surveyed house-\\nholds in certain London parishes (or neighborhoods) and discovered that, on\\naverage, there were approximately 3 deaths for every 88 people. Dividing by 3\\nshows that, on average, there was roughly 1 death for every 88/3 people. Be-\\ncause the London bills cited 13,200 deaths in London for that year, Graunt'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 20}, page_content='1.5 A brief history of statistics\\n5\\nTable 1.2 John Graunt’s Mortality Table.\\nAge at Death\\nNumber of Deaths per\\n100 Births\\n0–6\\n36\\n6–16\\n24\\n16–26\\n15\\n26–36\\n9\\n36–46\\n6\\n46–56\\n4\\n56–66\\n3\\n66–76\\n2\\n76 and greater\\n1\\nNote: The categories go up to but do not include the right-hand\\nvalue. For instance, 0–6 means all ages from 0 up through 5.\\nestimated the London population to be about\\n13,200 × 88/3 = 387,200\\nGraunt used this estimate to project a ﬁgure for all England. In his book he\\nnoted that these ﬁgures would be of interest to the rulers of the country, as\\nindicators of both the number of men who could be drafted into an army and\\nthe number who could be taxed.\\nGraunt also used the London bills of mortality — and some intelligent guess-\\nwork as to what diseases killed whom and at what age — to infer ages at death.\\n(Recall that the bills of mortality listed only causes and places at death, not the\\nages of those dying.) Graunt then used this information to compute tables giv-\\ning the proportion of the population that dies at various ages. Table 1.2 is one\\nof Graunt’s mortality tables. It states, for instance, that of 100 births, 36 people\\nwill die before reaching age 6, 24 will die between the age of 6 and 15, and so\\non.\\nGraunt’s estimates of the ages at which people were dying were of great interest\\nto those in the business of selling annuities. Annuities are the opposite of life\\ninsurance in that one pays in a lump sum as an investment and then receives\\nregular payments for as long as one lives.\\nGraunt’s work on mortality tables inspired further work by Edmund Halley\\nin 1693. Halley, the discoverer of the comet bearing his name (and also the\\nman who was most responsible, by both his encouragement and his ﬁnancial\\nsupport, for the publication of Isaac Newton’s famous Principia Mathematica),\\nused tables of mortality to compute the odds that a person of any age would\\nlive to any other particular age. Halley was inﬂuential in convincing the insurers\\nof the time that an annual life insurance premium should depend on the age\\nof the person being insured.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 21}, page_content='6 CHAPTER 1: Introduction to statistics\\nFollowing Graunt and Halley, the collection of data steadily increased through-\\nout the remainder of the 17th and on into the 18th century. For instance, the\\ncity of Paris began collecting bills of mortality in 1667, and by 1730 it had\\nbecome common practice throughout Europe to record ages at death.\\nThe term statistics, which was used until the 18th century as a shorthand for the\\ndescriptive science of states, became in the 19th century increasingly identiﬁed\\nwith numbers. By the 1830s the term was almost universally regarded in Britain\\nand France as being synonymous with the “numerical science” of society. This\\nchange in meaning was caused by the large availability of census records and\\nother tabulations that began to be systematically collected and published by\\nthe governments of Western Europe and the United States beginning around\\n1800.\\nThroughout the 19th century, although probability theory had been developed\\nby such mathematicians as Jacob Bernoulli, Karl Friedrich Gauss, and Pierre-\\nSimon Laplace, its use in studying statistical ﬁndings was almost nonexistent,\\nbecause most social statisticians at the time were content to let the data speak\\nfor themselves. In particular, statisticians of that time were not interested in\\ndrawing inferences about individuals, but rather were concerned with the soci-\\nety as a whole. Thus, they were not concerned with sampling but rather tried\\nto obtain censuses of the entire population. As a result, probabilistic infer-\\nence from samples to a population was almost unknown in 19th century social\\nstatistics.\\nIt was not until the late 1800s that statistics became concerned with inferring\\nconclusions from numerical data. The movement began with Francis Galton’s\\nwork on analyzing hereditary genius through the uses of what we would now\\ncall regression and correlation analysis (see Chapter 9), and obtained much\\nof its impetus from the work of Karl Pearson. Pearson, who developed the\\nchi-square goodness of ﬁt tests (see Chapter 11), was the ﬁrst director of the\\nGalton Laboratory, endowed by Francis Galton in 1904. There Pearson origi-\\nnated a research program aimed at developing new methods of using statistics\\nin inference. His laboratory invited advanced students from science and in-\\ndustry to learn statistical methods that could then be applied in their ﬁelds.\\nOne of his earliest visiting researchers was W.S. Gosset, a chemist by training,\\nwho showed his devotion to Pearson by publishing his own works under the\\nname “Student.” (A famous story has it that Gosset was afraid to publish un-\\nder his own name for fear that his employers, the Guinness brewery, would be\\nunhappy to discover that one of its chemists was doing research in statistics.)\\nGosset is famous for his development of the t-test (see Chapter 8).\\nTwo of the most important areas of applied statistics in the early 20th century\\nwere population biology and agriculture. This was due to the interest of Pear-\\nson and others at his laboratory and also to the remarkable accomplishments\\nof the English scientist Ronald A. Fisher. The theory of inference developed by\\nthese pioneers, including among others Karl Pearson’s son Egon and the Pol-'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 22}, page_content='Problems\\n7\\nTable 1.3 The Changing Deﬁnition of Statistics.\\nStatistics has then for its object that of presenting a faithful representation of a state at a\\ndetermined epoch. (Quetelet, 1849)\\nStatistics are the only tools by which an opening can be cut through the formidable thicket\\nof difﬁculties that bars the path of those who pursue the Science of man. (Galton, 1889)\\nStatistics may be regarded (i) as the study of populations, (ii) as the study of variation, and\\n(iii) as the study of methods of the reduction of data. (Fisher, 1925)\\nStatistics is a scientiﬁc discipline concerned with collection, analysis, and interpretation of\\ndata obtained from observation or experiment. The subject has a coherent structure based\\non the theory of Probability and includes many different procedures which contribute to\\nresearch and development throughout the whole of Science and Technology. (E. Pearson,\\n1936)\\nStatistics is the name for that science and art which deals with uncertain inferences —\\nwhich uses numbers to ﬁnd out something about nature and experience. (Weaver, 1952)\\nStatistics has become known in the 20th century as the mathematical tool for analyzing\\nexperimental and observational data. (Porter, 1986)\\nStatistics is the art of learning from data. (this book, 2020)\\nish born mathematical statistician Jerzy Neyman, was general enough to deal\\nwith a wide range of quantitative and practical problems. As a result, after the\\nearly years of the 20th century a rapidly increasing number of people in sci-\\nence, business, and government began to regard statistics as a tool that was\\nable to provide quantitative solutions to scientiﬁc and practical problems (see\\nTable 1.3).\\nNowadays the ideas of statistics are everywhere. Descriptive statistics are fea-\\ntured in every newspaper and magazine. Statistical inference has become indis-\\npensable to public health and medical research, to engineering and scientiﬁc\\nstudies, to marketing and quality control, to education, to accounting, to eco-\\nnomics, to meteorological forecasting, to polling and surveys, to sports, to\\ninsurance, to gambling, and to all research that makes any claim to being sci-\\nentiﬁc. Statistics has indeed become ingrained in our intellectual heritage.\\nProblems\\n1. An election will be held next week and, by polling a sample of the voting\\npopulation, we are trying to predict whether the Republican or Demo-\\ncratic candidate will prevail. Which of the following methods of selection\\nis likely to yield a representative sample?\\na.\\nPoll all people of voting age attending a college basketball game.\\nb.\\nPoll all people of voting age leaving a fancy midtown restaurant.\\nc.\\nObtain a copy of the voter registration list, randomly choose 100\\nnames, and question them.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 23}, page_content='8 CHAPTER 1: Introduction to statistics\\nd.\\nUse the results of a television call-in poll, in which the station asked\\nits listeners to call in and name their choice.\\ne.\\nChoose names from the telephone directory and call these people.\\n2. The approach used in Problem 1(e) led to a disastrous prediction in the\\n1936 presidential election, in which Franklin Roosevelt defeated Alfred\\nLandon by a landslide. A Landon victory had been predicted by the Lit-\\nerary Digest. The magazine based its prediction on the preferences of a\\nsample of voters chosen from lists of automobile and telephone owners.\\na.\\nWhy do you think the Literary Digest’s prediction was so far off?\\nb.\\nHas anything changed between 1936 and now that would make you\\nbelieve that the approach used by the Literary Digest would work\\nbetter today?\\n3. A researcher is trying to discover the average age at death for people in the\\nUnited States today. To obtain data, the obituary columns of the New York\\nTimes are read for 30 days, and the ages at death of people in the United\\nStates are noted. Do you think this approach will lead to a representative\\nsample?\\n4. To determine the proportion of people in your town who are smokers, it\\nhas been decided to poll people at one of the following local spots:\\na.\\nthe pool hall;\\nb.\\nthe bowling alley;\\nc.\\nthe shopping mall;\\nd.\\nthe library.\\nWhich of these potential polling places would most likely result in a rea-\\nsonable approximation to the desired proportion? Why?\\n5. A university plans on conducting a survey of its recent graduates to deter-\\nmine information on their yearly salaries. It randomly selected 200 recent\\ngraduates and sent them questionnaires dealing with their present jobs.\\nOf these 200, however, only 86 were returned. Suppose that the average\\nof the yearly salaries reported was $75,000.\\na.\\nWould the university be correct in thinking that $75,000 was a good\\napproximation to the average salary level of all of its graduates? Ex-\\nplain the reasoning behind your answer.\\nb.\\nIf your answer to part (a) is no, can you think of any set of conditions\\nrelating to the group that returned questionnaires for which it would\\nbe a good approximation?\\n6. An article reported that a survey of clothing worn by pedestrians killed\\nat night in trafﬁc accidents revealed that about 80 percent of the victims\\nwere wearing dark-colored clothing and 20 percent were wearing light-\\ncolored clothing. The conclusion drawn in the article was that it is safer\\nto wear light-colored clothing at night.\\na.\\nIs this conclusion justiﬁed? Explain.\\nb.\\nIf your answer to part (a) is no, what other information would be\\nneeded before a ﬁnal conclusion could be drawn?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 24}, page_content='Problems 9\\n7. Critique Graunt’s method for estimating the population of London.\\nWhat implicit assumption is he making?\\n8. The London bills of mortality listed 12,246 deaths in 1658. Supposing\\nthat a survey of London parishes showed that roughly 2 percent of the\\npopulation died that year, use Graunt’s method to estimate London’s\\npopulation in 1658.\\n9. Suppose you were a seller of annuities in 1662 when Graunt’s book was\\npublished. Explain how you would make use of his data on the ages at\\nwhich people were dying.\\n10. Based on Graunt’s mortality table:\\na.\\nWhat proportion of people survived to age 6?\\nb.\\nWhat proportion survived to age 46?\\nc.\\nWhat proportion died between the ages of 6 and 36?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 25}, page_content='CHAPTER 2\\nDescriptive statistics\\n2.1\\nIntroduction\\nIn this chapter we introduce the subject matter of descriptive statistics, and in\\ndoing so learn ways to describe and summarize a set of data. Section 2.2 deals\\nwith ways of describing a data set. Subsections 2.2.1 and 2.2.2 indicate how\\ndata that take on only a relatively few distinct values can be described by using\\nfrequency tables or graphs, whereas Subsection 2.2.3 deals with data whose set\\nof values is grouped into different intervals. Section 2.3 discusses ways of sum-\\nmarizing data sets by use of statistics, which are numerical quantities whose\\nvalues are determined by the data. Subsection 2.3.1 considers three statistics\\nthat are used to indicate the “center” of the data set: the sample mean, the\\nsample median, and the sample mode. Subsection 2.3.2 introduces the sam-\\nple variance and its square root, called the sample standard deviation. These\\nstatistics are used to indicate the spread of the values in the data set. Subsection\\n2.3.3 deals with sample percentiles, which are statistics that tell us, for instance,\\nwhich data value is greater than 95 percent of all the data. In Section 2.4 we\\npresent Chebyshev’s inequality for sample data. This famous inequality gives\\nan upper bound to the proportion of the data that can differ from the sample\\nmean by more than k times the sample standard deviation. Whereas Cheby-\\nshev’s inequality holds for all data sets, we can in certain situations, which are\\ndiscussed in Section 2.5, obtain more precise estimates of the proportion of\\nthe data that is within k sample standard deviations of the sample mean. In\\nSection 2.5 we note that when a graph of the data follows a bell-shaped form\\nthe data set is said to be approximately normal, and more precise estimates are\\ngiven by the so-called empirical rule. Section 2.6 is concerned with situations\\nin which the data consist of paired values. A graphical technique, called the\\nscatter diagram, for presenting such data is introduced, as is the sample corre-\\nlation coefﬁcient, a statistic that indicates the degree to which a large value of\\nthe ﬁrst member of the pair tends to go along with a large value of the second.\\nSection 2.7 is concerned with the income distribution of a population. It intro-\\nduces the Lorenz curve L(p), which gives the proportion of the total income of\\na population that is earned by the lower 100p percent of wage earners. Also in-\\ntroduced is the Gini index, which is a measure of the inequality of the income\\ndistribution. Section 2.8 shows how to use R to analyze data sets.\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00011-9\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n11'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 26}, page_content='12 CHAPTER 2: Descriptive statistics\\n2.2\\nDescribing data sets\\nThe numerical ﬁndings of a study should be presented clearly, concisely, and in\\nsuch a manner that an observer can quickly obtain a feel for the essential char-\\nacteristics of the data. Over the years it has been found that tables and graphs\\nare particularly useful ways of presenting data, often revealing important fea-\\ntures such as the range, the degree of concentration, and the symmetry of the\\ndata. In this section we present some common graphical and tabular ways for\\npresenting data.\\n2.2.1\\nFrequency tables and graphs\\nA data set having a relatively small number of distinct values can be conve-\\nniently presented in a frequency table. For instance, Table 2.1 is a frequency table\\nfor a data set consisting of the starting yearly salaries (to the nearest thousand\\ndollars) of 42 recently graduated students with B.S. degrees in electrical engi-\\nneering. Table 2.1 tells us, among other things, that the lowest starting salary\\nof $57,000 was received by four of the graduates, whereas the highest salary\\nof $70,000 was received by a single student. The most common starting salary\\nwas $62,000, and was received by 10 of the students.\\nTable 2.1 Starting Yearly Salaries.\\nStarting Salary\\nFrequency\\n57\\n4\\n58\\n1\\n59\\n3\\n60\\n5\\n61\\n8\\n62\\n10\\n63\\n0\\n64\\n5\\n66\\n2\\n67\\n3\\n70\\n1\\nData from a frequency table can be graphically represented by a line graph that\\nplots the distinct data values on the horizontal axis and indicates their fre-\\nquencies by the heights of vertical lines. A line graph of the data presented in\\nTable 2.1 is shown in Figure 2.1.\\nWhen the lines in a line graph are given added thickness, the graph is called a\\nbar graph. Figure 2.2 presents a bar graph.\\nAnother type of graph used to represent a frequency table is the frequency poly-\\ngon, which plots the frequencies of the different data values on the vertical axis,'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 27}, page_content='2.2 Describing data sets\\n13\\nFIGURE 2.1\\nStarting salary data.\\nFIGURE 2.2\\nBar graph for starting salary data.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 28}, page_content='14 CHAPTER 2: Descriptive statistics\\nFIGURE 2.3\\nFrequency polygon for starting salary data.\\nand then connects the plotted points with straight lines. Figure 2.3 presents a\\nfrequency polygon for the data of Table 2.1.\\n2.2.2\\nRelative frequency tables and graphs\\nConsider a data set consisting of n values. If f is the frequency of a particular\\nvalue, then the ratio f/n is called its relative frequency. That is, the relative fre-\\nquency of a data value is the proportion of the data that have that value. The\\nrelative frequencies can be represented graphically by a relative frequency line\\nor bar graph or by a relative frequency polygon. Indeed, these relative frequency\\ngraphs will look like the corresponding graphs of the absolute frequencies ex-\\ncept that the labels on the vertical axis are now the old labels (that gave the\\nfrequencies) divided by the total number of data points.\\nExample 2.2.a. Table 2.2 is a relative frequency table for the data of Table 2.1.\\nThe relative frequencies are obtained by dividing the corresponding frequencies\\nof Table 2.1 by 42, the size of the data set.\\n■\\nA pie chart is often used to indicate relative frequencies when the data are not\\nnumerical in nature. A circle is constructed and then sliced into different sec-'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 29}, page_content='2.2 Describing data sets\\n15\\ntors; one for each distinct type of data value. The relative frequency of a data\\nvalue is indicated by the area of its sector, this area being equal to the total area\\nof the circle multiplied by the relative frequency of the data value.\\nExample 2.2.b. The following data relate to the different types of cancers af-\\nfecting the 200 most recent patients to enroll at a clinic specializing in cancer.\\nThese data are represented in the pie chart presented in Figure 2.4.\\n■\\nTable 2.2\\nStarting Salary\\nFrequency\\n47\\n4/42=.0952\\n48\\n1/42=.0238\\n49\\n3/42\\n50\\n5/42\\n51\\n8/42\\n52\\n10/42\\n53\\n0\\n54\\n5/42\\n56\\n2/42\\n57\\n3/42\\n60\\n1/42\\nFIGURE 2.4'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 30}, page_content='16 CHAPTER 2: Descriptive statistics\\nType of Cancer\\nNumber of New Cases\\nRelative Frequency\\nLung\\n42\\n.21\\nBreast\\n50\\n.25\\nColon\\n32\\n.16\\nProstate\\n55\\n.275\\nMelanoma\\n9\\n.045\\nBladder\\n12\\n.06\\n2.2.3\\nGrouped data, histograms, ogives, and\\nstem and leaf plots\\nAs seen in Subsection 2.2.2, using a line or a bar graph to plot the frequen-\\ncies of data values is often an effective way of portraying a data set. However,\\nfor some data sets the number of distinct values is too large to utilize this ap-\\nproach. Instead, in such cases, it is useful to divide the values into groupings,\\nor class intervals, and then plot the number of data values falling in each class\\ninterval. The number of class intervals chosen should be a trade-off between\\n(1) choosing too few classes at a cost of losing too much information about\\nthe actual data values in a class and (2) choosing too many classes, which will\\nresult in the frequencies of each class being too small for a pattern to be dis-\\ncernible. Although 5 to 10 class intervals are typical, the appropriate number is\\na subjective choice, and of course, you can try different numbers of class inter-\\nvals to see which of the resulting charts appears to be most revealing about the\\ndata. It is common, although not essential, to choose class intervals of equal\\nlength.\\nThe endpoints of a class interval are called the class boundaries. We will adopt\\nthe left-end inclusion convention, which stipulates that a class interval contains\\nits left-end but not its right-end boundary point. Thus, for instance, the class\\ninterval 20–30 contains all values that are both greater than or equal to 20 and\\nless than 30.\\nTable 2.3 presents the lifetimes of 200 incandescent lamps. A class frequency\\ntable for the data of Table 2.3 is presented in Table 2.4. The class intervals are\\nof length 100, with the ﬁrst one starting at 500.\\nA bar graph plot of class data, with the bars placed adjacent to each other, is\\ncalled a histogram. The vertical axis of a histogram can represent either the class\\nfrequency or the relative class frequency; in the former case the graph is called\\na frequency histogram and in the latter a relative frequency histogram. Figure 2.5\\npresents a frequency histogram of the data in Table 2.4.\\nWe are sometimes interested in plotting a cumulative frequency (or cumulative\\nrelative frequency) graph. A point on the horizontal axis of such a graph rep-\\nresents a possible data value; its corresponding vertical plot gives the number\\n(or proportion) of the data whose values are less than or equal to it. A cumu-\\nlative relative frequency plot of the data of Table 2.3 is given in Figure 2.6. We'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 31}, page_content='2.2 Describing data sets\\n17\\ncan conclude from this ﬁgure that 100 percent of the data values are less than\\n1500, approximately 40 percent are less than or equal to 900, approximately\\n80 percent are less than or equal to 1100, and so on. A cumulative frequency\\nplot is called an ogive.\\nTable 2.3 Life in Hours of 200 Incandescent Lamps.\\nItem Lifetimes\\n1067\\n919\\n1196\\n785\\n1126\\n936\\n918\\n1156\\n920\\n948\\n855\\n1092\\n1162\\n1170\\n929\\n950\\n905\\n972\\n1035\\n1045\\n1157\\n1195\\n1195\\n1340\\n1122\\n938\\n970\\n1237\\n956\\n1102\\n1022\\n978\\n832\\n1009\\n1157\\n1151\\n1009\\n765\\n958\\n902\\n923\\n1333\\n811\\n1217\\n1085\\n896\\n958\\n1311\\n1037\\n702\\n521\\n933\\n928\\n1153\\n946\\n858\\n1071\\n1069\\n830\\n1063\\n930\\n807\\n954\\n1063\\n1002\\n909\\n1077\\n1021\\n1062\\n1157\\n999\\n932\\n1035\\n944\\n1049\\n940\\n1122\\n1115\\n833\\n1320\\n901\\n1324\\n818\\n1250\\n1203\\n1078\\n890\\n1303\\n1011\\n1102\\n996\\n780\\n900\\n1106\\n704\\n621\\n854\\n1178\\n1138\\n951\\n1187\\n1067\\n1118\\n1037\\n958\\n760\\n1101\\n949\\n992\\n966\\n824\\n653\\n980\\n935\\n878\\n934\\n910\\n1058\\n730\\n980\\n844\\n814\\n1103\\n1000\\n788\\n1143\\n935\\n1069\\n1170\\n1067\\n1037\\n1151\\n863\\n990\\n1035\\n1112\\n931\\n970\\n932\\n904\\n1026\\n1147\\n883\\n867\\n990\\n1258\\n1192\\n922\\n1150\\n1091\\n1039\\n1083\\n1040\\n1289\\n699\\n1083\\n880\\n1029\\n658\\n912\\n1023\\n984\\n856\\n924\\n801\\n1122\\n1292\\n1116\\n880\\n1173\\n1134\\n932\\n938\\n1078\\n1180\\n1106\\n1184\\n954\\n824\\n529\\n998\\n996\\n1133\\n765\\n775\\n1105\\n1081\\n1171\\n705\\n1425\\n610\\n916\\n1001\\n895\\n709\\n860\\n1110\\n1149\\n972\\n1002\\nTable 2.4 A Class Frequency Table.\\nClass Interval\\nFrequency\\n(Number of Data\\nValues in the Interval)\\n500–600\\n2\\n600–700\\n5\\n700–800\\n12\\n800–900\\n25\\n900–1000\\n58\\n1000–1100\\n41\\n1100–1200\\n43\\n1200–1300\\n7\\n1300–1400\\n6\\n1400–1500\\n1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 32}, page_content='18 CHAPTER 2: Descriptive statistics\\nFIGURE 2.5\\nA frequency histogram.\\nFIGURE 2.6\\nA cumulative frequency plot.\\nAn efﬁcient way of organizing a small- to moderate-sized data set is to utilize\\na stem and leaf plot. Such a plot is obtained by ﬁrst dividing each data value\\ninto two parts — its stem and its leaf. For instance, if the data are all two-digit\\nnumbers, then we could let the stem part of a data value be its tens digit and\\nlet the leaf be its ones digit. Thus, for instance, the value 62 is expressed as\\nStem\\nLeaf\\n6\\n2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 33}, page_content='2.3 Summarizing data sets\\n19\\nand the two data values 62 and 67 can be represented as\\nStem\\nLeaf\\n6\\n2,7\\nExample 2.2.c. Table 2.5 gives the monthly and yearly average daily minimum\\ntemperatures in 35 U.S. cities.\\nThe annual average daily minimum temperatures from Table 2.5 are repre-\\nsented in the following stem and leaf plot.\\n7\\n0.0\\n6\\n9.0\\n5\\n1.0,1.3,2.0,5.5,7.1,7.4,7.6,8.5,9.3\\n4\\n0.0,1.0,2.4,3.6,3.7,4.8,5.0,5.2,6.0,6.7,8.1,9.0,9.2\\n3\\n3.1,4.1,5.3,5.8,6.2,9.0,9.5,9.5\\n2\\n9.0,9.8\\n■\\n2.3\\nSummarizing data sets\\nModern-day experiments often deal with huge sets of data. For instance, in an\\nattempt to learn about the health consequences of certain common practices,\\nin 1951 the medical statisticians R. Doll and A. B. Hill sent questionnaires to\\nall doctors in the United Kingdom and received approximately 40,000 replies.\\nTheir questions dealt with age, eating habits, and smoking habits. The respon-\\ndents were then tracked for the ensuing 10 years and the causes of death for\\nthose who died were monitored. To obtain a feel for such a large amount of\\ndata, it is useful to be able to summarize it by some suitably chosen measures.\\nIn this section we present some summarizing statistics, where a statistic is a\\nnumerical quantity whose value is determined by the data.\\n2.3.1\\nSample mean, sample median, and sample mode\\nIn this section we introduce some statistics that are used for describing the\\ncenter of a set of data values. To begin, suppose that we have a data set consist-\\ning of the n numerical values x1,x2,...,xn. The sample mean is the arithmetic\\naverage of these values.\\nDeﬁnition. The sample mean, designated by ¯x, is deﬁned by\\n¯x =\\nn\\n\\x02\\ni=1\\nxi/n\\nThe computation of the sample mean can often be simpliﬁed by noting that if\\nfor constants a and b\\nyi = axi + b,\\ni = 1,...,n'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 34}, page_content='20 CHAPTER 2: Descriptive statistics\\nTable 2.5 Normal Daily Minimum Temperature — Selected Cities\\nIn Fahrenheit degrees. Airport data except as noted. Based on standard 30-year\\nperiod, 1961 through 1990\\nState\\nStation\\nAnnual\\navg.\\nJan.\\nFeb.\\nMar.\\nApr.\\nMay\\nJune July\\nAug.\\nSept. Oct.\\nNov.\\nDec.\\nAL\\nMobile . . . . . . .\\n40.0\\n42.7\\n50.1\\n57.1\\n64.4\\n70.7\\n73.2\\n72.9\\n68.7\\n57.3\\n49.1\\n43.1\\n57.4\\nAK\\nJuneau . . . . . .\\n19.0\\n22.7\\n26.7\\n32.1\\n38.9\\n45.0\\n48.1\\n47.3\\n42.9\\n37.2\\n27.2\\n22.6\\n34.1\\nAZ\\nPhoenix. . . . . .\\n41.2\\n44.7\\n48.8\\n55.3\\n63.9\\n72.9\\n81.0\\n79.2\\n72.8\\n60.8\\n48.9\\n41.8\\n59.3\\nAR\\nLittle Rock . . .\\n29.1\\n33.2\\n42.2\\n50.7\\n59.0\\n67.4\\n71.5\\n69.8\\n63.5\\n50.9\\n41.5\\n33.1\\n51.0\\nCA\\nLos Angeles .\\n47.8\\n49.3\\n50.5\\n52.8\\n56.3\\n59.5\\n62.8\\n64.2\\n63.2\\n59.2\\n52.8\\n47.9\\n55.5\\nSacramento. .\\n37.7\\n41.4\\n43.2\\n45.5\\n50.3\\n55.3\\n58.1\\n58.0\\n55.7\\n50.4\\n43.4\\n37.8\\n48.1\\nSan Diego . . .\\n48.9\\n50.7\\n52.8\\n55.6\\n59.1\\n61.9\\n65.7\\n67.3\\n65.6\\n60.9\\n53.9\\n48.8\\n57.6\\nSan Francisco\\n41.8\\n45.0\\n45.8\\n47.2\\n49.7\\n52.6\\n53.9\\n55.0\\n55.2\\n51.8\\n47.1\\n42.7\\n49.0\\nCO\\nDenver. . . . . . .\\n16.1\\n20.2\\n25.8\\n34.5\\n43.6\\n52.4\\n58.6\\n56.9\\n47.6\\n36.4\\n25.4\\n17.4\\n36.2\\nCT\\nHartford . . . . .\\n15.8\\n18.6\\n28.1\\n37.5\\n47.6\\n56.9\\n62.2\\n60.4\\n51.8\\n40.7\\n32.8\\n21.3\\n39.5\\nDE\\nWilmington. . .\\n22.4\\n24.8\\n33.1\\n41.8\\n52.2\\n61.6\\n67.1\\n65.9\\n58.2\\n45.7\\n37.0\\n27.6\\n44.8\\nDC\\nWashington . .\\n26.8\\n29.1\\n37.7\\n46.4\\n56.6\\n66.5\\n71.4\\n70.0\\n62.5\\n50.3\\n41.1\\n31.7\\n49.2\\nFL\\nJacksonville. .\\n40.5\\n43.3\\n49.2\\n54.9\\n62.1\\n69.1\\n71.9\\n71.8\\n69.0\\n59.3\\n50.2\\n43.4\\n57.1\\nMiami. . . . . . . .\\n59.2\\n60.4\\n64.2\\n67.8\\n72.1\\n75.1\\n76.2\\n76.7\\n75.9\\n72.1\\n66.7\\n61.5\\n69.0\\nGA\\nAtlanta. . . . . . .\\n31.5\\n34.5\\n42.5\\n50.2\\n58.7\\n66.2\\n69.5\\n69.0\\n63.5\\n51.9\\n42.8\\n35.0\\n51.3\\nHI\\nHonolulu . . . . .\\n65.6\\n65.4\\n67.2\\n68.7\\n70.3\\n72.2\\n73.5\\n74.2\\n73.5\\n72.3\\n70.3\\n67.0\\n70.0\\nID\\nBoise . . . . . . . .\\n21.6\\n27.5\\n31.9\\n36.7\\n43.9\\n52.1\\n57.7\\n56.8\\n48.2\\n39.0\\n31.1\\n22.5\\n39.1\\nIL\\nChicago . . . . .\\n12.9\\n17.2\\n28.5\\n38.6\\n47.7\\n57.5\\n62.6\\n61.6\\n53.9\\n42.2\\n31.6\\n19.1\\n39.5\\nPeoria . . . . . . .\\n13.2\\n17.7\\n29.8\\n40.8\\n50.9\\n60.7\\n65.4\\n63.1\\n55.2\\n43.1\\n32.5\\n19.3\\n41.0\\nIN\\nIndianapolis . .\\n17.2\\n20.9\\n31.9\\n41.5\\n51.7\\n61.0\\n65.2\\n62.8\\n55.6\\n43.5\\n34.1\\n23.2\\n42.4\\nIA\\nDes Moines . .\\n10.7\\n15.6\\n27.6\\n40.0\\n51.5\\n61.2\\n66.5\\n63.6\\n54.5\\n42.7\\n29.9\\n16.1\\n40.0\\nKS\\nWichita . . . . . .\\n19.2\\n23.7\\n33.6\\n44.5\\n54.3\\n64.6\\n69.9\\n67.9\\n59.2\\n46.6\\n33.9\\n23.0\\n45.0\\nKY\\nLouisville. . . . .\\n23.2\\n26.5\\n36.2\\n45.4\\n54.7\\n62.9\\n67.3\\n65.8\\n58.7\\n45.8\\n37.3\\n28.6\\n46.0\\nLA\\nNew Orleans .\\n41.8\\n44.4\\n51.6\\n58.4\\n65.2\\n70.8\\n73.1\\n72.8\\n69.5\\n58.7\\n51.0\\n44.8\\n58.5\\nME\\nPortland . . . . .\\n11.4\\n13.5\\n24.5\\n34.1\\n43.4\\n52.1\\n58.3\\n57.1\\n48.9\\n38.3\\n30.4\\n17.8\\n35.8\\nMD\\nBaltimore . . . .\\n23.4\\n25.9\\n34.1\\n42.5\\n52.6\\n61.8\\n66.8\\n65.7\\n58.4\\n45.9\\n37.1\\n28.2\\n45.2\\nMA\\nBoston . . . . . .\\n21.6\\n23.0\\n31.3\\n40.2\\n49.8\\n59.1\\n65.1\\n64.0\\n56.8\\n46.9\\n38.3\\n26.7\\n43.6\\nMI\\nDetroit . . . . . . .\\n15.6\\n17.6\\n27.0\\n36.8\\n47.1\\n56.3\\n61.3\\n59.6\\n52.5\\n40.9\\n32.2\\n21.4\\n39.0\\nSault Ste.\\nMarie . . . . . . . .\\n4.6\\n4.8\\n15.3\\n28.4\\n38.4\\n45.5\\n51.3\\n51.3\\n44.3\\n36.2\\n25.9\\n11.8\\n29.8\\nMN\\nDuluth . . . . . . .\\n−2.2 2.8\\n15.7\\n28.9\\n39.6\\n48.5\\n55.1\\n53.3\\n44.5\\n35.1\\n21.5\\n4.9\\n29.0\\nMinneapolis-\\nSt. Paul . . . . . .\\n2.8\\n9.2\\n22.7\\n36.2\\n47.6\\n57.6\\n63.1\\n60.3\\n50.3\\n38.8\\n25.2\\n10.2\\n35.3\\nMS\\nJackson . . . . .\\n32.7\\n35.7\\n44.1\\n51.9\\n60.0\\n67.1\\n70.5\\n69.7\\n63.7\\n50.3\\n42.3\\n36.1\\n52.0\\nMO\\nKansas City . .\\n16.7\\n21.8\\n32.6\\n43.8\\n53.9\\n63.1\\n68.2\\n65.7\\n56.9\\n45.7\\n33.6\\n21.9\\n43.7\\nSt. Louis . . . . .\\n20.8\\n25.1\\n35.5\\n46.4\\n56.0\\n65.7\\n70.4\\n67.9\\n60.5\\n48.3\\n37.7\\n26.0\\n46.7\\nMT\\nGreat Falls . . .\\n11.6\\n17.2\\n22.8\\n31.9\\n40.9\\n48.6\\n53.2\\n52.2\\n43.5\\n35.8\\n24.3\\n14.6\\n33.1\\nSource: U.S. National Oceanic and Atmospheric Administration, Climatography of the United States,\\nNo. 81.\\nthen the sample mean of the data set y1,...,yn is\\n¯y =\\nn\\n\\x02\\ni=1\\n(axi + b)/n =\\nn\\n\\x02\\ni=1\\naxi/n +\\nn\\n\\x02\\ni=1\\nb/n = a ¯x + b\\nExample 2.3.a. The winning scores in the U.S. Masters golf tournament in the\\nyears from 2004 to 2013 were as follows:\\n280, 278, 272, 276, 281, 279, 276, 281, 289, 280'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 35}, page_content='2.3 Summarizing data sets\\n21\\nFind the sample mean of these scores.\\nSolution. Rather than directly adding these values, it is easier to ﬁrst subtract\\n280 from each one to obtain the new values yi = xi −280:\\n0,−2,−8,−4,1,−1,−4,1,9,0\\nBecause the arithmetic average of the transformed data set is\\n¯y= −8/10\\nit follows that\\n¯x = ¯y + 280 = 279.2\\n■\\nSometimes we want to determine the sample mean of a data set that is pre-\\nsented in a frequency table listing the k distinct values v1,...,vk having corre-\\nsponding frequencies f1,...,fk. Since such a data set consists of n = \\x03k\\ni=1 fi\\nobservations, with the value vi appearing fi times, for each i = 1,...,k, it fol-\\nlows that the sample mean of these n data values is\\n¯x =\\nk\\n\\x02\\ni=1\\nvifi/n\\nBy writing the preceding as\\n¯x = f1\\nn v1 + f2\\nn v2 + ··· + fk\\nn vk\\nwe see that the sample mean is a weighted average of the distinct values, where\\nthe weight given to the value vi is equal to the proportion of the n data values\\nthat are equal to vi,i = 1,...,k.\\nExample 2.3.b. The following is a frequency table giving the ages of members\\nof a symphony orchestra for young adults.\\nAge\\nFrequency\\n15\\n2\\n16\\n5\\n17\\n11\\n18\\n9\\n19\\n14\\n20\\n13\\nFind the sample mean of the ages of the 54 members of the symphony.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 36}, page_content='22 CHAPTER 2: Descriptive statistics\\nSolution.\\n¯x = (15·2 + 16·5 + 17·11 + 18·9 + 19·14 + 20·13)/54≈18.24\\n■\\nAnother statistic used to indicate the center of a data set is the sample median;\\nloosely speaking, it is the middle value when the data set is arranged in increas-\\ning order.\\nDeﬁnition. Order the values of a data set of size n from smallest to largest. If\\nn is odd, the sample median is the value in position (n + 1)/2; if n is even, it is\\nthe average of the values in positions n/2 and n/2 + 1.\\nThus the sample median of a set of three values is the second smallest; of a set\\nof four values, it is the average of the second and third smallest.\\nExample 2.3.c. Find the sample median for the data described in Exam-\\nple 2.3.b.\\nSolution. Since there are 54 data values, it follows that when the data are put\\nin increasing order, the sample median is the average of the values in positions\\n27 and 28. Thus, the sample median is 18.5.\\n■\\nThe sample mean and sample median are both useful statistics for describing\\nthe central tendency of a data set. The sample mean makes use of all the data\\nvalues and is affected by extreme values that are much larger or smaller than the\\nothers; the sample median makes use of only one or two of the middle values\\nand is thus not affected by extreme values. Which of them is more useful de-\\npends on what one is trying to learn from the data. For instance, if a city govern-\\nment has a ﬂat rate income tax and is trying to estimate its total revenue from\\nthe tax, then the sample mean of its residents’ income would be a more useful\\nstatistic. On the other hand, if the city was thinking about constructing middle-\\nincome housing, and wanted to determine the proportion of its population\\nable to afford it, then the sample median would probably be more useful.\\nExample 2.3.d. In a study reported in Hoel, D. G., “A representation of mor-\\ntality data by competing risks,” Biometrics, 28, pp. 475–488, 1972, a group of\\n5-week-old mice were each given a radiation dose of 300 rad. The mice were\\nthen divided into two groups; the ﬁrst group was kept in a germ-free environ-\\nment, and the second in conventional laboratory conditions. The numbers of\\ndays until death were then observed. The data for those whose death was due\\nto thymic lymphoma are given in the following stem and leaf plots (whose\\nstems are in units of hundreds of days); the ﬁrst plot is for mice living in the\\ngerm-free conditions and the second for mice living under ordinary laboratory\\nconditions.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 37}, page_content='2.3 Summarizing data sets\\n23\\nGerm-Free Mice\\n1\\n58,92,93,94,95\\n2\\n02,12,15,29,30,37,40,44,47,59\\n3\\n01,01,21,37\\n4\\n15,34,44,85,96\\n5\\n29,37\\n6\\n24\\n7\\n07\\n8\\n00\\nConventional Mice\\n1\\n59,89,91,98\\n2\\n35,45,50,56,61,65,66,80\\n3\\n43,56,83\\n4\\n03,14,28,32\\nDetermine the sample means and the sample medians for the two sets of mice.\\nSolution. It is clear from the stem and leaf plots that the sample mean for\\nthe set of mice put in the germ-free setting is larger than the sample mean for\\nthe set of mice in the usual laboratory setting; indeed, a calculation gives that\\nthe former sample mean is 344.07, whereas the latter one is 292.32. On the\\nother hand, since there are 29 data values for the germ-free mice, the sample\\nmedian is the 15th largest data value, namely, 259; similarly, the sample me-\\ndian for the other set of mice is the 10th largest data value, namely, 265. Thus,\\nwhereas the sample mean is quite a bit larger for the ﬁrst data set, the sample\\nmedians are approximately equal. The reason for this is that whereas the sam-\\nple mean for the ﬁrst set is greatly affected by the ﬁve data values greater than\\n500, these values have a much smaller effect on the sample median. Indeed,\\nthe sample median would remain unchanged if these values were replaced by\\nany other ﬁve values greater than or equal to 259. It appears from the stem and\\nleaf plots that the germ-free conditions probably improved the life span of the\\nﬁve longest living rats, but it is unclear what, if any, effect it had on the life\\nspans of the other rats.\\n■\\nAnother statistic that has been used to indicate the central tendency of a data\\nset is the sample mode, deﬁned to be the value that occurs with the greatest\\nfrequency. If no single value occurs most frequently, then all the values that\\noccur at the highest frequency are called modal values.\\nExample 2.3.e. The following frequency table gives the values obtained in 40\\nrolls of a die.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 38}, page_content='24 CHAPTER 2: Descriptive statistics\\nValue\\nFrequency\\n1\\n9\\n2\\n8\\n3\\n5\\n4\\n5\\n5\\n6\\n6\\n7\\nFind (a) the sample mean, (b) the sample median, and (c) the sample mode.\\nSolution. (a) The sample mean is\\n¯x = (9 + 16 + 15 + 20 + 30 + 42)/40 = 3.05\\n(b) The sample median is the average of the 20th and 21st smallest values,\\nand is thus equal to 3. (c) The sample mode is 1, the value that occurred most\\nfrequently.\\n■\\n2.3.2\\nSample variance and sample standard deviation\\nWhereas we have presented statistics that describe the central tendencies of a\\ndata set, we are also interested in ones that describe the spread or variability\\nof the data values. A statistic that could be used for this purpose would be one\\nthat measures the average value of the squares of the distances between the\\ndata values and the sample mean. This is accomplished by the sample variance,\\nwhich for technical reasons divides the sum of the squares of the differences by\\nn −1 rather than n, where n is the size of the data set.\\nDeﬁnition. The sample variance, call it s2, of the data set x1,...,xn is deﬁned\\nby\\ns2 =\\nn\\n\\x02\\ni=1\\n(xi −¯x)2/(n −1)\\nExample 2.3.f. Find the sample variances of the data sets A and B given below.\\nA:3,4,6,7,10\\nB:−20,5,15,24\\nSolution. As the sample mean for data set A is ¯x = (3 + 4 + 6 + 7 + 10)/5 = 6,\\nit follows that its sample variance is\\ns2 = [(−3)2 + (−2)2 + 02 + 12 +42]/4 = 7.5\\nThe sample mean for data set B is also 6; its sample variance is\\ns2 = [(−26)2 + (−1)2 + 92 + (18)2]/3≈360.67\\nThus, although both data sets have the same sample mean, there is a much\\ngreater variability in the values of the B set than in the A set.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 39}, page_content='2.3 Summarizing data sets\\n25\\nThe following algebraic identity is often useful for computing the sample vari-\\nance:\\nAn algebraic identity\\nn\\n\\x02\\ni=1\\n(xi −¯x)2 =\\nn\\n\\x02\\ni=1\\nx2\\ni −n¯x2\\nThe identity is proven as follows:\\nn\\n\\x02\\ni=1\\n(xi −¯x)2 =\\nn\\n\\x02\\ni=1\\n\\x04\\nx2\\ni −2xi ¯x + ¯x2\\x05\\n=\\nn\\n\\x02\\ni=1\\nx2\\ni −2¯x\\nn\\n\\x02\\ni=1\\nxi +\\nn\\n\\x02\\ni=1\\n¯x2\\n=\\nn\\n\\x02\\ni=1\\nx2\\ni −2n¯x2 + n¯x2\\n=\\nn\\n\\x02\\ni=1\\nx2\\ni −n¯x2\\nThe computation of the sample variance can also be eased by noting that if\\nyi = a + bxi,\\ni = 1,...,n\\nthen ¯y = a + b ¯x, and so\\nn\\n\\x02\\ni=1\\n(yi −¯y)2 = b2\\nn\\n\\x02\\ni=1\\n(xi −¯x)2\\nThat is, if s2\\ny and s2\\nx are the respective sample variances, then\\ns2\\ny = b2s2\\nx\\nIn other words, adding a constant to each data value does not change the sam-\\nple variance; whereas multiplying each data value by a constant results in a\\nnew sample variance that is equal to the old one multiplied by the square of\\nthe constant.\\n■\\nExample 2.3.g. The following data give the worldwide number of fatal airline\\naccidents of commercially scheduled air transports in the years from 1997 to\\n2005.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 40}, page_content='26 CHAPTER 2: Descriptive statistics\\nYear\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\n2003\\n2004\\n2005\\nAccidents\\n25\\n20\\n21\\n18\\n13\\n13\\n7\\n9\\n18\\nSource: National Safety Council.\\nFind the sample variance of the number of accidents in these years.\\nSolution. Let us start by subtracting 18 from each value, to obtain the new\\ndata set:\\n7,2,3,0,−5,−5,−11,−9,0\\nCalling the transformed data y1,...,y9, we have\\n¯y =\\n9\\n\\x02\\ni=1\\nyi/9 = −2,\\nn\\n\\x02\\ni=1\\ny2\\ni = 49 + 4 + 9 + 25 + 25 + 121 + 81 = 314\\nHence, since the sample variance of the transformed data is equal to that of the\\noriginal data, upon using the algebraic identity we obtain\\ns2 = 314 −9(4)\\n8\\n= 34.75\\n■\\nProgram 2.3 on the text disk can be used to obtain the sample variance for large\\ndata sets.\\nThe positive square root of the sample variance is called the sample standard\\ndeviation.\\nDeﬁnition. The quantity s, deﬁned by\\ns =\\n\\x06\\n\\x07\\n\\x07\\n\\x08\\nn\\n\\x02\\ni=1\\n(xi −¯x)2/(n −1)\\nis called the sample standard deviation.\\nThe sample standard deviation is measured in the same units as the data.\\n2.3.3\\nSample percentiles and box plots\\nLoosely speaking, the sample 100p percentile of a data set is that value such\\nthat 100p percent of the data values are less than or equal to it, 0 ≤p ≤1. More\\nformally, we have the following deﬁnition.\\nDeﬁnition. The sample 100p percentile is that data value such that at least 100p\\npercent of the data are less than or equal to it and at least 100(1 −p) percent\\nare greater than or equal to it. If two data values satisfy this condition, then the\\nsample 100p percentile is the arithmetic average of these two values.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 41}, page_content='2.3 Summarizing data sets\\n27\\nTo determine the sample 100p percentile of a data set of size n, we need to\\ndetermine the data values such that\\n1. At least np of the values are less than or equal to it.\\n2. At least n(1 −p) of the values are greater than or equal to it.\\nTo accomplish this, ﬁrst arrange the data in increasing order. Then, note that\\nif np is not an integer, then the only data value that satisﬁes the preceding\\nconditions is the one whose position when the data are ordered from smallest\\nto largest is the smallest integer exceeding np. For instance, if n = 22, p =.8,\\nthen we require a data value such that at least 17.6 of the values are less than\\nor equal to it, and at least 4.4 of them are greater than or equal to it. Clearly,\\nonly the 18th smallest value satisﬁes both conditions and this is the sample 80\\npercentile. On the other hand, if np is an integer, then it is easy to check that\\nboth the values in positions np and np+1 satisfy the preceding conditions, and\\nso the sample 100p percentile is the average of these values. For instance, if we\\nwanted the 90 percentile of a data set of size 20, then both the (18)th and (19)th\\nsmallest values would be such that at least 90 percent of the data values are less\\nthan or equal to them, and at least 10 percent of the data values are greater than\\nor equal to them. Thus, the 90 percentile is the average of these two values.\\nExample 2.3.h. Table 2.6 lists the populations of the 25 most populous U.S.\\ncities for the year 2006. For this data set, ﬁnd (a) the sample 10 percentile and\\n(b) the sample 80 percentile.\\nSolution. (a) Because the sample size is 25 and 25(.10)= 2.5, the sample 10\\npercentile is the third smallest value, equal to 590, 763.\\n(b) Because 25(.80)= 20, the sample 80 percentile is the average of the twenti-\\neth and the twenty-ﬁrst smallest values. Hence, the sample 80 percentile is\\n1,512,986 + 1,448,394\\n2\\n=1,480,690\\n■\\nThe sample 50 percentile is, of course, just the sample median. Along with the\\nsample 25 and 75 percentiles, it makes up the sample quartiles.\\nDeﬁnition. The sample 25 percentile is called the ﬁrst quartile; the sample 50\\npercentile is called the sample median or the second quartile; the sample 75\\npercentile is called the third quartile.\\nThe quartiles break up a data set into four parts, with roughly 25 percent of the\\ndata being less than the ﬁrst quartile, 25 percent being between the ﬁrst and\\nsecond quartile, 25 percent being between the second and third quartile, and\\n25 percent being greater than the third quartile.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 42}, page_content='28 CHAPTER 2: Descriptive statistics\\nTable 2.6 Population of 25 Largest U.S. Cities, July 2006.\\nRank\\nCity\\nPopulation\\n1\\nNew York, NY . . . . . .\\n8,250,567\\n2\\nLos Angeles, CA . . .\\n3,849,378\\n3\\nChicago, IL . . . . . . . .\\n2,833,321\\n4\\nHouston, TX . . . . . . .\\n2,144,491\\n5\\nPhoenix, AR. . . . . . . .\\n1,512,986\\n6\\nPhiladelphia, PA . . . .\\n1,448,394\\n7\\nSan Antonio, TX. . . .\\n1,296,682\\n8\\nSan Diego, CA . . . . .\\n1,256,951\\n9\\nDallas, TX . . . . . . . . . .\\n1,232,940\\n10\\nSan Jose, CA . . . . . .\\n929,936\\n11\\nDetroit, MI . . . . . . . . .\\n918,849\\n12\\nJacksonville, FL . . . .\\n794,555\\n13\\nIndianapolis, IN. . . . .\\n785,597\\n14\\nSan Francisco, CA .\\n744,041\\n15\\nColumbus, OH . . . . .\\n733,203\\n16\\nAustin, TX. . . . . . . . . .\\n709,893\\n17\\nMemphis, TN . . . . . .\\n670,902\\n18\\nFort Worth, TX . . . . .\\n653,320\\n19\\nBaltimore, MD. . . . . .\\n640,961\\n20\\nCharlotte, NC . . . . . .\\n630,478\\n21\\nEl Paso, TX . . . . . . . .\\n609,415\\n22\\nMilwaukee, WI . . . . .\\n602,782\\n23\\nBoston, MA . . . . . . . .\\n590,763\\n24\\nSeattle, WA . . . . . . . .\\n582,454\\n25\\nWashington, DC. . . .\\n581,530\\nExample 2.3.i. Noise is measured in decibels, denoted as dB. One decibel is\\nabout the level of the weakest sound that can be heard in a quiet surrounding\\nby someone with good hearing; a whisper measures about 30 dB; a human\\nvoice in normal conversation is about 70 dB; a loud radio is about 100 dB. Ear\\ndiscomfort usually occurs at a noise level of about 120 dB.\\nThe following data give noise levels measured at 36 different times directly\\noutside of Grand Central Station in Manhattan.\\n82,89,94,110,74,122,112,95,100,78,65,60,90,83,87,75,114,85\\n69,94,124,115,107,88,97,74,72,68,83,91,90,102,77,125,108,65\\nDetermine the quartiles.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 43}, page_content='2.4 Chebyshev’s inequality 29\\nFIGURE 2.7\\nA box plot.\\nSolution. A stem and leaf plot of the data is as follows:\\n6\\n0,5,5,8,9\\n7\\n2,4,4,5,7,8\\n8\\n2,3,3,5,7,8,9\\n9\\n0,0,1,4,4,5,7\\n10\\n0,2,7,8\\n11\\n0,2,4,5\\n12\\n2,4,5\\nBecause 36/4 = 9, the ﬁrst quartile is 74.5, the average of the 9th and 10th\\nsmallest data values; the second quartile is 89.5, the average of the 18th and\\n19th smallest values; the third quartile is 104.5, the average of the 27th and\\n28th smallest values.\\n■\\nA box plot is often used to plot some of the summarizing statistics of a data set.\\nA straight line segment stretching from the smallest to the largest data value\\nis drawn on a horizontal axis; imposed on the line is a “box,” which starts\\nat the ﬁrst and continues to the third quartile, with the value of the second\\nquartile indicated by a vertical line. For instance, the 42 data values presented\\nin Table 2.1 go from a low value of 57 to a high value of 70. The value of the ﬁrst\\nquartile (equal to the value of the 11th smallest on the list) is 60; the value of\\nthe second quartile (equal to the average of the 21st and 22nd smallest values)\\nis 61.5; and the value of the third quartile (equal to the value of the 32nd\\nsmallest on the list) is 64. The box plot for this data set is shown in Figure 2.7.\\nThe length of the line segment on the box plot, equal to the largest minus the\\nsmallest data value, is called the range of the data. Also, the length of the box it-\\nself, equal to the third quartile minus the ﬁrst quartile, is called the interquartile\\nrange.\\n2.4\\nChebyshev’s inequality\\nLet ¯x and s be the sample mean and sample standard deviation of a data set.\\nAssuming that s >0, Chebyshev’s inequality states that for any value of k ≥1,\\ngreater than 100(1−1/k2) percent of the data lie within the interval from ¯x−ks\\nto ¯x+ks. Thus, by letting k = 3/2, we obtain from Chebyshev’s inequality that\\ngreater than 100(5/9)=55.56 percent of the data from any data set lies within\\na distance 1.5s of the sample mean ¯x; letting k=2 shows that greater than 75\\npercent of the data lies within 2s of the sample mean; and letting k=3 shows'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 44}, page_content='30 CHAPTER 2: Descriptive statistics\\nthat greater than 800/9≈88.9 percent of the data lies within 3 sample standard\\ndeviations of ¯x.\\nWhen the size of the data set is speciﬁed, Chebyshev’s inequality can be sharp-\\nened, as indicated in the following formal statement and proof.\\nChebyshev’s inequality\\nLet ¯x and s be the sample mean and sample standard deviation of the data set\\nconsisting of the data x1,...,xn, where s >0. Let\\nSk = {i,1 ≤i ≤n:|xi −¯x| < ks}\\nand let |Sk| be the number of elements in the set Sk. Then, for any k ≥1,\\n|Sk|\\nn\\n≥1 −n −1\\nnk2 >1 −1\\nk2\\nProof.\\n(n −1)s2 =\\nn\\n\\x02\\ni=1\\n(xi −¯x)2\\n=\\n\\x02\\ni∈Sk\\n(xi −¯x)2 +\\n\\x02\\ni̸∈Sk\\n(xi −¯x)2\\n≥\\n\\x02\\ni̸∈Sk\\n(xi −¯x)2\\n≥\\n\\x02\\ni̸∈Sk\\nk2s2\\n= k2s2(n −|Sk|)\\nwhere the ﬁrst inequality follows because all terms being summed are nonneg-\\native, and the second follows since (x1 −¯x)2 ≥k2s2 when i ̸∈Sk. Dividing both\\nsides of the preceding inequality by nk2s2 yields that\\nn −1\\nnk2 ≥n −|Sk|\\nn\\n= 1 −|Sk|\\nn\\nand the result is proven.\\n■\\nBecause Chebyshev’s inequality holds universally, it might be expected for\\ngiven data that the actual percentage of the data values that lie within the in-\\nterval from ¯x −ks to ¯x + ks might be quite a bit larger than the bound given by\\nthe inequality.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 45}, page_content='2.4 Chebyshev’s inequality 31\\nTable 2.7 Top Selling Vehicles.\\nJune 2013 Sales (in thousands of vehicles)\\nFord F Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68.0\\nChevrolet Silverado . . . . . . . . . . . . . . . . . . . . . . . . .\\n43.3\\nToyota Camry. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35.9\\nChevrolet Cruze . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32.9\\nHonda Accord . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31.7\\nHonda Civic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29.7\\nDodge Ram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29.6\\nFord Escape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28.7\\nNissan Altima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26.9\\nHonda CR – V . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26.6\\nExample 2.4.a. Table 2.7 lists the 10 top-selling passenger cars in the United\\nStates in the month of June 2013.\\nA simple calculation yields that the sample mean and sample standard devia-\\ntion of these data are\\n¯x = 35.33\\ns = 11.86\\nThus Chebyshev’s inequality states that at least 100(5/9) = 55.55 percent of the\\ndata lies in the interval\\n\\t\\n¯x −3\\n2s, ¯x + 3\\n2s\\n\\n= (17.54, 53.12)\\nwhereas, in actuality, 90 percent of the data falls within these limits.\\n■\\nSuppose now that we are interested in the fraction of data values that exceed\\nthe sample mean by at least k sample standard deviations, where k is positive.\\nThat is, suppose that ¯x and s are the sample mean and the sample standard\\ndeviation of the data set x1,x2,...,xn. Then, with\\nN(k) = number of i : xi −¯x ≥ks\\nwhat can we say about N(k)/n? Clearly,\\nN(k)\\nn\\n≤number of i : |xi −¯x| ≥ks\\nn\\n≤1\\nk2\\nby Chebyshev’s inequality\\nHowever, we can make a stronger statement, as is shown in the following one-\\nsided version of Chebyshev’s inequality.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 46}, page_content='32 CHAPTER 2: Descriptive statistics\\nThe one-sided Chebyshev inequality\\nLet ¯x and s be the sample mean and sample standard deviation of the data set\\nconsisting of the data x1,...,xn. Suppose s > 0, and let N(k) = number of i :\\nxi −¯x ≥ks. Then, for any k > 0,\\nN(k)\\nn\\n≤\\n1\\n1 + k2\\nProof. Let yi = xi −¯x, i = 1,...,n. For any b >0, we have that\\nn\\n\\x02\\ni=1\\n(yi + b)2 ≥\\n\\x02\\ni:yi≥ks\\n(yi + b)2\\n≥\\n\\x02\\ni:yi≥ks\\n(ks + b)2\\n= N(k)(ks + b)2\\n(2.4.1)\\nwhere the ﬁrst inequality follows because (yi + b)2 ≥0, the second because\\nboth ks and b are positive, and the ﬁnal equality because N(k) is equal to the\\nnumber of i such that yi ≥ks. However,\\nn\\n\\x02\\ni=1\\n(yi + b)2 =\\nn\\n\\x02\\ni=1\\n(y2\\ni + 2byi + b2)\\n=\\nn\\n\\x02\\ni=1\\ny2\\ni + 2b\\nn\\n\\x02\\ni=1\\nyi + nb2\\n=\\nn\\n\\x02\\ni=1\\ny2\\ni + nb2\\n= (n −1)s2 + nb2\\nwhere the next to last equation used that \\x03n\\ni=1 yi = \\x03n\\ni=1(xi −¯x) = \\x03n\\ni=1 xi −\\nn¯x = 0. Therefore, we obtain from Equation (2.4.1) that\\nN(k) ≤(n −1)s2 + nb2\\n(ks + b)2\\n< ns2 + nb2\\n(ks + b)2\\nimplying that\\nN(k)\\nn\\n≤s2 + b2\\n(ks + b)2\\nBecause the preceding is valid for all b >0, we can set b = s/k (which is the\\nvalue of b that minimizes the right-hand side of the preceding) to obtain that'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 47}, page_content='2.5 Normal data sets\\n33\\nN(k)\\nn\\n≤s2 + s2/k2\\n(ks + s/k)2\\nMultiplying the numerator and the denominator of the right side of the pre-\\nceding by k2/s2 gives\\nN(k)\\nn\\n≤\\nk2 + 1\\n(k2 + 1)2 =\\n1\\nk2 + 1\\nand the result is proven. Thus, for instance, where the usual Chebyshev in-\\nequality shows that at most 25 percent of data values are at least 2 standard\\ndeviations greater than the sample mean, the one-sided Chebyshev inequality\\nlowers the bound to “at most 20 percent.”\\n■\\n2.5\\nNormal data sets\\nMany of the large data sets observed in practice have histograms that are similar\\nin shape. These histograms often reach their peaks at the sample median and\\nthen decrease on both sides of this point in a bell-shaped symmetric fashion.\\nSuch data sets are said to be normal and their histograms are called normal\\nhistograms. Figure 2.8 is the histogram of a normal data set.\\nIf the histogram of a data set is close to being a normal histogram, then we\\nsay that the data set is approximately normal. For instance, we would say that\\nthe histogram given in Figure 2.9 is from an approximately normal data set,\\nwhereas the ones presented in Figures 2.10 and 2.11 are not (because each is\\ntoo nonsymmetric). Any data set that is not approximately symmetric about its\\nsample median is said to be skewed. It is “skewed to the right” if it has a long\\ntail to the right and “skewed to the left” if it has a long tail to the left. Thus the\\ndata set presented in Figure 2.10 is skewed to the left and the one of Figure 2.11\\nis skewed to the right.\\nFIGURE 2.8\\nHistogram of a normal data set.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 48}, page_content='34 CHAPTER 2: Descriptive statistics\\nFIGURE 2.9\\nHistogram of an approximately normal data set.\\nFIGURE 2.10\\nHistogram of a data set skewed to the left.\\nFIGURE 2.11\\nHistogram of a data set skewed to the right.\\nIt follows from the symmetry of the normal histogram that a data set that\\nis approximately normal will have its sample mean and sample median ap-\\nproximately equal.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 49}, page_content='2.5 Normal data sets\\n35\\nSuppose that ¯x and s are the sample mean and sample standard deviation of an\\napproximately normal data set. The following rule, known as the empirical rule,\\nspeciﬁes the approximate proportions of the data observations that are within\\ns, 2s, and 3s of the sample mean ¯x.\\nThe empirical rule\\nIf a data set is approximately normal with sample mean ¯x and sample standard\\ndeviation s, then the following statements are true.\\n1. Approximately 68 percent of the observations lie within\\n¯x ±s\\n2. Approximately 95 percent of the observations lie within\\n¯x ±2s\\n3. Approximately 99.7 percent of the observations lie within\\n¯x ±3s\\nExample 2.5.a. The following stem and leaf plot gives the scores on a statistics\\nexam taken by industrial engineering students.\\n9\\n0,1,4\\n8\\n3,5,5,7,8\\n7\\n2,4,4,5,7,7,8\\n6\\n0,2,3,4,6,6\\n5\\n2,5,5,6,8\\n4\\n3,6\\nBy standing the stem and leaf plot on its side we can see that the corresponding\\nhistogram is approximately normal. Use it to assess the empirical rule.\\nSolution. A calculation gives that\\n¯x ≈70.571,\\ns≈14.354\\nThus the empirical rule states that approximately 68 percent of the data are be-\\ntween 56.2 and 84.9; the actual percentage is 1500/28≈53.6. Similarly, the\\nempirical rule gives that approximately 95 percent of the data are between\\n41.86 and 99.28, whereas the actual percentage is 100.\\n■\\nA data set that is obtained by sampling from a population that is itself made\\nup of subpopulations of different types is usually not normal. Rather, the his-\\ntogram from such a data set often appears to resemble a combining, or super-\\nposition, of normal histograms and thus will often have more than one local'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 50}, page_content='36 CHAPTER 2: Descriptive statistics\\nFIGURE 2.12\\nHistogram of a bimodal data set.\\npeak or hump. Because the histogram will be higher at these local peaks than\\nat their neighboring values, these peaks are similar to modes. A data set whose\\nhistogram has two local peaks is said to be bimodal. The data set represented in\\nFigure 2.12 is bimodal.\\n2.6\\nPaired data sets and the sample\\ncorrelation coefﬁcient\\nWe are often concerned with data sets that consist of pairs of values that have\\nsome relationship to each other. If each element in such a data set has an x\\nvalue and a y value, then we represent the ith data point by the pair (xi, yi). For\\ninstance, in an attempt to determine the relationship between the daily midday\\ntemperature (measured in degrees Celsius) and the number of defective parts\\nproduced during that day, a company recorded the data presented in Table 2.8.\\nFor this data set, xi represents the temperature in degrees Celsius and yi the\\nnumber of defective parts produced on day i.\\nA useful way of portraying a data set of paired values is to plot the data on\\na two-dimensional graph, with the x-axis representing the x value of the data\\nand the y-axis representing the y value. Such a plot is called a scatter diagram.\\nFigure 2.13 presents a scatter diagram for the data of Table 2.8.\\nA question of interest concerning paired data sets is whether large x values\\ntend to be paired with large y values, and small x values with small y values;\\nif this is not the case, then we might question whether large values of one of\\nthe variables tend to be paired with small values of the other. A rough answer\\nto these questions can often be provided by the scatter diagram. For instance,\\nFigure 2.13 indicates that there appears to be some connection between high\\ntemperatures and large numbers of defective items. To obtain a quantitative\\nmeasure of this relationship, we now develop a statistic that attempts to mea-'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 51}, page_content='2.6 Paired data sets and the sample correlation coefﬁcient 37\\nTable 2.8 Temperature and Defect Data.\\nDay\\nTemperature\\nNumber of Defects\\n1\\n24.2\\n25\\n2\\n22.7\\n31\\n3\\n30.5\\n36\\n4\\n28.6\\n33\\n5\\n25.5\\n19\\n6\\n32.0\\n24\\n7\\n28.6\\n27\\n8\\n26.5\\n25\\n9\\n25.3\\n16\\n10\\n26.0\\n14\\n11\\n24.4\\n22\\n12\\n24.8\\n23\\n13\\n20.6\\n20\\n14\\n25.1\\n25\\n15\\n21.4\\n25\\n16\\n23.7\\n23\\n17\\n23.9\\n27\\n18\\n25.2\\n30\\n19\\n27.4\\n33\\n20\\n28.3\\n32\\n21\\n28.8\\n35\\n22\\n26.6\\n24\\nsure the degree to which larger x values go with larger y values and smaller x\\nvalues with smaller y values.\\nSuppose that the data set consists of the paired values (xi, yi), i = 1,...,n.\\nTo obtain a statistic that can be used to measure the association between the\\nindividual values of a set of paired data, let ¯x and ¯y denote the sample means\\nof the x values and the y values, respectively. For data pair i, consider xi −¯x the\\ndeviation of its x value from the sample mean, and yi −¯y the deviation of its y\\nvalue from the sample mean. Now if xi is a large x value, then it will be larger\\nthan the average value of all the x’s, so the deviation xi −¯x will be a positive\\nvalue. Similarly, when xi is a small x value, then the deviation xi −¯x will be a\\nnegative value. Because the same statements are true about the y deviations, we\\ncan conclude the following:\\nWhen large values of the x variable tend to be associated with large values of\\nthe y variable and small values of the x variable tend to be associated with\\nsmall values of the y variable, then the signs, either positive or negative, of\\nxi −¯x and yi −¯y will tend to be the same.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 52}, page_content='38 CHAPTER 2: Descriptive statistics\\nFIGURE 2.13\\nA scatter diagram.\\nNow, if xi −¯x and yi −¯y both have the same sign (either positive or negative),\\nthen their product (xi −¯x)(yi −¯y) will be positive. Thus, it follows that when\\nlarge x values tend to be associated with large y values and small x values are\\nassociated with small y values, then \\x03n\\ni=1(xi −¯x)(yi −¯y) will tend to be a\\nlarge positive number. [In fact, not only will all the products have a positive\\nsign when large (small) x values are paired with large (small) y values, but\\nit also follows from a mathematical result known as Hardy’s lemma that the\\nlargest possible value of the sum of paired products will be obtained when the\\nlargest xi −¯x is paired with the largest yi −¯y, the second largest xi −¯x is paired\\nwith the second largest yi −¯y, and so on.] In addition, it similarly follows that\\nwhen large values of xi tend to be paired with small values of yi then the signs\\nof xi −¯x and yi −¯y will be opposite and so \\x03n\\ni=1(xi −¯x)(yi −¯y) will be a large\\nnegative number.\\nTo determine what it means for \\x03n\\ni=1(xi −¯x)(yi −¯y) to be “large,” we standard-\\nize this sum ﬁrst by dividing by n −1 and then by dividing by the product of\\nthe two sample standard deviations. The resulting statistic is called the sample\\ncorrelation coefﬁcient.\\nDeﬁnition. Consider the data pairs (xi,yi), i = 1,...,n and let sx and sy de-\\nnote, respectively, the sample standard deviations of the x values and the y'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 53}, page_content='2.6 Paired data sets and the sample correlation coefﬁcient 39\\nvalues. The sample correlation coefﬁcient, call it r, of the data pairs (xi,yi), i = 1,\\n..., n is deﬁned by\\nr =\\nn\\x03\\ni=1\\n(xi −¯x)(yi −¯y)\\n(n −1)sxsy\\n=\\nn\\x03\\ni=1\\n(xi −¯x)(yi −¯y)\\n\\x0b\\nn\\x03\\ni=1\\n(xi −¯x)2\\nn\\x03\\ni=1\\n(yi −¯y)2\\nWhen r >0 we say that the sample data pairs are positively correlated, and when\\nr <0 we say that they are negatively correlated.\\nThe following are properties of the sample correlation coefﬁcient.\\nProperties of r\\n1. −1 ≤r ≤1\\n2. If for constants a and b, with b >0,\\nyi = a + bxi,\\ni = 1,...,n\\nthen r = 1.\\n3. If for constants a and b, with b <0,\\nyi = a + bxi,\\ni = 1,...,n\\nthen r = −1.\\n4. If r is the sample correlation coefﬁcient for the data pairs xi,yi,i =\\n1,...,n then it is also the sample correlation coefﬁcient for the data pairs\\na + bxi,\\nc + dyi,\\ni =1,...,n\\nprovided that b and d are both positive or both negative.\\nProperty 1 says that the sample correlation coefﬁcient r is always between −1\\nand +1. Property 2 says that r will equal +1 when there is a straight line (also\\ncalled a linear) relation between the paired data such that large y values are\\nattached to large x values. Property 3 says that r will equal −1 when the relation\\nis linear and large y values are attached to small x values. Property 4 states\\nthat the value of r is unchanged when a constant is added to each of the x\\nvariables (or to each of the y variables) or when each x variable (or each y\\nvariable) is multiplied by a positive constant. This property implies that r does\\nnot depend on the dimensions chosen to measure the data. For instance, the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 54}, page_content='40 CHAPTER 2: Descriptive statistics\\nsample correlation coefﬁcient between a person’s height and weight does not\\ndepend on whether the height is measured in feet or in inches or whether the\\nweight is measured in pounds or in kilograms. Also, if one of the values in the\\npair is temperature, then the sample correlation coefﬁcient is the same whether\\nit is measured in Fahrenheit or in Celsius.\\nThe absolute value of the sample correlation coefﬁcient r (that is, |r|, its value\\nwithout regard to its sign) is a measure of the strength of the linear relationship\\nbetween the x and the y values of a data pair. A value of |r| equal to 1 means\\nthat there is a perfect linear relation — that is, a straight line can pass through\\nall the data points (xi,yi), i = 1, ..., n. A value of |r| of around .8 means that\\nthe linear relation is relatively strong; although there is no straight line that\\npasses through all of the data points, there is one that is “close” to them all.\\nA value for |r| of around .3 means that the linear relation is relatively weak.\\nThe sign of r gives the direction of the relation. It is positive when the linear\\nrelation is such that smaller y values tend to go with smaller x values and larger\\ny values with larger x values (and so a straight line approximation points up-\\nward), and it is negative when larger y values tend to go with smaller x values\\nand smaller y values with larger x values (and so a straight line approxima-\\ntion points downward). Figure 2.14 displays scatter diagrams for data sets with\\nvarious values of r.\\nFIGURE 2.14\\nSample correlation coefﬁcients.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 55}, page_content='2.6 Paired data sets and the sample correlation coefﬁcient\\n41\\nExample 2.6.a. Find the sample correlation coefﬁcient for the data presented\\nin Table 2.8.\\nSolution. A computation gives the solution\\nr=.4189\\nthus indicating a relatively weak positive correlation between the daily temper-\\nature and the number of defective items produced that day.\\n■\\nExample 2.6.b. The following data give the resting pulse rates (in beats per\\nminute) and the years of schooling of 10 individuals. A scatter diagram of these\\ndata is presented in Figure 2.15. The sample correlation coefﬁcient for these\\ndata is r = −.7638. This negative correlation indicates that for this data set a\\nhigh pulse rate is strongly associated with a small number of years in school,\\nand a low pulse rate with a large number of years in school.\\nPerson\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nYears of School\\n12\\n16\\n13\\n18\\n19\\n12\\n18\\n19\\n12\\n14\\nPulse Rate\\n73\\n67\\n74\\n63\\n73\\n84\\n60\\n62\\n76\\n71\\n■\\nFIGURE 2.15\\nScatter diagram of years in school and pulse rate.\\nCorrelation measures association, not causation\\nThe data set of Example 2.6.b only considers 10 students and, as such, is not large enough\\nfor one to draw any ﬁrm conclusions about the relationship between years of school and'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 56}, page_content='42 CHAPTER 2: Descriptive statistics\\npulse rate. Moreover, even if the data set were of larger size and with the same strong\\nnegative correlation between an individual’s years of education and that individual’s resting\\npulse rate, we would not be justiﬁed to conclude that additional years of school will directly\\nreduce one’s pulse rate. That is, whereas additional years of school tend to be associated\\nwith a lower resting pulse rate, this does not mean that it is a direct cause of it. Often, the\\nexplanation for such an association lies with an unexpressed factor that is related to both\\nvariables under consideration. In this instance, it may be that a person who has spent ad-\\nditional time in school is more aware of the latest ﬁndings in the area of health, and thus\\nmay be more aware of the importance of exercise and good nutrition; or it may be that it is\\nnot knowledge that is making the difference but rather it is that people who have had more\\neducation tend to end up in jobs that allow them more time for exercise and money for good\\nnutrition. The strong negative correlation between years in school and resting pulse rate\\nprobably results from a combination of these as well as other underlying factors.\\nWe will now prove the ﬁrst three properties of the sample correlation coefﬁcient\\nr. That is, we will prove that |r| ≤1 with equality when the data lie on a straight\\nline. To begin, note that\\n\\x02\\txi −¯x\\nsx\\n−yi −¯y\\nsy\\n\\n2\\n≥0\\n(2.6.1)\\nor\\n\\x02 (xi −¯x)2\\ns2x\\n+\\n\\x02 (yi −¯y)\\ns2y\\n2\\n−2\\n\\x02 (xi −¯x)(yi −¯y)\\nsxsy\\n≥0\\nor\\nn −1 + n −1 −2(n −1)r ≥0\\nshowing that\\nr ≤1\\nTo see when r = 1, suppose ﬁrst that the points (xi,yi),i = 1,...,n lie on the\\nstraight line\\nyi = a + bxi, i = 1,...,n\\nwith positive slope b. If this is so, then\\ns2\\ny = b2s2\\nx ,\\n¯y = a + b ¯x\\nshowing that\\nb = sy\\nsx\\n,\\na = ¯y −sy\\nsx\\n¯x'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 57}, page_content='2.7 The Lorenz curve and Gini index 43\\nNow, note also that r = 1 if and only if there is equality in Equation (2.6.1).\\nThat is, r = 1 if and only if for all i,\\nyi −¯y\\nsy\\n= xi −¯x\\nsx\\nor, equivalently,\\nyi = ¯y −sy\\nsx\\n¯x + sy\\nsx\\nxi\\nConsequently, r = 1 if and only if the data values (xi,yi) lie on a straight line\\nhaving a positive slope.\\nTo show that r ≥−1, with equality if and only if the data values (xi,yi) lie on\\na straight line having a negative slope, start with\\n\\x02\\txi −¯x\\nsx\\n+ yi −¯y\\nsy\\n\\n2\\n≥0\\nand use an argument analogous to the one used to show that r ≤1.\\n2.7\\nThe Lorenz curve and Gini index\\nThe Lorenz curve L(p), 0 ≤p ≤1 is a plot related to the incomes of work-\\ning members of a population, with L(p) representing the proportion of total\\nincome that is earned by those in the lower 100p percent of wage earners.\\nFor instance, L(.3) is the proportion of total income that is earned by all\\nthose among the lowest paid 30 percent of the population. In general, sup-\\npose there are n working individuals whose incomes are, in increasing order,\\nx1 ≤x2 ≤x3 ··· ≤xn. Because x1 + ... + xj is the total of the incomes of the\\nj lowest paid members of the population, and x1 + ... + xn is the total of all\\nthe incomes of the members of the population, it follows that the proportion\\nof the total income that is earned by the lowest paid 100j/n percent of the\\npopulation is\\nL(j/n) = x1 + ... + xj\\nx1 + ... + xn\\n,\\nj = 1,...,n.\\nIt is traditional to let L(0) = 0, and to complete the curve by connecting with\\nstraight lines between the values j\\nn and j+1\\nn .\\nExample 2.7.a. Suppose we want the Lorenz curve when n = 5 and the in-\\ncomes of this group are 9,7,22,5,17. The incomes in increasing order are\\n5,7,9,17,22. Because the sum of all incomes is 60, the Lorenz curve has\\nL(.2) = 5/60,'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 58}, page_content='44 CHAPTER 2: Descriptive statistics\\nFIGURE 2.16\\nThe Lorenz curve for Example 2.7.a.\\nL(.4) = 12/60,\\nL(.6) = 21/60,\\nL(.8) = 38/60,\\nL(1) = 60/60.\\nConnecting with straight lines between these points gives the Lorenz curve (Fig-\\nure 2.16).\\n■\\nLorenz curves are also used to plot the wealths of members of a population,\\nwith L(p) now representing the proportion of all wealth owned by the least\\nwealthy 100p percent of the population. For instance, if the population consists\\nof 1000 people, then L(.22) is the proportion of the populations’s wealth that\\nis owned by its 220 poorest members.\\nNow, the average of the numbers in a set always increases when a new value\\nthat is larger than any of the previous ones is added to the set. Consequently,\\nif the incomes, in increasing value, of the members of the population are x1 ≤\\nx2 ≤x3 ··· ≤xn, then for any j = 1,...,n\\nx1 + ... + xj\\nj\\n≤x1 + ... + xj+1\\nj + 1\\n≤... ≤x1 + ... + xn\\nn\\n,'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 59}, page_content='2.7 The Lorenz curve and Gini index 45\\nFIGURE 2.17\\nThe Lorenz curve and the straight line.\\nimplying that\\nx1 + ... + xj\\nx1 + ... + xn\\n≤j\\nn.\\nThus,\\nL(j/n) ≤j/n.\\nIn addition, it can be veriﬁed that L(j/n) = j/n, j = 1,...,n, only when all\\nn incomes are equal. Consequently, unless all incomes are equal, the Lorenz\\ncurve is always below the straight line from (0,0) to (1,1). (Figure 2.17 indi-\\ncates this for the data of Example 2.7.a.)\\nThus, the Lorenz curve is equal to the straight line from (0,0) to (1,1) when all\\nincomes are equal and dips below the line when incomes are not all equal. The\\nmore unequal the incomes the greater the “hump”, equal to the region between\\nthe straight line and the Lorenz curve (the shaded region in Figure 2.18).\\nA measure of the hump and thus of the inequality of the incomes is given by\\nthe Gini index, which is the ratio of the area of the hump divided by the area\\nunder the straight line. Because the area of a triangle is one half its base times\\nits height, the area under the straight line from (0,0) to (1,1) is equal to 1/2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 60}, page_content='46 CHAPTER 2: Descriptive statistics\\nFIGURE 2.18\\nThe hump of the Lorenz curve.\\nHence, the Gini index, call it G, is given by\\nG = area of the hump of the Lorenz curve\\n1/2\\n.\\nLetting B be the area beneath the Lorenz curve, and using that the area of the\\nhump is equal to the area under the straight line minus the area under the\\nLorenz curve, we see that the value of the Gini index is\\nG = 1/2 −B\\n1/2\\n= 1 −2B.\\nExample 2.7.b. Compute the Gini index for the data of Example 2.7.a.\\nSolution. To compute B, the area under the Lorenz curve, we write B = B1 +\\nB2 + B3 + B4 + B5, where B1 is the area of the region under the Lorenz curve\\nfrom 0 to .2; B2 is the area of the region between .2 and .4; B3 is the area of the\\nregion between .4 and .6; B4 is the area of the region between .6 and .8; and\\nB5 is the area of the region between .8 and 1. Now, B1 is the area of a triangle\\nwhose base is .2 and whose height is 5/60, showing that\\nB1 = (1/2)(.2)(5/60) = 5/600.\\nNow, B2,B3,B4, and B5 are the areas of regions composed of a triangle on top\\nof a rectangle. For instance, the following is the region B2:'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 61}, page_content='2.7 The Lorenz curve and Gini index 47\\nNow all of the triangles in these regions have base .2 and respective heights\\n5/60,7/60,9/60,17/60,22/60. Consequently, the sum of the areas of all these\\ntriangles is\\nArea of triangles = 1\\n2(.2)(5 + 7 + 9 + 17 + 22)/60 = .1.\\nThe four rectangles all have base length .2 and respective heights 5/60, 12/60,\\n21/60, 38/60, yielding\\nArea of rectangles = .2(5 + 12 + 21 + 38)/60 ≈.25333.\\nAdding the area of the rectangles and the area of the triangles gives B = .1 +\\n.25333 = .35333, and thus\\nG = 1 −.70666 = .29334.\\nIn general, suppose the incomes are, in increasing order, x1 ≤x2 ≤x3 ··· ≤xn.\\nLet\\nsj = x1 + ... + xj, j = 1,...,n.\\nThen all the triangles will have base 1/n and will have respective heights\\nx1/sn,x2/sn,...,xn/sn. Consequently, the sum of the area of all the triangles\\nis\\nArea of triangles = 1\\n2n(x1 + ... + xn)/sn = 1\\n2n.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 62}, page_content='48 CHAPTER 2: Descriptive statistics\\nFIGURE 2.19\\nLargest possible Gini index value.\\nThe rectangles will all have length 1/n and will have respective heights\\ns1/sn,s2/sn,...,sn−1/sn. Hence,\\nArea of rectangles = s1 + s2 + ... + sn−1\\nnsn\\n,\\ngiving\\nB = 1\\n2n + s1 + s2 + ... + sn−1\\nnsn\\n.\\n■\\nThe Gini index is 0 when all incomes are equal (and so the area between the\\nstraight line and the Lorenz curve is 0). On the opposite extreme, the largest\\npossible value of the Gini index occurs when only one of the n members of the\\npopulation has a positive income. In this case, the area under the Lorenz curve\\nis the area of a triangle whose base is of length 1/n and whose height is 1 (see\\nFigure 2.19). Thus, B = 1\\n2n, and G = 1 −1/n.\\n2.8\\nUsing R\\nSuppose you want the sample mean and sample variance of the values\\nx1,...,xn. Type\\nx = c(x1,...,xn)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 63}, page_content='2.8 Using R 49\\nand hit return. [Interpret x = c(x1,...,xn) as the sequence of values x1,...,xn.]\\nOn the next line type\\nmean(x)\\nand when you hit return, the sample mean is displayed. On the next line, type\\nvar(x)\\nand when you hit return, the variance is displayed.\\nFor instance, suppose you wanted the sample mean and sample variance of the\\nvalues 4,6,12,9,21,14. Then\\n> x=c(4,6,12,9,21,14)\\n> mean(x)\\n[1] 11\\n> var(x)\\n[1] 37.6\\nBy the way, you do not type > at the beginning of a line. R always starts a line\\nwith this symbol, which is a prompt asking what to do next. (The [1] in front\\nof the answers returned is to indicate that you only asked for a single item.)\\nOther commands are sum(x), which returns the sum of the values; median(x),\\nwhich returns the sample median; and sd(x), which returns the sample stan-\\ndard deviation (equal to the square root of var(x)). Also, if x = c(x1,...,xn) and\\ny = c(y1,...,yn), then x +y = c(x1 +y1,...,xn +yn), x −y = c(x1 −y1,...,xn −\\nyn), x ∗y = c(x1y1,...,xnyn), and if all yi ̸= 0, x/y = c(x1/y1,...,xn/yn). Also,\\nx2 = x ∗x = c(x2\\n1,...,x2\\nn). If a is a real number, then a + x = c(a + x1,...,a +\\nxn), a ∗x = c(ax1,...,axn) and, for a ̸= 0, x/a = c(x1/a,...,xn/a). In addition,\\nif all xi ̸= 0, then a/x = c(a/x1,...,a/xn).\\nIf you have data pairs (x1,y1),...,(xn,yn), then to obtain the sample correla-\\ntion coefﬁcient, do the following:\\n> x = c(x1,...,xn)\\n> y = c(y1,...,yn)\\n> cor(x, y)\\nTo obtain a scatter diagram of the data, type\\n> plot(x,y)\\nand hit return.\\nFor instance, suppose we have the data pairs (4,10), (6,13), (12,22), (9,15),\\n(21,30), (14,15), then we obtain the sample correlation coefﬁcient and scatter\\ndiagram as follows:\\n> x = c(4,6,12,9,21,14)\\n> y = c(10,13,22,15,30,15)\\n> cor(x, y)\\n[1] 0.9041494\\n> plot(x,y)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 64}, page_content='50 CHAPTER 2: Descriptive statistics\\nR can also be used for computation. For instance, suppose we wanted to com-\\npute 18\\n√\\n177\\n677\\n. This is accomplished by\\n> 18 ∗sqrt(177)/677\\n[1] 0.3537288\\nSuppose now that we want to select an element of the vector x = c(x1,...,xn),\\nsay xi, its value in position i. This is accomplished by the R command > x[i].\\nIf we wanted the new vector, consisting of the elements of the vector x, going\\nfrom positions i to j, use the R command x[i : j].\\nFor instance,\\n> x = c(3,18,9,7,22,5,17)\\n> x[3]\\n[1] 9\\n> x[2 : 5]\\n[1]18 9 7 22'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 65}, page_content='2.8 Using R 51\\nWe can also use R to compute a Gini index. First note that if x = c(x1,...,xn),\\nthen the R command\\n> sort(x)\\nreturns the vector whose values are that of x but are now given in increasing\\norder. For instance,\\n> x = c(9,7,22,5,17)\\n> sort(x)\\n[1] 5 7 9 17 22\\nSuppose, as in Example 2.7.b, we wanted to compute the Gini index of the\\nvalues 9,7,22,5,17. Using\\nG = 1 −2B\\nwhere\\nB = 1\\n2n + s1 + s2 + ... + sn−1\\nnsn\\n,\\nwe do the following:\\n> y = c(9,7,22,5,17)\\n> x = sort(y)\\n> s = c(x[1],sum(x[1 : 2]),sum(x[1 : 3]),sum(x[1 : 4]))\\n> B = 1/10 + sum(s)/(5 ∗sum(x))\\n> G = 1 −2 ∗B\\n> G\\n[1] 0.2933333\\nR also allows us to deﬁne functions. For instance, suppose we wanted to deﬁne\\nthe function f such that f (x) = x2. This is done as follows:\\n> f = function(x){x ∗x}\\n> f (4)\\n[1] 16\\nIf we wanted to deﬁne the function f (x) = ex, do the following:\\n> f = function(x){exp(x)}\\n> e = f (1)\\n> e\\n[1] 2.718282'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 66}, page_content='52 CHAPTER 2: Descriptive statistics\\nUsing the function concept, here is another way we could have obtained the\\nGini index of the values 9,7,22,5,17.\\n> y = c(9,7,22,5,17)\\n> x = sort(y)\\n> s = function(j){sum(x[1 : j]}\\n> B = 1/10 + (s(1) + s(2) + s(3) + s(4))/(5 ∗s(5))\\n> G = 1 −2 ∗B\\n> G\\n[1] 0.2933333\\nProblems\\n1. The following is a sample of prices, rounded to the nearest cent, charged\\nper gallon of standard unleaded gasoline in the San Francisco Bay area in\\nJune 1997.\\n3.88,3.90,3.93,3.90,3.93,3.96,3.88,3.94,3.96,3.88,3.94,3.99,3.98\\nRepresent these data in\\na.\\na frequency table;\\nb.\\na relative frequency line graph.\\n2. Explain how a pie chart can be constructed. If a data value had relative\\nfrequency r, at what angle would the lines deﬁning its sector meet?\\n3. The following are the estimated oil reserves, in billions of barrels, for four\\nregions in the Western Hemisphere:\\nUnited States\\n38.7\\nSouth America 22.6\\nCanada\\n8.8\\nMexico\\n60.0\\nRepresent these data in a pie chart.\\n4. Choose a book or article and count the number of words in each of the\\nﬁrst 100 sentences. Present the data in a stem and leaf plot. Now choose\\nanother book or article, by a different author, and do the same. Do the\\ntwo stem and leaf plots look similar? Do you think this could be a viable\\nmethod for telling whether different articles were written by different au-\\nthors?\\n5. The following is a frequency table of daily travel times (in minutes):\\na.\\nHow many days are reported in the frequency table?\\nb.\\nFind the sum of the travel times of all those days.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 67}, page_content='Problems\\n53\\nTravel time\\nFrequency\\n15\\n6\\n18\\n5\\n22\\n4\\n23\\n3\\n24\\n4\\n25\\n2\\n26\\n4\\n32\\n3\\n36\\n1\\n48\\n1\\n6. Table 2.9 gives the number of commercial airline accidents and the total\\nnumber of resulting fatalities in the United States in the years from 1985\\nto 2006.\\na.\\nRepresent the number of yearly airline accidents in a frequency table.\\nb.\\nGive a frequency polygon graph of the number of yearly airline ac-\\ncidents.\\nc.\\nGive a cumulative relative frequency plot of the number of yearly\\nairline accidents.\\nd.\\nFind the sample mean of the number of yearly airline accidents.\\ne.\\nFind the sample median of the number of yearly airline accidents.\\nf.\\nFind the sample mode of the number of yearly airline accidents.\\ng.\\nFind the sample standard deviation of the number of yearly airline\\naccidents.\\nTable 2.9 U.S. Airline Safety, Scheduled Commercial Carriers, 1985–2006.\\nYear\\nDepar-\\ntures\\nAcci-\\ndents\\nFatali-\\nties\\nYear\\nDepar-\\ntures\\nAcci-\\ndents\\nFatali-\\nties\\n(millions)\\n(millions)\\n1985\\n6.1\\n4\\n197\\n1996\\n7.9\\n3\\n342\\n1986\\n6.4\\n2\\n5\\n1997\\n9.9\\n3\\n3\\n1987\\n6.6\\n4\\n231\\n1998\\n10.5\\n1\\n1\\n1988\\n6.7\\n3\\n285\\n1999\\n10.9\\n2\\n12\\n1989\\n6.6\\n11\\n278\\n2000\\n11.1\\n2\\n89\\n1990\\n7.8\\n6\\n39\\n2001\\n10.6\\n6\\n531\\n1991\\n7.5\\n4\\n62\\n2002\\n10.3\\n0\\n0\\n1992\\n7.5\\n4\\n33\\n2003\\n10.2\\n2\\n22\\n1993\\n7.7\\n1\\n1\\n2004\\n10.8\\n1\\n13\\n1994\\n7.8\\n4\\n239\\n2005\\n10.9\\n3\\n22\\n1995\\n8.1\\n2\\n166\\n2006\\n11.2\\n2\\n50\\nSource: National Transportation Safety Board.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 68}, page_content='54 CHAPTER 2: Descriptive statistics\\n7. (Use the table from Problem 6.)\\na.\\nRepresent the number of yearly airline fatalities in a histogram.\\nb.\\nRepresent the number of yearly airline fatalities in a stem and leaf\\nplot.\\nc.\\nFind the sample mean of the number of yearly airline fatalities.\\nd.\\nFind the sample median of the number of yearly airline fatalities.\\ne.\\nFind the sample standard deviation of the number of yearly airline\\nfatalities.\\n8. The sample mean of the weights of the adult women of town A is larger\\nthan the sample mean of the weights of the adult women of town B.\\nMoreover, the sample mean of the weights of the adult men of town A\\nis larger than the sample mean of the weights of the adult men of town\\nB. Can we conclude that the sample mean of the weights of the adults\\nof town A is larger than the sample mean of the weights of the adults of\\ntown B? Explain your answer.\\n9. Benford’s law, also called the ﬁrst-digit law, is the observation that the\\nﬁrst digits in many real-life sets of numerical data do not occur in equal\\nproportions, but rather are biased towards the smaller numbers. More\\nprecisely, it states that the proportion of the data whose ﬁrst non-zero\\ndigit is i,i = 1,...,9 is approximately log( i+1\\ni ) with the logarithm having\\nbase 10. For instance, because log(2) = .301 it says that approximately\\n30.1 percent of the data will have 1 as their ﬁrst digit. Table 2.10 gives the\\nproportions, according to Benford’s law, of the data values having each\\nof 1,...,9 as ﬁrst digit.\\nInterestingly, it has been shown that this result applies to a wide vari-\\nety of real life data sets, including electricity bills, street addresses, stock\\nprices, population numbers, death rates, lengths of rivers, physical and\\nmathematical constants, and seems to be most accurate when the data\\nvalues are widely spread. The law was ﬁrst published by the American\\nastronomer Simon Newcomb in 1881. In 1938 the physicist Frank Ben-\\nford tested it on data from 20 different domains and showed it was a\\nTable 2.10 Benford’s law for ﬁrst digits.\\nFirst digit\\nProportion of data having it as\\nﬁrst digit\\n1\\n.301\\n2\\n.176\\n3\\n.125\\n4\\n.097\\n5\\n.079\\n6\\n.067\\n7\\n.058\\n8\\n.051\\n9\\n.046'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 69}, page_content='Problems 55\\ngood ﬁt in most of these cases. Among others, he considered the surface\\nareas of rivers, the population sizes of US cities, physical constants, and\\nmolecular weights.\\nA multiple choice question on a physics exam asks whether (a) 7.3316,\\n(b) 6.2421, (c) 1.4512 or (d) 8.1818 is the density (in grams per cubed\\ncentimeter) of a 100 percent solution of hydrogen peroxide at a tem-\\nperature of 20 degrees C. Without knowing anything about hydrogen\\nperoxide, which answer would you guess to be correct?\\n10. A total of 100 people work at company A, whereas a total of 110 work at\\ncompany B. Suppose the total employee payroll is larger at company A\\nthan at company B.\\na.\\nWhat does this imply about the median of the salaries at com-\\npany A with regard to the median of the salaries at company\\nB?\\nb.\\nWhat does this imply about the average of the salaries at com-\\npany A with regard to the average of the salaries at company\\nB?\\n11. The sample mean of the initial 99 values of a data set consisting of 198\\nvalues is equal to 120, whereas the sample mean of the ﬁnal 99 values\\nis equal to 100. What can you conclude about the sample mean of the\\nentire data set\\na.\\nRepeat when “sample mean” is replaced by “sample median.”\\nb.\\nRepeat when “sample mean” is replaced by “sample mode.”\\n12. The following table gives the number of pedestrians, classiﬁed accord-\\ning to age group and sex, killed in fatal road accidents in England in\\n1922.\\na.\\nApproximate the sample means of the ages of the males.\\nb.\\nApproximate the sample means of the ages of the females.\\nc.\\nApproximate the quartiles of the males killed.\\nd.\\nApproximate the quartiles of the females killed.\\nAge\\nNumber of Males\\nNumber of Females\\n0–5\\n120\\n67\\n5–10\\n184\\n120\\n10–15\\n44\\n22\\n15–20\\n24\\n15\\n20–30\\n23\\n25\\n30–40\\n50\\n22\\n40–50\\n60\\n40\\n50–60\\n102\\n76\\n60–70\\n167\\n104\\n70–80\\n150\\n90\\n80–100\\n49\\n27'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 70}, page_content='56 CHAPTER 2: Descriptive statistics\\n13. The following are the percentages of ash content in 12 samples of coal\\nfound in close proximity:\\n9.2,14.1,9.8,12.4,16.0,12.6,22.7,18.9,21.0,14.5,20.4,16.9\\nFind the\\na.\\nsample mean, and\\nb.\\nsample standard deviation of these percentages.\\n14. The sample mean and sample variance of ﬁve data values are, respec-\\ntively, ¯x = 104 and s2 = 16. If three of the data values are 102, 100, 105,\\nwhat are the other two data values?\\n15. Suppose you are given the average pay of all working people in each of\\nthe 50 states of the United States.\\na.\\nDo you think that the sample mean of the averages for the 50 states\\nwill equal the value given for the entire United States?\\nb.\\nIf the answer to part (a) is no, explain what other information\\naside from just the 50 averages would be needed to determine\\nthe sample mean salary for the entire country. Also, explain how\\nyou would use the additional information to compute this quan-\\ntity.\\n16. The following data represent the lifetimes (in hours) of a sample of 40\\ntransistors:\\n112,121,126,108,141,104,136,134\\n121,118,143,116,108,122,127,140\\n113,117,126,130,134,120,131,133\\n118,125,151,147,137,140,132,119\\n110,124,132,152,135,130,136,128\\na.\\nDetermine the sample mean, median, and mode.\\nb.\\nGive a cumulative relative frequency plot of these data.\\n17. An experiment measuring the percent shrinkage on drying of 50 clay\\nspecimens produced the following data:\\n18.2\\n21.2\\n23.1 18.5 15.6\\n20.8\\n19.4\\n15.4\\n21.2\\n13.4\\n16.4 18.7\\n18.2 19.6 14.3\\n16.6 24.0\\n17.6 17.8\\n20.2\\n17.4 23.6\\n17.5\\n20.3\\n16.6\\n19.3 18.5\\n19.3\\n21.2\\n13.9\\n20.5\\n19.0\\n17.6 22.3 18.4\\n21.2\\n20.4\\n21.4\\n20.3\\n20.1\\n19.6\\n20.6\\n14.8 19.7\\n20.5\\n18.0\\n20.8\\n15.8 23.1 17.0\\na.\\nDraw a stem and leaf plot of these data.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 71}, page_content='Problems\\n57\\nb.\\nCompute the sample mean, median, and mode.\\nc.\\nCompute the sample variance.\\nd.\\nGroup the data into class intervals of size 1 percent starting with the\\nvalue 13.0, and draw the resulting histogram.\\ne.\\nFor the grouped data acting as if each of the data points in an in-\\nterval was actually located at the midpoint of that interval, com-\\npute the sample mean and sample variance and compare this\\nwith the results obtained in parts (b) and (c). Why do they dif-\\nfer?\\n18. A computationally efﬁcient way to compute the sample mean and sam-\\nple variance of the data set x1,x2,...,xn is as follows. Let\\n¯xj =\\nj\\x03\\ni=1\\nxi\\nj\\n,\\nj =1,...,n\\nbe the sample mean of the ﬁrst j data values, and let\\ns2\\nj =\\nj\\x03\\ni=1\\n(xi −¯xj)2\\nj −1\\n,\\nj = 2,...,n\\nbe the sample variance of the ﬁrst j,j ≥2, values. Then, with s2\\n1 = 0, it\\ncan be shown that\\n¯xj+1 = ¯xj + xj+1 −¯xj\\nj + 1\\nand\\ns2\\nj+1 =\\n\\t\\n1 −1\\nj\\n\\ns2\\nj + (j + 1)(¯xj+1 −¯xj)2\\na.\\nUse the preceding formulas to compute the sample mean and sam-\\nple variance of the data values 3, 4, 7, 2, 9, 6.\\nb.\\nVerify your results in part (a) by computing as usual.\\nc.\\nVerify the formula given above for ¯xj+1 in terms of ¯xj.\\n19. Use the data of Table 2.5 to ﬁnd the\\na.\\n90 percentile of the average temperature for January;\\nb.\\n75 percentile of the average temperature for July.\\n20. Find the quartiles of the following ages at death as given in obit-\\nuaries of the New York Times in the 2 weeks preceding 1 August\\n2013.\\n92, 90, 92, 74, 69, 80, 94, 98, 65, 96, 84, 69, 86, 91, 88'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 72}, page_content='58 CHAPTER 2: Descriptive statistics\\n74, 97, 85, 88, 68, 77, 94, 88, 65, 76, 75, 60\\n69, 97, 92, 85, 70, 80, 93, 91, 68, 82, 78, 89\\n21. The universities having the largest number of months in which they\\nranked in the top 10 for the number of Google searches over the past\\n114 months (as of June 2013) are as follows.\\nUniversity\\nNumber of Months in Top 10\\nHarvard University\\n114\\nUniversity of Texas, Austin\\n114\\nUniversity of Michigan\\n114\\nStanford University\\n113\\nUniversity of California Los Angeles (UCLA)\\n111\\nUniversity of California Berkeley\\n97\\nPenn State University\\n94\\nMassachusetts Institute of Technology (MIT)\\n66\\nUniversity of Southern California (USC)\\n63\\nOhio State University\\n52\\nYale University\\n48\\nUniversity of Washington\\n33\\na.\\nFind the sample mean of the data.\\nb.\\nFind the sample variance of the data.\\nc.\\nFind the sample quartiles of the data.\\n22. Fill in the missing word or phrase to complete the following sentence,\\n“If a new value is added to a set of numbers, then the sample mean will\\nincrease over what it was if the new value is —.”\\n23. Represent the data of Problem 20 in a box plot.\\n24. The average particulate concentration, in micrograms per cubic meter,\\nwas measured in a petrochemical complex at 36 randomly chosen times,\\nwith the following concentrations resulting:\\n5,18,15,7,23,220,130,85,103,25,80,7,24,6,13,65,37,25,\\n24,65,82,95,77,15,70,110,44,28,33,81,29,14,45,92,17,53\\na.\\nRepresent the data in a histogram.\\nb.\\nIs the histogram approximately normal?\\n25. A chemical engineer desiring to study the evaporation rate of water from\\nbrine evaporation beds obtained data on the number of inches of evap-\\noration in each of 55 July days spread over 4 years. The data are given\\nin the following stem and leaf plot, which shows that the smallest data'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 73}, page_content='Problems 59\\nvalue was .02 inch, and the largest .56 inch.\\n.0\\n2,6\\n.1\\n1,4\\n.2\\n1,1,1,3,3,4,5,5,5,6,9\\n.3\\n0,0,2,2,2,3,3,3,3,4,4,5,5,5,6,6,7,8,9\\n.4\\n0,1,2,2,2,3,4,4,4,5,5,5,7,8,8,8,9,9\\n.5\\n2,5,6\\nFind the\\na.\\nsample mean;\\nb.\\nsample median;\\nc.\\nsample standard deviation of these data.\\nd.\\nDo the data appear to be approximately normal?\\ne.\\nWhat percentage of data values are within 1 standard deviation of\\nthe mean?\\n26. The following are the grade point averages of 30 students recently ad-\\nmitted to the graduate program in the Department of Industrial Engi-\\nneering and Operations Research at the University of California at Berke-\\nley.\\n3.46,3.72,3.95,3.55,3.62,3.80,3.86,3.71,3.56,3.49,3.96,3.90,3.70,3.61,\\n3.72,3.65,3.48,3.87,3.82,3.91,3.69,3.67,3.72,3.66,3.79,3.75,3.93,3.74,\\n3.50,3.83\\na.\\nRepresent the preceding data in a stem and leaf plot.\\nb.\\nCalculate the sample mean ¯x.\\nc.\\nCalculate the sample standard deviation s.\\nd.\\nDetermine the proportion of the data values that lies within ¯x ±1.5s\\nand compare with the lower bound given by Chebyshev’s inequal-\\nity.\\ne.\\nDetermine the proportion of the data values that lies within ¯x ±2s\\nand compare with the lower bound given by Chebyshev’s inequal-\\nity.\\n27. Do the data in Problem 26 appear to be approximately normal? For parts\\n(c) and (d) of this problem, compare the approximate proportions given\\nby the empirical rule with the actual proportions.\\n28. Would you expect that a histogram of the weights of all the members of\\na health club would be approximately normal?\\n29. Use the data of Problem 16.\\na.\\nCompute the sample mean and sample median.\\nb.\\nAre the data approximately normal?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 74}, page_content='60 CHAPTER 2: Descriptive statistics\\nc.\\nCompute the sample standard deviation s.\\nd.\\nWhat percentage of the data fall within ¯x ±1.5s?\\ne.\\nCompare your answer in part (d) to that given by the empirical\\nrule.\\nf.\\nCompare your answer in part (d) to the bound given by Chebyshev’s\\ninequality.\\n30. The following are the heights and starting salaries of 12 law school\\nclassmates whose law school examination scores were roughly the same.\\nHeight\\nSalary\\n64\\n91\\n65\\n94\\n66\\n88\\n67\\n103\\n69\\n77\\n70\\n96\\n72\\n105\\n72\\n88\\n74\\n122\\n74\\n102\\n75\\n90\\n76\\n114\\na.\\nRepresent these data in a scatter diagram.\\nb.\\nFind the sample correlation coefﬁcient.\\n31. A random sample of individuals were rated as to their standing posture.\\nIn addition, the numbers of days of back pain each had experienced dur-\\ning the past year were also recorded. Surprisingly to the researcher these\\ndata indicated a positive correlation between good posture and number\\nof days of back pain. Does this indicate that good posture causes back\\npain?\\n32. If for each of the ﬁfty states we plot the paired data consisting of the\\naverage income of residents of the state and the number of foreign-born\\nimmigrants who reside in the state, then the data pairs will have a pos-\\nitive correlation. Can we conclude that immigrants tend to have higher\\nincomes than native-born Americans? If not, how else could this phe-\\nnomenon be explained?\\n33. A random group of 12 high school juniors were asked to estimate the\\naverage number of hours they study each week. The following give these\\nhours along with the student’s grade point average.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 75}, page_content='Problems 61\\nHours\\nGPA\\n6\\n2.8\\n14\\n3.2\\n3\\n3.1\\n22\\n3.6\\n9\\n3.0\\n11\\n3.3\\n12\\n3.4\\n5\\n2.7\\n18\\n3.1\\n24\\n3.8\\n15\\n3.0\\n17\\n3.9\\nFind the sample correlation coefﬁcient between hours reported and\\nGPA.\\n34. Verify property 3 of the sample correlation coefﬁcient.\\n35. Verify property 4 of the sample correlation coefﬁcient.\\n36. In a study of children in grades 2 through 4, a researcher gave each stu-\\ndent a reading test. When looking at the resulting data the researcher\\nnoted a positive correlation between a student’s reading test score and\\nheight. The researcher concluded that taller children read better because\\nthey can more easily see the blackboard. What do you think?\\n37. A recent study yielded a positive correlation between breast-fed babies\\nand scores on a vocabulary test taken at age 6. Discuss the potential dif-\\nﬁculties in interpreting the results of this study.\\n38. Draw the Lorenz curve and compute the Gini index of a group whose\\nincomes are 25,32,60,40,38,50.\\n39. Draw the Lorenz curve and compute the Gini index of a group whose\\nannual incomes (in thousands of dollars) are given by the following fre-\\nquency table.\\nValue\\nFrequency\\n30\\n2\\n50\\n4\\n60\\n5\\n90\\n4\\n100\\n3\\n120\\n2\\n40.\\na.\\nHow is the Gini index changed if all values are multiplied by a pos-\\nitive constant c?\\nb.\\nDoes the Gini index increase, decrease, remain the same, or is it not\\npossible to tell, when a positive constant c is added to all values?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 76}, page_content='CHAPTER 3\\nElements of probability\\n3.1\\nIntroduction\\nThe concept of the probability of a particular event of an experiment is subject\\nto various meanings or interpretations. For instance, if a geologist is quoted\\nas saying that “there is a 60 percent chance of oil in a certain region,” we all\\nprobably have some intuitive idea as to what is being said. Indeed, most of us\\nwould probably interpret this statement in one of two possible ways: either by\\nimagining that\\n1. the geologist feels that, over the long run, in 60 percent of the regions\\nwhose outward environmental conditions are very similar to the condi-\\ntions that prevail in the region under consideration, there will be oil; or\\n2. the geologist believes that it is more likely that the region will contain\\noil than it is that it will not; and in fact .6 is a measure of the geologist’s\\nbelief in the hypothesis that the region will contain oil.\\nThe two foregoing interpretations of the probability of an event are referred to\\nas being the frequency interpretation and the subjective (or personal) interpre-\\ntation of probability. In the frequency interpretation, the probability of a given\\noutcome of an experiment is considered as being a “property” of that outcome.\\nIt is imagined that this property can be operationally determined by continual\\nrepetition of the experiment — the probability of the outcome will then be ob-\\nservable as being the proportion of the experiments that result in the outcome.\\nThis is the interpretation of probability that is most prevalent among scientists.\\nIn the subjective interpretation, the probability of an outcome is not thought of\\nas being a property of the outcome but rather is considered a statement about\\nthe beliefs of the person who is quoting the probability, concerning the chance\\nthat the outcome will occur. Thus, in this interpretation, probability becomes\\na subjective or personal concept and has no meaning outside of expressing\\none’s degree of belief. This interpretation of probability is often favored by\\nphilosophers and certain economic decision makers.\\nRegardless of which interpretation one gives to probability, however, there is\\na consensus that the mathematics of probability are the same in either case.\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00012-0\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n63'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 77}, page_content='64 CHAPTER 3: Elements of probability\\nFor instance, if you think that the probability that it will rain tomorrow is .3\\nand you feel that the probability that it will be cloudy but without any rain\\nis .2, then you should feel that the probability that it will either be cloudy\\nor rainy is .5 independently of your individual interpretation of the concept\\nof probability. In this chapter, we present the accepted rules, or axioms, used\\nin probability theory. As a preliminary to this, however, we need to study the\\nconcept of the sample space and the events of an experiment.\\n3.2\\nSample space and events\\nConsider an experiment whose outcome is not predictable with certainty in\\nadvance. Although the outcome of the experiment will not be known in ad-\\nvance, let us suppose that the set of all possible outcomes is known. This set\\nof all possible outcomes of an experiment is known as the sample space of the\\nexperiment and is denoted by S. Some examples are the following.\\n1. If the outcome of an experiment consists in the determination of the sex\\nof a newborn child, then\\nS = {g,b}\\nwhere the outcome g means that the child is a girl and b that it is a boy.\\n2. If the experiment consists of the running of a race among the seven horses\\nhaving post positions 1, 2, 3, 4, 5, 6, 7, then\\nS = {all orderings of (1,2,3,4,5,6,7)}\\nThe outcome (2, 3, 1, 6, 5, 4, 7) means, for instance, that the number 2\\nhorse is ﬁrst, then the number 3 horse, then the number 1 horse, and so\\non.\\n3. Suppose we are interested in determining the amount of dosage that\\nmust be given to a patient until that patient reacts positively. One possi-\\nble sample space for this experiment is to let S consist of all the positive\\nnumbers. That is, let\\nS = (0,∞)\\nwhere the outcome would be x if the patient reacts to a dosage of value x\\nbut not to any smaller dosage.\\nAny subset E of the sample space is known as an event. That is, an event is a\\nset consisting of possible outcomes of the experiment. If the outcome of the\\nexperiment is contained in E, then we say that E has occurred. Some examples\\nof events are the following.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 78}, page_content='3.2 Sample space and events\\n65\\nIn Example 1 if E = {g}, then E is the event that the child is a girl. Similarly, if\\nF = {b}, then F is the event that the child is a boy.\\nIn Example 2 if\\nE = {all outcomes in S starting with a 3}\\nthen E is the event that the number 3 horse wins the race.\\nFor any two events E and F of a sample space S, we deﬁne the new event E∪F,\\ncalled the union of the events E and F, to consist of all outcomes that are ei-\\nther in E or in F or in both E and F. That is, the event E∪F will occur if\\neither E or F occurs. For instance, in Example 1 if E = {g} and F = {b}, then\\nE∪F = {g,b}. That is, E∪F would be the whole sample space S. In Example 2\\nif E = {all outcomes starting with 6} is the event that the number 6 horse wins\\nand F = {all outcomes having 6 in the second position} is the event that the\\nnumber 6 horse comes in second, then E ∪F is the event that the number 6\\nhorse comes in either ﬁrst or second.\\nSimilarly, for any two events E and F, we may also deﬁne the new event EF,\\nsometimes written as E ∩F, called the intersection of E and F, to consist of\\nall outcomes that are in both E and F. That is, the event EF will occur only if\\nboth E and F occur. For instance, in Example 3 if E = (0,5) is the event that the\\nrequired dosage is less than 5 and F=(2,10) is the event that it is between 2 and\\n10, then EF=(2,5) is the event that the required dosage is between 2 and 5.\\nIn Example 2 if E = {all outcomes ending in 5} is the event that horse number\\n5 comes in last and F = {all outcomes starting with 5} is the event that horse\\nnumber 5 comes in ﬁrst, then the event EF does not contain any outcomes and\\nhence cannot occur. To give such an event a name, we shall refer to it as the null\\nevent and denote it by ∅. Thus ∅refers to the event consisting of no outcomes.\\nIf EF = ∅, implying that E and F cannot both occur, then E and F are said to be\\nmutually exclusive.\\nFor any event E, we deﬁne the event Ec, referred to as the complement of E, to\\nconsist of all outcomes in the sample space S that are not in E. That is, Ec will\\noccur if and only if E does not occur. In Example 1 if E = {b} is the event that\\nthe child is a boy, then Ec = {g} is the event that it is a girl. Also note that since\\nthe experiment must result in some outcome, it follows that Sc = ∅.\\nFor any two events E and F, if all of the outcomes in E are also in F, then we\\nsay that E is contained in F and write E ⊂F (or equivalently, F ⊃E). Thus\\nif E ⊂F, then the occurrence of E necessarily implies the occurrence of F. If\\nE ⊂F and F ⊂E, then we say that E and F are equal (or identical) and we\\nwrite E = F.\\nWe can also deﬁne unions and intersections of more than two events. In partic-\\nular, the union of the events E1,E2, ... ,En, denoted either by E1∪E2 ∪···∪En\\nor by ∪n\\n1Ei, is deﬁned to be the event consisting of all outcomes that are in'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 79}, page_content='66 CHAPTER 3: Elements of probability\\nEi for at least one i = 1,2,...,n. Similarly, the intersection of the events Ei,\\ni = 1,2,...,n, denoted by E1E2 ···En, is deﬁned to be the event consisting of\\nthose outcomes that are in all of the events Ei,i = 1,2,...,n. In other words,\\nthe union of the Ei occurs when at least one of the events Ei occurs; the inter-\\nsection occurs when all of the events Ei occur.\\n3.3\\nVenn diagrams and the algebra of events\\nA graphical representation of events that is very useful for illustrating logical\\nrelations among them is the Venn diagram. The sample space S is represented\\nas consisting of all the points in a large rectangle, and the events E,F,G,..., are\\nrepresented as consisting of all the points in given circles within the rectangle.\\nEvents of interest can then be indicated by shading appropriate regions of the\\ndiagram. For instance, in the three Venn diagrams shown in Figure 3.1, the\\nshaded areas represent, respectively, the events E∪F, EF, and Ec. The Venn\\ndiagram of Figure 3.2 indicates that E ⊂F.\\nThe operations of forming unions, intersections, and complements of events\\nobey certain rules not dissimilar to the rules of algebra. We list a few of these.\\nCommutative law\\nE ∪F = F ∪E\\nEF =FE\\nAssociative law\\n(E ∪F) ∪G = E ∪(F ∪G) (EF)G = E(FG)\\nDistributive law\\n(E ∪F)G = EG ∪FG\\nEF ∪G = (E ∪G)(F ∪G)\\nFIGURE 3.1\\nVenn diagrams. (A) Shaded region: E∪F; (B) shaded region: EF; (C) shaded region: Ec.\\nFIGURE 3.2\\nVenn diagram.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 80}, page_content='3.4 Axioms of probability 67\\nFIGURE 3.3\\nProving the distributive law. (A) Shaded region: EG; (B) shaded region: FG; (C) shaded region:\\n(E ∪F)G, (E ∪F)G = EG ∪FG.\\nThese relations are veriﬁed by showing that any outcome that is contained in\\nthe event on the left side of the equality is also contained in the event on\\nthe right side and vice versa. One way of showing this is by means of Venn\\ndiagrams. For instance, the distributive law may be veriﬁed by the sequence of\\ndiagrams shown in Figure 3.3.\\nThe following useful relationship between the three basic operations of form-\\ning unions, intersections, and complements of events is known as DeMorgan’s\\nlaws.\\n(E ∪F)c = EcF c\\n(EF)c = Ec ∪F c\\n3.4\\nAxioms of probability\\nIt appears to be an empirical fact that if an experiment is continually repeated\\nunder the exact same conditions, then for any event E, the proportion of time\\nthat the outcome is contained in E approaches some constant value as the\\nnumber of repetitions increases. For instance, if a coin is continually ﬂipped,\\nthen the proportion of ﬂips resulting in heads will approach some value as\\nthe number of ﬂips increases. It is this constant limiting frequency that we\\noften have in mind when we speak of the probability of an event.\\nFrom a purely mathematical viewpoint, we shall suppose that for each event E\\nof an experiment having a sample space S there is a number, denoted by P(E),\\nthat is in accord with the following three axioms:\\nAxiom 1.\\n0 ≤P(E) ≤1\\nAxiom 2.\\nP(S) = 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 81}, page_content='68 CHAPTER 3: Elements of probability\\nAxiom 3. For any sequence of mutually exclusive events E1,E2,... (that is,\\nevents for which EiEj = ∅when i̸=j),\\nP\\n\\x02 n\\n\\x03\\ni=1\\nEi\\n\\x04\\n=\\nn\\n\\x05\\ni=1\\nP(Ei),\\nn = 1,2,...,∞\\nWe call P(E ) the probability of the event E.\\nThus, Axiom 1 states that the probability that the outcome of the experiment\\nis contained in E is some number between 0 and 1. Axiom 2 states that, with\\nprobability 1, the outcome will be a member of the sample space S. Axiom 3\\nstates that for any set of mutually exclusive events the probability that at least\\none of these events occurs is equal to the sum of their respective probabilities.\\nIt should be noted that if we interpret P(E) as the relative frequency of the\\nevent E when a large number of repetitions of the experiment are performed,\\nthen P(E) would indeed satisfy the above axioms. For instance, the proportion\\n(or frequency) of time that the outcome is in E is clearly between 0 and 1, and\\nthe proportion of time that it is in S is 1 (since all outcomes are in S). Also,\\nif E and F have no outcomes in common, then the proportion of time that\\nthe outcome is in either E or F is the sum of their respective frequencies. As\\nan illustration of this last statement, suppose the experiment consists of the\\nrolling of a pair of dice and suppose that E is the event that the sum is 2, 3,\\nor 12 and F is the event that the sum is 7 or 11. Then if outcome E occurs 11\\npercent of the time and outcome F 22 percent of the time, then 33 percent of\\nthe time the outcome will be either 2, 3, 12, 7, or 11.\\nThese axioms will now be used to prove two simple propositions concerning\\nprobabilities. We ﬁrst note that E and Ec are always mutually exclusive, and\\nsince E ∪Ec = S, we have by Axioms 2 and 3 that\\n1 = P(S) = P(E ∪Ec) = P(E) + P(Ec)\\nOr equivalently, we have the following:\\nProposition 3.4.1.\\nP(Ec) = 1 −P(E)\\nIn other words, Proposition 3.4.1 states that the probability that an event does\\nnot occur is 1 minus the probability that it does occur. For instance, if the\\nprobability of obtaining a head on the toss of a coin is 3\\n8, the probability of\\nobtaining a tail must be 5\\n8.\\nOur second proposition gives the relationship between the probability of the\\nunion of two events in terms of the individual probabilities and the probability\\nof the intersection.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 82}, page_content='3.4 Axioms of probability 69\\nFIGURE 3.4\\nVenn diagram.\\nProposition 3.4.2.\\nP(E ∪F) = P(E) + P(F) −P(EF)\\nProof. This proposition is most easily proven by the use of a Venn diagram\\nas shown in Figure 3.4. As the regions I, II, and III are mutually exclusive, it\\nfollows that\\nP(E ∪F) = P(I) + P(II) + P(III)\\nP(E) = P(I) + P(II)\\nP(F) = P(II) + P(III)\\nwhich shows that\\nP(E ∪F) = P(E) + P(F) −P(II)\\nand the proof is complete since II = EF.\\n■\\nExample 3.4.a. A total of 28 percent of males living in Nevada smoke cigarettes,\\n6 percent smoke cigars, and 3 percent smoke both cigars and cigarettes. What\\npercentage of males smoke neither cigars nor cigarettes?\\nSolution. Let E be the event that a randomly chosen male is a cigarette smoker\\nand let F be the event that he is a cigar smoker. Then, the probability this person\\nis either a cigarette or a cigar smoker is\\nP(E ∪F) = P(E) + P(F) −P(EF) = .28 + .06 −.03 = .31\\nThus the probability that the person is not a smoker is 1 −.31 = .69, implying\\nthat 69 percent of males smoke neither cigarettes nor cigars.\\n■\\nThe odds of an event A is deﬁned by\\nP(A)\\nP(Ac) =\\nP(A)\\n1 −P(A)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 83}, page_content='70 CHAPTER 3: Elements of probability\\nThus the odds of an event A tells how much more likely it is that A occurs than\\nthat it does not occur. For instance, if P(A) = 3/4, then P(A)/(1 −P(A)) = 3,\\nso the odds are 3. Consequently, it is 3 times as likely that A occurs as it is that\\nit does not. (Common terminology is to say that the odds are 3 to 1 in favor of\\nthe event A.)\\n3.5\\nSample spaces having equally likely outcomes\\nFor a large number of experiments, it is natural to assume that each point in\\nthe sample space is equally likely to occur. That is, for many experiments whose\\nsample space S is a ﬁnite set, say S = {1,2,...,N}, it is often natural to assume\\nthat\\nP({1}) = P({2}) = ··· = P({N}) = p\\n(say)\\nNow it follows from Axioms 2 and 3 that\\n1 = P(S ) = P({1}) + ··· + P({N}) = Np\\nwhich shows that\\nP({i}) = p = 1/N\\nFrom this it follows from Axiom 3 that for any event E,\\nP(E) = Number of points in E\\nN\\nIn words, if we assume that each outcome of an experiment is equally likely to\\noccur, then the probability of any event E equals the proportion of points in\\nthe sample space that are contained in E.\\nThus, to compute probabilities it is often necessary to be able to effectively\\ncount the number of different ways that a given event can occur. To do this, we\\nwill make use of the following rule.\\nBasic principle of counting\\nSuppose that two experiments are to be performed. Then if experiment 1 can\\nresult in any one of m possible outcomes and if, for each outcome of experi-\\nment 1, there are n possible outcomes of experiment 2, then together there are\\nmn possible outcomes of the two experiments.\\nProof of the Basic Principle\\nThe basic principle can be proven by enumerating all the possible outcomes of\\nthe two experiments as follows:\\n(1,1), (1,2),...,(1,n)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 84}, page_content='3.5 Sample spaces having equally likely outcomes\\n71\\n(2,1), (2,2),...,(2,n)\\n...\\n(m,1), (m,2),...,(m,n)\\nwhere we say that the outcome is (i, j) if experiment 1 results in its ith possible\\nout-come and experiment 2 then results in the jth of its possible outcomes.\\nHence, the set of possible outcomes consists of m rows, each row containing n\\nelements, which proves the result.\\n■\\nExample 3.5.a. Two balls are “randomly drawn” from a bowl containing 6\\nwhite and 5 black balls. What is the probability that one of the drawn balls is\\nwhite and the other black?\\nSolution. If we regard the order in which the balls are selected as being signiﬁ-\\ncant, then as the ﬁrst drawn ball may be any of the 11 and the second any of the\\nremaining 10, it follows that the sample space consists of 11·10 = 110 points.\\nFurthermore, there are 6·5 = 30 ways in which the ﬁrst ball selected is white\\nand the second black, and similarly there are 5·6 = 30 ways in which the ﬁrst\\nball is black and the second white. Hence, assuming that “randomly drawn”\\nmeans that each of the 110 points in the sample space is equally likely to occur,\\nthen we see that the desired probability is\\n30 + 30\\n110\\n= 6\\n11\\n■\\nWhen there are more than two experiments to be performed the basic principle\\ncan be generalized as follows:\\nGeneralized basic principle of counting\\nIf r experiments that are to be performed are such that the ﬁrst one may result in any of\\nn1 possible outcomes, and if for each of these n1 possible outcomes there are n2 possible\\noutcomes of the second experiment, and if for each of the possible outcomes of the ﬁrst two\\nexperiments there are n3 possible outcomes of the third experiment, and if, ..., then there\\nare a total of n1 ·n2 ···nr possible outcomes of the r experiments.\\nAs an illustration of this, let us determine the number of different ways n dis-\\ntinct objects can be arranged in a linear order. For instance, how many different\\nordered arrangements of the letters a, b, c are possible? By direct enumeration\\nwe see that there are 6; namely, abc, acb, bac, bca, cab, cba. Each one of these\\nordered arrangements is known as a permutation. Thus, there are 6 possible per-\\nmutations of a set of 3 objects. This result could also have been obtained from\\nthe basic principle, since the ﬁrst object in the permutation can be any of the\\n3, the second object in the permutation can then be chosen from any of the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 85}, page_content='72 CHAPTER 3: Elements of probability\\nremaining 2, and the third object in the permutation is then chosen from the\\nremaining one. Thus, there are 3·2·1 = 6 possible permutations.\\nSuppose now that we have n objects. Similar reasoning shows that there are\\nn(n −1)(n −2)···3·2·1\\ndifferent permutations of the n objects. It is convenient to introduce the nota-\\ntion n!, which is read “n factorial,” for the foregoing expression. That is,\\nn! = n(n −1)(n −2)···3·2·1\\nThus, for instance, 1! = 1, 2! = 2·1 = 2, 3! = 3·2·1 = 6, 4! = 4·3·2·1 = 24, and\\nso on. It is convenient to deﬁne 0! = 1.\\nExample 3.5.b. Mr. Jones has 10 books that he is going to put on his bookshelf.\\nOf these, 4 are mathematics books, 3 are chemistry books, 2 are history books,\\nand 1 is a language book. Jones wants to arrange his books so that all the books\\ndealing with the same subject are together on the shelf. How many different\\narrangements are possible?\\nSolution. There are 4! 3! 2! 1! arrangements such that the mathematics books\\nare ﬁrst in line, then the chemistry books, then the history books, and then the\\nlanguage book. Similarly, for each possible ordering of the subjects, there are\\n4! 3! 2! 1! possible arrangements. Hence, as there are 4! possible orderings of\\nthe subjects, the desired answer is 4! 4! 3! 2! 1! = 6912.\\n■\\nExample 3.5.c. A class in probability theory consists of 6 men and 4 women.\\nAn exam is given and the students are ranked according to their performance.\\nAssuming that no two students obtain the same score, (a) how many different\\nrankings are possible? (b) If all rankings are considered equally likely, what is\\nthe probability that women receive the top 4 scores?\\nSolution.\\na. Because each ranking corresponds to a particular ordered ar-\\nrangement of the 10 people, we see the answer to this part is 10! =\\n3,628,800.\\nb. Because there are 4! possible rankings of the women among themselves\\nand 6! possible rankings of the men among themselves, it follows from\\nthe basic principle that there are (6!)(4!) = (720)(24) = 17,280 possible\\nrankings in which the women receive the top 4 scores. Hence, the desired\\nprobability is\\n6!4!\\n10! = 4·3·2·1\\n10·9·8·7 =\\n1\\n210\\n■\\nSuppose now that we are interested in determining the number of different\\ngroups of r objects that could be formed from a total of n objects. For instance,\\nhow many different groups of three could be selected from the ﬁve items A, B,'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 86}, page_content='3.5 Sample spaces having equally likely outcomes\\n73\\nC, D, E? To answer this, reason as follows. Since there are 5 ways to select the\\ninitial item, 4 ways to then select the next item, and 3 ways to then select the\\nﬁnal item, there are thus 5·4·3 ways of selecting the group of 3 when the order\\nin which the items are selected is relevant. However, since every group of 3,\\nsay the group consisting of items A, B, and C, will be counted 6 times (that is,\\nall of the permutations ABC, ACB, BAC, BCA, CAB, CBA will be counted when\\nthe order of selection is relevant), it follows that the total number of different\\ngroups that can be formed is (5·4·3)/(3·2·1) = 10.\\nIn general, as n(n−1)···(n−r +1) represents the number of different ways that\\na group of r items could be selected from n items when the order of selection\\nis considered relevant (since the ﬁrst one selected can be any one of the n, and\\nthe second selected any one of the remaining n −1, etc.), and since each group\\nof r items will be counted r! times in this count, it follows that the number of\\ndifferent groups of r items that could be formed from a set of n items is\\nn(n −1)···(n −r + 1)\\nr!\\n=\\nn!\\n(n −r)!r!\\nNotation and terminology\\nWe deﬁne\\n\\x06n\\nr\\n\\x07\\n, for r ≤n, by\\n\\x08n\\nr\\n\\t\\n=\\nn!\\n(n −r)!r!\\nand call\\n\\x06n\\nr\\n\\x07\\nthe number of combinations of n objects taken r at a time.\\nThus\\n\\x06n\\nr\\n\\x07\\nrepresents the number of different groups of size r that can be selected\\nfrom a set of size n when the order of selection is not considered relevant. For\\nexample, there are\\n\\x088\\n2\\n\\t\\n= 8·7\\n2·1 = 28\\ndifferent groups of size 2 that can be chosen from a set of 8 people, and\\n\\x0810\\n2\\n\\t\\n= 10·9\\n2·1 = 45\\ndifferent groups of size 2 that can be chosen from a set of 10 people. Also, since\\n0! = 1, note that\\n\\x08n\\n0\\n\\t\\n=\\n\\x08n\\nn\\n\\t\\n= 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 87}, page_content='74 CHAPTER 3: Elements of probability\\nExample 3.5.d. A committee of size 5 is to be selected from a group of 6 men\\nand 9 women. If the selection is made randomly, what is the probability that\\nthe committee consists of 3 men and 2 women?\\nSolution. Let us assume that “randomly selected” means that each of the\\n\\x0615\\n5\\n\\x07\\npossible combinations is equally likely to be selected. Hence, since there are\\n\\x066\\n3\\n\\x07\\npossible choices of 3 men and\\n\\x069\\n2\\n\\x07\\npossible choices of 2 women, it follows\\nfrom the basic principle of counting that the desired probability is given by\\n\\x08 6\\n3\\n\\t\\x08 9\\n2\\n\\t\\n\\x08 15\\n5\\n\\t\\n= 240\\n1001\\n■\\nExample 3.5.e. From a set of n items a random sample of size k is to be se-\\nlected. What is the probability a given item will be among the k selected?\\nSolution. Because there is\\n\\x061\\n1\\n\\x07\\nway of choosing the given item and\\n\\x06n−1\\nk−1\\n\\x07\\ndiffer-\\nent choices of k −1 of the other n −1 items, it follows from the basic principle\\nof counting that there are\\n\\x061\\n1\\n\\x07\\x06n−1\\nk−1\\n\\x07\\n=\\n\\x06n−1\\nk−1\\n\\x07\\ndifferent subsets of k of the n items\\nthat include the given item. As there are a total of\\n\\x06n\\nk\\n\\x07\\ndifferent choices of k of\\nthe n items, it follows that the probability that a particular item is among the k\\nselected is\\n\\x08n −1\\nk −1\\n\\t\\n\\x08n\\nk\\n\\t\\n=\\n(n −1)!\\n(n −k)!(k −1)!\\n\\nn!\\n(n −k)!k! = k\\nn\\n■\\nExample 3.5.f. A basketball team consists of 6 black and 6 white players. The\\nplayers are to be paired in groups of two for the purpose of determining room-\\nmates. If the pairings are done at random, what is the probability that none of\\nthe black players will have a white roommate?\\nSolution. Let us start by imagining that the 6 pairs are numbered — that is,\\nthere is a ﬁrst pair, a second pair, and so on. Since there are\\n\\x0612\\n2\\n\\x07\\ndifferent choices\\nof a ﬁrst pair; and for each choice of a ﬁrst pair there are\\n\\x0610\\n2\\n\\x07\\ndifferent choices\\nof a second pair; and for each choice of the ﬁrst 2 pairs there are\\n\\x068\\n2\\n\\x07\\nchoices\\nfor a third pair; and so on, it follows from the generalized basic principle of\\ncounting that there are\\n\\x0812\\n2\\n\\t\\x0810\\n2\\n\\t\\x088\\n2\\n\\t\\x086\\n2\\n\\t\\x084\\n2\\n\\t\\x082\\n2\\n\\t\\n= 12!\\n(2!)6\\nways of dividing the players into a ﬁrst pair, a second pair, and so on. Hence there\\nare (12)!/266! ways of dividing the players into 6 (unordered) pairs of 2 each.\\nFurthermore, since there are, by the same reasoning, 6!/233! ways of pairing the\\nwhite players among themselves and 6!/233! ways of pairing the black players'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 88}, page_content='3.6 Conditional probability 75\\namong themselves, it follows that there are (6!/233!)2 pairings that do not result\\nin any black–white roommate pairs. Hence, if the pairings are done at random\\n(so that all outcomes are equally likely), then the desired probability is\\n\\x08 6!\\n233!\\n\\t2 \\n(12)!\\n266! =\\n5\\n231 = .0216\\nHence, there are roughly only two chances in a hundred that a random pairing\\nwill not result in any of the white and black players rooming together.\\n■\\nExample 3.5.g. If n people are present in a room, what is the probability that\\nno two of them celebrate their birthday on the same day of the year? How large\\nneed n be so that this probability is less than 1\\n2?\\nSolution. Because each person can celebrate his or her birthday on any one of\\n365 days, there are a total of (365)n possible outcomes. (We are ignoring the\\npossibility of someone having been born on February 29.) Furthermore, there\\nare (365)(364)(363)·(365−n+1) possible outcomes that result in no two of the\\npeople having the same birthday. This is so because the ﬁrst person could have\\nany one of 365 birthdays, the next person any of the remaining 364 days, the\\nnext any of the remaining 363, and so on. Hence, assuming that each outcome\\nis equally likely, we see that the desired probability is\\n(365)(364)(363)···(365 −n + 1)\\n(365)n\\nIt is a rather surprising fact that when n ≥23, this probability is less than 1\\n2.\\nThat is, if there are 23 or more people in a room, then the probability that at\\nleast two of them have the same birthday exceeds 1\\n2. Many people are initially\\nsurprised by this result, since 23 seems so small in relation to 365, the number\\nof days of the year. However, every pair of individuals has probability\\n365\\n(365)2 =\\n1\\n365 of having the same birthday, and in a group of 23 people there are\\n\\x0623\\n2\\n\\x07\\n=253\\ndifferent pairs of individuals. Looked at this way, the result no longer seems so\\nsurprising.\\n■\\n3.6\\nConditional probability\\nIn this section, we introduce one of the most important concepts in all of prob-\\nability theory — that of conditional probability. Its importance is twofold. In\\nthe ﬁrst place, we are often interested in calculating probabilities when some\\npartial information concerning the result of the experiment is available, or in\\nrecalculating them in light of additional information. In such situations, the\\ndesired probabilities are conditional ones. Second, as a kind of a bonus, it of-\\nten turns out that the easiest way to compute the probability of an event is to\\nﬁrst “condition” on the occurrence or nonoccurrence of a secondary event.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 89}, page_content='76 CHAPTER 3: Elements of probability\\nAs an illustration of a conditional probability, suppose that one rolls a pair of\\ndice. The sample space S of this experiment can be taken to be the following\\nset of 36 outcomes\\nS = {(i,j),\\ni = 1,2,3,4,5,6,\\nj = 1,2,3,4,5,6}\\nwhere we say that the outcome is (i, j) if the ﬁrst die lands on side i and the\\nsecond on side j. Suppose now that each of the 36 possible outcomes is equally\\nlikely to occur and thus has probability 1\\n36. (In such a situation we say that the\\ndice are fair.) Suppose further that we observe that the ﬁrst die lands on side 3.\\nThen, given this information, what is the probability that the sum of the two\\ndice equals 8? To calculate this probability, we reason as follows: Given that the\\ninitial die is a 3, there can be at most 6 possible outcomes of our experiment,\\nnamely, (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), and (3, 6). In addition, because\\neach of these outcomes originally had the same probability of occurring, they\\nshould still have equal probabilities. That is, given that the ﬁrst die is a 3, then\\nthe (conditional) probability of each of the outcomes (3, 1), (3, 2), (3, 3), (3,\\n4), (3, 5), (3, 6) is 1\\n6, whereas the (conditional) probability of the other 30\\npoints in the sample space is 0. Hence, the desired probability will be 1\\n6.\\nIf we let E and F denote, respectively, the event that the sum of the dice is 8 and\\nthe event that the ﬁrst die is a 3, then the probability just obtained is called the\\nconditional probability of E given that F has occurred, and is denoted by\\nP(E|F)\\nA general formula for P(E|F) that is valid for all events E and F is derived\\nin the same manner as just described. Namely, if the event F occurs, then in\\norder for E to occur it is necessary that the actual occurrence be a point in\\nboth E and F; that is, it must be in EF. However, because we know that F has\\noccurred, it follows that we can regard F as the new sample space and hence\\nthe probability that the event EF occurs will equal the probability of EF relative\\nto the probability of F. That is,\\nP(E|F) = P(EF)\\nP(F)\\n(3.6.1)\\nNote that Equation (3.6.1) is well deﬁned only when P(F)>0 and hence\\nP(E|F) is deﬁned only when P(F)>0. (See Figure 3.5.)\\nThe deﬁnition of conditional probability given by Equation (3.6.1) is con-\\nsistent with the interpretation of probability as being a long-run relative fre-\\nquency. To see this, suppose that a large number n of repetitions of the ex-\\nperiment are performed. Then, since P(F) is the long-run proportion of ex-\\nperiments in which F occurs, it follows that F will occur approximately nP(F)\\ntimes. Similarly, in approximately nP(EF) of these experiments, both E and F\\nwill occur. Hence, of the approximately nP(F) experiments whose outcome is'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 90}, page_content='3.6 Conditional probability 77\\nFIGURE 3.5\\nP(E|F) = P(EF )\\nP(F) .\\nin F, approximately nP(EF) of them will also have their outcome in E. That is,\\nfor those experiments whose outcome is in F, the proportion whose outcome\\nis also in E is approximately\\nnP(EF)\\nnP(F) = P(EF)\\nP(F)\\nSince this approximation becomes exact as n becomes larger and larger, it fol-\\nlows that (3.6.1) gives the appropriate deﬁnition of the conditional probability\\nof E given that F has occurred.\\nExample 3.6.a. A bin contains 5 defective (that immediately fail when put in\\nuse), 10 partially defective (that fail after a couple of hours of use), and 25\\nacceptable transistors. A transistor is chosen at random from the bin and put\\ninto use. If it does not immediately fail, what is the probability it is acceptable?\\nSolution. Since the transistor did not immediately fail, we know that it is not\\none of the 5 defectives and so the desired probability is:\\nP{acceptable|not defective}\\n= P{acceptable, not defective}\\nP{not defective}\\n=\\nP{acceptable}\\nP{not defective}\\nwhere the last equality follows since the transistor will be both acceptable and\\nnot defective if it is acceptable. Hence, assuming that each of the 40 transistors\\nis equally likely to be chosen, we obtain that\\nP{acceptable|not defective} = 25/40\\n35/40 = 5/7\\nIt should be noted that we could also have derived this probability by working\\ndirectly with the reduced sample space. That is, since we know that the chosen\\ntransistor is not defective, the problem reduces to computing the probability\\nthat a transistor, chosen at random from a bin containing 25 acceptable and 10\\npartially defective transistors, is acceptable. This is clearly equal to 25\\n35.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 91}, page_content='78 CHAPTER 3: Elements of probability\\nExample 3.6.b. The organization that Jones works for is running a father–son\\ndinner for those employees having at least one son. Each of these employees\\nis invited to attend along with his youngest son. If Jones is known to have two\\nchildren, what is the conditional probability that they are both boys given that\\nhe is invited to the dinner? Assume that the sample space S is given by S =\\n{(b,b),(b,g),(g,b),(g,g)} and all outcomes are equally likely [(b,g) means,\\nfor instance, that the younger child is a boy and the older child is a girl].\\nSolution. The knowledge that Jones has been invited to the dinner is equiva-\\nlent to knowing that he has at least one son. Hence, letting B denote the event\\nthat both children are boys, and A the event that at least one of them is a boy,\\nwe have that the desired probability P(B|A) is given by\\nP(B|A) = P(BA)\\nP(A)\\n=\\nP({(b,b)})\\nP({(b,b),(b,g),(g,b)})\\n=\\n1\\n4\\n3\\n4\\n= 1\\n3\\nMany readers incorrectly reason that the conditional probability of two boys\\ngiven at least one is 1\\n2, as opposed to the correct 1\\n3, since they reason that the\\nJones child not attending the dinner is equally likely to be a boy or a girl.\\nTheir mistake, however, is in assuming that these two possibilities are equally\\nlikely. Remember that initially there were four equally likely outcomes. Now\\nthe information that at least one child is a boy is equivalent to knowing that the\\noutcome is not (g, g). Hence we are left with the three equally likely outcomes\\n(b, b), (b, g), (g, b), thus showing that the Jones child not attending the dinner\\nis twice as likely to be a girl as a boy.\\n■\\nBy multiplying both sides of Equation (3.6.1) by P(F) we obtain that\\nP(EF ) = P(F)P(E|F)\\n(3.6.2)\\nIn words, Equation (3.6.2) states that the probability that both E and F occur is\\nequal to the probability that F occurs multiplied by the conditional probability\\nof E given that F occurred. Equation (3.6.2) is often quite useful in computing\\nthe probability of the intersection of events. This is illustrated by the following\\nexample.\\nExample 3.6.c. Ms. Perez ﬁgures that there is a 30 percent chance that her com-\\npany will set up a branch ofﬁce in Phoenix. If it does, she is 60 percent certain\\nthat she will be made manager of this new operation. What is the probability\\nthat Perez will be a Phoenix branch ofﬁce manager?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 92}, page_content='3.7 Bayes’ formula 79\\nFIGURE 3.6\\nE = EF ∪EF c.\\nSolution. If we let B denote the event that the company sets up a branch ofﬁce\\nin Phoenix and M the event that Perez is made the Phoenix manager, then the\\ndesired probability is P(BM), which is obtained as follows:\\nP(BM) = P(B)P(M|B)\\n= (.3)(.6)\\n= .18\\nHence, there is an 18 percent chance that Perez will be the Phoenix manager. ■\\n3.7\\nBayes’ formula\\nLet E and F be events. We may express E as\\nE = EF ∪EF c\\nfor, in order for a point to be in E, it must either be in both E and F or be in E\\nbut not in F. (See Figure 3.6.) As EF and EF c are clearly mutually exclusive, we\\nhave by Axiom 3 that\\nP(E) = P(EF) + P(EF c)\\n= P(E|F)P(F) + P(E|F c)P(F c)\\n= P(E|F)P(F) + P(E|F c)[1 −P(F)]\\n(3.7.1)\\nEquation (3.7.1) states that the probability of the event E is a weighted average\\nof the conditional probability of E given that F has occurred and the condi-\\ntional probability of E given that F has not occurred, with each conditional\\nprobability being given as much weight as the event it is conditioned on has\\nof occurring. It is an extremely useful formula, for its use often enables us to\\ndetermine the probability of an event by ﬁrst “conditioning” on whether or\\nnot some second event has occurred. That is, there are many instances where\\nit is difﬁcult to compute the probability of an event directly, but it is straight-\\nforward to compute it once we know whether or not some second event has\\noccurred.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 93}, page_content='80 CHAPTER 3: Elements of probability\\nExample 3.7.a. An insurance company believes that people can be divided\\ninto two classes — those that are accident prone and those that are not. Their\\nstatistics show that an accident-prone person will have an accident at some\\ntime within a ﬁxed 1-year period with probability .4, whereas this probability\\ndecreases to .2 for a non-accident-prone person. If we assume that 30 percent\\nof the population is accident prone, what is the probability that a new policy\\nholder will have an accident within a year of purchasing a policy?\\nSolution. We obtain the desired probability by ﬁrst conditioning on whether\\nor not the policy holder is accident prone. Let A1 denote the event that the pol-\\nicy holder will have an accident within a year of purchase; and let A denote the\\nevent that the policy holder is accident prone. Hence, the desired probability,\\nP(A1), is given by\\nP(A1) = P(A1|A)P(A) + P(A1|Ac)P(Ac)\\n= (.4)(.3) + (.2)(.7) = .26\\n■\\nIn the next series of examples, we will indicate how to reevaluate an initial\\nprobability assessment in light of additional (or new) information. That is,\\nwe will show how to incorporate new information with an initial probability\\nassessment to obtain an updated probability.\\nExample 3.7.b. Twins can either be identical or fraternal. Identical, also called\\nmonozygotic, twins form when a single fertilized egg splits into two genetically\\nidentical parts. Consequently, identical twins always have the same set of genes.\\nFraternal, also called dizygotic, twins develop when two separate eggs are fertil-\\nized and implant in the uterus. The genetic connection of fraternal twins is no\\nmore or less the same as siblings born at separate times. A Los Angeles county\\nscientist wishing to know the current fraction of twin pairs born in the county\\nthat are identical twins has assigned a county statistician to study this issue.\\nThe statistician initially requested each hospital in the county to record all twin\\nbirths, indicating whether the resulting twins were identical or not. The hospi-\\ntals, however, told her that to determine whether newborn twins were identical\\nwas not a simple task, as it involved the permission of the twins’ parents to\\nperform complicated and expensive DNA studies that the hospitals could not\\nafford. After some deliberation, the statistician just asked the hospitals for data\\nlisting all twin births along with an indication as to whether the twins were of\\nthe same sex. When such data indicated that approximately 64 percent of twin\\nbirths were same-sexed, the statistician declared that approximately 28 percent\\nof all twins were identical. How did she come to this conclusion?\\nSolution. The statistician reasoned that identical twins are always of the same\\nsex, whereas fraternal twins, having the same relationship to each other as any\\npair of siblings, will have probability 1\\n2 of being of the same sex. Letting I be\\nthe event that a pair of twins are identical, and SS be the event that a pair of'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 94}, page_content='3.7 Bayes’ formula 81\\ntwins are of the same sex, she computed the probability P(SS) by conditioning\\non whether the twin pair was identical. This gave\\nP(SS) = P(SS|I)P(I) + P(SS|I c)P(I c)\\nor\\nP(SS) = 1 × P(I) + 1\\n2 × [1 −P(I)] = 1\\n2 + 1\\n2 P(I)\\nwhich, using that P(SS) ≈.64 yielded the result\\nP(I) ≈.28\\n■\\nExample 3.7.c. Reconsider Example 3.7.a and suppose that a new policy\\nholder has an accident within a year of purchasing his policy. What is the prob-\\nability that he is accident prone?\\nSolution. Initially, at the moment when the policy holder purchased his pol-\\nicy, we assumed there was a 30 percent chance that he was accident prone. That\\nis, P(A) = .3. However, based on the fact that he has had an accident within a\\nyear, we now reevaluate his probability of being accident prone as follows.\\nP(A|A1) = P(AA1)\\nP(A1)\\n= P(A)P(A1|A)\\nP(A1)\\n= (.3)(.4)\\n.26\\n= 6\\n13 = .4615\\n■\\nExample 3.7.d. In answering a question on a multiple-choice test, a student\\neither knows the answer or she guesses. Let p be the probability that she knows\\nthe answer and 1 −p the probability that she guesses. Assume that a student\\nwho guesses at the answer will be correct with probability 1/m, where m is\\nthe number of multiple-choice alternatives. What is the conditional probabil-\\nity that a student knew the answer to a question given that she answered it\\ncorrectly?\\nSolution. Let C and K denote, respectively, the events that the student answers\\nthe question correctly and the event that she actually knows the answer. To\\ncompute\\nP(K|C) = P(KC)\\nP(C)\\nwe ﬁrst note that\\nP(KC) = P(K)P(C|K)\\n= p ·1\\n= p'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 95}, page_content='82 CHAPTER 3: Elements of probability\\nTo compute the probability that the student answers correctly, we condition on\\nwhether or not she knows the answer. That is,\\nP(C) = P(C|K)P(K) + P(C|Kc)P(Kc)\\n= p + (1/m)(1 −p)\\nHence, the desired probability is given by\\nP(K|C) =\\np\\np + (1/m)(1 −p) =\\nmp\\n1 + (m −1)p\\nThus, for example, if m = 5, p = 1\\n2, then the probability that a student knew the\\nanswer to a question she correctly answered is 5\\n6.\\n■\\nExample 3.7.e. A laboratory blood test is 99 percent effective in detecting a\\ncertain disease when it is, in fact, present. However, the test also yields a “false\\npositive” result for 1 percent of the healthy persons tested. (That is, if a healthy\\nperson is tested, then, with probability .01, the test result will imply he or she\\nhas the disease.) If .5 percent of the population actually has the disease, what\\nis the probability a person has the disease given that his test result is positive?\\nSolution. Let D be the event that the tested person has the disease and E the\\nevent that his test result is positive. The desired probability P(D|E) is obtained\\nby\\nP(D|E) = P(DE)\\nP(E)\\n=\\nP(E|D)P(D)\\nP(E|D)P(D) + P(E|Dc)P(Dc)\\n=\\n(.99)(.005)\\n(.99)(.005) + (.01)(.995)\\n= .3322\\nThus, only 33 percent of those persons whose test results are positive actually\\nhave the disease. Since many students are often surprised at this result (because\\nthey expected this ﬁgure to be much higher since the blood test seems to be a\\ngood one), it is probably worthwhile to present a second argument which,\\nthough less rigorous than the foregoing, is probably more revealing. We now\\ndo so.\\nSince .5 percent of the population actually has the disease, it follows that, on\\nthe average, 1 person out of every 200 tested will have it. The test will correctly\\nconﬁrm that this person has the disease with probability .99. Thus, on the av-\\nerage, out of every 200 persons tested, the test will correctly conﬁrm that .99\\nperson has the disease. On the other hand, out of the (on the average) 199\\nhealthy people, the test will incorrectly state that (199) (.01) of these people'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 96}, page_content='3.7 Bayes’ formula 83\\nhave the disease. Hence, for every .99 diseased person that the test correctly\\nstates is ill, there are (on the average) 1.99 healthy persons that the test incor-\\nrectly states are ill. Hence, the proportion of time that the test result is correct\\nwhen it states that a person is ill is\\n.99\\n.99 + 1.99 = .3322\\n■\\nEquation (3.7.1) is also useful when one has to reassess one’s (personal) prob-\\nabilities in the light of additional information. For instance, consider the fol-\\nlowing examples.\\nExample 3.7.f. At a certain stage of a criminal investigation, the inspector in\\ncharge is 60 percent convinced of the guilt of a certain suspect. Suppose now\\nthat a new piece of evidence that shows that the criminal has a certain char-\\nacteristic (such as left-handedness, baldness, brown hair, etc.) is uncovered. If\\n20 percent of the population possesses this characteristic, how certain of the\\nguilt of the suspect should the inspector now be if it turns out that the sus-\\npect is among this group?\\nSolution. Letting G denote the event that the suspect is guilty and C the event\\nthat he possesses the characteristic of the criminal, we have\\nP(G|C) = P(GC)\\nP(C )\\nNow\\nP(GC) = P(G)P(C|G)\\n= (.6)(1)\\n= .6\\nTo compute the probability that the suspect has the characteristic, we condition\\non whether or not he is guilty. That is,\\nP(C) = P(C|G)P(G) + P(C|Gc)P(Gc)\\n= (1)(.6) + (.2)(.4)\\n= .68\\nwhere we have supposed that the probability of the suspect having the charac-\\nteristic if he is, in fact, innocent is equal to .2, the proportion of the population\\npossessing the characteristic. Hence\\nP(G|C) = 60\\n68 = .882'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 97}, page_content='84 CHAPTER 3: Elements of probability\\nand so the inspector should now be 88 percent certain of the guilt of the sus-\\npect.\\n■\\nExample 3.7.f (continued). Let us now suppose that the new evidence is sub-\\nject to different possible interpretations, and in fact only shows that it is 90\\npercent likely that the criminal possesses this certain characteristic. In this case,\\nhow likely would it be that the suspect is guilty (assuming, as before, that he\\nhas this characteristic)?\\nSolution. In this case, the situation is as before with the exception that the\\nprobability of the suspect having the characteristic given that he is guilty is\\nnow .9 (rather than 1). Hence,\\nP(G|C) = P( GC)\\nP(C)\\n=\\nP(G)P(C|G)\\nP(C|G)P(G) + P(C|Gc)P(Gc)\\n=\\n(.6)(.9)\\n(.9)(.6) + (.2)(.4)\\n= 54\\n62 = .871\\nwhich is slightly less than in the previous case (why?).\\n■\\nEquation (3.7.1) may be generalized in the following manner. Suppose that\\nF1, F2, ..., Fn are mutually exclusive events such that\\nn\\n\\x03\\ni=1\\nFi = S\\nIn other words, exactly one of the events F1,F2,...,Fn must occur. By writing\\nE =\\nn\\n\\x03\\ni=1\\nEFi\\nand using the fact that the events EFi,i = 1,...,n are mutually exclusive, we\\nobtain that\\nP(E) =\\nn\\n\\x05\\ni=1\\nP(EFi)\\n=\\nn\\n\\x05\\ni=1\\nP(E|Fi)P(Fi)\\n(3.7.2)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 98}, page_content='3.7 Bayes’ formula 85\\nThus, Equation (3.7.2) shows how, for given events F1,F2,...,Fn of which\\none and only one must occur, we can compute P(E) by ﬁrst “conditioning” on\\nwhich one of the Fi occurs. That is, it states that P(E) is equal to a weighted\\naverage of P(E|Fi), each term being weighted by the probability of the event\\non which it is conditioned.\\nSuppose now that E has occurred and we are interested in determining which\\none of Fj also occurred. By Equation (3.7.2), we have that\\nP(Fj|E) = P(EFj)\\nP(E)\\n=\\nP(E|Fj)P(Fj)\\nn\\x0b\\ni=1\\nP(E|Fi)P(Fi)\\n(3.7.3)\\nEquation (3.7.3) is known as Bayes’ formula, after the English philosopher\\nThomas Bayes. If we think of the events Fj as being possible “hypotheses”\\nabout some subject matter, then\\nBayes’ formula may be interpreted as showing us how opinions about these\\nhypotheses held before the experiment [that is, the P(Fj)] should be modiﬁed\\nby the evidence of the experiment.\\nExample 3.7.g. A plane is missing and it is presumed that it was equally likely\\nto have gone down in any of three possible regions. Let 1 −αi denote the\\nprobability the plane will be found upon a search of the ith region when\\nthe plane is, in fact, in that region, i =1,2,3. (The constants αi are called\\noverlook probabilities because they represent the probability of overlooking the\\nplane; they are generally attributable to the geographical and environmental\\nconditions of the regions.) What is the conditional probability that the plane\\nis in the ith region, given that a search of region 1 is unsuccessful, i = 1,2,3?\\nSolution. Let Ri,i =1,2,3, be the event that the plane is in region i; and let E\\nbe the event that a search of region 1 is unsuccessful. From Bayes’ formula, we\\nobtain\\nP(R1|E) = P(ER1)\\nP(E)\\n=\\nP(E|R1)P(R1)\\n3\\x0b\\ni=1\\nP(E|Ri)P(Ri)\\n=\\n(α1)(1/3)\\n(α1)(1/3) + (1)(1/3) + (1)(1/3)\\n=\\nα1\\nα1 + 2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 99}, page_content='86 CHAPTER 3: Elements of probability\\nFor j = 2,3,\\nP(Rj|E) = P(E|Rj)P(Rj)\\nP(E)\\n=\\n(1)(1/3)\\n(α1)1/3 + 1/3 + 1/3\\n=\\n1\\nα1 + 2,\\nj = 2,3\\nThus, for instance, if α1 = .4, then the conditional probability that the plane is\\nin region 1 given that a search of that region did not uncover it is 1\\n6, whereas\\nthe conditional probabilities that it is in region 2 and that it is in region 3 are\\nboth equal to\\n1\\n2.4 = 5\\n12.\\n■\\n3.8\\nIndependent events\\nThe previous examples in this chapter show that P(E|F), the conditional prob-\\nability of E given F, is not generally equal to P(E), the unconditional proba-\\nbility of E. In other words, knowing that F has occurred generally changes the\\nchances of E ’s occurrence. In the special cases where P(E|F) does in fact equal\\nP(E), we say that E is independent of F. That is, E is independent of F if knowl-\\nedge that F has occurred does not change the probability that E occurs.\\nSince P(E|F) = P(EF)/P(F), we see that E is independent of F if\\nP(EF) = P(E)P(F)\\n(3.8.1)\\nSince this equation is symmetric in E and F, it shows that whenever E is inde-\\npendent of F so is F of E. We thus have the following.\\nDeﬁnition. Two events E and F are said to be independent if Equation (3.8.1)\\nholds. Two events E and F that are not independent are said to be dependent.\\nExample 3.8.a. A card is selected at random from an ordinary deck of 52 play-\\ning cards. If A is the event that the selected card is an ace and H is the event that\\nit is a heart, then A and H are independent, since P(AH ) = 1\\n52, while P(A) = 4\\n52\\nand P(H) = 13\\n52.\\n■\\nExample 3.8.b. If we let E denote the event that the next president is a Repub-\\nlican and F the event that there will be a major earthquake within the next year,\\nthen most people would probably be willing to assume that E and F are inde-\\npendent. However, there would probably be some controversy over whether it\\nis reasonable to assume that E is independent of G, where G is the event that\\nthere will be a recession within the next two years.\\n■\\nWe now show that if E is independent of F then E is also independent of F c.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 100}, page_content='3.8 Independent events\\n87\\nProposition 3.8.1. If E and F are independent, then so are E and F c.\\nProof. Assume that E and F are independent. Since E = EF ∪EF c, and EF and\\nEF c are obviously mutually exclusive, we have that\\nP(E) = P(EF ) + P(EF c)\\n= P(E)P(F) + P(EF c)\\nby the independence of E and F\\nor equivalently,\\nP(EF c) = P(E)(1 −P(F))\\n= P(E)P(F c)\\nand the result is proven.\\n■\\nThus if E is independent of F, then the probability of E ’s occurrence is un-\\nchanged by information as to whether or not F has occurred.\\nSuppose now that E is independent of F and is also independent of G. Is E\\nthen necessarily independent of FG ? The answer, somewhat surprisingly, is\\nno. Consider the following example.\\nExample 3.8.c. Two fair dice are thrown. Let E7 denote the event that the sum\\nof the dice is 7. Let F denote the event that the ﬁrst die equals 4 and let T be\\nthe event that the second die equals 3. Now it can be shown (see Problem 36)\\nthat E7 is independent of F and that E7 is also independent of T; but clearly\\nE7 is not independent of FT [since P(E7|FT ) = 1].\\n■\\nIt would appear to follow from the foregoing example that an appropriate deﬁ-\\nnition of the independence of three events E, F, and G would have to go further\\nthan merely assuming that all of the\\n\\x063\\n2\\n\\x07\\npairs of events are independent. We are\\nthus led to the following deﬁnition.\\nDeﬁnition. The three events E, F, and G are said to be independent if\\nP(EFG) = P(E)P(F)P(G)\\nP(EF) = P(E)P(F)\\nP(EG) = P(E)P(G)\\nP(FG) = P(F)P(G)\\nIt should be noted that if the events E, F, G are independent, then E will be\\nindependent of any event formed from F and G. For instance, E is independent\\nof F ∪G since\\nP(E(F ∪G)) = P(EF ∪EG)\\n= P(EF ) + P(EG) −P(EFG)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 101}, page_content='88 CHAPTER 3: Elements of probability\\n= P(E)P(F) + P(E )P(G) −P(E)P(FG)\\n= P(E)[P(F) + P(G) −P(FG)]\\n= P(E)P(F ∪G)\\nOf course we may also extend the deﬁnition of independence to more than\\nthree events. The events E1,E2,...,En are said to be independent if for every\\nsubset E1′,E2′,...,Er ′, r ≤n, of these events\\nP(E1′E2′ ···Er ′) = P(E1′)P(E2′)···P(Er ′)\\nIt is sometimes the case that the probability experiment under consideration\\nconsists of performing a sequence of subexperiments. For instance, if the exper-\\niment consists of continually tossing a coin, then we may think of each toss as\\nbeing a subexperiment. In many cases it is reasonable to assume that the out-\\ncomes of any group of the subexperiments have no effect on the probabilities\\nof the outcomes of the other subexperiments. If such is the case, then we say\\nthat the subexperiments are independent.\\nExample 3.8.d. A system composed of n separate components is said to be a\\nparallel system if it functions when at least one of the components functions.\\n(See Figure 3.7.) For such a system, if component i, independent of other com-\\nponents, functions with probability pi,i = 1,...,n, what is the probability the\\nsystem functions?\\nSolution. Let Ai denote the event that component i functions. Then\\nP{system functions} = 1 −P{system does not function}\\n= 1 −P{all components do not function}\\n= 1 −P\\n\\x06\\nAc\\n1Ac\\n2 ···Ac\\nn\\n\\x07\\n= 1 −\\nn\\n\\x0c\\ni=1\\n(1 −pi)\\nby independence\\n■\\nExample 3.8.e. A set of k coupons, each of which is independently a type\\nj coupon with probability pj, \\x0bn\\nj=1 pj =1, is collected. Find the probability\\nthat the set contains a type j coupon given that it contains a type i, i ̸= j.\\nFIGURE 3.7\\nParallel system: functions if current ﬂows from A to B.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 102}, page_content='Problems 89\\nSolution. Let Ar be the event that the set contains a type r coupon. Then\\nP(Aj|Ai) = P(AjAi)\\nP(Ai)\\nTo compute P(Ai) and P(AjAi), consider the probability of their comple-\\nments:\\nP(Ai) = 1 −P(Ac\\ni )\\n= 1 −P{no coupon is type i}\\n= 1 −(1 −pi)k\\nP(AiAj) = 1 −P(Ac\\ni ∪Ac\\nj)\\n= 1 −[P(Ac\\ni ) + P(Ac\\nj) −P(Ac\\ni Ac\\nj)]\\n= 1 −(1 −pi)k −(1 −pj)k + P{no coupon is type i or type j}\\n= 1 −(1 −pi)k −(1 −pj)k + (1 −pi −pj)k\\nwhere the ﬁnal equality follows because each of the k coupons is, indepen-\\ndently, neither of type i or of type j with probability 1 −pi −pj. Consequently,\\nP(Aj|Ai) = 1 −(1 −pi)k −(1 −pj)k + (1 −pi −pj)k\\n1 −(1 −pi)k\\n■\\nProblems\\n1. A box contains three marbles — one red, one green, and one blue. Con-\\nsider an experiment that consists of taking one marble from the box, then\\nreplacing it in the box and drawing a second marble from the box. De-\\nscribe the sample space. Repeat for the case in which the second marble\\nis drawn without ﬁrst replacing the ﬁrst marble.\\n2. An experiment consists of tossing a coin three times. What is the sample\\nspace of this experiment? Which event corresponds to the experiment\\nresulting in more heads than tails?\\n3. Let S = {1,2,3,4,5,6,7}, E = {1,3,5,7}, F = {7,4,6}, G = {1,4}. Find\\na. EF ;\\nc. EGc;\\ne. Ec(F ∪G);\\nb. E ∪FG ; d. EF c ∪G; f. EG ∪FG.\\n4. Two dice are thrown. Let E be the event that the sum of the dice is odd,\\nlet F be the event that the ﬁrst die lands on 1, and let G be the event that\\nthe sum is 5. Describe the events EF, E ∪F, FG, EF c, EFG.\\n5. A system is composed of four components, each of which is either work-\\ning or failed. Consider an experiment that consists of observing the status\\nof each component, and let the outcome of the experiment be given by\\nthe vector (x1, x2, x3, x4) where xi is equal to 1 if component i is working\\nand is equal to 0 if component i is failed.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 103}, page_content='90 CHAPTER 3: Elements of probability\\na.\\nHow many outcomes are in the sample space of this experiment?\\nb.\\nSuppose that the system will work if components 1 and 2 are both\\nworking, or if components 3 and 4 are both working. Specify all the\\noutcomes in the event that the system works.\\nc.\\nLet E be the event that components 1 and 3 are both failed. How\\nmany outcomes are contained in event E?\\n6. Let E, F, G be three events. Find expressions for the events that of E, F, G\\na.\\nonly E occurs;\\nb.\\nboth E and G but not F occur;\\nc.\\nat least one of the events occurs;\\nd.\\nat least two of the events occur;\\ne.\\nall three occur;\\nf.\\nnone of the events occurs;\\ng.\\nat most one of them occurs;\\nh.\\nat most two of them occur;\\ni.\\nexactly two of them occur;\\nj.\\nat most three of them occur.\\n7. Find simple expressions for the events\\na.\\nE ∪Ec;\\nb.\\nEE c;\\nc.\\n(E ∪F)(E ∪F c);\\nd.\\n(E ∪F)(Ec ∪F)(E ∪F c);\\ne.\\n(E ∪F)(F ∪G).\\n8. Use Venn diagrams (or any other method) to show that\\na.\\nEF ⊂E, E ⊂E ∪F;\\nb.\\nif E ⊂F then F c ⊂Ec;\\nc.\\nthe commutative laws are valid;\\nd.\\nthe associative laws are valid;\\ne.\\nF =FE∪FE c;\\nf.\\nE ∪F =E ∪EcF;\\ng.\\nDeMorgan’s laws are valid.\\n9. For the following Venn diagram, describe in terms of E, F, and G the\\nevents denoted in the diagram by the Roman numerals I through VII.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 104}, page_content='Problems 91\\n10. Show that if E⊂F then P(E)≤P(F). (Hint: Write F as the union of two\\nmutually exclusive events, one of them being E.)\\n11. Prove Boole’s inequality, namely that\\nP\\n\\x02 n\\n\\x03\\ni=1\\nEi\\n\\x04\\n≤\\nn\\n\\x05\\ni=1\\nP(Ei)\\n12. If P(E)=.9 and P(F)=.9, show that P(EF )≥.8. In general, prove Bon-\\nferroni’s inequality, namely that\\nP(EF)≥P(E) + P(F) −1\\n13. Prove that\\na.\\nP(EF c) = P(E) −P(EF )\\nb.\\nP(EcF c) = 1 −P(E) −P(F) + P(EF )\\n14. Show that the probability that exactly one of the events E or F occurs is\\nequal to P(E) + P(F) −2P(EF ).\\n15. Calculate\\n\\x069\\n3\\n\\x07\\n,\\n\\x069\\n6\\n\\x07\\n,\\n\\x067\\n2\\n\\x07\\n,\\n\\x067\\n5\\n\\x07\\n,\\n\\x0610\\n7\\n\\x07\\n.\\n16. Show that\\n\\x08n\\nr\\n\\t\\n=\\n\\x08\\nn\\nn −r\\n\\t\\nNow present a combinatorial argument for the foregoing by explaining\\nwhy a choice of r items from a set of size n is equivalent to a choice of\\nn −r items from that set.\\n17. Show that\\n\\x08n\\nr\\n\\t\\n=\\n\\x08n −1\\nr −1\\n\\t\\n+\\n\\x08n −1\\nr\\n\\t\\nFor a combinatorial argument, consider a set of n items and ﬁx attention\\non one of these items. How many different sets of size r contain this item,\\nand how many do not?\\n18. A group of 5 boys and 10 girls is lined up in random order — that is, each\\nof the 15! permutations is assumed to be equally likely.\\na.\\nWhat is the probability that the person in the 4th position is a boy?\\nb.\\nWhat about the person in the 12th position?\\nc.\\nWhat is the probability that a particular boy is in the 3rd position?\\n19. Consider a set of 23 unrelated people. Because each pair of people shares\\nthe same birthday with probability 1/365, and there are\\n\\x0623\\n2\\n\\x07\\n=253 pairs,\\nwhy isn’t the probability that at least two people have the same birthday\\nequal to 253/365?\\n20. Suppose that distinct integer values are written on each of 3 cards. These\\ncards are then randomly given the designations A, B, and C. The values\\non cards A and B are then compared. If the smaller of these values is then'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 105}, page_content='92 CHAPTER 3: Elements of probability\\ncompared with the value on card C, what is the probability that it is also\\nsmaller than the value on card C?\\n21. There is a 60 percent chance that the event A will occur. If A does not\\noccur, then there is a 10 percent chance that B will occur.\\na.\\nWhat is the probability that at least one of the events A or B occurs?\\nb.\\nIf A is the event that the democratic candidate wins the presidential\\nelection in 2012 and B is the event that there is a 6.2 or higher earth-\\nquake in Los Angeles sometime in 2013, what would you take as\\nthe probability that both A and B occur? What assumption are you\\nmaking?\\n22. The sample mean of the annual salaries of a group of 100 accountants\\nwho work at a large accounting ﬁrm is $130,000 with a sample standard\\ndeviation of $20,000. If a member of this group is randomly chosen,\\nwhat can we say about\\na.\\nthe probability that his or her salary is between $90,000 and\\n$170,000?\\nb.\\nthe probability that his or her salary exceeds $150,000?\\nHint: Use the Chebyshev inequality.\\n23. Of three cards, one is painted red on both sides; one is painted black on\\nboth sides; and one is painted red on one side and black on the other.\\nA card is randomly chosen and placed on a table. If the side facing up is\\nred, what is the probability that the other side is also red?\\n24. A couple has 2 children. What is the probability that both are girls if the\\neldest is a girl?\\n25. Fifty-two percent of the students at a certain college are females. Five per-\\ncent of the students in this college are majoring in computer science.\\nTwo percent of the students are women majoring in computer science. If\\na student is selected at random, ﬁnd the conditional probability that\\na.\\nthis student is female, given that the student is majoring in computer\\nscience;\\nb.\\nthis student is majoring in computer science, given that the student\\nis female.\\n26. A total of 500 married working couples were polled about their annual\\nsalaries, with the following information resulting.\\nWife\\nHusband\\nLess than $50,000\\nMore than $50,000\\nLess than $50,000\\n212\\n198\\nMore than $50,000\\n36\\n54\\nThus, for instance, in 36 of the couples the wife earned more and the\\nhusband earned less than $50,000. If one of the couples is randomly\\nchosen, what is\\na.\\nthe probability that the husband earns less than $50,000;'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 106}, page_content='Problems 93\\nb.\\nthe conditional probability that the wife earns more than $50,000\\ngiven that the husband earns more than this amount;\\nc.\\nthe conditional probability that the wife earns more than $50,000\\ngiven that the husband earns less than this amount?\\n27. There are two local factories that produce microwaves. Each microwave\\nproduced at factory A is defective with probability .05, whereas each one\\nproduced at factory B is defective with probability .01. Suppose you pur-\\nchase two microwaves that were produced at the same factory, which is\\nequally likely to have been either factory A or factory B. If the ﬁrst mi-\\ncrowave that you check is defective, what is the conditional probability\\nthat the other one is also defective?\\n28. A red die, a blue die, and a yellow die (all six-sided) are rolled. We are\\ninterested in the probability that the number appearing on the blue die\\nis less than that appearing on the yellow die which is less than that ap-\\npearing on the red die. (That is, if B (R) [Y] is the number appearing on\\nthe blue (red) [yellow] die, then we are interested in P(B <Y <R).)\\na.\\nWhat is the probability that no two of the dice land on the same\\nnumber?\\nb.\\nGiven that no two of the dice land on the same number, what is the\\nconditional probability that B <Y <R?\\nc.\\nWhat is P(B <Y <R)?\\nd.\\nIf we regard the outcome of the experiment as the vector B, R, Y,\\nhow many outcomes are there in the sample space?\\ne.\\nWithout using the answer to (c), determine the number of outcomes\\nthat result in B <Y <R.\\nf.\\nUse the results of parts (d) and (e) to verify your answer to part (c).\\n29. You ask your neighbor to water a sickly plant while you are on vaca-\\ntion. Without water it will die with probability .8; with water it will die\\nwith probability .15. You are 90 percent certain that your neighbor will\\nremember to water the plant.\\na.\\nWhat is the probability that the plant will be alive when you return?\\nb.\\nIf it is dead, what is the probability your neighbor forgot to water it?\\n30. Two balls, each equally likely to be colored either red or blue, are put\\nin an urn. At each stage one of the balls is randomly chosen, its color is\\nnoted, and it is then returned to the urn. If the ﬁrst two balls chosen are\\ncolored red, what is the probability that\\na.\\nboth balls in the urn are colored red;\\nb.\\nthe next ball chosen will be red?\\n31. A total of 600 of the 1000 people in a retirement community clas-\\nsify themselves as Republicans, while the others classify themselves as\\nDemocrats. In a local election in which everyone voted, 60 Republicans\\nvoted for the Democratic candidate, and 50 Democrats voted for the Re-\\npublican candidate. If a randomly chosen community member voted for\\nthe Republican, what is the probability that she or he is a Democrat?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 107}, page_content='94 CHAPTER 3: Elements of probability\\n32. Each of 2 balls is painted black or gold and then placed in an urn. Sup-\\npose that each ball is colored black with probability 1\\n2, and that these\\nevents are independent.\\na.\\nSuppose that you obtain information that the gold paint has been\\nused (and thus at least one of the balls is painted gold). Compute\\nthe conditional probability that both balls are painted gold.\\nb.\\nSuppose, now, that the urn tips over and 1 ball falls out. It is painted\\ngold. What is the probability that both balls are gold in this case?\\nExplain.\\n33. Each of 2 cabinets identical in appearance has 2 drawers. Cabinet A con-\\ntains a silver coin in each drawer, and cabinet B contains a silver coin in\\none of its drawers and a gold coin in the other. A cabinet is randomly\\nselected, one of its drawers is opened, and a silver coin is found. What is\\nthe probability that there is a silver coin in the other drawer?\\n34. Prostate cancer is the most common type of cancer found in males. As an\\nindicator of whether a male has prostate cancer, doctors often perform\\na test that measures the level of the PSA protein (prostate speciﬁc anti-\\ngen) that is produced only by the prostate gland. Although higher PSA\\nlevels are indicative of cancer, the test is notoriously unreliable. Indeed,\\nthe probability that a noncancerous man will have an elevated PSA level\\nis approximately .135, with this probability increasing to approximately\\n.268 if the man does have cancer. If, based on other factors, a physician is\\n70 percent certain that a male has prostate cancer, what is the conditional\\nprobability that he has the cancer given that\\na.\\nthe test indicates an elevated PSA level;\\nb.\\nthe test does not indicate an elevated PSA level?\\nRepeat the preceding, this time assuming that the physician initially be-\\nlieves there is a 30 percent chance the man has prostate cancer.\\n35. Suppose that an insurance company classiﬁes people into one of three\\nclasses — good risks, average risks, and bad risks. Their records indicate\\nthat the probabilities that good, average, and bad risk persons will be in-\\nvolved in an accident over a 1-year span are, respectively, .05, .15, and .30.\\nIf 20 percent of the population are “good risks,” 50 percent are “average\\nrisks,” and 30 percent are “bad risks,” what proportion of people have\\naccidents in a ﬁxed year? If policy holder A had no accidents in 1987,\\nwhat is the probability that he or she is a good (average) risk?\\n36. A pair of fair dice is rolled. Let E denote the event that the sum of the dice\\nis equal to 7.\\na.\\nShow that E is independent of the event that the ﬁrst die lands on 4.\\nb.\\nShow that E is independent of the event that the second die lands\\non 3.\\n37. The probability of the closing of the ith relay in the circuits shown is\\ngiven by pi,i =1,2,3,4,5. If all relays function independently, what is\\nthe probability that a current ﬂows between A and B for the respective\\ncircuits?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 108}, page_content='Problems 95\\n38. An engineering system consisting of n components is said to be a\\nk-out-of-n system (k ≤n) if the system functions if and only if at least\\nk of the n components function. Suppose that all components function\\nindependently of each other.\\na.\\nIf the ith component functions with probability Pi,i =1,2,3,4,\\ncompute the probability that a 2-out-of-4 system functions.\\nb.\\nRepeat (a) for a 3-out-of-5 system.\\n39. Five independent ﬂips of a fair coin are made. Find the probability that\\na.\\nthe ﬁrst three ﬂips are the same;\\nb.\\neither the ﬁrst three ﬂips are the same, or the last three ﬂips are the\\nsame;\\nc.\\nthere are at least two heads among the ﬁrst three ﬂips, and at least\\ntwo tails among the last three ﬂips.\\n40. Suppose that n independent trials, each of which results in any of the\\noutcomes 0, 1, or 2, with respective probabilities .3, .5, and .2, are per-\\nformed. Find the probability that both outcome 1 and outcome 2 occur\\nat least once. (Hint: Consider the complementary probability.)\\n41. A parallel system functions whenever at least one of its components\\nworks. Consider a parallel system of n components, and suppose that\\neach component independently works with probability 1\\n2. Find the con-\\nditional probability that component 1 works, given that the system is\\nfunctioning.\\n42. A certain organism possesses a pair of each of 5 different genes (which\\nwe will designate by the ﬁrst 5 letters of the English alphabet). Each\\ngene appears in 2 forms (which we designate by lowercase and capital\\nletters). The capital letter will be assumed to be the dominant gene in\\nthe sense that if an organism possesses the gene pair xX, then it will\\noutwardly have the appearance of the X gene. For instance, if X stands\\nfor brown eyes and x for blue eyes, then an individual having either'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 109}, page_content='96 CHAPTER 3: Elements of probability\\ngene pair XX or xX will have brown eyes, whereas one having gene pair\\nxx will be blue-eyed. The characteristic appearance of an organism is\\ncalled its phenotype, whereas its genetic constitution is called its geno-\\ntype. (Thus 2 organisms with respective genotypes aA, bB, cc, dD, ee and\\nAA, BB, cc, DD, ee would have different genotypes but the same pheno-\\ntype.) In a mating between 2 organisms each one contributes, at random,\\none of its gene pairs of each type. The 5 contributions of an organism\\n(one of each of the 5 types) are assumed to be independent and are also\\nindependent of the contributions of its mate. In a mating between or-\\nganisms having genotypes aA, bB, cC, dD, eE, and aa, bB, cc, Dd, ee, what\\nis the probability that the progeny will (1) phenotypically, (2) genotypi-\\ncally resemble\\na.\\nthe ﬁrst parent;\\nb.\\nthe second parent;\\nc.\\neither parent;\\nd.\\nneither parent?\\n43. Three prisoners are informed by their jailer that one of them has been\\nchosen at random to be executed, and the other two are to be freed. Pris-\\noner A asks the jailer to tell him privately which of his fellow prisoners\\nwill be set free, claiming that there would be no harm in divulging this\\ninformation because he already knows that at least one of the two will go\\nfree. The jailer refuses to answer this question, pointing out that if A knew\\nwhich of his fellow prisoners were to be set free, then his own probability\\nof being executed would rise from 1\\n3 to 1\\n2 because he would then be one\\nof two prisoners. What do you think of the jailer’s reasoning?\\n44. Although both my parents have brown eyes, I have blue eyes. What is\\nthe probability that my sister has blue eyes? (As stated in Problem 42, an\\nindividual who receives a blue-eyed gene from each parent will have blue\\neyes, whereas one who receives one blue-eyed and one brown-eyed gene\\nwill have brown eyes.)\\n45. In a 7-game series played with two teams, the ﬁrst team to win a total of\\n4 games is the winner. Suppose that each game played is independently\\nwon by team A with probability p.\\na.\\nGiven that one team leads 3 to 0, what is the probability that it is\\nteam A that is leading?\\nb.\\nGiven that one team leads 3 to 0, what is the probability that team\\nwins the series?\\nc.\\nIf p = 1\\n2, what is the probability that the team that wins the ﬁrst\\ngame wins the series?\\n46. Suppose that distinct integer values are written on each of 3 cards. Sup-\\npose you are to be offered these cards in a random order. When you are\\noffered a card you must immediately either accept it or reject it. If you\\naccept a card, the process ends. If you reject a card, then the next card (if\\na card remains) is offered. If you reject the ﬁrst two cards offered, then\\nyou must accept the ﬁnal card.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 110}, page_content='Problems 97\\na.\\nIf you plan to accept the ﬁrst card offered, what is the probability\\nthat you will accept the highest valued card?\\nb.\\nIf you plan to reject the ﬁrst card offered, and to then accept the\\nsecond card if and only if its value is greater than the value of the\\nﬁrst card, what is the probability that you will accept the highest\\nvalued card?\\n47. Let A, B, C be events such that P(A) = .2, P(B) = .3, P(C) = .4.\\nFind the probability that at least one of the events A and B occurs if\\na.\\nA and B are mutually exclusive;\\nb.\\nA and B are independent.\\nFind the probability that all of the events A, B, C occur if\\na.\\nA, B, C are independent;\\nb.\\nA, B, C are mutually exclusive.\\n48. Two percent of women of age 45 who participate in routine screening\\nhave breast cancer. Ninety percent of those with breast cancer have posi-\\ntive mammographies. Ten percent of the women who do not have breast\\ncancer will also have positive mammographies. Given a woman has a\\npositive mammography, what is the probability she has breast cancer?\\n49. Twelve percent of all US households are in California. A total of 3.3 per-\\ncent of all US households earn over $250,000 per year, while a total of\\n6.3 percent of California households earn over $250,000 per year. If a\\nrandomly chosen US household earns over $250,000 per year, what is\\nthe probability it is from California?\\n50. There is a 60 percent chance that the event A will occur. If A does not oc-\\ncur, there is a 10 percent chance that B will occur. What is the probability\\nthat at least one of the events A or B occur?\\n51. Suppose distinct values are written on each of three cards, which are then\\nrandomly given the designations A, B, and C. The values on cards A and\\nB are then compared. What is the probability that the smaller of these\\nvalues is also smaller than the value on card C?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 111}, page_content='CHAPTER 4\\nRandom variables and expectation\\n4.1\\nRandom variables\\nWhen a random experiment is performed, we are often not interested in all of\\nthe details of the experimental result but only in the value of some numerical\\nquantity determined by the result. For instance, in tossing dice we are often\\ninterested in the sum of the two dice and are not really concerned about the\\nvalues of the individual dice. That is, we may be interested in knowing that the\\nsum is 7 and not be concerned over whether the actual outcome was (1, 6)\\nor (2, 5) or (3, 4) or (4, 3) or (5, 2) or (6, 1). Also, a civil engineer may not\\nbe directly concerned with the daily risings and declines of the water level of\\na reservoir (which we can take as the experimental result) but may only care\\nabout the level at the end of a rainy season. These quantities of interest that are\\ndetermined by the result of the experiment are known as random variables.\\nSince the value of a random variable is determined by the outcome of the ex-\\nperiment, we may assign probabilities of its possible values.\\nExample 4.1.a. Letting X denote the random variable that is deﬁned as the\\nsum of two fair dice, then\\nP{X = 2} = P {(1,1)} = 1\\n36\\n(4.1.1)\\nP{X = 3} = P {(1,2),(2,1)} = 2\\n36\\nP{X = 4} = P {(1,3),(2,2),(3,1)} = 3\\n36\\nP{X = 5} = P {(1,4),(2,3),(3,2),(4,1)} = 4\\n36\\nP{X = 6} = P {(1,5),(2,4),(3,3),(4,2),(5,1)} = 5\\n36\\nP{X = 7} = P {(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)} = 6\\n36\\nP{X = 8} = P {(2,6),(3,5),(4,4),(5,3),(6,2)} = 5\\n36\\nP{X = 9} = P {(3,6),(4,5),(5,4),(6,3)} = 4\\n36\\nP{X = 10} = P{(4,6),(5,5),(6,4)} = 3\\n36\\nP{X = 11} = P{(5,6),(6,5)} = 2\\n36\\nP{X = 12} = P{(6,6)} = 1\\n36\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00013-2\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n99'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 112}, page_content='100 CHAPTER 4: Random variables and expectation\\nIn other words, the random variable X can take on any integral value between\\n2 and 12 and the probability that it takes on each value is given by Equa-\\ntion (4.1.1). Since X must take on some value, we must have\\n1 = P(S) = P\\n\\x02 12\\n\\x03\\ni=2\\n{X = i}\\n\\x04\\n=\\n12\\n\\x05\\ni=2\\nP{X = i}\\nwhich is easily veriﬁed from Equation (4.1.1).\\nAnother random variable of possible interest in this experiment is the value of\\nthe ﬁrst die. Letting Y denote this random variable, then Y is equally likely to\\ntake on any of the values 1 through 6. That is,\\nP{Y = i} = 1/6,\\ni = 1,2,3,4,5,6\\n■\\nExample 4.1.b. Suppose that an individual purchases two electronic compo-\\nnents, each of which may be either defective or acceptable. In addition, suppose\\nthat the four possible results — (d, d), (d, a), (a, d), (a, a) — have respective\\nprobabilities .09, .21, .21, .49 [where (d, d) means that both components are\\ndefective, (d, a) that the ﬁrst component is defective and the second acceptable,\\nand so on]. If we let X denote the number of acceptable components obtained\\nin the purchase, then X is a random variable taking on one of the values 0, 1,\\n2 with respective probabilities\\nP{X = 0} = .09\\nP{X = 1} = .42\\nP{X = 2} = .49\\nIf we were mainly concerned with whether there was at least one acceptable\\ncomponent, we could deﬁne the random variable I by\\nI =\\n\\x06\\n1\\nif X = 1 or 2\\n0\\nif X = 0\\nIf A denotes the event that at least one acceptable component is obtained, then\\nthe random variable I is called the indicator random variable for the event A,\\nsince I will equal 1 or 0 depending upon whether A occurs. The probabilities\\nattached to the possible values of I are\\nP{I = 1} = .91\\nP{I = 0} = .09\\n■\\nIn the two foregoing examples, the random variables of interest took on a ﬁnite\\nnumber of possible values. Random variables whose set of possible values can'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 113}, page_content='4.1 Random variables\\n101\\nbe written either as a ﬁnite sequence x1,...,xn, or as an inﬁnite sequence x1,...\\nare said to be discrete. For instance, a random variable whose set of possible\\nvalues is the set of nonnegative integers is a discrete random variable. However,\\nthere also exist random variables that take on a continuum of possible values.\\nThese are known as continuous random variables. One example is the random\\nvariable denoting the lifetime of a car, when the car’s lifetime is assumed to\\ntake on any value in some interval (a, b).\\nThe cumulative distribution function, or more simply the distribution function, F\\nof the random variable X is deﬁned for any real number x by\\nF(x) = P{X ≤x}\\nThat is, F(x) is the probability that the random variable X takes on a value that\\nis less than or equal to x.\\nNotation: We will use the notation X ∼F to signify that F is the distribution\\nfunction of X.\\nAll probability questions about X can be answered in terms of its distribution\\nfunction F. For example, suppose we wanted to compute P{a < X ≤b}. This\\ncan be accomplished by ﬁrst noting that the event {X ≤b} can be expressed\\nas the union of the two mutually exclusive events {X ≤a} and {a < X ≤b}.\\nTherefore, applying Axiom 3, we obtain that\\nP{X ≤b} = P{X ≤a} + P{a < X ≤b}\\nor\\nP{a < X ≤b} = F(b) −F(a)\\nExample 4.1.c. Suppose the random variable X has distribution function\\nF(x) =\\n\\x06\\n0\\nx ≤0\\n1 −exp{−x2}\\nx > 0\\nWhat is the probability that X exceeds 1?\\nSolution. The desired probability is computed as follows:\\nP{X > 1} = 1 −P{X ≤1}\\n= 1 −F(1)\\n= e−1\\n= .368\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 114}, page_content='102 CHAPTER 4: Random variables and expectation\\nFIGURE 4.1\\nGraph of p(x), Example 4.2.a.\\n4.2\\nTypes of random variables\\nAs was previously mentioned, a random variable whose set of possible values\\nis a sequence is said to be discrete. For a discrete random variable X, we deﬁne\\nthe probability mass function p(a) of X by\\np(a) = P{X = a}\\nThe probability mass function p(a) is positive for at most a countable number\\nof values of a. That is, if X must assume one of the values x1,x2,..., then\\np(xi) > 0,\\ni = 1,2,...\\np(x) = 0,\\nall other values of x\\nSince X must take on one of the values xi, we have\\n∞\\n\\x05\\ni=1\\np(xi) = 1\\nExample 4.2.a. Consider a random variable X that is equal to 1, 2, or 3. If we\\nknow that\\np(1) = 1\\n2\\nand\\np(2) = 1\\n3\\nthen it follows (since p(1) + p(2) + p(3) = 1) that\\np(3) = 1\\n6\\nA graph of p(x) is presented in Figure 4.1.\\n■\\nThe cumulative distribution function F can be expressed in terms of p(x) by\\nF(a) =\\n\\x05\\nall x ≤a\\np(x)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 115}, page_content='4.2 Types of random variables\\n103\\nFIGURE 4.2\\nGraph of F(x).\\nIf X is a discrete random variable whose set of possible values are x1,x2,x3,...,\\nwhere x1 < x2 < x3 < ···, then its distribution function F is a step function.\\nThat is, the value of F is constant in the intervals [xi−1,xi) and then takes a\\nstep (or jump) of size p(xi) at xi.\\nFor instance, suppose X has a probability mass function given (as in Exam-\\nple 4.2.a) by\\np(1) = 1\\n2,\\np(2) = 1\\n3,\\np(3) = 1\\n6\\nThen the cumulative distribution function F of X is given by\\nF(a) =\\n⎧\\n⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎩\\n0\\na < 1\\n1\\n2\\n1 ≤a < 2\\n5\\n6\\n2 ≤a < 3\\n1\\n3 ≤a\\nThis is graphically presented in Figure 4.2.\\nWhereas the set of possible values of a discrete random variable is a sequence,\\nwe often must consider random variables whose set of possible values is an\\ninterval. Let X be such a random variable. We say that X is a continuous random\\nvariable if there exists a nonnegative function f (x), deﬁned for all real x ∈\\n(−∞,∞), having the property that for any set B of real numbers\\nP{X ∈B} =\\n\\x0b\\nB\\nf (x)dx\\n(4.2.1)\\nThe function f (x) is called the probability density function of the random vari-\\nable X.\\nIn words, Equation (4.2.1) states that the probability that X will be in B may\\nbe obtained by integrating the probability density function over the set B. Since\\nX must assume some value, f (x) must satisfy\\n1 = P{X ∈(−∞,∞)} =\\n\\x0b ∞\\n−∞\\nf (x)dx'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 116}, page_content='104 CHAPTER 4: Random variables and expectation\\nAll probability statements about X can be answered in terms of f (x). For in-\\nstance, letting B = [a,b], we obtain from Equation (4.2.1) that\\nP{a ≤X ≤b} =\\n\\x0b b\\na\\nf (x) dx\\n(4.2.2)\\nIf we let a = b in the above, then\\nP{X = a} =\\n\\x0b a\\na\\nf (x)dx = 0\\nIn words, this equation states that the probability that a continuous random\\nvariable will assume any particular value is zero. (See Figure 4.3.)\\nThe relationship between the cumulative distribution F(·) and the probability\\ndensity f (·) is expressed by\\nF(a) = P{X ∈(−∞,a]} =\\n\\x0b a\\n−∞\\nf (x)dx\\nDifferentiating both sides yields\\nd\\nda F(a) = f (a)\\nThat is, the density is the derivative of the cumulative distribution function.\\nA somewhat more intuitive interpretation of the density function may be ob-\\ntained from Equation 4.2.2 as follows:\\nP\\n\\x0c\\na −ε\\n2 ≤X ≤a + ε\\n2\\n\\r\\n=\\n\\x0b a+ε/2\\na−ε/2\\nf (x)dx ≈εf (a)\\nwhen ε is small. In other words, the probability that X will be contained in an\\ninterval of length ε around the point a is approximately εf (a). From this, we\\nFIGURE 4.3\\nThe probability density function f (x) =\\n\\x06\\ne−x\\nx ≥0\\n0\\nx < 0\\n.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 117}, page_content='4.3 Jointly distributed random variables\\n105\\nsee that f (a) is a measure of how likely it is that the random variable will be\\nnear a.\\nExample 4.2.b. Suppose that X is a continuous random variable whose prob-\\nability density function is given by\\nf (x) =\\n\\x06\\nC(4x −2x2)\\n0 < x < 2\\n0\\notherwise\\n(a) What is the value of C?\\n(b) Find P{X > 1}.\\nSolution. (a) Since f is a probability density function, we must have that\\n\\x0e ∞\\n−∞f (x)dx = 1, implying that\\nC\\n\\x0b 2\\n0\\n(4x −2x2)dx = 1\\nor\\nC\\n\\x0f\\n2x2 −2x3\\n3\\n\\x10\\x11\\x11\\x11\\nx=2\\nx=0 = 1\\nor\\nC = 3\\n8\\n(b) Hence\\nP{X > 1} =\\n\\x0b ∞\\n1\\nf (x)dx = 3\\n8\\n\\x0b 2\\n1\\n(4x −2x2)dx = 1\\n2\\n■\\n4.3\\nJointly distributed random variables\\nFor a given experiment, we are often interested not only in probability dis-\\ntribution functions of individual random variables but also in the relation-\\nships between two or more random variables. For instance, in an experiment\\ninto the possible causes of cancer, we might be interested in the relationship\\nbetween the average number of cigarettes smoked daily and the age at which\\nan individual contracts cancer. Similarly, an engineer might be interested in the\\nrelationship between the shear strength and the diameter of a spot weld in a\\nfabricated sheet steel specimen.\\nTo specify the relationship between two random variables, we deﬁne the\\njoint cumulative probability distribution function of X and Y by\\nF(x,y) = P{X ≤x,Y ≤y}.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 118}, page_content='106 CHAPTER 4: Random variables and expectation\\nA knowledge of the joint probability distribution function enables one, at least\\nin theory, to compute the probability of any statement concerning the values\\nof X and Y. For instance, the distribution function of X — call it FX — can be\\nobtained from the joint distribution function F of X and Y as follows:\\nFX(x) = P{X ≤x}\\n= P{X ≤x,Y < ∞}\\n= F(x,∞)\\nSimilarly, the cumulative distribution function of Y is given by\\nFY (y) = F(∞,y)\\nIn the case where X and Y are both discrete random variables whose possible\\nvalues are, respectively, x1,x2,..., and y1,y2,..., we deﬁne the joint probability\\nmass function of X and Y, p(xi,yj), by\\np(xi,yj) = P{X = xi,Y = yj}\\nThe individual probability mass functions of X and Y are easily obtained from\\nthe joint probability mass function by the following reasoning. Since Y must\\ntake on some value yj, it follows that the event {X = xi} can be written as the\\nunion, over all j, of the mutually exclusive events {X = xi,Y = yj}. That is,\\n{X = xi} =\\n\\x03\\nj\\n{X = xi,Y = yj}\\nand so, using Axiom 3 of the probability function, we see that\\nP{X = xi} = P\\n⎛\\n⎝\\x03\\nj\\n{X = xi,Y = yj}\\n⎞\\n⎠\\n(4.3.1)\\n=\\n\\x05\\nj\\nP{X = xi,Y = yj}\\n=\\n\\x05\\nj\\np(xi,yj)\\nSimilarly, we can obtain P{Y = yj} by summing p(xi,yj) over all possible val-\\nues of xi, that is,\\nP{Y = yj} =\\n\\x05\\ni\\nP{X = xi,Y = yj}\\n(4.3.2)\\n=\\n\\x05\\ni\\np(xi,yj)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 119}, page_content='4.3 Jointly distributed random variables\\n107\\nHence, specifying the joint probability mass function always determines the\\nindividual mass functions. However, it should be noted that the reverse is not\\ntrue. Namely, knowledge of P{X = xi} and P{Y = yj} does not determine the\\nvalue of P{X = xi,Y = yj}.\\nExample 4.3.a. Suppose that 3 batteries are randomly chosen from a group\\nof 3 new, 4 used but still working, and 5 defective batteries. If we let X and\\nY denote, respectively, the number of new and used but still working batteries\\nthat are chosen, then the joint probability mass function of X and Y,p(i,j) =\\nP{X = i,Y = j}, is given by\\np(i,j) =\\n\\x163\\ni\\n\\x17\\x164\\nj\\n\\x17\\x16\\n5\\n3−i−j\\n\\x17\\n\\x1612\\n3\\n\\x17\\nwhere the preceding follows because of the\\n\\x1612\\n3\\n\\x17\\nequally likely outcomes, there\\nare, by the basic principle of counting,\\n\\x163\\ni\\n\\x17\\x164\\nj\\n\\x17\\x16\\n5\\n3−i−j\\n\\x17\\npossible choices that con-\\ntain exactly i new, j used, and 3 −i −j defective batteries. Consequently,\\np(0,0) =\\n\\x18 5\\n3\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 10/220\\np(0,1) =\\n\\x18 4\\n1\\n\\x19\\x18 5\\n2\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 40/220\\np(0,2) =\\n\\x18 4\\n2\\n\\x19\\x18 5\\n1\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 30/220\\np(0,3) =\\n\\x18 4\\n3\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 4/220\\np(1,0) =\\n\\x18 3\\n1\\n\\x19\\x18 5\\n2\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 30/220\\np(1,1) =\\n\\x18 3\\n1\\n\\x19\\x18 4\\n1\\n\\x19\\x18 5\\n1\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 60/220\\np(1,2) =\\n\\x18 3\\n1\\n\\x19\\x18 4\\n2\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 18/220\\np(2,0) =\\n\\x18 3\\n2\\n\\x19\\x18 5\\n1\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 15/220\\np(2,1) =\\n\\x18 3\\n2\\n\\x19\\x18 4\\n1\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 12/220\\np(3,0) =\\n\\x18 3\\n3\\n\\x19\\x1a\\x18 12\\n3\\n\\x19\\n= 1/220\\nThese probabilities can most easily be expressed in tabular form as shown in\\nTable 4.1.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 120}, page_content='108 CHAPTER 4: Random variables and expectation\\nTable 4.1 P {X = i,Y = j}.\\ni\\nj\\n0\\n1\\n2\\n3\\nRow Sum\\n= P {X = i}\\n0\\n10\\n220\\n40\\n220\\n30\\n220\\n4\\n220\\n84\\n220\\n1\\n30\\n220\\n60\\n220\\n18\\n220\\n0\\n108\\n220\\n2\\n15\\n220\\n12\\n220\\n0\\n0\\n27\\n220\\n3\\n1\\n220\\n0\\n0\\n0\\n1\\n220\\nColumn\\nSums =\\nP {Y = j}\\n56\\n220\\n112\\n220\\n48\\n220\\n4\\n220\\nTable 4.2 P {B = i,G = j}.\\ni\\nj\\n0\\n1\\n2\\n3\\nRow Sum\\n= P {B = i}\\n0\\n.15\\n.10\\n.0875\\n.0375\\n.3750\\n1\\n.10\\n.175\\n.1125\\n0\\n.3875\\n2\\n.0875\\n.1125\\n0\\n0\\n.2000\\n3\\n.0375\\n0\\n0\\n0\\n.0375\\nColumn\\nSum =\\nP {G = j}\\n.3750\\n.3875\\n.2000\\n.0375\\nThe reader should note that the probability mass function of X is obtained\\nby computing the row sums, in accordance with the Equation (4.3.1), whereas\\nthe probability mass function of Y is obtained by computing the column sums,\\nin accordance with Equation (4.3.2). Because the individual probability mass\\nfunctions of X and Y thus appear in the margin of such a table, they are of-\\nten referred to as being the marginal probability mass functions of X and Y,\\nrespectively. It should be noted that to check the correctness of such a table\\nwe could sum the marginal row (or the marginal column) and verify that its\\nsum is 1. (Why must the sum of the entries in the marginal row (or column)\\nequal 1?)\\n■\\nExample 4.3.b. Suppose that 15 percent of the families in a certain community\\nhave no children, 20 percent have 1, 35 percent have 2, and 30 percent have 3\\nchildren; suppose further that each child is equally likely (and independently)\\nto be a boy or a girl. If a family is chosen at random from this community, then\\nB, the number of boys, and G, the number of girls, in this family will have the\\njoint probability mass function shown in Table 4.2.\\nThese probabilities are obtained as follows:\\nP{B = 0,G = 0} = P {no children}\\n= .15\\nP{B = 0,G = 1} = P {1 girl and total of 1 child}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 121}, page_content='4.3 Jointly distributed random variables\\n109\\n= P{1 child}P{1 girl|1 child}\\n= (.20)\\n\\x18\\n1\\n2\\n\\x19\\n= .1\\nP{B = 0,G = 2} = P{2 girls and total of 2 children}\\n= P{2 children}P{2 girls|2 children}\\n= (.35)\\n\\x18\\n1\\n2\\n\\x192\\n= .0875\\nP{B = 0,G = 3} = P{3 girls and total of 3 children}\\n= P{3 children}P{3 girls|3 children}\\n= (.30)\\n\\x18\\n1\\n2\\n\\x193\\n= .0375\\nWe leave it to the reader to verify the remainder of Table 4.2, which tells us,\\namong other things, that the family chosen will have at least 1 girl with proba-\\nbility .625.\\n■\\nWe say that X and Y are jointly continuous if there exists a function f (x,y)\\ndeﬁned for all real x and y, having the property that for every set C of pairs\\nof real numbers (that is, C is a set in the two-dimensional plane)\\nP{(X,Y) ∈C} =\\n\\x0b\\x0b\\n(x,y)∈C\\nf (x,y)dx dy\\n(4.3.3)\\nThe function f (x,y) is called the joint probability density function of X and Y. If A\\nand B are any sets of real numbers, then by deﬁning C = {(x,y) : x ∈A,y ∈B},\\nwe see from Equation (4.3.3) that\\nP{X ∈A,Y ∈B} =\\n\\x0b\\nB\\n\\x0b\\nA\\nf (x,y)dx dy\\n(4.3.4)\\nBecause\\nF(a,b) = P{X ∈(−∞,a],Y ∈(−∞,b]}\\n=\\n\\x0b b\\n−∞\\n\\x0b a\\n−∞\\nf (x,y)dx dy\\nit follows, upon differentiation, that\\nf (a,b) =\\n∂2\\n∂a ∂bF(a,b)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 122}, page_content='110 CHAPTER 4: Random variables and expectation\\nwherever the partial derivatives are deﬁned. Another interpretation of the joint\\ndensity function is obtained from Equation (4.3.4) as follows:\\nP{a < X < a + da,b < Y < b + db} =\\n\\x0b d+db\\nb\\n\\x0b a+da\\na\\nf (x,y)dx dy\\n≈f (a,b)da db\\nwhen da and db are small and f (x,y) is continuous at a, b. Hence f (a,b) is a\\nmeasure of how likely it is that the random vector (X,Y) will be near (a,b).\\nIf X and Y are jointly continuous, they are individually continuous, and their\\nprobability density functions can be obtained as follows:\\nP{X ∈A} = P{X ∈A,Y ∈(−∞,∞)}\\n(4.3.5)\\n=\\n\\x0b\\nA\\n\\x0b ∞\\n−∞\\nf (x,y)dy dx\\n=\\n\\x0b\\nA\\nfX(x)dx\\nwhere\\nfX(x) =\\n\\x0b ∞\\n−∞\\nf (x,y)dy\\nis thus the probability density function of X. Similarly, the probability density\\nfunction of Y is given by\\nfY (y) =\\n\\x0b ∞\\n−∞\\nf (x,y)dx\\n(4.3.6)\\nExample 4.3.c. The joint density function of X and Y is given by\\nf (x,y) =\\n⎧\\n⎨\\n⎩\\n2e−xe−2y\\n0 < x < ∞,0 < y < ∞\\n0\\notherwise\\nCompute (a) P{X > 1,Y < 1}; (b) P{X < Y}; and (c) P{X < a}.\\nSolution.\\n(a)\\nP{X > 1,Y < 1} =\\n\\x0b 1\\n0\\n\\x0b ∞\\n1\\n2e−xe−2y dx dy\\n=\\n\\x0b 1\\n0\\n2e−2y(−e−x|∞\\n1 )dy'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 123}, page_content='4.3 Jointly distributed random variables\\n111\\n= e−1\\n\\x0b 1\\n0\\n2e−2y dy\\n= e−1(1 −e−2)\\n(b)\\nP{X < Y} =\\n\\x0b\\x0b\\n(x,y):x<y\\n2e−xe−2y dx dy\\n=\\n\\x0b ∞\\n0\\n\\x0b y\\n0\\n2e−xe−2y dx dy\\n=\\n\\x0b ∞\\n0\\n2e−2y(1 −e−y)dy\\n=\\n\\x0b ∞\\n0\\n2e−2y dy −\\n\\x0b ∞\\n0\\n2e−3y dy\\n= 1 −2\\n3\\n= 1\\n3\\n(c)\\nP{X < a} =\\n\\x0b a\\n0\\n\\x0b ∞\\n0\\n2e−2ye−x dy dx\\n=\\n\\x0b a\\n0\\ne−x dx\\n= 1 −e−a\\n■\\n4.3.1\\nIndependent random variables\\nThe random variables X and Y are said to be independent if for any two sets of\\nreal numbers A and B\\nP{X ∈A,Y ∈B} = P{X ∈A}P{Y ∈B}\\n(4.3.7)\\nIn other words, X and Y are independent if, for all A and B, the events EA =\\n{X ∈A} and FB = {Y ∈B} are independent.\\nIt can be shown by using the three axioms of probability that Equation (4.3.7)\\nwill follow if and only if for all a, b\\nP{X ≤a,Y ≤b} = P{X ≤a}P{Y ≤b}\\nHence, in terms of the joint distribution function F of X and Y, we have that\\nX and Y are independent if\\nF(a,b) = FX(a)FY (b)\\nfor all a,b'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 124}, page_content='112 CHAPTER 4: Random variables and expectation\\nWhen X and Y are discrete random variables, the condition of independence\\nEquation (4.3.7) is equivalent to\\np(x,y) = pX(x)pY (y)\\nfor all x,y\\n(4.3.8)\\nwhere pX and pY are the probability mass functions of X and Y. The equiv-\\nalence follows because, if Equation (4.3.7) is satisﬁed, then we obtain Equa-\\ntion (4.3.8) by letting A and B be, respectively, the one-point sets A = {x},\\nB = {y}. Furthermore, if Equation (4.3.8) is valid, then for any sets A, B\\nP{X ∈A,Y ∈B} =\\n\\x05\\ny∈B\\n\\x05\\nx∈A\\np(x,y)\\n=\\n\\x05\\ny∈B\\n\\x05\\nx∈A\\npX(x)pY (y)\\n=\\n\\x05\\ny∈B\\npY (y)\\n\\x05\\nx∈A\\npX(x)\\n= P{Y ∈B}P{X ∈A}\\nand thus Equation (4.3.7) is established.\\nIn the jointly continuous case, the condition of independence is equivalent to\\nf (x,y) = fX(x)fY (y)\\nfor all x,y\\nLoosely speaking, X and Y are independent if knowing the value of one does\\nnot change the distribution of the other. Random variables that are not inde-\\npendent are said to be dependent.\\nExample 4.3.d. Suppose that X and Y are independent random variables hav-\\ning the common density function\\nf (x) =\\n\\x06\\ne−x\\nx > 0\\n0\\notherwise\\nFind the density function of the random variable X/Y.\\nSolution. We start by determining the distribution function of X/Y. For a > 0\\nFX/Y (a) = P{X/Y ≤a}\\n=\\n\\x0b\\x0b\\nx/y≤a\\nf (x,y)dx dy\\n=\\n\\x0b\\x0b\\nx/y≤a\\ne−xe−y dx dy'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 125}, page_content='4.3 Jointly distributed random variables\\n113\\n=\\n\\x0b ∞\\n0\\n\\x0b ay\\n0\\ne−xe−y dx dy\\n=\\n\\x0b ∞\\n0\\n(1 −e−ay)e−y dy\\n=\\n\\x1b\\n−e−y + e−(a+1)y\\na + 1\\n\\x1c\\x11\\x11\\x11\\n∞\\n0\\n= 1 −\\n1\\na + 1\\nDifferentiation yields that the density function of X/Y is given by\\nfX/Y (a) = 1/(a + 1)2,\\n0 < a < ∞\\n■\\nWe can also deﬁne joint probability distributions for n random variables in\\nexactly the same manner as we did for n = 2. For instance, the joint cumulative\\nprobability distribution function F(a1,a2,...,an) of the n random variables\\nX1,X2,...,Xn is deﬁned by\\nF(a1,a2,...,an) = P{X1 ≤a1,X2 ≤a2,...,Xn ≤an}\\nIf these random variables are discrete, we deﬁne their joint probability mass\\nfunction p(x1,x2,...,xn) by\\np(x1,x2,...,xn) = P{X1 = x1,X2 = x2,...,Xn = xn}\\nFurther, the n random variables are said to be jointly continuous if there exists\\na function f (x1,x2,...,xn), called the joint probability density function, such\\nthat for any set C in n-space\\nP{(X1,X2,...,Xn) ∈C} =\\n\\x0b \\x0b\\n(x1,..., xn)∈C\\n...\\n\\x0b\\nf (x1,...,xn)dx1 dx2 ···dxn\\nIn particular, for any n sets of real numbers A1,A2,...,An\\nP{X1 ∈A1,X2 ∈A2,...,Xn ∈An}\\n=\\n\\x0b\\nAn\\n\\x0b\\nAn−1\\n...\\n\\x0b\\nA1\\nf (x1,...,xn)dx1 dx2 ...dxn\\nThe concept of independence may, of course, also be deﬁned for more than\\ntwo random variables. In general, the n random variables X1,X2,...,Xn are\\nsaid to be independent if, for all sets of real numbers A1,A2,...,An,\\nP{X1 ∈A1,X2 ∈A2,...,Xn ∈An} =\\nn\\n\\x1d\\ni=1\\nP{Xi ∈Ai}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 126}, page_content='114 CHAPTER 4: Random variables and expectation\\nAs before, it can be shown that this condition is equivalent to\\nP{X1 ≤a1,X2 ≤a2,...,Xn ≤an}\\n=\\nn\\n\\x1d\\ni=1\\nP{X1 ≤ai}\\nfor all a1,a2,...,an\\nFinally, we say that an inﬁnite collection of random variables is independent if\\nevery ﬁnite subcollection of them is independent.\\nExample 4.3.e. Suppose that the successive daily changes of the price of a\\ngiven stock are assumed to be independent and identically distributed random\\nvariables with probability mass function given by\\nP{daily change isi} =\\n⎧\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\\n−3\\nwith probability .05\\n−2\\nwith probability .10\\n−1\\nwith probability .20\\n0\\nwith probability .30\\n1\\nwith probability .20\\n2\\nwith probability .10\\n3\\nwith probability .05\\nThen the probability that the stock’s price will increase successively by 1, 2, and\\n0 points in the next three days is\\nP{X1 = 1,X2 = 2,X3 = 0} = (.20)(.10)(.30) = .006\\nwhere we have let Xi denote the change on the ith day.\\n■\\n4.3.2\\nConditional distributions1\\nThe relationship between two random variables can often be clariﬁed by con-\\nsideration of the conditional distribution of one given the value of the other.\\nRecall that for any two events E and F, the conditional probability of E given\\nF is deﬁned, provided that P(F) > 0, by\\nP(E|F) = P(EF)\\nP(F)\\nHence, if X and Y are discrete random variables, it is natural to deﬁne the\\nconditional probability mass function of X given that Y = y, by\\npX|Y (x|y) = P{X = x|Y = y}\\n1Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 127}, page_content='4.3 Jointly distributed random variables\\n115\\n= P{X = x,Y = y}\\nP{Y = y}\\n= p(x,y)\\npY (y)\\nfor all values of y such that pY (y) > 0.\\nExample 4.3.f. If we know, in Example 4.3.b, that the family chosen has one\\ngirl, compute the conditional probability mass function of the number of boys\\nin the family.\\nSolution. We ﬁrst note from Table 4.2 that\\nP{G = 1} = .3875\\nHence,\\nP{B = 0|G = 1} = P{B = 0,G = 1}\\nP{G = 1}\\n=\\n.10\\n.3875 = 8/31\\nP{B = 1|G = 1} = P{B = 1,G = 1}\\nP{G = 1}\\n= .175\\n.3875 = 14/31\\nP{B = 2|G = 1} = P{B = 2,G = 1}\\nP{G = 1}\\n= .1125\\n.3875 = 9/31\\nP{B = 3|G = 1} = P{B = 3,G = 1}\\nP{G = 1}\\n= 0\\nThus, for instance, given 1 girl, there are 23 chances out of 31 that there will\\nalso be at least 1 boy.\\n■\\nExample 4.3.g. Suppose that p(x,y), the joint probability mass function of X\\nand Y, is given by\\np(0,0) = .4,\\np(0,1) = .2,\\np(1,0) = .1,\\np(1,1) = .3\\nCalculate the conditional probability mass function of X given that Y = 1.\\nSolution. We ﬁrst note that\\nP{Y = 1} =\\n\\x05\\nx\\np(x,1) = p(0,1) + p(1,1) = .5\\nHence,\\nP{X = 0|Y = 1} =\\np(0,1)\\nP{Y = 1} = 2/5\\nP{X = 1|Y = 1} =\\np(1,1)\\nP{Y = 1} = 3/5\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 128}, page_content='116 CHAPTER 4: Random variables and expectation\\nIf X and Y have a joint probability density function f (x,y), then the condi-\\ntional probability density function of X, given that Y = y, is deﬁned for all\\nvalues of y such that fY (y) > 0, by\\nfX|Y (x|y) = f (x,y)\\nfY (y)\\nTo motivate this deﬁnition, multiply the left-hand side by dx and the right-\\nhand side by (dx dy)/dy to obtain\\nfX|Y (x|y)dx = f (x,y)dx dy\\nfY (y)dy\\n≈P{x ≤X ≤x + dx,y ≤Y ≤y + dy}\\nP{y ≤Y ≤y + dy}\\n= P{x ≤X ≤x + dy|y ≤Y ≤y + dy}\\nIn other words, for small values of dx and dy, fX|Y (x|y)dx represents the con-\\nditional probability that X is between x and x + dx, given that Y is between y\\nand y + dy.\\nThe use of conditional densities allows us to deﬁne conditional probabilities\\nof events associated with one random variable when we are given the value of\\na second random variable. That is, if X and Y are jointly continuous, then, for\\nany set A,\\nP{X ∈A|Y = y} =\\n\\x0b\\nA\\nfX|Y (x|y)dx\\nExample 4.3.h. The joint density of X and Y is given by\\nf (x,y) =\\n\\x06\\n12\\n5 x(2 −x −y)\\n0 < x < 1,0 < y < 1\\n0\\notherwise\\nCompute the conditional density of X, given that Y = y, where 0 < y < 1.\\nSolution. For 0 < x < 1, 0 < y < 1, we have\\nfX|Y (x|y) = f (x,y)\\nfY (y)\\n=\\nf (x,y)\\n\\x0e ∞\\n−∞f (x,y)dx\\n=\\nx(2 −x −y)\\n\\x0e 1\\n0 x(2 −x −y)dx\\n= x(2 −x −y)\\n2\\n3 −y/2\\n= 6x(2 −x −y)\\n4 −3y\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 129}, page_content='4.4 Expectation 117\\n4.4\\nExpectation\\nOne of the most important concepts in probability theory is that of the expec-\\ntation of a random variable. If X is a discrete random variable taking on the\\npossible values x1,x2,..., then the expectation or expected value of X, denoted\\nby E[X], is deﬁned by\\nE[X] =\\n\\x05\\ni\\nxiP{X = xi}\\nIn words, the expected value of X is a weighted average of the possible val-\\nues that X can take on, each value being weighted by the probability that X\\nassumes it. For instance, if the probability mass function of X is given by\\np(0) = 1\\n2 = p(1)\\nthen\\nE[X] = 0\\n\\x18\\n1\\n2\\n\\x19\\n+ 1\\n\\x18\\n1\\n2\\n\\x19\\n= 1\\n2\\nis just the ordinary average of the two possible values 0 and 1 that X can as-\\nsume. On the other hand, if\\np(0) = 1\\n3,\\np(1) = 2\\n3\\nthen\\nE[X] = 0\\n\\x18\\n1\\n3\\n\\x19\\n+ 1\\n\\x18\\n2\\n3\\n\\x19\\n= 2\\n3\\nis a weighted average of the two possible values 0 and 1 where the value 1 is\\ngiven twice as much weight as the value 0 since p(1) = 2p(0).\\nAnother motivation of the deﬁnition of expectation is provided by the fre-\\nquency interpretation of probabilities. This interpretation assumes that if an\\ninﬁnite sequence of independent replications of an experiment is performed,\\nthen for any event E, the proportion of time that E occurs will be P(E). Now,\\nconsider a random variable X that must take on one of the values x1,x2,...,xn\\nwith respective probabilities p(x1),p(x2),...,p(xn); and think of X as repre-\\nsenting our winnings in a single game of chance. That is, with probability p(xi)\\nwe shall win xi units i = 1,2,...,n. Now by the frequency interpretation, it\\nfollows that if we continually play this game, then the proportion of time that\\nwe win xi will be p(xi). Since this is true for all i, i = 1,2,...,n, it follows that\\nour average winnings per game will be\\nn\\n\\x05\\ni=1\\nxip(xi) = E[X]'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 130}, page_content='118 CHAPTER 4: Random variables and expectation\\nTo see this argument more clearly, suppose that we play N games where N is\\nvery large. Then in approximately Np(xi) of these games, we shall win xi, and\\nthus our total winnings in the N games will be\\nn\\n\\x05\\ni=1\\nxiN p(xi)\\nimplying that our average winnings per game are\\nn\\n\\x05\\ni=1\\nxiNp(xi)\\nN\\n=\\nn\\n\\x05\\ni=1\\nxip(xi) = E[X]\\nExample 4.4.a. Find E[X] where X is the outcome when we roll a fair die.\\nSolution. Since p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = 1\\n6, we obtain that\\nE[X] = 1\\n\\x18\\n1\\n6\\n\\x19\\n+ 2\\n\\x18\\n1\\n6\\n\\x19\\n+ 3\\n\\x18\\n1\\n6\\n\\x19\\n+ 4\\n\\x18\\n1\\n6\\n\\x19\\n+ 5\\n\\x18\\n1\\n6\\n\\x19\\n+ 6\\n\\x18\\n1\\n6\\n\\x19\\n= 7\\n2\\nThe reader should note that, for this example, the expected value of X is not a\\nvalue that X could possibly assume. (That is, rolling a die cannot possibly lead\\nto an outcome of 7/2.) Thus, even though we call E[X] the expectation of X, it\\nshould not be interpreted as the value that we expect X to have but rather as the\\naverage value of X in a large number of repetitions of the experiment. That is,\\nif we continually roll a fair die, then after a large number of rolls the average of\\nall the outcomes will be approximately 7/2. (The interested reader should try\\nthis as an experiment.)\\n■\\nExample 4.4.b. If I is an indicator random variable for the event A, that is, if\\nI =\\n\\x06\\n1\\nif A occurs\\n0\\nif A does not occur\\nthen\\nE[I] = 1P(A) + 0P(Ac) = P(A)\\nHence, the expectation of the indicator random variable for the event A is just\\nthe probability that A occurs.\\n■\\nExample 4.4.c (Entropy). For a given random variable X, how much infor-\\nmation is conveyed in the message that X = x? Let us begin our attempts at\\nquantifying this statement by agreeing that the amount of information in the\\nmessage that X = x should depend on how likely it was that X would equal\\nx. In addition, it seems reasonable that the more unlikely it was that X would\\nequal x, the more informative would be the message. For instance, if X repre-\\nsents the sum of two fair dice, then there seems to be more information in the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 131}, page_content='4.4 Expectation 119\\nmessage that X equals 12 than there would be in the message that X equals 7,\\nsince the former event has probability 1\\n36 and the latter 1\\n6.\\nLet us denote by I(p) the amount of information contained in the message\\nthat an event, whose probability is p, has occurred. Clearly I(p) should be a\\nnonnegative, decreasing function of p. To determine its form, let X and Y be\\nindependent random variables, and suppose that P{X = x} = p and P{Y =\\ny} = q. How much information is contained in the message that X equals x\\nand Y equals y? To answer this, note ﬁrst that the amount of information in\\nthe statement that X equals x is I(p). Also, since knowledge of the fact that X is\\nequal to x does not affect the probability that Y will equal y (since X and Y are\\nindependent), it seems reasonable that the additional amount of information\\ncontained in the statement that Y = y should equal I(q). Thus, it seems that\\nthe amount of information in the message that X equals x and Y equals y is\\nI(p) + I(q). On the other hand, however, we have that\\nP{X = x,Y = y} = P{X = x}P{Y = y} = pq\\nwhich implies that the amount of information in the message that X equals x\\nand Y equals y is I(pq). Therefore, it seems that the function I should satisfy\\nthe identity\\nI(pq) = I(p) + I(q)\\nHowever, if we deﬁne the function G by\\nG(p) = I(2−p)\\nthen we see from the above that\\nG(p + q) = I(2−( p+q))\\n= I(2−p2−q)\\n= I(2−p) + I(2−q)\\n= G(p) + G(q)\\nHowever, it can be shown that the only (monotone) functions G that satisfy\\nthe foregoing functional relationship are those of the form\\nG(p) = cp\\nfor some constant c. Therefore, we must have that\\nI(2−p) = cp\\nor, letting q = 2−p\\nI(q) = −c log2(q)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 132}, page_content='120 CHAPTER 4: Random variables and expectation\\nfor some positive constant c. It is traditional to let c = 1 and to say that the\\ninformation is measured in units of bits (short for binary digits).\\nConsider now a random variable X, which must take on one of the values\\nx1,...,xn with respective probabilities p1,...,pn. As −log2(pi) represents the\\ninformation conveyed by the message that X is equal to xi, it follows that the\\nexpected amount of information that will be conveyed when the value of X is\\ntransmitted is given by\\nH(X) = −\\nn\\n\\x05\\ni=1\\npi log2(pi)\\nThe quantity H(X) is known in information theory as the entropy of the random\\nvariable X.\\n■\\nWe can also deﬁne the expectation of a continuous random variable. Suppose\\nthat X is a continuous random variable with probability density function f .\\nSince, for dx small\\nf (x)dx ≈P{x < X < x + dx}\\nit follows that a weighted average of all possible values of X, with the weight\\ngiven to x equal to the probability that X is near x, is just the integral over all\\nx of xf (x)dx. Hence, it is natural to deﬁne the expected value of X by\\nE[X] =\\n\\x0b ∞\\n−∞\\nx f (x)dx\\nExample 4.4.d. Suppose that you are expecting a message at some time past\\n5 P.M. From experience you know that X, the number of hours after 5 P.M.\\nuntil the message arrives, is a random variable with the following probability\\ndensity function:\\nf (x) =\\n⎧\\n⎨\\n⎩\\n1\\n1.5\\nif 0 < x < 1.5\\n0\\notherwise\\nThe expected amount of time past 5 P.M. until the message arrives is given by\\nE[X] =\\n\\x0b 1.5\\n0\\nx\\n1.5 dx = .75\\nHence, on average, you would have to wait three-fourths of an hour.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 133}, page_content='4.5 Properties of the expected value\\n121\\nFIGURE 4.4\\nRemarks\\n(a) The concept of expectation is analogous to the physical concept of the cen-\\nter of gravity of a distribution of mass. Consider a discrete random variable X\\nhaving probability mass function p(xi),i ≥1. If we now imagine a weightless\\nrod in which weights with mass p(xi),i ≥1 are located at the points xi,i ≥1\\n(see Figure 4.4), then the point at which the rod would be in balance is known\\nas the center of gravity. For those readers acquainted with elementary statistics,\\nit is now a simple matter to show that this point is at E[X].2\\n(b) E[X] has the same units of measurement as does X.\\n4.5\\nProperties of the expected value\\nSuppose now that we are given a random variable X and its probability dis-\\ntribution (that is, its probability mass function in the discrete case or its\\nprobability density function in the continuous case). Suppose also that we are\\ninterested in calculating, not the expected value of X, but the expected value of\\nsome function of X, say g(X). How do we go about doing this? One way is as\\nfollows. Since g(X) is itself a random variable, it must have a probability dis-\\ntribution, which should be computable from a knowledge of the distribution\\nof X. Once we have obtained the distribution of g(X), we can then compute\\nE[g(X)] by the deﬁnition of the expectation.\\nExample 4.5.a. Suppose X has the following probability mass function\\np(0) = .2,\\np(1) = .5,\\np(2) = .3\\nCalculate E[X2].\\nSolution. Letting Y = X2, we have that Y is a random variable that can take on\\none of the values 02, 12, 22 with respective probabilities\\npY (0) = P{Y = 02} = .2\\npY (1) = P{Y = 12} = .5\\npY (4) = P{Y = 22} = .3\\n2To prove this, we must show that the sum of the torques tending to turn the point around E[X] is equal\\nto 0. That is, we must show that 0 = \\x1e\\ni(xi −E[X])p(xi), which is immediate.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 134}, page_content='122 CHAPTER 4: Random variables and expectation\\nHence,\\nE[X2] = E[Y] = 0(.2) + 1(.5) + 4(.3) = 1.7\\n■\\nExample 4.5.b. The time, in hours, it takes to locate and repair an electrical\\nbreakdown in a certain factory is a random variable — call it X — whose den-\\nsity function is given by\\nfX(x) =\\n\\x06\\n1\\nif 0 < x < 1\\n0\\notherwise\\nIf the cost involved in a breakdown of duration x is x3, what is the expected\\ncost of such a breakdown?\\nSolution. Letting Y = X3 denote the cost, we ﬁrst calculate its distribution\\nfunction as follows. For 0 ≤a ≤1,\\nFY (a) = P{Y ≤a}\\n= P{X3 ≤a}\\n= P{X ≤a1/3}\\n=\\n\\x0b a1/3\\n0\\ndx\\n= a1/3\\nBy differentiating FY (a), we obtain the density of Y,\\nfY (a) = 1\\n3a−2/3,\\n0 ≤a < 1\\nHence,\\nE[X3] = E[Y] =\\n\\x0b ∞\\n−∞\\nafY (a)da\\n=\\n\\x0b 1\\n0\\na 1\\n3a−2/3 da\\n= 1\\n3\\n\\x0b 1\\n0\\na1/3 da\\n= 1\\n3\\n3\\n4a4/3|1\\n0\\n= 1\\n4\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 135}, page_content='4.5 Properties of the expected value\\n123\\nWhile the foregoing procedure will, in theory, always enable us to compute the\\nexpectation of any function of X from a knowledge of the distribution of X,\\nthere is an easier way of doing this. Suppose, for instance, that we wanted to\\ncompute the expected value of g(X). Since g(X) takes on the value g(x) when\\nX = x, it seems intuitive that E[g(X)] should be a weighted average of the\\npossible values g(x) with, for a given x, the weight given to g(x) being equal\\nto the probability (or probability density in the continuous case) that X will\\nequal x. Indeed, the foregoing can be shown to be true and we thus have the\\nfollowing proposition.\\nProposition 4.5.1. (Expectation of a function of a random variable).\\n(a) If X is a discrete random variable with probability mass function p(x),\\nthen for any real-valued function g,\\nE[g(X)] =\\n\\x05\\nx\\ng(x)p(x)\\n(b) If X is a continuous random variable with probability density function\\nf (x), then for any real-valued function g,\\nE[g(X)] =\\n\\x0b ∞\\n−∞\\ng(x)f (x)dx\\nExample 4.5.c. Applying Proposition 4.5.1 to Example 4.5.a yields\\nE[X2] = 02(0.2) + (12)(0.5) + (22)(0.3) = 1.7\\nwhich, of course, checks with the result derived in Example 4.5.a.\\n■\\nExample 4.5.d. Applying the proposition to Example 4.5.b yields\\nE[X3] =\\n\\x0b 1\\n0\\nx3dx\\n(since f (x) = 1,0 < x < 1)\\n= 1\\n4\\n■\\nAn immediate corollary of Proposition 4.5.1 is the following.\\nCorollary 4.5.2. If a and b are constants, then\\nE[aX + b] = aE[X] + b\\nProof. In the discrete case,\\nE[aX + b] =\\n\\x05\\nx\\n(ax + b)p(x)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 136}, page_content='124 CHAPTER 4: Random variables and expectation\\n= a\\n\\x05\\nx\\nx p(x) + b\\n\\x05\\nx\\np(x)\\n= aE[X] + b\\nIn the continuous case,\\nE[aX + b] =\\n\\x0b ∞\\n−∞\\n(ax + b)f (x)dx\\n= a\\n\\x0b ∞\\n−∞\\nx f (x)dx + b\\n\\x0b ∞\\n−∞\\nf (x)dx\\n= aE[X] + b\\n■\\nIf we take a = 0 in Corollary 4.5.2, we see that\\nE[b] = b\\nThat is, the expected value of a constant is just its value. (Is this intuitive?) Also,\\nif we take b = 0, then we obtain\\nE[aX] = aE[X]\\nor, in words, the expected value of a constant multiplied by a random variable is\\njust the constant times the expected value of the random variable. The expected\\nvalue of a random variable X, E[X], is also referred to as the mean or the ﬁrst\\nmoment of X. The quantity E[Xn],n ≥1, is called the nth moment of X. By\\nProposition 4.5.1, we note that\\nE[Xn] =\\n⎧\\n⎪⎪⎨\\n⎪⎪⎩\\n\\x05\\nx\\nxnp(x)\\nif X is discrete\\n\\x0b ∞\\n−∞\\nxnf (x)dx\\nif X is continuous\\n4.5.1\\nExpected value of sums of random variables\\nThe two-dimensional version of Proposition 4.5.1 states that if X and Y are\\nrandom variables and g is a function of two variables, then\\nE[g(X,Y)] =\\n\\x05\\ny\\n\\x05\\nx\\ng(x,y)p(x,y)\\nin the discrete case\\n=\\n\\x0b ∞\\n−∞\\n\\x0b ∞\\n−∞\\ng(x,y)f (x,y)dx dy\\nin the continuous case'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 137}, page_content='4.5 Properties of the expected value\\n125\\nFor example, if g(X,Y) = X + Y, then, in the continuous case,\\nE[X + Y] =\\n\\x0b ∞\\n−∞\\n\\x0b ∞\\n−∞\\n(x + y)f (x,y)dx dy\\n=\\n\\x0b ∞\\n−∞\\n\\x0b ∞\\n−∞\\nx f (x,y)dx dy +\\n\\x0b ∞\\n−∞\\n\\x0b ∞\\n−∞\\ny f (x,y)dx dy\\n= E[X] + E[Y]\\nwhere the ﬁnal equality followed by applying the identity\\nE[g(X,Y)] =\\n\\x0b ∞\\n−∞\\n\\x0b ∞\\n−∞\\ng(x,y)f (x,y)dx dy\\nﬁrst to the function g(x,y) = x and then to the function g(x,y) = y.\\nA similar result can be shown in the discrete case and indeed, for any random\\nvariables X and Y,\\nE[X + Y] = E[X] + E[Y]\\n(4.5.1)\\nBy repeatedly applying Equation (4.5.1) we can show that the expected value of\\nthe sum of any number of random variables equals the sum of their individual\\nexpectations. For instance,\\nE[X + Y + Z] = E[(X + Y) + Z]\\n= E[X + Y] + E[Z]\\nby Equation (4.5.1)\\n= E[X] + E[Y] + E[Z]\\nagain by Equation (4.5.1)\\nAnd in general, for any n,\\nE[X1 + X2 ··· + Xn] = E[X1] + E[X2] + ··· + E[Xn]\\n(4.5.2)\\nEquation (4.5.2) is an extremely useful formula whose utility will now be il-\\nlustrated by a series of examples.\\nExample 4.5.e. Find the expected value of the sum obtained when two fair\\ndice are rolled.\\nSolution. If X is the sum, then E[X] can be obtained from the formula\\nE[X] =\\n12\\n\\x05\\ni=2\\ni P(X = i)\\nHowever, it is simpler to name the dice, and let Xi be the value on dice i,i =\\n1,2. As, X = X1 + X2, this yields that\\nE[X] = E[X1] + E[X2]\\nThus, from Example 4.4.a, we see that E[X] = 7.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 138}, page_content='126 CHAPTER 4: Random variables and expectation\\nExample 4.5.f. A construction ﬁrm has recently sent in bids for 3 jobs worth\\n(in proﬁts) 10, 20, and 40 (thousand) dollars. If its probabilities of winning\\nthe jobs are respectively .2, .8, and .3, what is the ﬁrm’s expected total proﬁt?\\nSolution. Letting Xi,i = 1,2,3 denote the ﬁrm’s proﬁt from job i, then\\ntotal proﬁt = X1 + X2 + X3\\nand so\\nE[total proﬁt] = E[X1] + E[X2] + E[X3]\\nNow\\nE[X1] = 10(.2) + 0(.8) = 2\\nE[X2] = 20(.8) + 0(.2) = 16\\nE[X3] = 40(.3) + 0(.7) = 12\\nand thus the ﬁrm’s expected total proﬁt is 30 thousand dollars.\\n■\\nExample 4.5.g. A secretary has typed N letters along with their respective en-\\nvelopes. The envelopes get mixed up when they fall on the ﬂoor. If the letters\\nare placed in the mixed-up envelopes in a completely random manner (that\\nis, each letter is equally likely to end up in any of the envelopes), what is the\\nexpected number of letters that are placed in the correct envelopes?\\nSolution. Letting X denote the number of letters that are placed in the correct\\nenvelope, we can most easily compute E[X] by noting that\\nX = X1 + X2 + ··· + XN\\nwhere\\nXi =\\n\\x1f\\n1\\nif the ith letter is placed in its proper envelope\\n0\\notherwise\\nNow, since the ith letter is equally likely to be put in any of the N envelopes, it\\nfollows that\\nP{Xi = 1} = P {ith letter is in its proper envelope} = 1/N\\nand so\\nE[Xi] = 1P{Xi = 1} + 0P{Xi = 0} = 1/N\\nHence, from Equation (4.5.2) we obtain that\\nE[X] = E[X1] + ··· + E[XN] =\\n 1\\nN\\n!\\nN = 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 139}, page_content='4.5 Properties of the expected value\\n127\\nHence, no matter how many letters there are, on the average, exactly one of the\\nletters will be in its own envelope.\\n■\\nExample 4.5.h. Suppose there are 20 different types of coupons and suppose\\nthat each time one obtains a coupon it is equally likely to be any one of the\\ntypes. Compute the expected number of different types that are contained in a\\nset for 10 coupons.\\nSolution. Let X denote the number of different types in the set of 10 coupons.\\nWe compute E[X] by using the representation\\nX = X1 + ··· + X20\\nwhere\\nXi =\\n\\x06\\n1\\nif at least one type i coupon is contained in the set of 10\\n0\\notherwise\\nNow\\nE[Xi] = P{Xi = 1}\\n= P{at least one type i coupon is in the set of 10}\\n= 1 −P{no type i coupons are contained in the set of 10}\\n= 1 −\\n\\x18\\n19\\n20\\n\\x1910\\nwhen the last equality follows since each of the 10 coupons will (indepen-\\ndently) not be a type i with probability 19\\n20. Hence,\\nE[X] = E[X1] + ··· + E[X20] = 20\\n\\x0f\\n1 −\\n\\x18\\n19\\n20\\n\\x1910\\x10\\n= 8.025\\n■\\nAn important property of the mean arises when one must predict the value of\\na random variable. That is, suppose that the value of a random variable X is to\\nbe predicted. If we predict that X will equal c, then the square of the “error”\\ninvolved will be (X −c)2. We will now show that the average squared error is\\nminimized when we predict that X will equal its mean μ. To see this, note that\\nfor any constant c\\nE[(X −c)2] = E[(X −μ + μ −c)2]\\n= E[(X −μ)2 + 2(μ −c)(X −μ) + (μ −c)2]\\n= E[(X −μ)2] + 2(μ −c)E[X −μ] + (μ −c)2\\n= E[(X −μ)2] + (μ −c)2\\nsince\\nE[X −μ] = E[X] −μ = 0'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 140}, page_content='128 CHAPTER 4: Random variables and expectation\\n≥E[(X −μ)2]\\nHence, the best predictor of a random variable, in terms of minimizing the\\nexpected square of its error, is just its mean.\\n4.6\\nVariance\\nGiven a random variable X along with its probability distribution function, it\\nwould be extremely useful if we were able to summarize the essential prop-\\nerties of the mass function by certain suitably deﬁned measures. One such\\nmeasure would be E[X], the expected value of X. However, while E[X] yields\\nthe weighted average of the possible values of X, it does not tell us anything\\nabout the variation, or spread, of these values. For instance, while the following\\nrandom variables W, Y, and Z having probability mass functions determined\\nby\\nW = 0\\nwith probability 1\\nY =\\n\\x06\\n−1\\nwith probability 1\\n2\\n1\\nwith probability 1\\n2\\nZ =\\n\\x06\\n−100\\nwith probability 1\\n2\\n100\\nwith probability 1\\n2\\nall have the same expectation — namely, 0 — there is much greater spread in\\nthe possible values of Y than in those of W (which is a constant) and in the\\npossible values of Z than in those of Y.\\nBecause we expect X to take on values around its mean E[X], it would ap-\\npear that a reasonable way of measuring the possible variation of X would\\nbe to look at how far apart X would be from its mean on the average. One\\npossible way to measure this would be to consider the quantity E[|X −μ|],\\nwhere μ = [X], and |X −μ| represents the absolute value of X −μ. However, it\\nturns out to be mathematically inconvenient to deal with this quantity and so a\\nmore tractable quantity is usually considered — namely, the expectation of the\\nsquare of the difference between X and its mean. We thus have the following\\ndeﬁnition.\\nDeﬁnition. If X is a random variable with mean μ, then the variance of X,\\ndenoted by Var(X), is deﬁned by\\nVar(X) = E[(X −μ)2]\\nAn alternative formula for Var(X) can be derived as follows:\\nVar(X) = E[(X −μ)2]'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 141}, page_content='4.6 Variance 129\\n= E[X2 −2μX + μ2]\\n= E[X2] −E[2μX] + E[μ2]\\n= E[X2] −2μE[X] + μ2\\n= E[X2] −μ2\\nThat is,\\nVar(X) = E[X2] −(E[X])2\\n(4.6.1)\\nor, in words, the variance of X is equal to the expected value of the square of\\nX minus the square of the expected value of X. This is, in practice, often the\\neasiest way to compute Var(X).\\nExample 4.6.a. Compute Var(X) when X represents the outcome when we roll\\na fair die.\\nSolution. Since P{X = i} = 1\\n6,i = 1,2,3,4,5,6, we obtain\\nE[X2] =\\n6\\n\\x05\\ni−1\\ni2P{X = i}\\n= 12 \\x18\\n1\\n6\\n\\x19\\n+ 22 \\x18\\n1\\n6\\n\\x19\\n+ 32 \\x18\\n1\\n6\\n\\x19\\n+ 42 \\x18\\n1\\n6\\n\\x19\\n+ 52 \\x18\\n1\\n6\\n\\x19\\n+ 62 \\x18\\n1\\n6\\n\\x19\\n= 91\\n6\\nHence, since it was shown in Example 4.4.a that E[X] = 7\\n2, we obtain from\\nEquation (4.6.1) that\\nVar(X) = E[X2] −(E[X])2\\n= 91\\n6 −\\n\\x16 7\\n2\\n\\x172 = 35\\n12\\n■\\nExample 4.6.b (Variance of an Indicator Random Variable.). If, for some\\nevent A,\\nI =\\n\\x06\\n1\\nif event A occurs\\n0\\nif event A does not occur\\nthen\\nVar(I) = E[I 2] −(E[I])2\\n= E[I] −(E[I])2\\nsince I 2 = I (as 12 = 1 and 02 = 0)\\n= E[I](1 −E[I])\\n= P(A)[1 −P(A)]\\nsince E[I] = P(A) from Example 4.4.b\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 142}, page_content='130 CHAPTER 4: Random variables and expectation\\nFIGURE 4.5\\nA friendship graph.\\nBecause (X −μ)2 ≥0, it follows that Var(X) = E[(X −μ)2] ≥0, implying from\\n(4.6.1) that\\nE[X2] ≥μ2.\\nThat is, it is always the case that the expected value of the square of a random\\nvariable is at least as large as the square of its expected value.\\nExample 4.6.c. The friendship paradox is often expressed as saying that on aver-\\nage your friends have more friends than you do. More formally, suppose that\\nthere are n people in a certain population, labeled 1,2,...,n, and that certain\\npairs of these individuals are friends. This friendship network can be graphically\\nrepresented by having a circle for each person and then having a line between\\ncircles to indicate that those people are friends. For instance, Figure 4.5 indi-\\ncates that there are 4 people in the community and that persons 1 and 2 are\\nfriends, persons 1 and 3 are friends, persons 1 and 4 are friends, and persons 2\\nand 4 are friends.\\nLet f (i) denote the number of friends of person i and let f = \\x1en\\ni=1 f (i). (Thus,\\nfor the network of Figure 4.5, f (1) = 3, f (2) = 2, f (3) = 1, f (4) = 2 and f =\\n8.) Now, let X be a randomly chosen individual, equally likely to be any of\\n1,2,...,n. That is,\\nP(X = i) = 1/n, i = 1,...,n.\\nIt then follows from Proposition 4.5.1 that E[f (X)], the expected number of\\nfriends of X, is\\nE[f (X)] =\\nn\\n\\x05\\ni=1\\nf (i)P(X = i) =\\nn\\n\\x05\\ni=1\\nf (i)/n = f/n.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 143}, page_content='4.6 Variance 131\\nNow suppose that each of the n individuals writes the names of all their friends,\\nwith each name written on a separate sheet of paper. Thus, an individual with\\nk friends will use k separate sheets. Because person i has f (i) friends, there will\\nbe f = \\x1en\\ni=1 f (i) separate pieces of paper, with each sheet containing one of\\nthe n names. Now choose one of these sheets at random and let Y denote the\\nname on that sheet. Let us compute E[f (Y)], the expected number of friends of\\nthe person whose name is on the chosen sheet. To do so, ﬁrst note that because\\nperson i has f (i) friends, it follows that i is the name on f (i) of the sheets, and\\nthus i is the name on the chosen sheet with probability f (i)\\nf . That is,\\nP(Y = i) = f (i)\\nf\\n,\\ni = 1,...,n.\\nConsequently,\\nE[f (Y)] =\\nn\\n\\x05\\ni=1\\nf (i)P(Y = i)\\n=\\n\\x1en\\ni=1 f 2(i)\\nf\\n.\\nUsing P(X = i) = 1/n and E[f (X)] = f/n we obtain, upon multiplying nu-\\nmerator and denominator by 1/n, that\\nE[f (Y)] =\\n\\x1en\\ni=1 f 2(i)P(X = i)\\nE[f (X)]\\n= E[f 2(X)]\\nE[f (X)]\\n≥E[f (X)]\\nwhere the inequality follows because the expected value of the square of any\\nrandom variable is always at least as large as the square of its expectation. Thus,\\nE[f (X)] ≤E[f (Y)], which says that the average number of friends that a ran-\\ndomly chosen individual has is less than or equal to the average number of\\nfriends of a randomly chosen member of a randomly chosen friendship pair.\\nRemark\\nThe intuitive reason for the friendship paradox is that X is equally likely to\\nbe any of the n individuals. On the other hand Y is chosen with a probabil-\\nity proportional to its number of friends; consequently, the more friends an\\nindividual has, the more likely that individual will be Y. Thus, Y is biased to-\\nwards individuals with a large number of friends, and so it is not surprising\\nthat the average number of friends that Y has is larger than the average number\\nof friends that X has.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 144}, page_content='132 CHAPTER 4: Random variables and expectation\\nA useful identity concerning variances is that for any constants a and b,\\nVar(aX + b) = a2Var(X)\\n(4.6.2)\\nTo prove Equation (4.6.2), let μ = E[X] and recall that E[aX + b] = aμ + b.\\nThus, by the deﬁnition of variance, we have\\nVar(aX + b) = E[(aX + b −E[aX + b])2]\\n= E[(aX + b −aμ −b)2]\\n= E[(aX −aμ)2]\\n= E[a2(X −μ)2]\\n= a2 E[(X −μ)2]\\n= a2 Var(X)\\nSpecifying particular values for a and b in Equation (4.6.2) leads to some inter-\\nesting corollaries. For instance, by setting a = 0 in Equation (4.6.2) we obtain\\nthat\\nVar(b) = 0\\nThat is, the variance of a constant is 0. (Is this intuitive?) Similarly, by setting\\na = 1 we obtain\\nVar(X + b) = Var(X)\\nThat is, the variance of a constant plus a random variable is equal to the vari-\\nance of the random variable. (Is this intuitive? Think about it.) Finally, setting\\nb = 0 yields\\nVar(aX) = a2Var(X)\\nThe quantity √Var(X) is called the standard deviation of X. The standard devia-\\ntion has the same units as does the mean.\\nRemark\\nAnalogous to the mean’s being the center of gravity of a distribution of mass,\\nthe variance represents, in the terminology of mechanics, the moment of iner-\\ntia.\\n4.7\\nCovariance and variance of sums of\\nrandom variables\\nWe showed in Section 4.5 that the expectation of a sum of random variables is\\nequal to the sum of their expectations. The corresponding result for variances'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 145}, page_content='4.7 Covariance and variance of sums of random variables\\n133\\nis, however, not generally valid. Consider\\nVar(X + X) = Var(2X)\\n= 22 Var(X)\\n= 4 Var(X)\\n̸= Var(X) + Var(X)\\nThere is, however, an important case in which the variance of a sum of random\\nvariables is equal to the sum of the variances; and this is when the random vari-\\nables are independent. Before proving this, however, let us deﬁne the concept\\nof the covariance of two random variables.\\nDeﬁnition. The covariance of two random variables X and Y, written Cov(X,Y),\\nis deﬁned by\\nCov(X,Y) = E[(X −μx)(Y −μy)]\\nwhere μx and μy are the means of X and Y, respectively.\\nA useful expression for Cov(X,Y) can be obtained by expanding the right side\\nof the deﬁnition. This yields\\nCov(X,Y) = E[XY −μxY −μyX + μxμy]\\n= E[XY] −μxE[Y] −μyE[X] + μxμy\\n= E[XY] −μxμy −μyμx + μxμy\\n= E[XY] −E[X]E[Y]\\n(4.7.1)\\nFrom its deﬁnition we see that covariance satisﬁes the following properties:\\nCov(X,Y) = Cov(Y,X)\\n(4.7.2)\\nand\\nCov(X,X) = Var(X)\\n(4.7.3)\\nAnother property of covariance, which immediately follows from its deﬁnition,\\nis that, for any constant a,\\nCov(aX,Y) = a Cov(X,Y)\\n(4.7.4)\\nThe proof of Equation (4.7.4) is left as an exercise.\\nCovariance, like expectation, possesses an additive property.\\nLemma 4.7.1.\\nCov(X1 + X2,Y) = Cov(X1,Y) + Cov(X2,Y)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 146}, page_content='134 CHAPTER 4: Random variables and expectation\\nProof.\\nCov(X1 + X2,Y)\\n= E[(X1 + X2)Y] −E[X1 + X2]E[Y]\\nfrom Equation 4.7.1\\n= E[X1Y] + E[X2Y] −(E[X1] + E[X2])E[Y]\\n= E[X1Y] −E[X1]E[Y] + E[X2Y] −E[X2]E[Y]\\n= Cov(X1,Y) + Cov(X2,Y)\\n■\\nLemma 4.7.1 can be easily generalized (see Problem 48) to show that\\nCov\\n\\x02 n\\n\\x05\\ni=1\\nXi,Y\\n\\x04\\n=\\nn\\n\\x05\\ni=1\\nCov(Xi,Y)\\n(4.7.5)\\nwhich gives rise to the following.\\nProposition 4.7.2.\\nCov\\n⎛\\n⎝\\nn\\n\\x05\\ni=1\\nXi,\\nm\\n\\x05\\nj=1\\nYj\\n⎞\\n⎠=\\nn\\n\\x05\\ni=1\\nm\\n\\x05\\nj=1\\nCov(Xi,Yj)\\nProof.\\nCov\\n⎛\\n⎝\\nn\\n\\x05\\ni=1\\nXi,\\nm\\n\\x05\\nj=1\\nYj\\n⎞\\n⎠\\n=\\nn\\n\\x05\\ni=1\\nCov\\n⎛\\n⎝Xi,\\nm\\n\\x05\\nj=1\\nYj\\n⎞\\n⎠\\nfrom Equation (4.7.5)\\n=\\nn\\n\\x05\\ni=1\\nCov\\n⎛\\n⎝\\nm\\n\\x05\\nj=1\\nYj,Xi\\n⎞\\n⎠\\nby the symmetry property Equation (4.7.2)\\n=\\nn\\n\\x05\\ni=1\\nm\\n\\x05\\nj=1\\nCov(Yj,Xi)\\nagain from Equation (4.7.5)\\nand the result now follows by again applying the symmetry property Equation\\n(4.7.2).\\n■\\nUsing Equation (4.7.3) gives rise to the following formula for the variance of a\\nsum of random variables.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 147}, page_content='4.7 Covariance and variance of sums of random variables\\n135\\nCorollary 4.7.3.\\nVar\\n\\x02 n\\n\\x05\\ni=1\\nXi\\n\\x04\\n=\\nn\\n\\x05\\ni=1\\nVar(Xi) +\\nn\\n\\x05\\ni=1\\nn\\n\\x05\\nj=1\\nj̸=i\\nCov(Xi,Xj)\\nProof. Because Cov(X,X) = Var(X), for any random variable X, we obtain from\\nProposition 4.7.2 that\\nVar\\n\\x02 n\\n\\x05\\ni=1\\nXi\\n\\x04\\n=\\nCov\\n⎛\\n⎝\\nn\\n\\x05\\ni=1\\nXi,\\nn\\n\\x05\\nj=1\\nXj\\n⎞\\n⎠\\n=\\nn\\n\\x05\\ni=1\\nn\\n\\x05\\nj=1\\nCov(Xi, Xj)\\n=\\nn\\n\\x05\\ni=1\\n⎡\\n⎣\\x05\\nj̸=i\\nCov(Xi, Xj) + Cov(Xi, Xi)\\n⎤\\n⎦\\n=\\nn\\n\\x05\\ni=1\\n\\x05\\nj̸=i\\nCov(Xi, Xj) +\\nn\\n\\x05\\ni=1\\nCov(Xi, Xi)\\n=\\nn\\n\\x05\\ni=1\\n\\x05\\nj̸=i\\nCov(Xi, Xj) +\\nn\\n\\x05\\ni=1\\nVar(Xi)\\n■\\nIn the case of n = 2, Corollary 4.7.3 yields that\\nVar(X + Y) = Var(X) + Var(Y) + Cov(X,Y) + Cov(Y,X)\\nor, using Equation (4.7.2),\\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)\\n(4.7.6)\\nTheorem 4.7.4. If X and Y are independent random variables, then\\nCov(X,Y) = 0\\nand so for independent X1,...,Xn,\\nVar\\n\\x02 n\\n\\x05\\ni=1\\nXi\\n\\x04\\n=\\nn\\n\\x05\\ni=1\\nVar(Xi)\\nProof. We need to prove that E[XY] = E[X]E[Y]. Now, in the discrete case,\\nE[XY] =\\n\\x05\\nj\\n\\x05\\ni\\nxiyjP{X = xi,Y = yj}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 148}, page_content='136 CHAPTER 4: Random variables and expectation\\n=\\n\\x05\\nj\\n\\x05\\ni\\nxiyjP{X = xi}P{Y = yj}\\nby independence\\n=\\n\\x05\\ny\\nyjP{Y = yj}\\n\\x05\\ni\\nxiP{X = xi}\\n= E[Y]E[X]\\nBecause a similar argument holds in all other cases, the result is proven.\\n■\\nExample 4.7.a. Compute the variance of the sum obtained when 10 indepen-\\ndent rolls of a fair die are made.\\nSolution. Letting Xi denote the outcome of the ith roll, we have that\\nVar\\n\\x02 10\\n\\x05\\n1\\nXi\\n\\x04\\n=\\n10\\n\\x05\\n1\\nVar(Xi)\\n= 10 35\\n12\\nfrom Example 4.6.a\\n= 175\\n6\\n■\\nExample 4.7.b. Compute the variance of the number of heads resulting from\\n10 independent tosses of a fair coin.\\nSolution. Letting\\nIj =\\n\\x06\\n1\\nif the jth toss lands heads\\n0\\nif the jth toss lands tails\\nthen the total number of heads is equal to\\n10\\n\\x05\\nj=1\\nIj\\nHence, from Theorem 4.7.4,\\nVar\\n⎛\\n⎝\\n10\\n\\x05\\nj=1\\nIj\\n⎞\\n⎠=\\n10\\n\\x05\\nj=1\\nVar(Ij)\\nNow, since Ij is an indicator random variable for an event having probability\\n1\\n2, it follows from Example 4.6.b that\\nVar(Ij) = 1\\n2\\n \\n1 −1\\n2\\n!\\n= 1\\n4'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 149}, page_content='4.7 Covariance and variance of sums of random variables\\n137\\nand thus\\nVar\\n⎛\\n⎝\\n10\\n\\x05\\nj=1\\nIj\\n⎞\\n⎠= 10\\n4\\n■\\nThe covariance of two random variables is important as an indicator of the\\nrelationship between them. For instance, consider the situation where X and Y\\nare indicator variables for whether or not the events A and B occur. That is, for\\nevents A and B, deﬁne\\nX =\\n\\x06\\n1\\nif A occurs\\n0\\notherwise ,\\nY =\\n\\x06\\n1\\nif B occurs\\n0\\notherwise\\nand note that\\nXY =\\n\\x06\\n1\\nif X = 1,Y = 1\\n0\\notherwise\\nThus,\\nCov(X,Y) = E[XY] −E[X]E[Y]\\n= P{X = 1,Y = 1} −P{X = 1}P{Y = 1}\\nFrom this we see that\\nCov(X,Y) > 0 ⇔P{X = 1,Y = 1} > P{X = 1}P{Y = 1}\\n⇔P{X = 1,Y = 1}\\nP{X = 1}\\n> P{Y = 1}\\n⇔P{Y = 1|X = 1} > P{Y = 1}\\nthat Y = 1; whereas the covariance of X and Y is negative if the outcome X = 1\\nmakes it less likely that Y = 1, and so makes it more likely that Y = 0. (By\\nthe symmetry of the covariance, the preceding remains true when X and Y are\\ninterchanged.)\\nIn general, it can be shown that a positive value of Cov(X,Y) is an indica-\\ntion that Y tends to increase as X does, whereas a negative value indicates that\\nY tends to decrease as X increases. The strength of the relationship between\\nX and Y is indicated by the correlation between X and Y, a dimensionless\\nquantity obtained by dividing the covariance by the product of the standard\\ndeviations of X and Y. That is,\\nCorr(X,Y) =\\nCov(X,Y)\\n√Var(X)Var(Y)\\nIt can be shown (see Problem 49) that this quantity always has a value between\\n−1 and +1.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 150}, page_content='138 CHAPTER 4: Random variables and expectation\\n4.8\\nMoment generating functions\\nThe moment generating function φ(t) of the random variable X is deﬁned for\\nall values t by\\nφ(t) = E[et X] =\\n⎧\\n⎪⎪⎪⎨\\n⎪⎪⎪⎩\\n\\x05\\nx\\net xp(x)\\nif X is discrete\\n\\x0b ∞\\n−∞\\net xf (x)dx\\nif X is continuous\\nWe call φ(t) the moment generating function because all of the moments of X\\ncan be obtained by successively differentiating φ(t). For example,\\nφ′(t) = d\\ndt E[et X]\\n= E\\n\\x0f d\\ndt (et X)\\n\\x10\\n= E[Xet X]\\nHence,\\nφ′(0) = E[X]\\nSimilarly,\\nφ′′(t) = d\\ndt φ′(t)\\n= d\\ndt E[Xet X]\\n= E\\n\\x0f d\\ndt (Xet X)\\n\\x10\\n= E[X2et X]\\nand so\\nφ′′(0) = E[X2]\\nIn general, the nth derivative of φ(t) evaluated at t = 0 equals E[Xn]; that is,\\nφn(0) = E[Xn],\\nn ≥1\\nAn important property of moment generating functions is that the moment gen-\\nerating function of the sum of independent random variables is just the product of the\\nindividual moment generating functions. To see this, suppose that X and Y are'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 151}, page_content='4.9 Chebyshev’s inequality and the weak law of large numbers\\n139\\nindependent and have moment generating functions φX(t) and φY (t), respec-\\ntively. Then φX+Y (t), the moment generating function of X + Y, is given by\\nφX+Y (t) = E[et(X+Y)]\\n= E[etXetY ]\\n= E[etX]E[etY ]\\n= φX(t)φY (t)\\nwhere the next to the last equality follows from Theorem 4.7.4 since X and Y,\\nand thus etX and etY , are independent.\\nAnother important result is that the moment generating function uniquely deter-\\nmines the distribution. That is, there exists a one-to-one correspondence between\\nthe moment generating function and the distribution function of a random\\nvariable.\\n4.9\\nChebyshev’s inequality and the weak law of large\\nnumbers\\nWe start this section by proving a result known as Markov’s inequality.\\nProposition 4.9.1 (Markov’s inequality). If X is a random variable that takes\\nonly nonnegative values, then for any value a > 0\\nP{X ≥a} ≤E[X]\\na\\nProof. We give a proof for the case where X is continuous with density f .\\nE[X] =\\n\\x0b ∞\\n0\\nx f (x)dx\\n=\\n\\x0b a\\n0\\nx f (x)dx +\\n\\x0b ∞\\na\\nxf (x)dx\\n≥\\n\\x0b ∞\\na\\nx f (x)dx\\n≥\\n\\x0b ∞\\na\\na f (x)dx\\n= a\\n\\x0b ∞\\na\\nf (x)dx\\n= aP{X ≥a}\\nand the result is proved.\\n■\\nAs a corollary, we obtain Proposition 4.9.2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 152}, page_content='140 CHAPTER 4: Random variables and expectation\\nProposition 4.9.2 (Chebyshev’s inequality). If X is a random variable with\\nmean μ and variance σ 2, then for any value k > 0\\nP{|X −μ| ≥k} ≤σ 2\\nk2\\nProof. Since (X −μ)2 is a nonnegative random variable, we can apply Markov’s\\ninequality (with a = k2) to obtain\\nP{(X −μ)2 ≥k2} ≤E[(X −μ)2]\\nk2\\n(4.9.1)\\nBut since (X −μ) ≥k2 if and only if |X −μ| ≥k, Equation (4.9.1) is equivalent\\nto\\nP{|X −μ| ≥k} ≤E[(X −μ)2]\\nk2\\n= σ 2\\nk2\\nand the proof is complete.\\n■\\nThe importance of Markov’s and Chebyshev’s inequalities is that they enable\\nus to derive bounds on probabilities when only the mean, or both the mean\\nand the variance, of the probability distribution are known. Of course, if the\\nactual distribution were known, then the desired probabilities could be exactly\\ncomputed and we would not need to resort to bounds.\\nExample 4.9.a. Suppose that it is known that the number of items produced\\nin a factory during a week is a random variable with mean 50.\\n(a) What can be said about the probability that this week’s production will\\nexceed 75?\\n(b) If the variance of a week’s production is known to equal 25, then what\\ncan be said about the probability that this week’s production will be be-\\ntween 40 and 60?\\nSolution. Let X be the number of items that will be produced in a week:\\n(a) By Markov’s inequality\\nP{X > 75} ≤E[X]\\n75\\n= 50\\n75 = 2\\n3\\n(b) By Chebyshev’s inequality\\nP{|X −50| ≥10} ≤σ 2\\n102 = 1\\n4'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 153}, page_content='4.9 Chebyshev’s inequality and the weak law of large numbers\\n141\\nHence\\nP{|X −50| < 10} ≥1 −1\\n4 = 3\\n4\\nand so the probability that this week’s production will be between 40 and 60\\nis at least .75.\\n■\\nBy replacing k by kσ in Equation (4.9.1), we can write Chebyshev’s inequality as\\nP{|X −μ| > kσ} ≤1/k2\\nThus it states that the probability a random variable differs from its mean by\\nmore than k standard deviations is bounded by 1/k2.\\nWe will end this section by using Chebyshev’s inequality to prove the weak\\nlaw of large numbers, which states that the probability that the average of the\\nﬁrst n terms in a sequence of independent and identically distributed random\\nvariables differs by its mean by more than ε goes to 0 as n goes to inﬁnity.\\nTheorem 4.9.3 (The weak law of large numbers). Let X1,X2, ..., be a se-\\nquence of independent and identically distributed random variables, each hav-\\ning mean E[Xi] = μ. Then, for any ε > 0,\\nP\\n\\x1f\\x11\\x11\\x11X1 + ··· + Xn\\nn\\n−μ\\n\\x11\\x11\\x11 > ε\\n&\\n→0\\nas n →∞\\nProof. We shall prove the result only under the additional assumption that the\\nrandom variables have a ﬁnite variance σ 2. Now, as\\nE\\n\\x0fX1 + ··· + Xn\\nn\\n\\x10\\n= μ\\nand\\nVar\\n X1 + ··· + Xn\\nn\\n!\\n= σ 2\\nn\\nit follows from Chebyshev’s inequality that\\nP\\n\\x1f\\x11\\x11\\x11X1 + ··· + Xn\\nn\\n−μ\\n\\x11\\x11\\x11 > ϵ\\n&\\n≤σ 2\\nnϵ2\\nand the result is proved.\\n■\\nFor an application of the above, suppose that a sequence of independent trials\\nis performed. Let E be a ﬁxed event and denote by P(E) the probability that E\\noccurs on a given trial. Letting\\nXi =\\n\\x06\\n1\\nif E occurs on trial i\\n0\\nif E does not occur on trial i\\nit follows that X1 + X2 + ··· + Xn represents the number of times that E occurs\\nin the ﬁrst n trials. Because E[Xi] = P(E), it thus follows from the weak law'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 154}, page_content='142 CHAPTER 4: Random variables and expectation\\nof large numbers that for any positive number ε, no matter how small, the\\nprobability that the proportion of the ﬁrst n trials in which E occurs differs\\nfrom P(E) by more than ε goes to 0 as n increases.\\nProblems\\n1. Five men and 5 women are ranked according to their scores on an exami-\\nnation. Assume that no two scores are alike and all 10! possible rankings\\nare equally likely. Let X denote the highest ranking achieved by a woman\\n(for instance, X = 2 if the top-ranked person was male and the next-\\nranked person was female). Find P{X = i},i = 1,2,3,...,8,9,10.\\n2. Let X represent the difference between the number of heads and the\\nnumber of tails obtained when a coin is tossed n times. What are the\\npossible values of X?\\n3. In Problem 2, if the coin is assumed fair, for n = 3, what are the proba-\\nbilities associated with the values that X can take on?\\n4. The distribution function of the random variable X is given\\nF(x) =\\n⎧\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\\n0\\nx < 0\\nx\\n2\\n0 ≤x < 1\\n2\\n3\\n1 ≤x < 2\\n11\\n12\\n2 ≤x < 3\\n1\\n3 ≤x\\na.\\nPlot this distribution function.\\nb.\\nWhat is P{X > 1\\n2}?\\nc.\\nWhat is P{2 < X ≤4}?\\nd.\\nWhat is P{X < 3}?\\ne.\\nWhat is P{X = 1}?\\n5. Suppose the random variable X has probability density function\\nf (x) =\\n\\x06\\nc x3,\\nif 0 ≤x ≤1\\n0,\\notherwise\\na.\\nFind the value of c.\\nb.\\nFind P{.4 < X < .8}.\\n6. The amount of time, in hours, that a computer functions before breaking\\ndown is a continuous random variable with probability density function\\ngiven by\\nf (x) =\\n\\x06\\nλe−x/100\\nx ≥0\\n0\\nx < 0'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 155}, page_content='Problems 143\\nWhat is the probability that a computer will function between 50 and 150\\nhours before breaking down? What is the probability that it will function\\nless than 100 hours?\\n7. The lifetime in hours of a certain kind of radio tube is a random variable\\nhaving a probability density function given by\\nf (x) =\\n⎧\\n⎨\\n⎩\\n0\\nx ≤100\\n100\\nx2\\nx > 100\\nWhat is the probability that exactly 2 of 5 such tubes in a radio set will\\nhave to be replaced within the ﬁrst 150 hours of operation? Assume that\\nthe events Ei,i = 1,2,3,4,5, that the ith such tube will have to be re-\\nplaced within this time are independent.\\n8. If the density function of X equals\\nf (x) =\\n\\x06\\nc e−2x\\n0 < x < ∞\\n0\\nx < 0\\nﬁnd c. What is P{X > 2}?\\n9. A set of ﬁve transistors are to be tested, one at a time in a random order,\\nto see which of them are defective. Suppose that three of the ﬁve tran-\\nsistors are defective, and let N1 denote the number of tests made until\\nthe ﬁrst defective is spotted, and let N2 denote the number of additional\\ntests until the second defective is spotted. Find the joint probability mass\\nfunction of N1 and N2.\\n10. The joint probability density function of X and Y is given by\\nf (x,y) = 6\\n7\\n\\x18\\nx2 + xy\\n2\\n\\x19\\n,\\n0 < x < 1,\\n0 < y < 2\\na.\\nVerify that this is indeed a joint density function.\\nb.\\nCompute the density function of X.\\nc.\\nFind P{X > Y}.\\n11. Let X1,X2,...,Xn be independent random variables, each having a uni-\\nform distribution over (0, 1). Let M = maximum (X1,X2,...,Xn). Show\\nthat the distribution function of M is given by\\nFM(x) = xn,\\n0 ≤x ≤1\\nWhat is the probability density function of M?\\n12. The joint density of X and Y is given by\\nf (x,y) =\\n\\x06\\nx e−(x+y)\\nx > 0, y > 0\\n0\\notherwise'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 156}, page_content='144 CHAPTER 4: Random variables and expectation\\na.\\nCompute the density of X.\\nb.\\nCompute the density of Y.\\nc.\\nAre X and Y independent?\\n13. The joint density of X and Y is\\nf (x, y) =\\n\\x06\\n2\\n0 < x < y, 0 < y < 1\\n0\\notherwise\\na.\\nCompute the density of X.\\nb.\\nCompute the density of Y.\\nc.\\nAre X and Y independent?\\n14. If the joint density function of X and Y factors into one part depending\\nonly on x and one depending only on y, show that X and Y are indepen-\\ndent. That is, if\\nf (x,y) = k(x)h(y),\\n−∞< x < ∞,\\n−∞< y < ∞\\nshow that X and Y are independent.\\n15. Is Problem 14 consistent with the results of Problems 12 and 13?\\n16. Suppose that X and Y are independent continuous random variables.\\nShow that\\na.\\nP{X + Y ≤a} =\\n\\x0b ∞\\n−∞\\nFX(a −y)fY (y)dy\\nb.\\nP{X ≤Y} =\\n\\x0b ∞\\n−∞\\nFX(y)fY (y)dy\\nwhere fY is the density function of Y, and FX is the distribution\\nfunction of X.\\n17. When a current I (measured in amperes) ﬂows through a resistance R\\n(measured in ohms), the power generated (measured in watts) is given\\nby W = I 2R. Suppose that I and R are independent random variables\\nwith densities\\nfI(x) = 6x(1 −x)\\n0 ≤x ≤1\\nfR(x) = 2x\\n0 ≤x ≤1\\nDetermine the density function of W.\\n18. In Example 4.3.b, determine the conditional probability mass function\\nof the size of a randomly chosen family containing 2 girls.\\n19. Compute the conditional density function of X given Y = y in (a) Prob-\\nlem 10 and (b) Problem 13.\\n20. Show that X and Y are independent if and only if\\na.\\npX|Y (x|y) = pX(x)\\nin the discrete case\\nb.\\nfX|Y (x|y) = fX(x)\\nin the continuous case\\n21. Compute the expected value of the random variable in Problem 1.\\n22. Compute the expected value of the random variable in Problem 3.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 157}, page_content='Problems 145\\n23. Each night different meteorologists give us the “probability” that it will\\nrain the next day. To judge how well these people predict, we will score\\neach of them as follows: If a meteorologist says that it will rain with prob-\\nability p, then he or she will receive a score of\\n1 −(1 −p)2\\nif it does rain\\n1 −p2\\nif it does not rain\\nWe will then keep track of scores over a certain time span and conclude\\nthat the meteorologist with the highest average score is the best predictor\\nof weather. Suppose now that a given meteorologist is aware of this and\\nso wants to maximize his or her expected score. If this individual truly\\nbelieves that it will rain tomorrow with probability p∗, what value of p\\nshould he or she assert so as to maximize the expected score?\\n24. An insurance company writes a policy to the effect that an amount of\\nmoney A must be paid if some event E occurs within a year. If the com-\\npany estimates that E will occur within a year with probability p, what\\nshould it charge the customer so that its expected proﬁt will be 10 percent\\nof A?\\n25. A total of 4 buses carrying 148 students from the same school arrive at\\na football stadium. The buses carry, respectively, 40, 33, 25, and 50 stu-\\ndents. One of the students is randomly selected. Let X denote the number\\nof students that were on the bus carrying this randomly selected student.\\nOne of the 4 bus drivers is also randomly selected. Let Y denote the num-\\nber of students on her bus.\\na.\\nWhich of E[X] or E[Y] do you think is larger? Why?\\nb.\\nCompute E[X] and E[Y].\\n26. Suppose that two teams play a series of games that end when one of them\\nhas won i games. Suppose that each game played is, independently, won\\nby team A with probability p. Find the expected number of games that\\nare played when i = 2. Also show that this number is maximized when\\np = 1\\n2.\\n27. The density function of X is given by\\nf (x) =\\n\\x06\\na + b x2\\n0 ≤x ≤1\\n0\\notherwise\\nIf E[X] = 3\\n5, ﬁnd a, b.\\n28. The lifetime in hours of electronic tubes is a random variable having a\\nprobability density function given by\\nf (x) = a2 x e−ax,\\nx ≥0\\nCompute the expected lifetime of such a tube.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 158}, page_content='146 CHAPTER 4: Random variables and expectation\\n29. Let X1, X2,...,Xn be independent random variables having the common\\ndensity function\\nf (x) =\\n\\x06\\n1\\n0 < x < 1\\n0\\notherwise\\nFind (a) E[Max(X1,...,Xn)] and (b) E[Min(X1,...,Xn)].\\n30. Suppose that X has density function\\nf (x) =\\n\\x06\\n1\\n0 < x < 1\\n0\\notherwise\\nCompute E[Xn] (a) by computing the density of Xn and then using the\\ndeﬁnition of expectation and (b) by using Proposition 4.5.1.\\n31. The time it takes to repair a personal computer is a random variable\\nwhose density, in hours, is given by\\nf (x) =\\n\\x06\\n1\\n2\\n0 < x < 2\\n0\\notherwise\\nThe cost of the repair depends on the time it takes and is equal to 40 +\\n30√x when the time is x. Compute the expected cost to repair a personal\\ncomputer.\\n32. If E[X] = 2 and E[X2] = 8, calculate (a) E[(2 + 4X)2] and (b) E[X2 +\\n(X + 1)2].\\n33. Ten balls are randomly chosen from an urn containing 17 white and 23\\nblack balls. Let X denote the number of white balls chosen. Compute\\nE[X]\\na.\\nby deﬁning appropriate indicator variables Xi,i = 1, ..., 10 so that\\nX =\\n10\\n\\x05\\ni=1\\nXi\\nb.\\nby deﬁning appropriate indicator variables Yi = 1,..., 17 so that\\nX =\\n17\\n\\x05\\ni=1\\nYi\\n34. If X is a continuous random variable having distribution function F,\\nthen its median is deﬁned as that value of m for which\\nF(m) = 1/2\\nFind the median of the random variables with density function'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 159}, page_content='Problems 147\\na.\\nf (x) = e−x,\\nx ≥0;\\nb.\\nf (x) = 1,\\n0 ≤x ≤1.\\n35. The median, like the mean, is important in predicting the value of a\\nrandom variable. Whereas it was shown in the text that the mean of a\\nrandom variable is the best predictor from the point of view of mini-\\nmizing the expected value of the square of the error, the median is the\\nbest predictor if one wants to minimize the expected value of the abso-\\nlute error. That is, E[|X −c|] is minimized when c is the median of the\\ndistribution function of X. Prove this result when X is continuous with\\ndistribution function F and density function f . Hint: Write\\nE[|X −c|] =\\n\\x0b ∞\\n−∞\\n|x −c|f (x)dx\\n=\\n\\x0b c\\n−∞\\n|x −c|f (x)dx +\\n\\x0b ∞\\nc\\n|x −c|f (x)dx\\n=\\n\\x0b c\\n−∞\\n(c −x)f (x)dx +\\n\\x0b ∞\\nc\\n(x −c)f (x)dx\\n= c F(c) −\\n\\x0b c\\n−∞\\nx f (x)dx +\\n\\x0b ∞\\nc\\nx f (x)dx −c[1 −F(c)]\\nNow, use calculus to ﬁnd the minimizing value of c.\\n36. We say that mp is the 100p percentile of the distribution function F if\\nF(mp) = p\\nFind mp for the distribution having density function\\nf (x) = 2e−2x,\\nx ≥0\\n37. A community consists of 100 married couples. If 50 members of the com-\\nmunity die, what is the expected number of marriages that remain intact?\\nAssume that the set of people who die is equally likely to be any of the\\n\\x18\\n200\\n50\\n\\x19\\ngroups of size 50. Hint: For i = 1, ... ,100 let\\nXi =\\n\\x06\\n1\\nif neither member of couple i dies\\n0\\notherwise\\n38. Compute the expectation and variance of the number of successes in n\\nindependent trials, each of which results in a success with probability p.\\nIs independence necessary?\\n39. Suppose that X is equally likely to take on any of the values 1, 2, 3, 4.\\nCompute (a) E[X] and (b) Var(X).\\n40. Let pi = P{X = i} and suppose that p1 + p2 + p3 = 1. If E[X] = 2, what\\nvalues of p1,p2,p3 (a) maximize and (b) minimize Var(X)?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 160}, page_content='148 CHAPTER 4: Random variables and expectation\\n41. Compute the mean and variance of the number of heads that appear in\\n3 ﬂips of a fair coin.\\n42. This problem refers to Example 4.6.c concerning the friendship paradox.\\nLet W be a randomly chosen friend of the randomly chosen individual\\nX. That is, W is equally likely to be any of the friends of X. It can be\\nshown that E[f (W)] ≥E[f (X)]. That is, the average number of friends\\nof a randomly chosen friend of a randomly chosen person is at least the\\naverage number of friends of the randomly chosen person. Verify that\\nthis is true for the friendship community given in Figure 4.5.\\n43. A random variable X, which represents the weight (in ounces) of an arti-\\ncle, has density function,\\nf (z) =\\n⎧\\n⎪⎨\\n⎪⎩\\nz −8\\nfor 8 ≤z ≤9\\n10 −z\\nfor 9 < z ≤10\\n0\\notherwise\\na.\\nCalculate the mean and variance of the random variable X.\\nb.\\nThe manufacturer sells the article for a ﬁxed price of $2.00. He guar-\\nantees to refund the purchase money to any customer who ﬁnds the\\nweight of his article to be less than 8.25 oz. His cost of production\\nis related to the weight of the article by the relation x/15 + .35. Find\\nthe expected proﬁt per article.\\n44. Let Xi denote the percentage of votes cast in a given election that are for\\ncandidate i, and suppose that X1 and X2 have a joint density function\\nfX1,X2(x,y) =\\n\\x1f 3(x + y),\\nif x ≥0,y ≥0,0 ≤x + y ≤1\\n0,\\nif otherwise\\na.\\nFind the marginal densities of X1 and X2.\\nb.\\nFind E[Xi] and Var(Xi) for i = 1,2.\\n45. A product is classiﬁed according to the number of defects it contains and\\nthe factory that produces it. Let X1 and X2 be the random variables that\\nrepresent the number of defects per unit (taking on possible values of 0,\\n1, 2, or 3) and the factory number (taking on possible values 1 or 2),\\nrespectively. The entries in the table represent the joint possibility mass\\nfunction of a randomly chosen product.\\nX1\\nX2\\n1\\n2\\n0\\n1\\n8\\n1\\n16\\n1\\n1\\n16\\n1\\n16\\n2\\n3\\n16\\n1\\n8\\n3\\n1\\n8\\n1\\n4'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 161}, page_content='Problems\\n149\\na.\\nFind the marginal probability distributions of X1 and X2.\\nb.\\nFind E[X1],E[X2], Var(X1), Var(X2), and Cov(X1,X2).\\n46. Find Corr(X1,X2) for the random variables of Problem 44.\\n47. Verify Equation (4.7.4).\\n48. Prove Equation (4.7.5) by using mathematical induction.\\n49. Let X have variance σ 2\\nx and let Y have variance σ 2\\ny . Starting with\\n0 ≤Var(X/σx + Y/σy)\\nshow that\\n−1 ≤Corr(X,Y)\\nNow using that\\n0 ≤Var(X/σx −Y/σy)\\nconclude that\\n−1 ≤Corr(X,Y) ≤1\\nUsing the result that Var(Z) = 0 implies that Z is constant, argue that, if\\nCorr(X,Y) = 1 or −1, then X and Y are related by\\nY = a + bx\\nwhere the sign of b is positive when the correlation is 1 and negative\\nwhen it is −1.\\n50. Consider n independent trials, each of which results in any of the out-\\ncomes i,i = 1,2,3, with respective probabilities p1,p2,p3,\\x1e3\\ni=1 pi = 1.\\nLet Ni denote the number of trials that result in outcome i, and show\\nthat Cov(N1,N2) = −np1p2. Also explain why it is intuitive that this co-\\nvariance is negative. (Hint: For i = 1,...,n, let\\nXi =\\n\\x06\\n1\\nif trial i results in outcome 1\\n0\\nif trial i does not result in outcome 1\\nSimilarly, for j = 1,...,n, let\\nYj =\\n\\x06\\n1\\nif trial j results in outcome 2\\n0\\nif trial j does not result in outcome 2\\nArgue that\\nN1 =\\nn\\n\\x05\\ni=1\\nXi,\\nN2 =\\nn\\n\\x05\\nj=1\\nYj\\nThen use Proposition 4.7.2 and Theorem 4.7.4.)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 162}, page_content='150 CHAPTER 4: Random variables and expectation\\n51. In Example 4.5.f, compute Cov(Xi,Xj) and use this result to show that\\nVar(X) = 1.\\n52. If X1 and X2 have the same probability distribution function, show that\\nCov(X1 −X2, X1 + X2) = 0\\nNote that independence is not being assumed.\\n53. Suppose that X has density function\\nf (x) = e−x,\\nx > 0\\nCompute the moment generating function of X and use your result to\\ndetermine its mean and variance. Check your answer for the mean by a\\ndirect calculation.\\n54. If the density function of X is\\nf (x) = 1,\\n0 < x < 1\\ndetermine E[etX]. Differentiate to obtain E[Xn] and then check your an-\\nswer.\\n55. Suppose that X is a random variable with mean and variance both equal\\nto 20. What can be said about P{0 ≤X ≤40}?\\n56. From past experience, a professor knows that the test score of a student\\ntaking her ﬁnal examination is a random variable with mean 75.\\na.\\nGive an upper bound to the probability that a student’s test score\\nwill exceed 85.\\nSuppose in addition the professor knows that the variance of a stu-\\ndent’s test score is equal to 25.\\nb.\\nWhat can be said about the probability that a student will score\\nbetween 65 and 85?\\nc.\\nHow many students would have to take the examination so as to\\nensure, with probability at least .9, that the class average would be\\nwithin 5 of 75?\\n57. Let X and Y have respective distribution functions FX and FY , and sup-\\npose that for some constants a and b > 0,\\nFX(x) = FY\\n x −a\\nb\\n!\\na.\\nDetermine E[X] in terms of E[Y].\\nb.\\nDetermine Var(X) in terms of Var(Y).\\nHint: X has the same distribution as what other random variable?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 163}, page_content='CHAPTER 5\\nSpecial random variables\\nCertain types of random variables occur over and over again in applications. In\\nthis chapter, we will study a variety of them.\\n5.1\\nThe Bernoulli and binomial random variables\\nSuppose that a trial, or an experiment, whose outcome can be classiﬁed as ei-\\nther a “success” or as a “failure” is performed. If we let X = 1 when the outcome\\nis a success and X = 0 when it is a failure, then the probability mass function\\nof X is given by\\nP{X = 0} = 1 −p\\n(5.1.1)\\nP{X = 1} = p\\nwhere p,0 ≤p ≤1, is the probability that the trial is a “success.”\\nA random variable X is said to be a Bernoulli random variable (after the Swiss\\nmathematician James Bernoulli) if its probability mass function is given by\\nEquations (5.1.1) for some p ∈(0,1). Its expected value is\\nE[X] = 1 · P{X = 1} + 0 · P{X = 0} = p\\nThat is, the expectation of a Bernoulli random variable is the probability that\\nthe random variable equals 1.\\nSuppose now that n independent trials, each of which results in a “success”\\nwith probability p and in a “failure” with probability 1 −p, are to be per-\\nformed. If X represents the number of successes that occur in the n trials, then\\nX is said to be a binomial random variable with parameters (n, p).\\nThe probability mass function of a binomial random variable with parameters\\nn and p is given by\\nP{X = i} =\\n\\x02n\\ni\\n\\x03\\npi(1 −p)n−i,\\ni = 0,1,...,n\\n(5.1.2)\\nwhere\\n\\x04 n\\ni\\n\\x05\\n=n!/[i!(n −i)!] is the number of different groups of i objects that\\ncan be chosen from a set of n objects. The validity of Equation (5.1.2) may\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00014-4\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n151'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 164}, page_content='152 CHAPTER 5: Special random variables\\nbe veriﬁed by ﬁrst noting that the probability of any particular sequence of\\nthe n outcomes containing i successes and n −i failures is, by the assumed\\nindependence of trials, pi(1 −p)n−i. Equation (5.1.2) then follows since there\\nare\\n\\x04 n\\ni\\n\\x05\\ndifferent sequences of the n outcomes leading to i successes and n −i\\nfailures — which can perhaps most easily be seen by noting that there are\\n\\x04 n\\ni\\n\\x05\\ndifferent selections of the i trials that result in successes. For instance, if n = 5,\\ni = 2, then there are\\n\\x04 5\\n2\\n\\x05\\nchoices of the two trials that are to result in successes —\\nnamely, any of the outcomes\\n(s,s,f,f,f )\\n(f,s,s,f,f )\\n(f,f,s,f,s)\\n(s,f,s,f,f )\\n(f,s,f,s,f )\\n(s,f,f,s,f )\\n(f,s,f,f,s)\\n(f,f,f,s,s)\\n(s,f,f,f,s)\\n(f,f,s,s,f )\\nwhere the outcome (f,s,f,s,f ) means, for instance, that the two successes\\nappeared on trials 2 and 4. Since each of the\\n\\x04 5\\n2\\n\\x05\\noutcomes has probability\\np2(1−p)3, we see that the probability of a total of 2 successes in 5 independent\\ntrials is\\n\\x04 5\\n2\\n\\x05\\np2(1 −p)3. As a check, note that, by the binomial theorem, the\\nprobabilities sum to 1; that is,\\n∞\\n\\x06\\ni=0\\np(i) =\\nn\\n\\x06\\ni=0\\n\\x07 n\\ni\\n\\x08\\npi(1 −p)n−i = [p + (1 −p)]n = 1\\nThe probability mass function of three binomial random variables with respec-\\ntive parameters (10, .5), (10, .3), and (10, .6) are presented in Figure 5.1. The\\nﬁrst of these is symmetric about the value .5, whereas the second is somewhat\\nweighted, or skewed, to lower values and the third to higher values.\\nExample 5.1.a. It is known that disks produced by a certain company will be\\ndefective with probability .01 independently of each other. The company sells\\nthe disks in packages of 10 and offers a money-back guarantee that at most 1 of\\nthe 10 disks is defective. What proportion of packages is returned? If someone\\nbuys three packages, what is the probability that exactly one of them will be\\nreturned?\\nSolution. If X is the number of defective disks in a package, then assuming\\nthat customers always take advantage of the guarantee, it follows that X is a\\nbinomial random variable with parameters (10, .01). Hence the probability\\nthat a package will have to be replaced is\\nP{X > 1} = 1 −P{X = 0} −P{X = 1}\\n= 1 −\\n\\x07 10\\n0\\n\\x08\\n(.01)0(.99)10 −\\n\\x07 10\\n1\\n\\x08\\n(.01)1(.99)9 ≈.005'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 165}, page_content='5.1 The Bernoulli and binomial random variables\\n153\\nFIGURE 5.1\\nBinomial probability mass functions.\\nBecause each package will, independently, have to be replaced with probability\\n.005, it follows from the law of large numbers that in the long run .5 percent\\nof the packages will have to be replaced.\\nIt follows from the foregoing that the number of packages that will be returned\\nby a buyer of three packages is a binomial random variable with parameters\\nn = 3 and p = .005. Therefore, the probability that exactly one of the three\\npackages will be returned is\\n\\x07\\n3\\n1\\n\\x08\\n(.005)(.995)2 = .015.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 166}, page_content='154 CHAPTER 5: Special random variables\\nExample 5.1.b. The color of one’s eyes is determined by a single pair of genes,\\nwith the gene for brown eyes being dominant over the one for blue eyes. This\\nmeans that an individual having two blue-eyed genes will have blue eyes, while\\none having either two brown-eyed genes or one brown-eyed and one blue-\\neyed gene will have brown eyes. When two people mate, the resulting offspring\\nreceives one randomly chosen gene from each of its parents’ gene pair. If the el-\\ndest child of a pair of brown-eyed parents has blue eyes, what is the probability\\nthat exactly two of the four other children (none of whom is a twin) of this\\ncouple also have blue eyes?\\nSolution. To begin, note that since the eldest child has blue eyes, it follows\\nthat both parents must have one blue-eyed and one brown-eyed gene. (For if\\neither had two brown-eyed genes, then each child would receive at least one\\nbrown-eyed gene and would thus have brown eyes.) The probability that an\\noffspring of this couple will have blue eyes is equal to the probability that it\\nreceives the blue-eyed gene from both parents, which is\\n\\x04 1\\n2\\n\\x05\\x04 1\\n2\\n\\x05\\n= 1\\n4. Hence, be-\\ncause each of the other four children will have blue eyes with probability 1\\n4, it\\nfollows that the probability that exactly two of them have this eye color is\\n\\x024\\n2\\n\\x03\\n(1/4)2(3/4)2 = 27/128\\n■\\nExample 5.1.c. A communications system consists of n components, each of\\nwhich will, independently, function with probability p. The total system will\\nbe able to operate effectively if at least one-half of its components function.\\n(a) For what values of p is a 5-component system more likely to operate ef-\\nfectively than a 3-component system?\\n(b) In general, when is a 2k + 1 component system better than a 2k −1 com-\\nponent system?\\nSolution.\\n(a) Because the number of functioning components is a binomial\\nrandom variable with parameters (n, p), it follows that the probability\\nthat a 5-component system will be effective is\\n\\x025\\n3\\n\\x03\\np3(1 −p)2 +\\n\\x025\\n4\\n\\x03\\np4(1 −p) + p5\\nwhereas the corresponding probability for a 3-component system is\\n\\x023\\n2\\n\\x03\\np2(1 −p) + p3\\nHence, the 5-component system is better if\\n10p3(1 −p)2 + 5p4(1 −p) + p5 ≥3p2(1 −p) + p3'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 167}, page_content='5.1 The Bernoulli and binomial random variables\\n155\\nwhich reduces to\\n3(p −1)2(2p −1) ≥0\\nor\\np ≥1\\n2\\n(b) In general, a system with 2k + 1 components will be better than one with\\n2k −1 components if (and only if) p ≥1\\n2. To prove this, consider a system\\nof 2k +1 components and let X denote the number of the ﬁrst 2k −1 that\\nfunction. Then\\nP2k+1(effective)\\n= P{X ≥k + 1} + P{X = k}(1 −(1 −p)2) + P{X = k −1}p2\\nwhich follows since the 2k+1 component system will be effective if either\\n1.\\nX ≥k + 1;\\n2.\\nX = k and at least one of the remaining 2 components function; or\\n3.\\nX = k −1 and both of the next 2 function.\\nBecause\\nP2k−1(effective) = P{X ≥k}\\n= P{X = k} + P{X ≥k + 1}\\nwe obtain that\\nP2k+1(effective) −P2k−1(effective)\\n= P{X = k −1}p2 −(1 −p)2P{X = k}\\n=\\n\\x02\\n2k−1\\nk−1\\n\\x03\\npk−1(1 −p)kp2 −(1 −p)2 \\x07 2k−1\\nk\\n\\x08\\npk(1 −p)k−1\\n=\\n\\x07 2k−1\\nk\\n\\x08\\npk(1 −p)k[p −(1 −p)]\\nsince\\n\\x02\\n2k−1\\nk−1\\n\\x03\\n=\\n\\x07 2k−1\\nk\\n\\x08\\n≥0 ⇔p ≥1\\n2\\n■\\nExample 5.1.d. Suppose that 10 percent of the chips produced by a computer\\nhardware manufacturer are defective. If we order 100 such chips, will X, the\\nnumber of defective ones we receive, be a binomial random variable?\\nSolution. The random variable X will be a binomial random variable with pa-\\nrameters (100, .1) if each chip has probability .9 of being functional and if the\\nfunctioning of successive chips is independent. Whether this is a reasonable\\nassumption when we know that 10 percent of the chips produced are defective'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 168}, page_content='156 CHAPTER 5: Special random variables\\ndepends on additional factors. For instance, suppose that all the chips pro-\\nduced on a given day are always either functional or defective (with 90 percent\\nof the days resulting in functional chips). In this case, if we know that all of our\\n100 chips were manufactured on the same day, then X will not be a binomial\\nrandom variable. This is so since the independence of successive chips is not\\nvalid. In fact, in this case, we would have\\nP{X = 100} = .1\\nP{X = 0} = .9\\n■\\nSince a binomial random variable X, with parameters n and p, represents the\\nnumber of successes in n independent trials, each having success probability p,\\nwe can represent X as follows:\\nX =\\nn\\n\\x06\\ni=1\\nXi\\n(5.1.3)\\nwhere\\nXi =\\n\\t\\n1\\nif the ith trial is a success\\n0\\notherwise\\nBecause the Xi,i = 1,...,n are independent Bernoulli random variables, we\\nhave that\\nE[Xi] = P{Xi = 1} = p\\nVar(Xi) = E[X2\\ni ] −p2\\n= p(1 −p)\\nwhere the last equality follows since X2\\ni = Xi, and so E[X2\\ni ] = E[Xi] = p.\\nUsing the representation Equation (5.1.3), it is now an easy matter to compute\\nthe mean and variance of X:\\nE[X] =\\nn\\n\\x06\\ni=1\\nE[Xi]\\n= np\\nVar(X) =\\nn\\n\\x06\\ni=1\\nVar(Xi)\\nsince the Xi are independent\\n= np(1 −p)\\nIf X1 and X2 are independent binomial random variables having respec-\\ntive parameters (ni,p), i = 1,2, then their sum is binomial with parameters'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 169}, page_content='5.1 The Bernoulli and binomial random variables\\n157\\n(n1 + n2,p). This can most easily be seen by noting that because Xi,i = 1,2,\\nrepresents the number of successes in ni independent trials each of which is\\na success with probability p, then X1 + X2 represents the number of successes\\nin n1 + n2 independent trials each of which is a success with probability p.\\nTherefore, X1 + X2 is binomial with parameters (n1 + n2, p).\\n5.1.1\\nUsing R to calculate binomial probabilities\\nIf you want the probability that a binomial random variable with parameters n\\nand p is equal to i, just write\\n> dbinom(i,n,p)\\nhit return and the value appears. If you want the probability that such a random\\nvariable is less than or equal to i, write\\n> pbinom(i,n,p)\\nhit return and the value appears.\\nIn other words, if X is a binomial random variable with parameters n and p,\\nthen\\ndbinom(i,n,p)\\nreturns\\nP(X = i)\\npbinom(i,n,p)\\nreturns\\nP(X ≤i)\\nExample 5.1.e. If X is a binomial random variable with parameters n =\\n100,p = .75, ﬁnd P(X = 70) and P(X ≥80).\\nSolution. We use R to obtain P(X = 70) as follows:\\n>\\ndbinom(70,100,.75)\\n>\\n[1] 0.04575381\\nUsing P(X ≥80) = 1 −P(X ≤79), do the following:\\n>\\n1 −pbinom(79,100,.75)\\n>\\n[1] 0.1488311\\nThus, P(X = 70) = 0.04575381, and P(X ≥80) = 0.1488311.\\n■\\nWe can also use R to plot binomial probabilities. Suppose that X is a bino-\\nmial random variable with parameters n = 10,p = .4, and that we want to plot\\nP(X = i), i = 1,...,10. To do so, type\\n>\\ni = seq(0,10,1)\\n>\\np = dbinom(i,10,.4)\\n>\\nplot (i,p)\\nThe ﬁrst command means that i is to go from 0 to 10 in increments of size 1.\\nThe second gives, for each i = 0,...,10, the probability that X = i, and the third'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 170}, page_content='158 CHAPTER 5: Special random variables\\ncommand tells R to plot (i,p). Hitting return after the last command yields the\\noutput.\\n5.2\\nThe Poisson random variable\\nA random variable X, taking on one of the values 0, 1, 2,..., is said to be a Pois-\\nson random variable with parameter λ,λ > 0, if its probability mass function\\nis given by\\nP{X = i} = e−λ λi\\ni! ,\\ni = 0,1,...\\n(5.2.1)\\nThe symbol e stands for a constant approximately equal to 2.7183. It is a fa-\\nmous constant in mathematics, named after the Swiss mathematician L. Euler,\\nand it is also the base of the so-called natural logarithm.\\nEquation (5.2.1) deﬁnes a probability mass function, since\\n∞\\n\\x06\\ni=0\\np(i) = e−λ\\n∞\\n\\x06\\ni=0\\nλi/i! = e−λeλ = 1\\nA graph of this mass function when λ = 4 is given in Figure 5.2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 171}, page_content='5.2 The Poisson random variable 159\\nFIGURE 5.2\\nThe Poisson probability mass function with λ = 4.\\nThe Poisson probability distribution was introduced by S. D. Poisson in a book\\nhe wrote dealing with the application of probability theory to lawsuits, crim-\\ninal trials, and the like. This book, published in 1837, was entitled Recherches\\nsur la probabilité des jugements en matière criminelle et en matière civile.\\nAs a prelude to determining the mean and variance of a Poisson random vari-\\nable, let us ﬁrst determine its moment generating function.\\nφ(t) = E[etX]\\n=\\n∞\\n\\x06\\ni=0\\netie−λλi/i!\\n= e−λ\\n∞\\n\\x06\\ni=0\\n(λet)i/i!\\n= e−λeλet\\n= exp{λ(et −1)}\\nDifferentiation yields\\nφ′(t) = λet exp{λ(et −1)}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 172}, page_content='160 CHAPTER 5: Special random variables\\nφ′′(t) = (λet)2 exp{λ(et −1)} + λet exp{λ(et −1)}\\nEvaluating at t = 0 gives that\\nE[X] = φ′(0) = λ\\nVar(X) = φ′′(0) −(E[X])2\\n= λ2 + λ −λ2 = λ\\nThus both the mean and the variance of a Poisson random variable are equal\\nto the parameter λ.\\nThe Poisson random variable has a wide range of applications in a variety of\\nareas because it may be used as an approximation for a binomial random vari-\\nable with parameters (n, p) when n is large and p is small. To see this, suppose\\nthat X is a binomial random variable with parameters (n, p) and let λ = np.\\nThen\\nP{X = i} =\\nn!\\n(n −i)!i!pi(1 −p)n−i\\n=\\nn!\\n(n −i)!i!\\n\\x02λ\\nn\\n\\x03i \\x02\\n1 −λ\\nn\\n\\x03n−i\\n= n(n −1)...(n −i + 1)\\nni\\nλi\\ni!\\n(1 −λ/n)n\\n(1 −λ/n)i\\nNow, for n large and p small,\\n\\x02\\n1 −λ\\nn\\n\\x03n\\n≈e−λ\\nn(n −1)...(n −i + 1)\\nni\\n≈1\\n\\x02\\n1 −λ\\nn\\n\\x03i\\n≈1\\nHence, for n large and p small,\\nP{X = i} ≈e−λ λi\\ni!\\nIn other words, if n independent trials, each of which results in a “success” with\\nprobability p, are performed, then when n is large and p small, the number\\nof successes occurring is approximately a Poisson random variable with mean\\nλ = np.\\nSome examples of random variables that usually obey, to a good approxima-\\ntion, the Poisson probability law (that is, they usually obey Equation (5.2.1)\\nfor some value of λ) are:\\n1. The number of misprints on a page (or a group of pages) of a book.\\n2. The number of people in a community living to 100 years of age.\\n3. The number of wrong telephone numbers that are dialed in a day.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 173}, page_content='5.2 The Poisson random variable 161\\n4. The number of transistors that fail on their ﬁrst day of use.\\n5. The number of customers entering a post ofﬁce on a given day.\\n6. The number of α-particles discharged in a ﬁxed period of time from some\\nradioactive particle.\\nEach of the foregoing, and numerous other random variables, is approximately\\nPoisson for the same reason — namely, because of the Poisson approximation\\nto the binomial. For instance, we can suppose that there is a small probabil-\\nity p that each letter typed on a page will be misprinted, and so the number\\nof misprints on a given page will be approximately Poisson with mean λ=np\\nwhere n is the (presumably) large number of letters on that page. Similarly,\\nwe can suppose that each person in a given community, independently, has a\\nsmall probability p of reaching the age 100, and so the number of people that\\ndo will have approximately a Poisson distribution with mean np where n is the\\nlarge number of people in the community. We leave it for the reader to reason\\nout why the remaining random variables in examples 3 through 6 should have\\napproximately a Poisson distribution.\\nExample 5.2.a. Suppose that the average number of accidents occurring weekly\\non a particular stretch of a highway equals 3. Calculate the probability that\\nthere is at least one accident this week.\\nSolution. Let X denote the number of accidents occurring on the stretch of\\nhighway in question during this week. Because it is reasonable to suppose that\\nthere are a large number of cars passing along that stretch, each having a small\\nprobability of being involved in an accident, the number of such accidents\\nshould be approximately Poisson distributed. Hence,\\nP{X ≥1} = 1 −P{X = 0}\\n= 1 −e−3 30\\n0!\\n= 1 −e−3\\n≈.9502\\n■\\nExample 5.2.b. Suppose the probability that an item produced by a certain\\nmachine will be defective is .1. Find the probability that a sample of 10 items\\nwill contain at most one defective item. Assume that the quality of successive\\nitems is independent.\\nSolution. The desired probability is\\n\\x0410\\n0\\n\\x05\\n(.1)0(.9)10 +\\n\\x0410\\n1\\n\\x05\\n(.1)1(.9)9 = .7361,\\nwhereas the Poisson approximation yields the value\\ne−1 10\\n0! + e−1 11\\n1! = 2e−1 ≈.7358\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 174}, page_content='162 CHAPTER 5: Special random variables\\nExample 5.2.c. Consider an experiment that consists of counting the number\\nof α particles given off in a 1-second interval by 1 gram of radioactive material.\\nIf we know from past experience that, on the average, 3.2 such α-particles are\\ngiven off, what is a good approximation to the probability that no more than\\n2 α-particles will appear?\\nSolution. If we think of the gram of radioactive material as consisting of a large\\nnumber n of atoms each of which has probability 3.2/n of disintegrating and\\nsending off an α-particle during the second considered, then we see that, to a\\nvery close approximation, the number of α-particles given off will be a Poisson\\nrandom variable with parameter λ = 3.2. Hence the desired probability is\\nP{X ≤2} = e−3.2 + 3.2e−3.2 + (3.2)2\\n2\\ne−3.2\\n= .382\\n■\\nExample 5.2.d. If the average number of claims handled daily by an insurance\\ncompany is 5, what proportion of days have less than 3 claims? What is the\\nprobability that there will be 4 claims in exactly 3 of the next 5 days? Assume\\nthat the number of claims on different days is independent.\\nSolution. Because the company probably insures a large number of clients,\\neach having a small probability of making a claim on any given day, it is rea-\\nsonable to suppose that the number of claims handled daily, call it X, is a\\nPoisson random variable. Since E(X) = 5, the probability that there will be\\nfewer than 3 claims on any given day is\\nP{X ≤3} = P{X = 0} + P{X = 1} + P{X = 2}\\n= e−5 + e−5 51\\n1! + e−5 52\\n2!\\n= 37\\n2 e−5\\n≈.1247\\nSince any given day will have fewer than 3 claims with probability .125, it\\nfollows, from the law of large numbers, that over the long run 12.5 percent of\\ndays will have fewer than 3 claims.\\nIt follows from the assumed independence of the number of claims over suc-\\ncessive days that the number of days in a 5-day span that have exactly 4 claims\\nis a binomial random variable with parameters 5 and P{X = 4}. Because\\nP{X = 4} = e−5 54\\n4! ≈.1755'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 175}, page_content='5.2 The Poisson random variable 163\\nit follows that the probability that 3 of the next 5 days will have 4 claims is\\nequal to\\n\\x025\\n3\\n\\x03\\n(.1755)3(.8245)2 ≈.0367\\n■\\nThe Poisson approximation result can be shown to be valid under even more\\ngeneral conditions than those so far mentioned. For instance, suppose that n\\nindependent trials are to be performed, with the ith trial resulting in a success\\nwith probability pi, i = 1,...,n. Then it can be shown that if n is large and each\\npi is small, then the number of successful trials is approximately Poisson dis-\\ntributed with mean equal to \\nn\\ni=1 pi. In fact, this result will sometimes remain\\ntrue even when the trials are not independent, provided that their dependence\\nis “weak.” For instance, consider the following example.\\nExample 5.2.e. At a party n people put their hats in the center of a room,\\nwhere the hats are mixed together. Each person then randomly chooses a hat.\\nIf X denotes the number of people who select their own hat, then, for large n,\\nit can be shown that X has approximately a Poisson distribution with mean 1.\\nTo see why this might be true, let\\nXi =\\n\\t\\n1\\nif the ith person selects his or her own hat\\n0\\notherwise\\nThen we can express X as\\nX = X1 + ··· + Xn\\nand so X can be regarded as representing the number of “successes” in n “trials”\\nwhere trial i is said to be a success if the ith person chooses her own hat. Now,\\nsince the ith person is equally likely to end up with any of the n hats, one of\\nwhich is her own, it follows that\\nP{Xi = 1} = 1\\nn\\n(5.2.2)\\nSuppose now that i ̸=j and consider the conditional probability that the ith\\nperson chooses her own hat given that the jth person does — that is, consider\\nP{Xi = 1|Xj = 1}. Now given that the jth person indeed selects her own hat,\\nit follows that the ith individual is equally likely to end up with any of the\\nremaining n −1, one of which is her own. Hence, it follows that\\nP{Xi = 1|Xj = 1} =\\n1\\nn −1\\n(5.2.3)\\nThus, we see from Equations (5.2.2) and (5.2.3) that whereas the trials are not\\nindependent, their dependence is rather weak [since, if the above conditional'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 176}, page_content='164 CHAPTER 5: Special random variables\\nprobability were equal to 1/n rather than 1/(n −1), then trials i and j would\\nbe independent]; and thus it is not at all surprising that X has approximately a\\nPoisson distribution. The fact that E[X] = 1 follows since\\nE[X] = E[X1 + ··· + Xn]\\n= E[X1] + ··· + E[Xn]\\n= n\\n\\x021\\nn\\n\\x03\\n= 1\\nThe last equality follows since, from Equation (5.2.2),\\nE[Xi] = P{Xi = 1} = 1\\nn\\n■\\nThe Poisson distribution possesses the reproductive property that the sum of\\nindependent Poisson random variables is also a Poisson random variable. To\\nsee this, suppose that X1 and X2 are independent Poisson random variables\\nhaving respective means λ1 and λ2. Then the moment generating function of\\nX1 + X2 is as follows:\\nE[et(X1+X2)] = E[etX1etX2]\\n= E[etX1]E[etX2]\\nby independence\\n= exp{λ1(et −1)} exp{λ2(et −1)}\\n= exp{(λ1 + λ2)(et −1)}\\nBecause exp{(λ1 + λ2)(et −1)} is the moment generating function of a Poisson\\nrandom variable having mean λ1 + λ2, we may conclude, from the fact that the\\nmoment generating function uniquely speciﬁes the distribution, that X1 + X2\\nis Poisson with mean λ1 + λ2.\\nExample 5.2.f. It has been established that the number of defective stereos\\nproduced daily at a certain plant is Poisson distributed with mean 4. Over a\\n2-day span, what is the probability that the number of defective stereos does\\nnot exceed 3?\\nSolution. Assuming that X1, the number of defectives produced during the\\nﬁrst day, is independent of X2, the number produced during the second day,\\nthen X1 + X2 is Poisson with mean 8. Hence,\\nP{X1 + X2 ≤3} =\\n3\\n\\x06\\ni=0\\ne−8 8i\\ni! = .04238\\n■\\nConsider now a situation in which a random number, call it N, of events will\\noccur, and suppose that each of these events will independently be a type 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 177}, page_content='5.2 The Poisson random variable 165\\nevent with probability p or a type 2 event with probability 1 −p. Let N1 and\\nN2 denote, respectively, the numbers of type 1 and type 2 events that occur.\\n(So N = N1 + N2.) If N is Poisson distributed with mean λ, then the joint\\nprobability mass function of N1 and N2 is obtained as follows.\\nP{N1 = n,N2 = m} = P{N1 = n,N2 = m,N = n + m}\\n= P{N1 = n,N2 = m|N = n + m}P{N = n + m}\\n= P{N1 = n,N2 = m|N = n + m}e−λ\\nλn+m\\n(n + m)!\\nNow, given a total of n + m events, because each one of these events is inde-\\npendently type 1 with probability p, it follows that the conditional probability\\nthat there are exactly n type 1 events (and m type 2 events) is the probability\\nthat a binomial (n + m,p) random variable is equal to n. Consequently,\\nP{N1 = n,N2 = m} = (n + m)!\\nn!m!\\npn(1 −p)me−λ\\nλn+m\\n(n + m)!\\n= e−λp (λp)n\\nn!\\ne−λ(1−p) (λ(1 −p))m\\nm!\\n(5.2.4)\\nThe probability mass function of N1 is thus\\nP{N1 = n} =\\n∞\\n\\x06\\nm=0\\nP{N1 = n,N2 = m}\\n= e−λp (λp)n\\nn!\\n∞\\n\\x06\\nm=0\\ne−λ(1−p) (λ(1 −p))m\\nm!\\n= e−λp (λp)n\\nn!\\n(5.2.5)\\nSimilarly,\\nP{N2 = m} =\\n∞\\n\\x06\\nn=0\\nP{N1 = n,N2 = m} = e−λ(1−p) (λ(1 −p))m\\nm!\\n(5.2.6)\\nIt now follows from Equations (5.2.4), (5.2.5), and (5.2.6), that N1 and N2 are\\nindependent Poisson random variables with respective means λp and λ(1−p).\\nThe preceding result generalizes when each of the Poisson number of events\\ncan be classiﬁed into any of r categories, to yield the following important prop-\\nerty of the Poisson distribution: If each of a Poisson number of events having mean\\nλ is independently classiﬁed as being of one of the types 1,...,r, with respective\\nprobabilities p1,...,pr, \\nr\\ni=1 pi = 1, then the numbers of type 1,...,r events are\\nindependent Poisson random variables with respective means λp1,...,λpr.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 178}, page_content='166 CHAPTER 5: Special random variables\\n5.2.1\\nUsing R to calculate Poisson probabilities\\nR calculates Poisson probabilities similarly to how it computes binomial ones.\\nIf X is Poisson with parameter λ, then\\ndpois(i,λ)\\nreturns\\nP(X = i)\\nppois(i,λ)\\nreturns\\nP(X ≤i)\\nFor instance, we obtain the probability that a Poisson random variable with\\nmean 40 is less than or equal to 50 as follows:\\n>\\nppois(50,40)\\n>\\n[1] 0.947372\\nR can be used to plot Poisson mass functions. Suppose we wanted to plot\\nP(X = i),i = 0,...,25 when X is Poisson with mean 10. We just type the fol-\\nlowing:\\n>\\ni = seq(0,25,1)\\n>\\np = dpois(i,10)\\n>\\nplot (i,p)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 179}, page_content='5.3 The hypergeometric random variable 167\\n5.3\\nThe hypergeometric random variable1\\nA bin contains N + M batteries, of which N are of acceptable quality and the\\nother M are defective. A sample of size n is to be randomly chosen (without\\nreplacements) in the sense that the set of sampled batteries is equally likely\\nto be any of the\\n\\x04N+M\\nn\\n\\x05\\nsubsets of size n. If we let X denote the number of\\nacceptable batteries in the sample, then\\nP{X = i} =\\n\\x04N\\ni\\n\\x05\\x04 M\\nn−i\\n\\x05\\n\\x04N+M\\nn\\n\\x05 ,\\ni = 0,1,...,min(N,n)∗\\n(5.3.1)\\nAny random variable X whose probability mass function is given by Equa-\\ntion (5.3.1) is said to be a hypergeometric random variable with parameters N,\\nM, n.\\nExample 5.3.a. The components of a 6-component system are to be randomly\\nchosen from a bin of 20 used components. The resulting system will be func-\\ntional if at least 4 of its 6 components are in working condition. If 15 of the 20\\ncomponents in the bin are in working condition, what is the probability that\\nthe resulting system will be functional?\\nSolution. If X is the number of working components chosen, then X is hy-\\npergeometric with parameters 15, 5, 6. The probability that the system will be\\nfunctional is\\nP{X ≥4} =\\n6\\n\\x06\\ni = 4\\nP{X = i}\\n=\\n\\x0215\\n4\\n\\x03\\x025\\n2\\n\\x03\\n+\\n\\x0215\\n5\\n\\x03\\x025\\n1\\n\\x03\\n+\\n\\x0215\\n6\\n\\x03\\x025\\n0\\n\\x03\\n\\x0220\\n6\\n\\x03\\n≈.8687\\n■\\nTo compute the mean and variance of a hypergeometric random variable\\nwhose probability mass function is given by Equation (5.3.1), imagine that\\nthe batteries are drawn sequentially and let\\nXi =\\n\\t\\n1\\nif the ith selection is acceptable\\n0\\notherwise\\n1We are following the convention that \\x02 m\\nr\\n\\x03 = 0 if r > m or if r < 0.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 180}, page_content='168 CHAPTER 5: Special random variables\\nNow, since the ith selection is equally likely to be any of the N + M batteries,\\nof which N are acceptable, it follows that\\nP{Xi = 1} =\\nN\\nN + M\\n(5.3.2)\\nAlso, for i ̸= j,\\nP{Xi = 1,Xj = 1} = P {Xi = 1}P{Xj = 1|Xi = 1}\\n=\\nN\\nN + M\\nN −1\\nN + M −1\\n(5.3.3)\\nwhich follows since, given that the ith selection is acceptable, the jth selection\\nis equally likely to be any of the other N + M −1 batteries of which N −1 are\\nacceptable.\\nTo compute the mean and variance of X, the number of acceptable batteries in\\nthe sample of size n, use the representation\\nX =\\nn\\n\\x06\\ni=1\\nXi\\nThis gives\\nE[X] =\\nn\\n\\x06\\ni=1\\nE[Xi] =\\nn\\n\\x06\\ni=1\\nP{Xi = 1} =\\nnN\\nN + M\\n(5.3.4)\\nAlso, Corollary 4.7.3 for the variance of a sum of random variables gives\\nVar(X) =\\nn\\n\\x06\\ni=1\\nVar(Xi) + 2\\n\\x06\\x06\\n1≤i<j≤n\\nCov(Xi,Xj)\\n(5.3.5)\\nNow, Xi is a Bernoulli random variable and so\\nVar(Xi) = P{Xi = 1}(1 −P{Xi = 1}) =\\nN\\nN + M\\nM\\nN + M\\n(5.3.6)\\nAlso, for i < j,\\nCov(Xi,Xj) = E[XiXj] −E[Xi]E[Xj]\\nNow, because both Xi and Xj are Bernoulli (that is, 0 −1) random variables,\\nit follows that XiXj is a Bernoulli random variable, and so\\nE[XiXj] = P{XiXj = 1}\\n= P{Xi = 1,Xj = 1}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 181}, page_content='5.3 The hypergeometric random variable 169\\n=\\nN(N −1)\\n(N + M)(N + M −1)\\nfrom Equation (5.3.3)\\n(5.3.7)\\nSo from Equation (5.3.2) and the foregoing we see that for i ̸= j,\\nCov(Xi,Xj) =\\nN(N −1)\\n(N + M)(N + M −1) −\\n\\x02\\nN\\nN + M\\n\\x032\\n=\\n−NM\\n(N + M)2(N + M −1)\\nHence, since there are\\n\\x04 n\\n2\\n\\x05\\nterms in the second sum on the right side of Equa-\\ntion (5.3.5), we obtain from Equation (5.3.6)\\nVar(X) =\\nnNM\\n(N + M)2 −\\nn(n −1)NM\\n(N + M)2(N + M −1)\\n=\\nnNM\\n(N + M)2\\n\\x02\\n1 −\\nn −1\\nN + M −1\\n\\x03\\n(5.3.8)\\nIf we let p = N/(N + M) denote the proportion of batteries in the bin that are\\nacceptable, we can rewrite Equations (5.3.4) and (5.3.8) as follows.\\nE(X) = np\\nVar(X) = np(1 −p)\\n\\x0b\\n1 −\\nn −1\\nN + M −1\\n\\x0c\\nIt should be noted that, for ﬁxed p, as N + M increases to ∞, Var(X) converges\\nto np(1 −p), which is the variance of a binomial random variable with param-\\neters (n, p). (Why was this to be expected?)\\nExample 5.3.b. An unknown number, say N, of animals inhabit a certain re-\\ngion. To obtain some information about the population size, ecologists often\\nperform the following experiment: They ﬁrst catch a number, say r, of these ani-\\nmals, mark them in some manner, and release them. After allowing the marked\\nanimals time to disperse throughout the region, a new catch of size, say, n is\\nmade. Let X denote the number of marked animals in this second capture. If we\\nassume that the population of animals in the region remained ﬁxed between\\nthe time of the two catches and that each time an animal was caught it was\\nequally likely to be any of the remaining uncaught animals, it follows that X is\\na hypergeometric random variable such that\\nP{X = i} =\\n\\x04 r\\ni\\n\\x05\\x07\\nN−r\\nn−i\\n\\x08\\n\\x07\\nN\\nn\\n\\x08\\n≡Pi(N)\\nSuppose now that X is observed to equal i. That is, the fraction i/n of the an-\\nimals in the second catch were marked. By taking this as an approximation of'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 182}, page_content='170 CHAPTER 5: Special random variables\\nr/N, the proportion of animals in the region that are marked, we obtain the\\nestimate rn/i of the number of animals in the region. For instance, if r =50 an-\\nimals are initially caught, marked, and then released, and a subsequent catch\\nof n=100 animals revealed X =25 of them that were marked, then we would\\nestimate the number of animals in the region to be about 200.\\n■\\nThere is a relationship between binomial random variables and the hypergeo-\\nmetric distribution that will be useful to us in developing a statistical test\\nconcerning two binomial populations.\\nExample 5.3.c. Let X and Y be independent binomial random variables hav-\\ning respective parameters (n, p) and (m, p). The conditional probability mass\\nfunction of X given that X + Y = k is as follows.\\nP{X = i|X + Y = k} = P{X = i,X + Y = k}\\nP{X + Y = k}\\n= P{X = i,Y = k −i}\\nP{X + Y = k}\\n= P{X = i}P{Y = k −i}\\nP{X + Y = k}\\n=\\n\\x02n\\ni\\n\\x03\\npi(1 −p)n−i\\n\\x02 m\\nk −i\\n\\x03\\npk−i(1 −p)m−(k−i)\\n\\x02n + m\\nk\\n\\x03\\npk(1 −p)n+m−k\\n=\\n\\x02n\\ni\\n\\x03\\x02 m\\nk −i\\n\\x03\\n\\x02n + m\\nk\\n\\x03\\nwhere the next-to-last equality used the fact that X + Y is binomial with pa-\\nrameters (n + m, p). Hence, we see that the conditional distribution of X given\\nthe value of X + Y is hypergeometric.\\nIt is worth noting that the preceding is quite intuitive. For suppose that n + m\\nindependent trials, each of which has the same probability of being a success,\\nare performed; let X be the number of successes in the ﬁrst n trials, and let Y be\\nthe number of successes in the ﬁnal m trials. Given a total of k successes in the\\nn+m trials, it is quite intuitive that each subgroup of k trials is equally likely to\\nconsist of those trials that resulted in successes. That is, the k success trials are\\ndistributed as a random selection of k of the n + m trials, and so the number\\nthat are from the ﬁrst n trials is hypergeometric.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 183}, page_content='5.4 The uniform random variable\\n171\\nHypergeometric probabilities can be calculated using R, with dhyper(i,N,M,n)\\nand phyper(i,N,M,n) giving, respectively, the probabilities that a hypergeo-\\nmetric random variable with parameters (N,M,n) will, respectively, be less\\nthan or equal to i. Therefore, the probabilities that 10 balls randomly selected\\nfrom an urn having 8 red and 22 blue balls will contain exactly 4 red balls, and\\nat most 4 red balls are obtained thus:\\n>\\ndhyper(4,8,22,10)\\n>\\n[1] 0.1738362\\n>\\nphyper(4,8,22,10)\\n>\\n[1] 0.943682\\nThus, there is approximately a 17.38 percent chance that the selection will con-\\ntain exactly 4, and a 94.37 percent chance that it will contain at most 4 red\\nballs.\\n5.4\\nThe uniform random variable\\nA random variable X is said to be uniformly distributed over the interval [α,β]\\nif its probability density function is given by\\nf (x) =\\n⎧\\n⎨\\n⎩\\n1\\nβ −α\\nif α ≤x ≤β\\n0\\notherwise\\nA graph of this function is given in Figure 5.3. Note that the foregoing meets\\nthe requirements of being a probability density function since\\n1\\nβ −α\\n\\x10 β\\nα\\ndx = 1\\nThe uniform distribution arises in practice when we suppose a certain random\\nvariable is equally likely to be near any value in the interval [α,β].\\nThe probability that X lies in any subinterval of [α,β] is equal to the length of\\nthat subinterval divided by the length of the interval [α,β]. This follows since\\nwhen [a, b] is a subinterval of [α,β] (see Figure 5.4),\\nP{a < X < b} =\\n1\\nβ −α\\n\\x10 b\\na\\ndx\\n= b −a\\nβ −α\\nExample 5.4.a. If X is uniformly distributed over the interval [0, 10], compute\\nthe probability that (a) 2 < X < 9, (b) 1 < X < 4, (c) X < 5, (d) X > 6.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 184}, page_content='172 CHAPTER 5: Special random variables\\nFIGURE 5.3\\nGraph of f (x) for a uniform [α,β].\\nFIGURE 5.4\\nProbabilities of a uniform random variable.\\nSolution. The respective answers are (a) 7/10, (b) 3/10, (c) 5/10, (d) 4/10.\\n■\\nExample 5.4.b. Buses arrive at a speciﬁed stop at 15-minute intervals starting\\nat 7 A.M. That is, they arrive at 7, 7:15, 7:30, 7:45, and so on. If a passenger\\narrives at the stop at a time that is uniformly distributed between 7 and 7:30,\\nﬁnd the probability that he waits\\n(a) less than 5 minutes for a bus;\\n(b) at least 12 minutes for a bus.\\nSolution. Let X denote the time in minutes past 7 A.M. that the passenger\\narrives at the stop. Since X is a uniform random variable over the interval (0,\\n30), it follows that the passenger will have to wait less than 5 minutes if he\\narrives between 7:10 and 7:15 or between 7:25 and 7:30. Hence, the desired\\nprobability for (a) is\\nP{10 < X < 15} + P{25 < X < 30} = 5\\n30 + 5\\n30 = 1\\n3\\nSimilarly, he would have to wait at least 12 minutes if he arrives between 7 and\\n7:03 or between 7:15 and 7:18, and so the probability for (b) is\\nP{0 < X < 3} + P{15 < X < 18} = 3\\n30 + 3\\n30 = 1\\n5\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 185}, page_content='5.4 The uniform random variable 173\\nThe mean of a uniform [α,β] random variable is\\nE[X] =\\n\\x10 β\\nα\\nx\\nβ −α dx\\n= β2 −α2\\n2(β −α)\\n= (β −α)(β + α)\\n2(β −α)\\nor\\nE[X] = α + β\\n2\\nOr, in other words, the expected value of a uniform [α,β] random variable is\\nequal to the midpoint of the interval [α,β], which is clearly what one would\\nexpect. (Why?)\\nThe variance is computed as follows.\\nE[X2] =\\n1\\nβ −α\\n\\x10 β\\nα\\nx2dx\\n= β3 −α3\\n3(β −α)\\n= β2 + αβ + α2\\n3\\nwhere the ﬁnal equation used that\\nβ3 −α3 = (β2 + αβ + α2)(β −α)\\nHence,\\nVar(X) = β2 + αβ + α2\\n3\\n−\\n\\x02α + β\\n2\\n\\x032\\n= 4(β2 + αβ + α2) −3(α2 + 2αβ + β2)\\n12\\n= α2 + β2 −2αβ\\n12\\n= (β −α)2\\n12\\nExample 5.4.c. The current in a semiconductor diode is often measured by the\\nShockley equation\\nI = I0(eaV −1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 186}, page_content='174 CHAPTER 5: Special random variables\\nwhere V is the voltage across the diode; I0 is the reverse current; a is a constant;\\nand I is the resulting diode current. Find E[I] if a = 5, I0 = 10−6, and V is\\nuniformly distributed over (1, 3).\\nSolution.\\nE[I] = E[I0(eaV −1)]\\n= I0E[eaV −1]\\n= I0(E[eaV ] −1)\\n= 10−6\\n\\x10 3\\n1\\ne5x 1\\n2dx −10−6\\n= 10−7(e15 −e5) −10−6\\n≈.3269\\n■\\nThe value of a uniform (0, 1) random variable is called a random number. Most\\ncomputer systems have a built-in subroutine for generating (to a high level of\\napproximation) sequences of independent random numbers — for instance,\\nTable 5.1 presents a set of independent random numbers. Random numbers\\nare quite useful in probability and statistics because their use enables one to\\nempirically estimate various probabilities and expectations.\\nFor an illustration of the use of random numbers, suppose that a medical cen-\\nter is planning to test a new drug designed to reduce its users’ blood cholesterol\\nlevels. To test its effectiveness, the medical center has recruited 1000 volunteers\\nto be subjects in the test. To take into account the possibility that the subjects’\\nblood cholesterol levels may be affected by factors external to the test (such as\\nchanging weather conditions), it has been decided to split the volunteers into\\n2 groups of size 500 — a treatment group that will be given the drug and a\\ncontrol group that will be given a placebo. Both the volunteers and the admin-\\nistrators of the drug will not be told who is in each group (such a test is called\\na double-blind test). It remains to determine which of the volunteers should be\\nchosen to constitute the treatment group. Clearly, one would want the treat-\\nment group and the control group to be as similar as possible in all respects\\nwith the exception that members in the ﬁrst group are to receive the drug while\\nthose in the other group receive a placebo; then it will be possible to conclude\\nthat any difference in response between the groups is indeed due to the drug.\\nThere is general agreement that the best way to accomplish this is to choose the\\n500 volunteers to be in the treatment group in a completely random fashion.\\nThat is, the choice should be made so that each of the\\n\\x041000\\n500\\n\\x05\\nsubsets of 500\\nvolunteers is equally likely to constitute the control group. How can this be\\naccomplished?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 187}, page_content='5.4 The uniform random variable\\n175\\nTable 5.1 A Random Number Table.\\n.68587\\n.25848\\n.85227\\n.78724\\n.05302\\n.70712\\n.76552\\n.70326\\n.80402\\n.49479\\n.73253\\n.41629\\n.37913\\n.00236\\n.60196\\n.59048\\n.59946\\n.75657\\n.61849\\n.90181\\n.84448\\n.42477\\n.94829\\n.86678\\n.14030\\n.04072\\n.45580\\n.36833\\n.10783\\n.33199\\n.49564\\n.98590\\n.92880\\n.69970\\n.83898\\n.21077\\n.71374\\n.85967\\n.20857\\n.51433\\n.68304\\n.46922\\n.14218\\n.63014\\n.50116\\n.33569\\n.97793\\n.84637\\n.27681\\n.04354\\n.76992\\n.70179\\n.75568\\n.21792\\n.50646\\n.07744\\n.38064\\n.06107\\n.41481\\n.93919\\n.37604\\n.27772\\n.75615\\n.51157\\n.73821\\n.29928\\n.62603\\n.06259\\n.21552\\n.72977\\n.43898\\n.06592\\n.44474\\n.07517\\n.44831\\n.01337\\n.04538\\n.15198\\n.50345\\n.65288\\n.86039\\n.28645\\n.44931\\n.59203\\n.98254\\n.56697\\n.55897\\n.25109\\n.47585\\n.59524\\n.28877\\n.84966\\n.97319\\n.66633\\n.71350\\n.28403\\n.28265\\n.61379\\n.13886\\n.78325\\n.44973\\n.12332\\n.16649\\n.88908\\n.31019\\n.33358\\n.68401\\n.10177\\n.92873\\n.13065\\n.42529\\n.37593\\n.90208\\n.50331\\n.37531\\n.72208\\n.42884\\n.07435\\n.58647\\n.84972\\n.82004\\n.74696\\n.10136\\n.35971\\n.72014\\n.08345\\n.49366\\n.68501\\n.14135\\n.15718\\n.67090\\n.08493\\n.47151\\n.06464\\n.14425\\n.28381\\n.40455\\n.87302\\n.07135\\n.04507\\n.62825\\n.83809\\n.37425\\n.17693\\n.69327\\n.04144\\n.00924\\n.68246\\n.48573\\n.24647\\n.10720\\n.89919\\n.90448\\n.80838\\n.70997\\n.98438\\n.51651\\n.71379\\n.10830\\n.69984\\n.69854\\n.89270\\n.54348\\n.22658\\n.94233\\n.08889\\n.52655\\n.83351\\n.73627\\n.39018\\n.71460\\n.25022\\n.06988\\n.64146\\n.69407\\n.39125\\n.10090\\n.08415\\n.07094\\n.14244\\n.69040\\n.33461\\n.79399\\n.22664\\n.68810\\n.56303\\n.65947\\n.88951\\n.40180\\n.87943\\n.13452\\n.36642\\n.98785\\n.62929\\n.88509\\n.64690\\n.38981\\n.99092\\n.91137\\n.02411\\n.94232\\n.91117\\n.98610\\n.71605\\n.89560\\n.92921\\n.51481\\n.20016\\n.56769\\n.60462\\n.99269\\n.98876\\n.47254\\n.93637\\n.83954\\n.60990\\n.10353\\n.13206\\n.33480\\n.29440\\n.75323\\n.86974\\n.91355\\n.12780\\n.01906\\n.96412\\n.61320\\n.47629\\n.33890\\n.22099\\n.75003\\n.98538\\n.63622\\n.94890\\n.96744\\n.73870\\n.72527\\n.17745\\n.01151\\n.47200\\nExample 5.4.d.2[Choosing a Random Subset] From a set of n elements — num-\\nbered 1,2,...,n — suppose we want to generate a random subset of size k that\\nis to be chosen in such a manner that each of the\\n\\x04n\\nk\\n\\x05\\nsubsets is equally likely\\nto be the subset chosen. How can we do this?\\nTo answer this question, let us work backward and suppose that we have indeed\\nrandomly generated such a subset of size k. Now for each j = 1,...,n, we set\\nIj =\\n\\t\\n1\\nif element j is in the subset\\n0\\notherwise\\nand compute the conditional distribution of Ij given I1,...,Ij−1. To start, note\\nthat the probability that element 1 is in the subset of size k is clearly k/n (which\\ncan be seen either by noting that there is probability 1/n that element 1 would\\nhave been the jth element chosen, j = 1,...,k; or by noting that the proportion\\n2Optional.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 188}, page_content='176 CHAPTER 5: Special random variables\\nof outcomes of the random selection that results in element 1 being chosen is\\n\\x07\\n1\\n1\\n\\x08\\x07\\nn−1\\nk−1\\n\\x08\\n/\\n\\x04 n\\nk\\n\\x05\\n= k/n). Therefore, we have that\\nP{I1 = 1} = k/n\\n(5.4.1)\\nTo compute the conditional probability that element 2 is in the subset given\\nI1, note that if I1 = 1, then aside from element 1 the remaining k −1 members\\nof the subset would have been chosen “at random” from the remaining n −1\\nelements (in the sense that each of the subsets of size k −1 of the numbers\\n2,...,n is equally likely to be the other elements of the subset). Hence, we\\nhave that\\nP{I2 = 1|I1 = 1} = k −1\\nn −1\\n(5.4.2)\\nSimilarly, if element 1 is not in the subgroup, then the k members of the sub-\\ngroup would have been chosen “at random” from the other n −1 elements,\\nand thus\\nP{I2 = 1|I1 = 0} =\\nk\\nn −1\\n(5.4.3)\\nFrom Equations (5.4.2) and (5.4.3), we see that\\nP{I2 = 1|I1} = k −I1\\nn −1\\nIn general, we have that\\nP{Ij = 1|I1,...,Ij−1} =\\nk −\\nj−1\\n\\ni=1\\nIi\\nn −j + 1,\\nj = 2,...,n\\n(5.4.4)\\nThe preceding formula follows since \\nj−1\\ni=1 Ii represents the number of the ﬁrst\\nj −1 elements that are included in the subset, and so given I1,...,Ij−1 there\\nremain k −\\nj−1\\ni=1 Ii elements to be selected from the remaining n −(j −1).\\nSince P{U <a} = a, 0 ≤a ≤1, when U is a uniform (0, 1) random variable,\\nEquations (5.4.1) and (5.4.4) lead to the following method for generating a\\nrandom subset of size k from a set of n elements: Namely, generate a sequence\\nof (at most n) random numbers U1,U2,... and set\\nI1 =\\n⎧\\n⎨\\n⎩\\n1\\nif U1 < k\\nn\\n0\\notherwise\\nI2 =\\n⎧\\n⎨\\n⎩\\n1\\nif U2 < k −I1\\nn −1\\n0\\notherwise'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 189}, page_content='5.4 The uniform random variable\\n177\\n...\\nIj =\\n\\t\\n1\\nif Uj < k−I1−···−Ij−1\\nn−j+1\\n0\\notherwise\\nThis process stops when I1 + ··· + Ij = k and the random subset consists of the\\nk elements whose I-value equals 1. That is, S = {i : Ii = 1} is the subset.\\nFor instance, if k =2, n=5, then the tree diagram of Figure 5.5 illustrates the\\nforegoing technique. The random subset S is given by the ﬁnal position on\\nthe tree. Note that the probability of ending up in any given ﬁnal position is\\nequal to 1/10, which can be seen by multiplying the probabilities of moving\\nthrough the tree to the desired endpoint. For instance, the probability of ending\\nat the point labeled S = {2,4} is P{U1 > .4}P{U2 < .5}P{U3 > 1\\n3}P{U4 > 1\\n2} =\\n(.6)(.5)\\n\\x07\\n2\\n3\\n\\x08\\x07\\n1\\n2\\n\\x08\\n= .1.\\nAs indicated in the tree diagram (see the rightmost branches that result in\\nS = {4,5}), we can stop generating random numbers when the number of re-\\nmaining places in the subset to be chosen is equal to the remaining number of\\nelements. That is, the general procedure would stop whenever either \\nj\\ni=1 Ii =\\nk or \\nj\\ni=1 Ii = k −(n −j). In the latter case, S = {i ≤j : Ii = 1,j + 1,...,n}.\\n■\\nFIGURE 5.5\\nTree diagram.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 190}, page_content='178 CHAPTER 5: Special random variables\\nExample 5.4.e. The random vector X, Y is said to have a uniform distribution\\nover the two-dimensional region R if its joint density function is constant for\\npoints in R, and is 0 for points outside of R. That is, if\\nf (x,y) =\\n\\t\\nc\\nif (x,y) ∈R\\n0\\nif otherwise\\nBecause\\n1 =\\n\\x10\\nR\\nf (x,y)dx dy\\n=\\n\\x10\\nR\\nc dx dy\\n= c × Area of R\\nit follows that\\nc =\\n1\\nArea of R\\nFor any region A ⊂R,\\nP{(X,Y) ∈A} =\\n\\x10 \\x10\\n(x,y) ∈A\\nf (x,y)dx dy\\n=\\n\\x10 \\x10\\n(x,y) ∈A\\nc dx dy\\n= Area of A\\nArea of R\\nSuppose now that X, Y is uniformly distributed over the following rectangular\\nregion R:\\nIts joint density function is\\nf (x,y) =\\n\\t\\nc\\nif 0 ≤x ≤a, 0 ≤y ≤b\\n0\\notherwise'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 191}, page_content='5.5 Normal random variables\\n179\\nFIGURE 5.6\\nThe normal density function (A) with μ = 0, σ = 1 and (B) with arbitrary μ and σ 2.\\nwhere c =\\n1\\nArea of rectangle = 1\\nab. In this case, X and Y are independent uniform\\nrandom variables. To show this, note that for 0 ≤x ≤a, 0 ≤y ≤b\\nP{X ≤x,Y ≤y} = c\\n\\x10 x\\n0\\n\\x10 y\\n0\\ndy dx = xy\\nab\\n(5.4.5)\\nFirst letting y = b, and then letting x = a, in the preceding shows that\\nP{X ≤x} = x\\na ,\\nP{Y ≤y} = y\\nb\\n(5.4.6)\\nThus, from Equations (5.4.5) and (5.4.6) we can conclude that X and Y are\\nindependent, with X being uniform on (0,a) and Y being uniform on (0,b).\\n■\\n5.5\\nNormal random variables\\nA random variable is said to be normally distributed with parameters μ and\\nσ 2, and we write X ∼N(μ,σ 2), if its density is\\nf (x) =\\n1\\n√\\n2πσ\\ne−(x−μ)2/2σ 2,\\n−∞< x < ∞∗\\nThe normal density f (x) is a bell-shaped curve that is symmetric about μ and\\nthat attains its maximum value of\\n1\\n√\\n2π σ ≈0.399/σ at x = μ (see Figure 5.6)3.\\nThe normal distribution was introduced by the French mathematician Abra-\\nham de Moivre in 1733 and was used by him to approximate probabilities\\n3To verify that this is indeed a density function, see Problem 29.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 192}, page_content='180 CHAPTER 5: Special random variables\\nassociated with binomial random variables when the binomial parameter n is\\nlarge. This result was later extended by Laplace and others and is now encom-\\npassed in a probability theorem known as the central limit theorem, which\\ngives a theoretical base to the often noted empirical observation that, in prac-\\ntice, many random phenomena obey, at least approximately, a normal proba-\\nbility distribution. Some examples of this behavior are the height of a person,\\nthe velocity in any direction of a molecule in gas, and the error made in mea-\\nsuring a physical quantity.\\nTo compute E[X] note that\\nE[X −μ] =\\n1\\n√\\n2πσ\\n\\x10 ∞\\n−∞\\n(x −μ)e−(x−μ)2/2σ 2dx\\nLetting y = (x −μ)/σ gives that\\nE[X −μ] =\\nσ\\n√\\n2π\\n\\x10 ∞\\n−∞\\nye−y2/2dy\\nBut\\n\\x10 ∞\\n−∞\\nye−y2/2dy = −e−y2/2|∞\\n−∞= 0\\nshowing that E[X −μ] = 0, or equivalently that\\nE[X] = μ\\nUsing this, we now compute Var(X) as follows:\\nVar(X) = E[(X −μ)2]\\n=\\n1\\n√\\n2πσ\\n\\x10 ∞\\n−∞\\n(x −μ)2e−(x−μ)2/2σ 2dx\\n=\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\nσ 2y2e−y2/2dy\\n(5.5.1)\\nWith u = y and dv = ye−y2/2, the integration by parts formula\\n\\x10\\nudv = uv −\\n\\x10\\nv du\\nyields that\\n\\x10 ∞\\n−∞\\ny2e−y2/2dy = −ye−y2/2\\x11\\x11∞\\n−∞+\\n\\x10 ∞\\n−∞\\ne−y2/2dy\\n=\\n\\x10 ∞\\n−∞\\ne−y2/2dy'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 193}, page_content='5.5 Normal random variables\\n181\\nHence, from (5.5.1)\\nVar(X) = σ 2\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\ne−y2/2dy\\n= σ 2\\nwhere the preceding used that\\n1\\n√\\n2π e−y2/2dy is the density function of a normal\\nrandom variable with parameters μ = 0 and σ = 1, so its integral must equal 1.\\nThus μ and σ 2 represent, respectively, the mean and variance of the normal\\ndistribution.\\nA very important property of normal random variables is that if X is normal\\nwith mean μ and variance σ 2, then for any constants a and b,b ̸= 0, the random\\nvariable Y = a + bX is also a normal random variable with parameters\\nE[Y] = E[a + bX] = a + bE[X] = a + bμ\\nand variance\\nVar(Y) = Var(a + bX) = b2Var(X) = b2σ 2\\nTo verify this, let FY (y) be the distribution function of Y. Then, for b > 0\\nFY (y) = P(Y ≤y)\\n= P(a + bX ≤y)\\n= P\\n\\x02\\nX ≤y −a\\nb\\n\\x03\\n= FX\\n\\x02y −a\\nb\\n\\x03\\nwhere FX is the distribution function of X. Similarly, if b < 0, then\\nFY (y) = P(a + bX ≤y)\\n= P\\n\\x02\\nX ≥y −a\\nb\\n\\x03\\n= 1 −FX\\n\\x02y −a\\nb\\n\\x03\\nDifferentiation yields that the density function of Y is\\nfY (y) =\\n\\t\\n1\\nb fX\\n\\x04 y−a\\nb\\n\\x05\\n,\\nif\\nb > 0\\n−1\\nb fX\\n\\x04 y−a\\nb\\n\\x05\\n,\\nif\\nb < 0'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 194}, page_content='182 CHAPTER 5: Special random variables\\nwhich can be written as\\nfY (y) = 1\\n|b|fX\\n\\x02y −a\\nb\\n\\x03\\n=\\n1\\n√\\n2πσ|b|\\ne\\n−\\n\\x07 y−a\\nb −μ\\n\\x082\\n/2σ 2\\n=\\n1\\n√\\n2πσ|b|\\ne−( y−a−bμ)2/2b2σ 2\\nshowing that Y = a + bX is normal with mean a + bμ and variance b2σ 2.\\nIt follows from the foregoing that if X ∼N(μ,σ 2), then\\nZ = X −μ\\nσ\\nis a normal random variable with mean 0 and variance 1. Such a random vari-\\nable Z is said to have a standard, or unit, normal distribution. Let \\x08(·) denote\\nits distribution function. That is,\\n\\x08(x) =\\n1\\n√\\n2π\\n\\x10 x\\n−∞\\ne−y2/2dy,\\n−∞< x < ∞\\nThis result that Z = (X −μ)/σ has a standard normal distribution when X is\\nnormal with parameters μ and σ 2 is quite important, for it enables us to write\\nall probability statements about X in terms of probabilities for Z. For instance,\\nto obtain P{X < b}, we note that X will be less than b if and only if (X −μ)/σ\\nis less than (b −μ)/σ, and so\\nP{X < b} = P\\n\\x12X −μ\\nσ\\n< b −μ\\nσ\\n\\x13\\n= \\x08\\n\\x02b −μ\\nσ\\n\\x03\\nSimilarly, for any a < b,\\nP{a < X < b} = P\\n\\x12a −μ\\nσ\\n< X −μ\\nσ\\n< b −μ\\nσ\\n\\x13\\n= P\\n\\x12a −μ\\nσ\\n< Z < b −μ\\nσ\\n\\x13\\n= P\\n\\x12\\nZ < b −μ\\nσ\\n\\x13\\n−P\\n\\x12\\nZ < a −μ\\nσ\\n\\x13\\n= \\x08\\n\\x02b −μ\\nσ\\n\\x03\\n−\\x08\\n\\x02a −μ\\nσ\\n\\x03'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 195}, page_content='5.5 Normal random variables\\n183\\nFIGURE 5.7\\nStandard normal probabilities.\\nIt remains for us to compute \\x08(x). This has been accomplished by an approx-\\nimation and the results are presented in Table A.1 of the Appendix, which\\ntabulates \\x08(x) (to a 4-digit level of accuracy) for a wide range of nonnegative\\nvalues of x.\\nWhile Table A.1 tabulates \\x08(x) only for nonnegative values of x, we can also\\nobtain \\x08(−x) from the table by making use of the symmetry (about 0) of the\\nstandard normal probability density function. That is, for x > 0, if Z represents\\na standard normal random variable, then (see Figure 5.7)\\n\\x08(−x) = P{Z < −x}\\n= P{Z > x}\\nby symmetry\\n= 1 −\\x08(x)\\nThus, for instance,\\nP{Z < −1} = \\x08(−1) = 1 −\\x08(1) = 1 −.8413 = .1587\\nExample 5.5.a. If X is a normal random variable with mean μ=3 and variance\\nσ 2 = 16, ﬁnd\\n(a) P{X < 11};\\n(b) P{X > −1};\\n(c) P{2 < X < 7}.\\nSolution.\\n(a)\\nP{X < 11} = P\\n\\x12X −3\\n4\\n< 11 −3\\n4\\n\\x13\\n= \\x08(2)\\n= .9772\\n(b)\\nP{X > −1} = P\\n\\x12X −3\\n4\\n> −1 −3\\n4\\n\\x13\\n= P{Z > −1}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 196}, page_content='184 CHAPTER 5: Special random variables\\n= P{Z < 1}\\n= .8413\\n(c)\\nP{2 < X < 7} = P\\n\\x122 −3\\n4\\n< X −3\\n4\\n< 7 −3\\n4\\n\\x13\\n= \\x08(1) −\\x08(−1/4)\\n= \\x08(1) −(1 −\\x08(1/4))\\n= .8413 + .5987 −1 = .4400\\n■\\nExample 5.5.b. Suppose that a binary message — either “0” or “1” — must\\nbe transmitted by wire from location A to location B. However, the data sent\\nover the wire are subject to a channel noise disturbance and so to reduce the\\npossibility of error, the value 2 is sent over the wire when the message is “1”\\nand the value −2 is sent when the message is “0.” If x, x = ±2, is the value sent\\nat location A then R, the value received at location B, is given by R = x + N,\\nwhere N is the channel noise disturbance. When the message is received at\\nlocation B, the receiver decodes it according to the following rule:\\nif R ≥.5, then “1” is concluded\\nif R < .5, then “0” is concluded\\nBecause the channel noise is often normally distributed, we will determine the\\nerror probabilities when N is a standard normal random variable.\\nThere are two types of errors that can occur: One is that the message “1” can be\\nincorrectly concluded to be “0” and the other that “0” is incorrectly concluded\\nto be “1.” The ﬁrst type of error will occur if the message is “1” and 2 + N < .5,\\nwhereas the second will occur if the message is “0” and −2 + N ≥.5.\\nHence,\\nP{error|message is “1”} = P{N < −1.5}\\n= 1 −\\x08(1.5) = .0668\\nand\\nP{error|message is “0”} = P{N > 2.5}\\n= 1 −\\x08(2.5) = .0062\\n■\\nWhile we could use a standard normal probability table to compute all proba-\\nbilities related to normal random variables, we can also use R to make this task\\neven easier. If Z is a standard normal random variable, then\\npnorm(x) returns P(Z ≤x).'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 197}, page_content='5.5 Normal random variables\\n185\\nMore generally, if X is normal with mean μ and variance σ 2, then\\npnorm(x,μ,σ) returns P(X ≤x).\\nOf course, we could also have obtained P(X ≤x) by\\n>\\ny = (x −μ)/σ\\n>\\npnorm(y)\\nFor instance, if X is normal with mean 10 and variance 8, then P(X ≤12) can\\nbe obtained either by\\n>\\npnorm(12,10,sqrt(8))\\n[1]\\n0.7602499\\nor by\\n>\\ny = (12 −10)/sqrt(8)\\n>\\npnorm(y)\\n[1]\\n0.7602499\\nor by\\n>\\npnorm((12 −10)/sqrt(8))\\n[1]\\n0.7602499\\nExample 5.5.c. The power W dissipated in a resistor is proportional to the\\nsquare of the voltage V. That is,\\nW = rV 2\\nwhere r is a constant. If r = 3, and V can be assumed (to a very good approxi-\\nmation) to be a normal random variable with mean 6 and standard deviation\\n1, ﬁnd\\n(a) E [W];\\n(b) P{W > 120}.\\nSolution.\\n(a)\\nE[W] = E[3V 2]\\n= 3E[V 2]\\n= 3(Var[V ] + E2[V ])\\n= 3(1 + 36) = 111'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 198}, page_content='186 CHAPTER 5: Special random variables\\n(b)\\nP(W > 120) = P(3V 2 > 120)\\n= P(V >\\n√\\n40)\\nBecause V is normal with mean 6 and standard deviation 1, R yields the solu-\\ntion\\n>\\n1 −pnorm(sqrt(40),6,1)\\n[1]\\n0.3727588\\n■\\nLet us now compute the moment generating function of a normal random\\nvariable. To start, we compute the moment generating function of a standard\\nnormal random variable Z.\\nE[etZ] =\\n\\x10 ∞\\n−∞\\netx\\n1\\n√\\n2π\\ne−x2/2 dx\\n=\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\ne−(x2−2tx)/2dx\\n= et2/2\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\ne−(x−t)2/2dx\\n= et2/2\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\ne−y2/2dy\\n= et2/2\\nNow, if Z is a standard normal, then X = μ + σZ is normal with mean μ and\\nvariance σ 2. Using the preceding, its moment generating function is\\nE[etX] = E[etμ+tσZ]\\n= E[etμetσZ]\\n= etμE[etσZ]\\n= etμe(σt)2/2\\n= eμt+σ 2t2/2\\nAnother important result is that the sum of independent normal random\\nvariables is also a normal random variable. To see this, suppose that Xi,i =\\n1,...,n, are independent, with Xi being normal with mean μi and variance\\nσ 2\\ni . The moment generating function of \\nn\\ni=1 Xi is as follows.\\nE\\n\\x14\\net \\nn\\ni=1 Xi\\x15\\n= E\\n\\x14\\netX1etX2 ···etXn\\x15\\n=\\nn\\n\\x16\\ni=1\\nE\\n\\x14\\netXi\\x15\\nby independence'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 199}, page_content='5.5 Normal random variables\\n187\\n=\\nn\\n\\x16\\ni=1\\neμit+σ 2\\ni t2/2\\n= eμt+σ 2t2/2\\nwhere\\nμ =\\nn\\n\\x06\\ni=1\\nμi,\\nσ 2 =\\nn\\n\\x06\\ni=1\\nσ 2\\ni\\nTherefore, \\nn\\ni=1 Xi has the same moment generating function as a normal ran-\\ndom variable having mean μ and variance σ 2. Hence, from the one-to-one\\ncorrespondence between moment generating functions and distributions, we\\ncan conclude that \\nn\\ni=1 Xi is normal with mean \\nn\\ni=1 μi and variance \\nn\\ni=1 σ 2\\ni .\\nExample 5.5.d. Data from the National Oceanic and Atmospheric Adminis-\\ntration indicate that the yearly precipitation in Los Angeles is a normal ran-\\ndom variable with a mean of 12.08 inches and a standard deviation of 3.1\\ninches.\\n(a) Find the probability that the total precipitation during the next 2 years\\nwill exceed 25 inches.\\n(b) Find the probability that next year’s precipitation will exceed that of the\\nfollowing year by more than 3 inches.\\nAssume that the precipitation totals for the next 2 years are indepen-\\ndent.\\nSolution. Let X1 and X2 be the precipitation totals for the next 2 years.\\n(a) Because X1 +X2 is normal with mean 24.16 and variance 2(3.1)2 = 19.22,\\nwe use R to obtain P(X1 + X2 > 25):\\n>\\n1 −pnorm(25,24.16,sqrt(19.22))\\n[1]\\n0.4240265\\n(b) Since −X2 is a normal random variable with mean −12.08 and vari-\\nance (−1)2(3.1)2, it follows that X1 −X2 is normal with mean 0 and\\nvariance 19.22. Hence, as\\nP(X1 > X2 + 3) = P(X1 −X2 > 3),\\nR yields the solution\\n>\\n1 −pnorm(3,0,sqrt(19.22))\\n[1]\\n0.2468939265'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 200}, page_content='188 CHAPTER 5: Special random variables\\nFIGURE 5.8\\nP{Z > zα} = α.\\nThus there is a 42.4 percent chance that the total precipitation in Los An-\\ngeles during the next 2 years will exceed 25 inches, and there is a 24.69\\npercent chance that next year’s precipitation will exceed that of the fol-\\nlowing year by more than 3 inches.\\n■\\nFor α ∈(0,1), let zα be such that\\nP{Z > zα} = 1 −\\x08(zα) = α\\nThat is, the probability that a standard normal random variable is greater than\\nzα is equal to α (see Figure 5.8).\\nThe value of zα can, for any α, be obtained from Table A.1. For instance, since\\n1 −\\x08(1.645) = .05\\n1 −\\x08(1.96) = .025\\n1 −\\x08(2.33) = .01\\nit follows that\\nz.05 = 1.645,\\nz.025 = 1.96,\\nz.01 = 2.33\\nTo use R to determine the zα, just type qnorm(1 −α). That is,\\nqnorm(1 −α) returns zα.\\nFor instance, z.01 is obtained as follows:\\n>\\nqnorm(1 −.01)\\n[1]\\n2.326348\\nThe command qnorm(β), where 0 < β < 1, returns the 100β percent quantile\\n(also called percentile) of the normal distribution, deﬁned as the value x such\\nthat the probability a standard normal is less than or equal to x is equal to β.\\nConsequently,\\nP(Z ≤qnorm(1 −α)) = 1 −α'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 201}, page_content='5.5 Normal random variables\\n189\\nor, equivalently,\\nP(Z > qnorm(1 −α)) = α,\\nshowing that qnorm(1 −α) = zα.\\nSince\\nP{Z < zα} = 1 −α\\nit follows that 100(1 −α) percent of the time a standard normal random vari-\\nable will be less than zα. As a result, we call zα the 100(1 −α) percentile of the\\nstandard normal distribution.\\nAdditional Uses of R. The R command dnorm(x) yields the density function\\nof the standard normal at value x. That is, dnorm(x) =\\n1\\n√\\n2π e−x2/2. Similarly,\\ndnorm(x,μ,σ) yields the density at x of a normal with mean μ and standard\\ndeviation σ. We can use these to plot normal densities. For instance, to plot\\nthe normal density of a normal with mean 3 and standard deviation 2, then\\nwe plot (x,dnorm(x)) for a range of values of x. The following does so for\\nall values starting at x = −5 to x = 11 in increments of size .001 (thus 16,000\\npoints are being plotted):\\n>\\nx = seq(−5,11,.001)\\n>\\nf = dnorm(x,3,2)\\n>\\nplot(x,f )'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 202}, page_content='190 CHAPTER 5: Special random variables\\n5.6\\nExponential random variables\\nA continuous random variable whose probability density function is given, for\\nsome λ > 0, by\\nf (x) =\\n\\t\\nλe−λx\\nif x ≥0\\n0\\nif x < 0\\nis said to be an exponential random variable (or, more simply, is said to be expo-\\nnentially distributed) with parameter λ. The cumulative distribution function\\nF(x) of an exponential random variable is given by\\nF(x) = P{X ≤x}\\n=\\n\\x10 x\\n0\\nλe−λy dy\\n= 1 −e−λx,\\nx ≥0\\nThe exponential distribution often arises, in practice, as being the distribu-\\ntion of the amount of time until some speciﬁc event occurs. For instance, the\\namount of time (starting from now) until an earthquake occurs, or until a new\\nwar breaks out, or until a telephone call you receive turns out to be a wrong\\nnumber are all random variables that tend in practice to have exponential dis-\\ntributions (see Section 5.6.1 for an explanation).\\nThe moment generating function of the exponential is given by\\nφ(t) = E[etX]\\n=\\n\\x10 ∞\\n0\\netxλe−λx dx\\n= λ\\n\\x10 ∞\\n0\\ne−(λ−t)x dx\\n=\\nλ\\nλ −t ,\\nt < λ\\nDifferentiation yields\\nφ′(t) =\\nλ\\n(λ −t)2\\nφ′′(t) =\\n2λ\\n(λ −t)3\\nand so\\nE[X] = φ′(0) = 1/λ'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 203}, page_content='5.6 Exponential random variables\\n191\\nVar(X) = φ′′(0) −(E[X])2\\n= 2/λ2 −1/λ2\\n= 1/λ2\\nThus λ is the reciprocal of the mean, and the variance is equal to the square of\\nthe mean.\\nThe key property of an exponential random variable is that it is memoryless,\\nwhere we say that a nonnegative random variable X is memoryless if\\nP{X > s + t|X > t} = P{X > s}\\nfor all s,t ≥0\\n(5.6.1)\\nTo understand why Equation (5.6.1) is called the memoryless property, imagine\\nthat X represents the length of time that a certain item functions before failing.\\nNow let us consider the probability that an item that is still functioning at age\\nt will continue to function for at least an additional time s. Since this will be\\nthe case if the total functional lifetime of the item exceeds t + s given that the\\nitem is still functioning at t, we see that\\nP{additional functional life of t-unit-old item exceeds s}\\n= P{X > t + s|X > t}\\nThus, we see that Equation (5.6.1) states that the distribution of additional\\nfunctional life of an item of age t is the same as that of a new item — in other\\nwords, when Equation (5.6.1) is satisﬁed, there is no need to remember the age\\nof a functional item since as long as it is still functional it is “as good as new.”\\nThe condition in Equation (5.6.1) is equivalent to\\nP{X > s + t,X > t}\\nP{X > t}\\n= P{X > s}\\nor\\nP{X > s + t} = P{X > s}P{X > t}\\n(5.6.2)\\nWhen X is an exponential random variable, then\\nP{X > x} = e−λx,\\nx > 0\\nand so Equation (5.6.2) is satisﬁed (since e−λ(s+t) = e−λse−λt). Hence, exponen-\\ntially distributed random variables are memoryless (and in fact it can be shown that\\nthey are the only random variables that are memoryless).\\nExample 5.6.a. Suppose that a number of miles that a car can run before its\\nbattery wears out is exponentially distributed with an average value of 10,000'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 204}, page_content='192 CHAPTER 5: Special random variables\\nmiles. If a person desires to take a 5000-mile trip, what is the probability that\\nshe will be able to complete her trip without having to replace her car battery?\\nWhat can be said when the distribution is not exponential?\\nSolution. It follows, by the memoryless property of the exponential distri-\\nbution, that the remaining lifetime (in thousands of miles) of the battery is\\nexponential with parameter λ = 1/10. Hence the desired probability is\\nP{remaining lifetime > 5} = 1 −F(5)\\n= e−5λ\\n= e−1/2 ≈.604\\nHowever, if the lifetime distribution F is not exponential, then the relevant\\nprobability is\\nP{lifetime > t + 5|lifetime > t} = 1 −F(t + 5)\\n1 −F(t)\\nwhere t is the number of miles that the battery had been in use prior to the start\\nof the trip. Therefore, if the distribution is not exponential, additional informa-\\ntion is needed (namely, t) before the desired probability can be calculated.\\n■\\nFor another illustration of the memoryless property, consider the following\\nexample.\\nExample 5.6.b. A crew of workers has 3 interchangeable machines, of which\\n2 must be working for the crew to do its job. When in use, each machine\\nwill function for an exponentially distributed time having parameter λ before\\nbreaking down. The workers decide initially to use machines A and B and keep\\nmachine C in reserve to replace whichever of A or B breaks down ﬁrst. They will\\nthen be able to continue working until one of the remaining machines breaks\\ndown. When the crew is forced to stop working because only one of the ma-\\nchines has not yet broken down, what is the probability that the still operable\\nmachine is machine C?\\nSolution. This can be easily answered, without any need for computations, by\\ninvoking the memoryless property of the exponential distribution. The argu-\\nment is as follows: Consider the moment at which machine C is ﬁrst put in\\nuse. At that time either A or B would have just broken down and the other one\\n— call it machine 0 — will still be functioning. Now even though 0 would\\nhave already been functioning for some time, by the memoryless property of\\nthe exponential distribution, it follows that its remaining lifetime has the same\\ndistribution as that of a machine that is just being put into use. Thus, the re-\\nmaining lifetimes of machine 0 and machine C have the same distribution and\\nso, by symmetry, the probability that 0 will fail before C is 1\\n2.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 205}, page_content='5.6 Exponential random variables\\n193\\nThe following proposition presents another property of the exponential distri-\\nbution.\\nProposition 5.6.1. If X1,X2,...,Xn are independent exponential random\\nvariables having respective parameters λ1,λ2,...,λn, then min(X1,X2,...,Xn)\\nis exponential with parameter \\nn\\nt=1 λi.\\nProof. Since the smallest value of a set of numbers is greater than x if and only\\nif all values are greater than x, we have\\nP{min(X1,X2,...,Xn) > x} = P{X1 > x,X2 > x,...,Xn > x}\\n=\\nn\\n\\x16\\ni=1\\nP{Xi > x}\\nby independence\\n=\\nn\\n\\x16\\ni=1\\ne−λix\\n= e−\\nn\\ni=1 λix\\n■\\nExample 5.6.c. A series system is one that needs all of its components to func-\\ntion in order for the system itself to be functional. For an n-component series\\nsystem in which the component lifetimes are independent exponential random\\nvariables with respective parameters λ1,λ2,...,λn, what is the probability that\\nthe system survives for a time t?\\nSolution. Since the system life is equal to the minimal component life, it fol-\\nlows from Proposition 5.6.1 that\\nP{system life exceeds t} = e−\\ni λit\\n■\\nAnother useful property of exponential random variables is that cX is expo-\\nnential with parameter λ/c when X is exponential with parameter λ, and c > 0.\\nThis follows since\\nP{cX ≤x} = P{X ≤x/c}\\n= 1 −e−λx/c\\nThe parameter λ is called the rate of the exponential distribution.\\n5.6.1\\nThe Poisson process4\\nSuppose that “events” are occurring at random time points, and let N(t) denote\\nthe number of events that occurs in the time interval [0, t]. These events are said\\nto constitute a Poisson process having rate λ, λ > 0, if\\n4Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 206}, page_content='194 CHAPTER 5: Special random variables\\nFIGURE 5.9\\n(a) N(0) = 0\\n(b) The number of events that occur in disjoint time intervals are indepen-\\ndent.\\n(c) The distribution of the number of events that occur in a given interval\\ndepends only on the length of the interval and not on its location.\\n(d) lim\\nh→0\\nP{N(h) = 1}\\nh\\n= λ\\n(e) lim\\nh→0\\nP{N(h) ≥2}\\nh\\n= 0\\nThus, Condition (a) states that the process begins at time 0. Condition (b),\\nthe independent increment assumption, states for instance that the number of\\nevents by time t [that is, N(t)] is independent of the number of events that\\noccurs between t and t +s [that is, N(t +s)−N(t)]. Condition (c), the stationary\\nincrement assumption, states that probability distribution of N(t + s) −N(t) is\\nthe same for all values of t. Conditions (d) and (e) state that in a small interval\\nof length h, the probability of one event occurring is approximately λh, whereas\\nthe probability of 2 or more is approximately 0.\\nWe will now show that these assumptions imply that the number of events oc-\\ncurring in any interval of length t is a Poisson random variable with parameter\\nλt. To be precise, let us call the interval [0, t] and denote by N(t) the number of\\nevents occurring in that interval. To obtain an expression for P{N(t) = k}, we\\nstart by breaking the interval [0, t] into n nonoverlapping subintervals each of\\nlength t/n (Figure 5.9). Now there will be k events in [0, t] if either\\n(i) N(t) equals k and there is at most one event in each subinterval;\\n(ii) N(t) equals k and at least one of the subintervals contains 2 or more\\nevents.\\nSince these two possibilities are clearly mutually exclusive, and since Condition\\n(i) is equivalent to the statement that k of the n subintervals contain exactly 1\\nevent and the other n −k contain 0 events, we have that\\nP{N(t) = k} = P{k of the n subintervals contain exactly 1 event\\n(5.6.3)\\nand the other n −k contain 0 events} + P{N(t) = k\\nand at least 1 subinterval contains 2 or more events}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 207}, page_content='5.6 Exponential random variables\\n195\\nNow it can be shown, using Condition (e), that\\nP{N(t) = k and at least 1 subinterval contains 2 or more events}\\n−→0as n →∞\\n(5.6.4)\\nAlso, it follows from Conditions (d) and (e) that\\nP{exactly 1 event in a subinterval} ≈λt\\nn\\nP{0 events in a subinterval} ≈1 −λt\\nn\\nHence, since the number of events that occur in different subintervals are inde-\\npendent [from Condition (b)], it follows that\\nP{k of the subintervals contain exactly 1 event\\nand the other n −k contain 0 events}\\n≈\\n\\x07 n\\nk\\n\\x08\\x02λt\\nn\\n\\x03k \\x02\\n1 −λt\\nn\\n\\x03n−k\\n(5.6.5)\\nwith the approximation becoming exact as the number of subintervals, n, goes\\nto ∞. However, the probability in Equation (5.6.5) is just the probability that a\\nbinomial random variable with parameters n and p = λt/n equals k. Hence, as\\nn becomes larger and larger, this approaches the probability that a Poisson ran-\\ndom variable with mean nλt/n = λt equals k. Hence, from Equations (5.6.3),\\n(5.6.4), and (5.6.5), we see upon letting n approach ∞that\\nP{N(t) = k} = e−λt (λt)k\\nk!\\nWe have shown:\\nProposition 5.6.2. For a Poisson process having rate λ\\nP{N(t) = k} = e−λt (λt)k\\nk! ,\\nk = 0,1,...\\nThat is, the number of events in any interval of length t has a Poisson distribu-\\ntion with mean λt.\\nFor a Poisson process, let X1 denote the time of the ﬁrst event. Further, for\\nn > 1, let Xn denote the elapsed time between (n −1)st and the nth events.\\nThe sequence {Xn,n = 1,2,...} is called the sequence of interarrival times. For\\ninstance, if X1 = 5 and X2 = 10, then the ﬁrst event of the Poisson process\\nwould have occurred at time 5 and the second at time 15.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 208}, page_content='196 CHAPTER 5: Special random variables\\nWe now determine the distribution of the Xn. To do so, we ﬁrst note that the\\nevent {X1 > t} takes place if and only if no events of the Poisson process occur\\nin the interval [0, t], and thus,\\nP{X1 > t} = P{N(t) = 0} = e−λt\\nHence, X1 has an exponential distribution with mean 1/λ. To obtain the distri-\\nbution of X2, note that\\nP{X2 > t|X1 = s} = P{0 events in (s,s + t]|X1 = s}\\n= P{0 events in (s,s + t]}\\n= e−λt\\nwhere the last two equations followed from independent and stationary in-\\ncrements. Therefore, from the foregoing we see that X2 is also an exponential\\nrandom variable with mean 1/λ, and furthermore, that X2 is independent of\\nX1. Repeating the same argument yields:\\nProposition 5.6.3. X1, X2,... are independent exponential random variables,\\neach with mean 1/λ.\\n5.6.2\\nThe Pareto distribution5\\nIf X is an exponential random variable with rate λ, then\\nY = α e X\\nis said to be a Pareto random variable with parameters α and λ. The parameter\\nλ>0 is called the index parameter, and α is called the minimum parameter\\n(because P(Y ≥α)=1). The distribution function of Y is derived as follows:\\nFor y ≥α,\\nP{Y > y} = P{α e X > y}\\n= P{e X > y/α}\\n= P{X > log(y/α)}\\n= e−λ log(y/α)\\n= e−log((y/α)λ)\\n= (α/y)λ\\nHence, the distribution function of Y is\\nFY (y) = 1 −P(Y > y) = 1 −αλy−λ ,\\ny ≥α\\n5Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 209}, page_content='5.6 Exponential random variables\\n197\\nDifferentiating the distribution function yields the density function of Y:\\nfY (y) = λαλy−(λ+1) ,\\ny ≥α\\nIt can be shown (see Problems 5–49) that E[Y] = ∞when λ ≤1. When λ > 1,\\nthe mean is obtained as follows.\\nE[Y] =\\n\\x10 ∞\\nα\\ny λαλ y−(λ+1) dy\\n= λαλ\\n\\x10 ∞\\nα\\ny−λ dy\\n= αλ\\nλ\\n1 −λy1−λ|∞\\nα\\n= αλ\\nλ\\nλ −1α1−λ\\n= α\\nλ\\nλ −1\\nAn important feature of Pareto distributions is that for y0 > α the conditional\\ndistribution of a Pareto random variable Y with parameters α and λ, given that\\nit exceeds y0, is the Pareto distribution with parameters y0 and λ. To verify this,\\nnote for y > y0 that\\nP{Y > y|Y > y0} = P{Y > y, Y > y0}\\nP{Y > y0}\\n= P{Y > y}\\nP{Y > y0} = αλ y−λ\\nαλ y−λ\\n0\\n= yλ\\n0 y−λ\\nThus, the conditional distribution is indeed Pareto with parameters y0 and λ.\\nOne of the earliest uses of the Pareto was as the distribution of the annual in-\\ncome of the members of a population. In fact, it has been widely supposed\\nthat incomes in many populations can be modeled as coming from a Pareto\\ndistribution with index parameter λ = log(5)/log(4) ≈1.161. Under this sup-\\nposition, it turns out that the total income of the top 20 percent of earners is 80\\npercent of the population’s total income earnings, and that the top 20 percent\\nof these high earners earn 80 percent of the total of all high earners income,\\nand that the top 20 percent of these very high earners earn 80 percent of the\\ntotal of all very high earners income, and so on.\\nTo verify the preceding claim, let y.8 be the 80 percentile of the Pareto dis-\\ntribution. Because FY (y) = 1 −(α/y)λ, we see that .8 = F(y.8) = 1 −(α/y.8)λ,\\nshowing that\\n(α/y.8)λ = .2\\nor\\n(y.8/α)λ = 5\\nand thus\\ny.8 = α 51/λ'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 210}, page_content='198 CHAPTER 5: Special random variables\\nNow suppose, from this point on, that λ = log(5)/log(4), and note that\\nlog(4) = (1/λ)log(5) = log(51/λ), showing that 4 = 51/λ, or equivalently that\\n1/λ = log5(4). Hence,\\ny.8 = α 5log5(4) = 4α\\nThe average income of a randomly chosen individual in the top 20 percent is\\nE[Y|Y > y.8], which is easily obtained by using that the conditional distribu-\\ntion of Y given that it exceeds y.8 is Pareto with parameters y.8 and λ. Using the\\npreviously derived formula for E[Y], this yields that\\nE[Y|Y > y.8] = y.8\\nλ\\nλ −1 = 4α\\nλ\\nλ −1\\nTo obtain E[Y|Y < y.8], the average income of a randomly chosen individual\\nin the bottom 80 percent, we use the identity\\nE[Y] = E[Y|Y < y.8](.8) + E[Y|Y > y.8](.2)\\nUsing the previously derived expressions for E[Y] and E[Y|Y > y.8], the pre-\\nceding equation yields that\\nα\\nλ\\nλ −1 = 4\\n5E[Y|Y < y.8] + 4\\n5α\\nλ\\nλ −1\\nshowing that\\nE[Y|Y < y.8] = α\\n4\\nλ\\nλ −1\\nThus,\\nE[Y|Y < y.8] = 1\\n16 E[Y|Y > y.8]\\nHence, the average earnings of someone in the top 20 percent of income earned\\nis 16 times that of someone in the lower 80 percent, thus showing that, al-\\nthough there are 4 times as many people in the lower earning group, the total\\nincome of the lower income group is only 20 percent of the total earnings of\\nthe population. (On average, for every 5 people in the population, 4 are in the\\nlower 80 percent and 1 is in the upper 20 percent; the 4 in the lower earnings\\ngroup earn on average a total of 4 a\\n4\\nλ\\nλ−1 = a\\nλ\\nλ−1, whereas the one in the higher\\nincome group earns on average 4a\\nλ\\nλ−1. Thus, 4 out of every 5 dollars of the\\npopulation’s total income is earned by someone in the highest 20 percent.)\\nBecause the conditional distribution of a high income earner (that is, one who\\nearns more than y.8) is Pareto with parameters y.8 and λ, it also follows from\\nthe preceding that 80 percent of the total of the earnings of this group are\\nearned by the top 20 percent of these high earners, and so on.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 211}, page_content='5.7 The gamma distribution\\n199\\nThe Pareto distribution has been applied in a variety of areas. For instance, it\\nhas been used as the distribution of\\n(a) the ﬁle size of internet trafﬁc (under the TCP protocol);\\n(b) the time to compete a job assigned to a supercomputer;\\n(c) the size of a meteorite;\\n(d) the yearly maximum one day rainfalls in different regions.\\n5.7\\nThe gamma distribution6\\nA random variable is said to have a gamma distribution with parameters\\n(α,λ),λ > 0, α > 0, if its density function is given by\\nf (x) =\\n⎧\\n⎨\\n⎩\\nλe−λx(λx)α−1\\n\\t(α)\\nx ≥0\\n0\\nx < 0\\nwhere\\n\\t(α) =\\n\\x10 ∞\\n0\\nλe−λx(λx)α−1 dx\\n=\\n\\x10 ∞\\n0\\ne−yyα−1 dy\\n(by letting y = λx)\\nThe integration by parts formula\\n\\x17\\nudv = uv −\\n\\x17\\nv du yields, with u = yα−1,\\ndv = e−ydy, v = −e−y, that for α > 1,\\n\\x10 ∞\\n0\\ne−yyα−1 dy = −e−yyα−1\\n\\x11\\x11\\x11\\x11\\ny = ∞\\ny = 0\\n+\\n\\x10 ∞\\n0\\ne−y(α −1)yα−2 dy\\n= (α −1)\\n\\x10 ∞\\n0\\ne−yyα−2 dy\\nor\\n\\t(α) = (α −1)\\t(α −1)\\n(5.7.1)\\nWhen α is an integer — say, α = n — we can iterate the foregoing to obtain that\\n\\t(n) = (n −1)\\t(n −1)\\n= (n −1)(n −2)\\t(n −2)\\nby letting α = n −1 in Equation (5.7.1)\\n= (n −1)(n −2)(n −3)\\t(n −3)\\nby letting α = n −2 in Equation (5.7.1)\\n...\\n= (n −1)!\\t(1)\\n6Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 212}, page_content='200 CHAPTER 5: Special random variables\\nBecause\\n\\t(1) =\\n\\x10 ∞\\n0\\ne−y dy = 1\\nwe see that\\n\\t(n) = (n −1)!\\nThe function \\t(α) is called the gamma function.\\nIt should be noted that when α = 1, the gamma distribution reduces to the\\nexponential with mean 1/λ.\\nThe moment generating function of a gamma random variable X with param-\\neters (α, λ) is obtained as follows:\\nφ(t) = E[et X]\\n=\\nλα\\n\\t(α)\\n\\x10 ∞\\n0\\net xe−λxxα−1 dx\\n=\\nλα\\n\\t(α)\\n\\x10 ∞\\n0\\ne−(λ−t)xxα−1 dx\\n=\\n\\x02\\nλ\\nλ −t\\n\\x03α\\n1\\n\\t(α)\\n\\x10 ∞\\n0\\ne−yyα−1 dy\\n[by y = (λ −t)x]\\n=\\n\\x02\\nλ\\nλ −t\\n\\x03α\\n(5.7.2)\\nwhere the ﬁnal equality used that e−y yα−1/\\t(α) is a density function, and\\nthus integrates to 1.\\nDifferentiation of Equation (5.7.2) yields\\nφ′(t) =\\nαλα\\n(λ −t)α+1\\nφ′′(t) = α(α + 1)λα\\n(λ −t)α+2\\nHence,\\nE[X] = φ′(0) = α\\nλ\\n(5.7.3)\\nVar(X) = E[X2] −(E[X])2\\n= φ′′(0) −\\n\\x07α\\nλ\\n\\x082\\n= α(α + 1)\\nλ2\\n−α2\\nλ2 = α\\nλ2\\n(5.7.4)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 213}, page_content='5.8 Distributions arising from the normal\\n201\\nAn important property of the gamma is that if X1 and X2 are independent\\ngamma random variables having respective parameters (α1,λ) and (α2,λ), then\\nX1 + X2 is a gamma random variable with parameters (α1 + α2,λ). This result\\neasily follows since\\nφX1+X2(t) = E[et(X1+X2)]\\n(5.7.5)\\n= φX1(t)φX2(t)\\n=\\n\\x02\\nλ\\nλ −t\\n\\x03α1 \\x02\\nλ\\nλ −t\\n\\x03α2\\nfrom Equation (5.7.2)\\n=\\n\\x02\\nλ\\nλ −t\\n\\x03α1+α2\\nwhich is seen to be the moment generating function of a gamma (α1 + α2,λ)\\nrandom variable. Since a moment generating function uniquely characterizes a\\ndistribution, the result entails.\\nThe foregoing result easily generalizes to yield the following proposition.\\nProposition 5.7.1. If Xi,i =1, ... ,n are independent gamma random vari-\\nables with respective parameters (αi,λ), then \\nn\\ni=1 Xi is gamma with parame-\\nters \\nn\\ni=1 αi, λ.\\nSince the gamma distribution with parameters (1, λ) reduces to the exponential\\nwith the rate λ, we have thus shown the following useful result.\\nCorollary 5.7.2. If X1,...,Xn are independent exponential random variables,\\neach having rate λ, then \\nn\\ni=1 Xi is a gamma random variable with parameters\\n(n, λ).\\nExample 5.7.a. The lifetime of a battery is exponentially distributed with rate\\nλ. If a stereo cassette requires one battery to operate, then the total playing\\ntime one can obtain from a total of n batteries is a gamma random variable\\nwith parameters (n, λ).\\n■\\nFigure 5.10 presents a graph of the gamma (α, 1) density for a variety of values\\nof α. It should be noted that as α becomes large, the density starts to resemble\\nthe normal density. This is theoretically explained by the central limit theorem,\\nwhich will be presented in the next chapter.\\n5.8\\nDistributions arising from the normal\\n5.8.1\\nThe chi-square distribution\\nDeﬁnition. If Z1,Z2,...,Zn are independent standard normal random vari-\\nables, then X, deﬁned by\\nX = Z2\\n1 + Z2\\n2 + ··· + Z2\\nn\\n(5.8.1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 214}, page_content='202 CHAPTER 5: Special random variables\\nFIGURE 5.10\\nGraphs of the gamma (α, 1) density for (A) α = .5, 2, 3, 4, 5 and (B) α = 50.\\nis said to have a chi-square distribution with n degrees of freedom. We will use the\\nnotation\\nX ∼χ2\\nn\\nto signify that X has a chi-square distribution with n degrees of freedom.\\nThe chi-square distribution has the additive property that if X1 and X2 are\\nindependent chi-square random variables with n1 and n2 degrees of freedom,\\nrespectively, then X1 + X2 is chi-square with n1 + n2 degrees of freedom. This\\ncan be formally shown either by the use of moment generating functions or,\\nmost easily, by noting that X1+X2 is the sum of squares of n1+n2 independent'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 215}, page_content='5.8 Distributions arising from the normal\\n203\\nFIGURE 5.11\\nThe chi-square density function with 8 degrees of freedom.\\nstandard normals and thus has a chi-square distribution with n1 + n2 degrees\\nof freedom.\\nIf X is a chi-square random variable with n degrees of freedom, then for any\\nα ∈(0,1), the quantity χ2\\nα,n is deﬁned to be such that\\nP{X ≥χ2\\nα,n} = α\\nThis is illustrated in Figure 5.11.\\nChi-square probabilities, as well as the values χ2\\nα,n can be obtained using R. To\\nobtain P(X ≤x), when X is a chi-squared random variable with n degrees of\\nfreedom, just use the R command pchisq(x,n). To obtain χ2\\nα,n , type qchisq(1−\\nα,n).\\nExample 5.8.a. Determine P{χ2\\n26 ≤30} when χ2\\n26 is a chi-square random vari-\\nable with 26 degrees of freedom.\\nSolution. R yields the solution\\n>\\npchisq(30,26)\\n[1]\\n0.732389\\n■\\nExample 5.8.b. Find χ2\\n.05,15.\\nSolution. R yields the result\\n>\\nqchisq(.95,15)\\n[1]\\n24.99579\\n■\\nExample 5.8.c. Suppose that we are attempting to locate a target in three-\\ndimensional space, and that the three coordinate errors (in meters) of the point\\nchosen are independent normal random variables with mean 0 and standard\\ndeviation 2. Find the probability that the distance between the point chosen\\nand the target exceeds 3 meters.\\nSolution. If D is the distance, then\\nD2 = X2\\n1 + X2\\n2 + X2\\n3'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 216}, page_content='204 CHAPTER 5: Special random variables\\nwhere Xi is the error in the ith coordinate. Since Zi = Xi/2, i = 1,2,3, are all\\nstandard normal random variables, it follows that\\nP{D2 > 9} = P {Z2\\n1 + Z2\\n2 + Z2\\n3 > 9/4}\\n= P{χ2\\n3 > 9/4}\\nR now gives\\n>\\n1 −pchisq(9/4,3)\\n[1]\\n0.5221672\\n■\\nWe can plot chi-square densities using R. To plot the density of a chi-square\\nwith 5 degrees of freedom, say going from 0 to 12 in increments of size .001,\\ndo the following:\\n>\\nx = seq(0,12,.001)\\n>\\nf = dchisq(x,5)\\n>\\nplot(x,f )\\n5.8.1.1\\nThe relation between chi-square and gamma random\\nvariables7\\nLet us compute the moment generating function of a chi-square random vari-\\nable with n degrees of freedom. To begin, we have, when n = 1, that\\nE[et X] = E[et Z2] where Z ∼N(0,1)\\n(5.8.2)\\n=\\n\\x10 ∞\\n−∞\\net x2fZ(x) dx\\n=\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\net x2e−x2/2 dx\\n=\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\ne−x2(1−2t)/2 dx\\n=\\n1\\n√\\n2π\\n\\x10 ∞\\n−∞\\ne−x2/2¯σ 2 dx\\nwhere ¯σ 2 = (1 −2t)−1\\n= (1 −2t)−1/2\\n1\\n√\\n2π ¯σ\\n\\x10 ∞\\n−∞\\ne−x2/2¯σ 2 dx\\n= (1 −2t)−1/2\\n7Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 217}, page_content='5.8 Distributions arising from the normal\\n205\\nFIGURE 5.12\\nThe chi-square density function with n degrees of freedom.\\nwhere the last equality follows since the integral of the normal (0, ¯σ 2) density\\nequals 1. Hence, in the general case of n degrees of freedom\\nE[e t X] = E\\n\\x18\\ne t \\nn\\ni=1 Z2\\ni\\n\\x19\\n= E\\n\\x1a n\\n\\x16\\ni=1\\ne t Z2\\ni\\n\\x1b\\n=\\nn\\n\\x16\\ni=1\\nE[e t Z2\\ni ]\\nby independence of the Zi\\n= (1 −2t)−n/2\\nfrom Equation (5.8.2)\\nwhich we recognize as being the moment generating function of a gamma ran-\\ndom variable with parameters (n/2,1/2). Hence, by the uniqueness of moment\\ngenerating functions, it follows that these two distributions — chi-square with\\nn degrees of freedom and gamma with parameters n/2 and 1/2 — are identical,\\nand thus we can conclude that the density of X is given by\\nf (x) =\\n1\\n2e−x/2 \\x07x\\n2\\n\\x08(n/2)−1\\n\\t\\n\\x07n\\n2\\n\\x08\\n,\\nx > 0\\nThe chi-square density functions having 1, 3, and 10 degrees of freedom, re-\\nspectively, are plotted in Figure 5.12.\\nLet us reconsider Example 5.8.c, this time supposing that the target is located\\nin the two-dimensional plane.\\nExample 5.8.d. When we attempt to locate a target in two-dimensional space,\\nsuppose that the coordinate errors are independent normal random variables\\nwith mean 0 and standard deviation 2. Find the probability that the distance\\nbetween the point chosen and the target exceeds 3.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 218}, page_content='206 CHAPTER 5: Special random variables\\nFIGURE 5.13\\nDensity function of Tn.\\nSolution. If D is the distance and Xi,i = 1,2, are the coordinate errors, then\\nD2 = X2\\n1 + X2\\n2\\nSince Zi = Xi/2, i = 1,2, are standard normal random variables, we obtain\\nP{D2 > 9} = P {Z2\\n1 + Z2\\n2 > 9/4} = P{χ2\\n2 > 9/4} = e−9/8 ≈.3247\\nwhere the preceding calculation used the fact that the chi-square distribution\\nwith 2 degrees of freedom is the same as the exponential distribution with\\nparameter 1/2.\\n■\\nSince the chi-square distribution with n degrees of freedom is identical to the\\ngamma distribution with parameters α =n/2 and λ=1/2, it follows from Equa-\\ntions (5.7.3) and (5.7.4) that the mean and variance of a random variable X\\nhaving this distribution is\\nE[X] = n,\\nVar(X) = 2n\\n5.8.2\\nThe t-distribution\\nIf Z and χ2\\nn are independent random variables, with Z having a standard nor-\\nmal distribution and χ2\\nn having a chi-square distribution with n degrees of\\nfreedom, then the random variable Tn deﬁned by\\nTn =\\nZ\\n\\x1c\\nχ2n/n\\nis said to have a t-distribution with n degrees of freedom. A graph of the density\\nfunction of Tn is given in Figure 5.13 for n = 1,5, and 10.\\nLike the standard normal density, the t-density is symmetric about zero. In ad-\\ndition, as n becomes larger, it becomes more and more like a standard normal'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 219}, page_content='5.8 Distributions arising from the normal\\n207\\nFIGURE 5.14\\nComparing standard normal density with the density of T5.\\ndensity. To understand why, recall that χ2\\nn can be expressed as the sum of the\\nsquares of n standard normals, and so\\nχ2\\nn\\nn = Z2\\n1 + ··· + Z2\\nn\\nn\\nwhere Z1,...,Zn are independent standard normal random variables. It now\\nfollows from the weak law of large numbers that, for large n, χ2\\nn/n will, with\\nprobability close to 1, be approximately equal to E[Z2\\ni ]=1. Hence, for n large,\\nTn =Z/\\n\\x1c\\nχ2n/n will have approximately the same distribution as Z.\\nFigure 5.14 shows a graph of the t-density function with 5 degrees of free-\\ndom compared with the standard normal density. Notice that the t-density has\\nthicker “tails,” indicating greater variability, than does the normal density.\\nThe mean and variance of Tn can be shown to equal\\nE[Tn] = 0,\\nn > 1\\nVar(Tn) =\\nn\\nn −2,\\nn > 2\\nThus the variance of Tn decreases to 1 — the variance of a standard normal\\nrandom variable — as n increases to ∞. For α,0 < α < 1, let tα,n be such that\\nP{Tn ≥tα,n} = α\\nIt follows from the symmetry about zero of the t-density function that −Tn has\\nthe same distribution as Tn, and so\\nα = P{−Tn ≥tα,n}\\n= P{Tn ≤−tα,n}\\n= 1 −P{Tn > −tα,n}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 220}, page_content='208 CHAPTER 5: Special random variables\\nFIGURE 5.15\\nt1−α,n = −tα,n.\\nTherefore,\\nP{Tn ≥−tα,n} = 1 −α\\nleading to the conclusion that\\n−tα,n = t1−α,n\\nwhich is illustrated in Figure 5.15.\\nR can be utilized to obtain probabilities of t random variables, as well as the\\nvalues tα,n. With Tn being t with n degrees of freedom\\npt(x,n)\\nreturns\\nP(Tn ≤x)\\nqt(1 −α,n)\\nreturns\\ntα,n\\nExample 5.8.e. Find (a) P{T12 ≤1.4} and (b) t.025,9.\\nSolution. R yields the result\\n>\\npt(1.4,12)\\n[1]\\n0.9065835\\n>\\nqt(1 −.025,9)\\n[1]\\n2.262157\\n■\\n5.8.3\\nThe F-distribution\\nIf χ2\\nn and χ2\\nm are independent chi-square random variables with n and m de-\\ngrees of freedom, respectively, then the random variable Fn,m deﬁned by\\nFn,m = χ2\\nn/n\\nχ2m/m\\nis said to have an F-distribution with n and m degrees of freedom.\\nFor any α ∈(0,1), let Fα,n,m be such that\\nP{Fn,m > Fα,n,m} = α'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 221}, page_content='5.9 The logistics distribution\\n209\\nFIGURE 5.16\\nDensity function of Fn,m.\\nThis is illustrated in Figure 5.16.\\nWith f being R notation for an F distribution with n numerator and m denom-\\ninator degrees of freedom, R can be used to obtain distribution probabilities\\nas well as values Fα,n,m. For instance, if F6,14 is an F random variable with nu-\\nmerator and denominator degrees of freedom 6 and 14, then P(F6,14 ≤1.5) is\\nobtained thus:\\n>\\npf(1.5,6,14)\\n[1]\\n0.7515004\\nSimilarly, to obtain F.01,6,14 do\\n>\\nqf(.99,6,14)\\n[1]\\n4.45582\\n5.9\\nThe logistics distribution8\\nA random variable X is said to have a logistics distribution with parameters μ\\nand v > 0 if its distribution function is\\nF(x) =\\ne(x−μ)/v\\n1 + e(x−μ)/v ,\\n−∞< x < ∞\\nDifferentiating F(x) = 1 −1/(1 + e(x−μ)/v) yields the density function\\nf (x) =\\ne(x−μ)/v\\nv(1 + e(x−μ)/v)2 ,\\n−∞< x < ∞\\nTo obtain the mean of a logistics random variable,\\nE[X] =\\n\\x10 ∞\\n−∞\\nx\\ne(x−μ)/v\\nv(1 + e(x−μ)/v)2 dx\\n8Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 222}, page_content='210 CHAPTER 5: Special random variables\\nmake the substitution y = (x −μ)/v. This yields\\nE[X] = v\\n\\x10 ∞\\n−∞\\nyey\\n(1 + ey)2 dy + μ\\n\\x10 ∞\\n−∞\\ney\\n(1 + ey)2 dy\\n= v\\n\\x10 ∞\\n−∞\\nyey\\n(1 + ey)2 dy + μ\\n(5.9.1)\\nwhere the preceding equality used that ey/((1 + ey)2) is the density function\\nof a logistic random variable with parameters μ = 0, v = 1 (such a random\\nvariable is called a standard logistic) and thus integrates to 1. Now,\\n\\x10 ∞\\n−∞\\nyey\\n(1 + ey)2 dy =\\n\\x10 0\\n−∞\\nyey\\n(1 + ey)2 dy +\\n\\x10 ∞\\n0\\nyey\\n(1 + ey)2 dy\\n= −\\n\\x10 ∞\\n0\\nxe−x\\n(1 + e−x)2 dx +\\n\\x10 ∞\\n0\\nyey\\n(1 + ey)2 dy\\n= −\\n\\x10 ∞\\n0\\nxex\\n(ex + 1)2 dx +\\n\\x10 ∞\\n0\\nyey\\n(1 + ey)2 dy\\n= 0\\n(5.9.2)\\nwhere the second equality is obtained by making the substitution x = −y, and\\nthe third by multiplying the numerator and denominator by e2x. From Equa-\\ntions (5.9.1) and (5.9.2) we obtain\\nE[X] = μ\\nThus μ is the mean of the logistic; v is called the dispersion parameter.\\n5.10\\nDistributions in R\\nTable 5.2 are the names of some common distributions in R.\\nTable 5.3 are some R commands.\\nFor instance, to plot the density of an F random variable with degrees of free-\\ndom 5 and 3 going from x = 0 to x = 10 in increments of size .01 do the\\nfollowing.\\n>\\nx = seq(0,10,.01)\\n>\\nf = df(x,5,3)\\n>\\nplot(x,f )\\nF5,3 Density'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 223}, page_content='5.10 Distributions in R 211\\nTable 5.2 Distribution Names in R.\\nDistribution\\nName in R\\nbinomial with parameters n,p\\nbinom(n,p)\\nPoisson with parameter λ\\npois(λ)\\nhypergeometric with parameters N,M,n\\nhyper(N,M,n)\\nstandard normal\\nnorm\\nnormal with parameters μ,σ 2\\nnorm(μ,σ)\\nchi-square with n degrees of freedom\\nchisq(n)\\nT -distribution with n degrees of freedom\\nt(n)\\nF-distribution with degrees of freedom n,m)\\nf (n,m)\\nexponential with rate 1\\nexp\\nexponential with rate λ\\nexp(λ)\\ngamma with parameters α,λ\\ngamma(α,λ)\\nlogistics with parameters μ,v\\nlogis(μ,v)\\nuniform on (0,1)\\nunif\\nuniform on (a,b)\\nunif(a,b)\\nTable 5.3 R commands.\\ndname(x )\\ndensity or mass function\\npname( x)\\ncumulative distribution function\\nqname(β)\\nquantile function\\nplot(x,y)\\nplots y as function of x for suitably\\ndeﬁned x and y'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 224}, page_content='212 CHAPTER 5: Special random variables\\nProblems\\n1. A satellite system consists of 4 components and can function adequately\\nif at least 2 of the 4 components are in working condition. If each com-\\nponent is, independently, in working condition with probability .6, what\\nis the probability that the system functions adequately?\\n2. A communications channel transmits the digits 0 and 1. However, due\\nto static, the digit transmitted is incorrectly received with probability .2.\\nSuppose that we want to transmit an important message consisting of\\none binary digit. To reduce the chance of error, we transmit 00000 instead\\nof 0 and 11111 instead of 1. If the receiver of the message uses “majority”\\ndecoding, what is the probability that the message will be incorrectly\\ndecoded? What independence assumptions are you making? (By majority\\ndecoding we mean that the message is decoded as “0” if there are at least\\nthree zeros in the message received and as “1” otherwise.)\\n3. If each voter is for Proposition A with probability .7, what is the proba-\\nbility that exactly 7 of 10 voters are for this proposition?\\n4. Suppose that a particular trait (such as eye color or left-handedness) of\\na person is classiﬁed on the basis of one pair of genes, and suppose that\\nd represents a dominant gene and r a recessive gene. Thus, a person with\\ndd genes is pure dominance, one with rr is pure recessive, and one with\\nrd is hybrid. The pure dominance and the hybrid are alike in appearance.\\nChildren receive 1 gene from each parent. If, with respect to a particular\\ntrait, 2 hybrid parents have a total of 4 children, what is the probability\\nthat 3 of the 4 children have the outward appearance of the dominant\\ngene?\\n5. At least one-half of an airplane’s engines are required to function in order\\nfor it to operate. If each engine independently functions with probability\\np, for what values of p is a 4-engine plane more likely to operate than a\\n2-engine plane?\\n6. Let X be a binomial random variable with\\nE[X] = 7\\nand\\nVar(X) = 2.1\\nFind\\na.\\nP{X = 4};\\nb.\\nP{X >12}.\\n7. If X and Y are binomial random variables with respective parameters\\n(n,p) and (n,1 −p), verify and explain the following identities:\\na.\\nP{X ≤i} = P{Y ≥n −i};\\nb.\\nP{X = k} = P{Y = n −k}.\\n8. If X is a binomial random variable with parameters n and p, where 0 <\\np < 1, show that\\na.\\nP{X = k + 1} =\\np\\n1 −p\\nn −k\\nk + 1P{X = k}, k = 0,1,...,n −1.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 225}, page_content='Problems 213\\nb.\\nAs k goes from 0 to n, P{X = k} ﬁrst increases and then decreases,\\nreaching its largest value when k is the largest integer less than or\\nequal to (n + 1)p.\\n9. An experiment has possible outcomes 1,2,...,r, with i being the out-\\ncome with probability pi, \\nr\\ni=1 pi = 1. Suppose there are n independent\\nreplications of this experiment, and let Ni be the number of them that\\nresult in outcome i, i = 1,...,r.\\na.\\nWhat is the distribution of N1?\\nb.\\nAre N1 and N2 independent?\\nc.\\nWhat is the distribution of N1 + N2?\\nHint: Use the fact that N1 + N2 is the number of the n experiments\\nthat result in either outcome 1 or outcome 2.\\nd.\\nFor k < r, what is the distribution of \\nk\\ni=1 Ni?\\n10. Compare the Poisson approximation with the correct binomial proba-\\nbility for the following cases:\\na.\\nP{X = 2} when n = 10, p = .1;\\nb.\\nP{X = 0} when n = 10, p = .1;\\nc.\\nP{X = 4} when n = 9, p = .2.\\n11. If you buy a lottery ticket in 50 lotteries, in each of which your chance\\nof winning a prize is\\n1\\n100, what is the (approximate) probability that you\\nwill win a prize (a) at least once, (b) exactly once, and (c) at least twice?\\n12. The number of times that an individual contracts a cold in a given year is\\na Poisson random variable with parameter λ = 3. Suppose a new wonder\\ndrug (based on large quantities of vitamin C) has just been marketed that\\nreduces the Poisson parameter to λ = 2 for 75 percent of the population.\\nFor the other 25 percent of the population, the drug has no appreciable\\neffect on colds. If an individual tries the drug for a year and has 0 colds\\nin that time, how likely is it that the drug is beneﬁcial for him or her?\\n13. In the 1980s, an average of 121.95 workers died on the job each week.\\nGive estimates of the following quantities:\\na.\\nthe proportion of weeks having 130 deaths or more;\\nb.\\nthe proportion of weeks having 100 deaths or less.\\nExplain your reasoning.\\n14. Approximately 80,000 marriages took place in the state of New York last\\nyear. Estimate the probability that for at least one of these couples\\na.\\nboth partners were born on April 30;\\nb.\\nboth partners celebrated their birthday on the same day of the year.\\nState your assumptions.\\n15. The game of frustration solitaire is played by turning the cards of a ran-\\ndomly shufﬂed deck of 52 playing cards over one at a time. Before you\\nturn over the ﬁrst card, say ace; before you turn over the second card,\\nsay two; before you turn over the third card, say three. Continue in this\\nmanner (saying ace again before turning over the fourteenth card, and\\nso on). You lose if you ever turn over a card that matches what you have'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 226}, page_content='214 CHAPTER 5: Special random variables\\njust said. Use the Poisson paradigm to approximate the probability of\\nwinning. (The actual probability is .01623.)\\n16. The probability of error in the transmission of a binary digit over a\\ncommunication channel is 1/103. Write an expression for the exact prob-\\nability of more than 3 errors when transmitting a block of 103 bits. What\\nis its approximate value? Assume independence.\\n17. If X is a Poisson random variable with mean λ, show that P{X = i } ﬁrst\\nincreases and then decreases as i increases, reaching its maximum value\\nwhen i is the largest integer less than or equal to λ.\\n18. A contractor purchases a shipment of 100 transistors. It is his policy to\\ntest 10 of these transistors and to keep the shipment only if at least 9 of\\nthe 10 are in working condition. If the shipment contains 20 defective\\ntransistors, what is the probability it will be kept?\\n19. Let X denote a hypergeometric random variable with parameters n, m,\\nand k. That is,\\nP{X = i} =\\n\\x07 n\\ni\\n\\x08\\x02\\nm\\nk−i\\n\\x03\\n\\x07 n+m\\nk\\n\\x08\\n,\\ni = 0,1,...,min(k,n)\\na.\\nDerive a formula for P{X = i} in terms of P{X = i −1}.\\nb.\\nUse part (a) to compute P{X = i} for i = 0,1,2,3,4,5 when n = m =\\n10, k = 5, by starting with P{X = 0}.\\nc.\\nBased on the recursion in part (a), write a program to compute the\\nhypergeometric distribution function.\\nd.\\nUse your program from part (c) to compute P{X ≤10} when n =\\nm = 30, k = 15.\\n20. Independent trials, each of which is a success with probability p, are suc-\\ncessively performed. Let X denote the ﬁrst trial resulting in a success. That\\nis, X will equal k if the ﬁrst k −1 trials are all failures and the kth a success.\\nX is called a geometric random variable. Compute\\na.\\nP{X = k}, k = 1,2,...;\\nb.\\nE[X].\\nLet Y denote the number of trials needed to obtain r successes. Y is\\ncalled a negative binomial random variable. Compute\\nc.\\nP{Y = k}, k = r,r + 1,....\\n(Hint: In order for Y to equal k, how many successes must result in\\nthe ﬁrst k −1 trials and what must be the outcome of trial k?)\\nd.\\nShow that\\nE[Y] = r/p\\n(Hint: Write Y = Y1 +···+Yr where Yi is the number of trials needed\\nto go from a total of i −1 to a total of i successes.)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 227}, page_content='Problems 215\\n21. If U is uniformly distributed on (0, 1), show that a + (b −a)U is uniform\\non (a,b).\\n22. You arrive at a bus stop at 10 o’clock, knowing that the bus will arrive\\nat some time uniformly distributed between 10 and 10:30. What is the\\nprobability that you will have to wait longer than 10 minutes? If at 10:15\\nthe bus has not yet arrived, what is the probability that you will have to\\nwait at least an additional 10 minutes?\\n23. Let X1 and X2 be independent normal random variables, each having\\nmean 10 and variance σ 2. Which probability is larger:\\na.\\nP(X1 > 15) or P(X1 + X2 > 25);\\nb.\\nP(X1 > 15) or P(X1 + X2 > 30)?\\nc.\\nFind x such that P(X1 + X2 > x) = P(X1 > 15).\\n24. The Scholastic Aptitude Test mathematics test scores across the popula-\\ntion of high school seniors follow a normal distribution with mean 500\\nand standard deviation 100. If ﬁve seniors are randomly chosen, ﬁnd the\\nprobability that (a) all scored below 600 and (b) exactly three of them\\nscored above 640.\\n25. The annual rainfall (in inches) in a certain region is normally distributed\\nwith μ = 40, σ = 4. What is the probability that in 2 of the next 4 years\\nthe rainfall will exceed 50 inches? Assume that the rainfalls in different\\nyears are independent.\\n26. The weekly demand for a product approximately has a normal distribu-\\ntion with mean 1000 and standard deviation 200. The current on hand\\ninventory is 2200 and no deliveries will be occurring in the next two\\nweeks. Assuming that the demands in different weeks are independent,\\na.\\nwhat is the probability that the demand in each of the next 2 weeks\\nis less than 1100?\\nb.\\nwhat is the probability that the total of the demands in the next 2\\nweeks exceeds 2200?\\n27. Let X be normal with mean μ and variance σ 2. For ﬁxed μ, show that\\nP(X > 10) is an increasing function of σ when μ < 10, and a decreasing\\nfunction of σ when μ > 10. Give an intuitive reason why the preceding is\\ntrue.\\n28. A manufacturer produces bolts that are speciﬁed to be between 1.19 and\\n1.21 inches in diameter. If its production process results in a bolt’s di-\\nameter being normally distributed with mean 1.20 inches and standard\\ndeviation .005, what percentage of bolts will not meet speciﬁcations?\\n29. Let I =\\n\\x17 ∞\\n−∞e−x2/2 dx.\\na.\\nShow that for any μ and σ\\n1\\n√\\n2πσ\\n\\x10 ∞\\n−∞\\ne−(x−μ)2/2σ 2 dx = 1\\nis equivalent to I =\\n√\\n2π.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 228}, page_content='216 CHAPTER 5: Special random variables\\nb.\\nShow that I =\\n√\\n2π by writing\\nI 2 =\\n\\x10 ∞\\n−∞\\ne−x2/2 dx\\n\\x10 ∞\\n−∞\\ne−y2/2 dy =\\n\\x10 ∞\\n−∞\\n\\x10 ∞\\n−∞\\ne−(x2+y2)/2 dx dy\\nand then evaluating the double integral by means of a change of\\nvariables to polar coordinates. (That is, let x = r cosθ, y = r sinθ,\\ndx dy = r dr dθ.)\\n30. A random variable X is said to have a lognormal distribution if log\\nX is normally distributed. If X is lognormal with E[logX] = μ and\\nVar(logX) = σ 2, determine the distribution function of X. That is, what\\nis P{X ≤x}?\\n31. The salaries of pediatric physicians are approximately normally dis-\\ntributed. If 25 percent of these physicians earn below 180,000 and 25\\npercent earn above 320,000, what fraction earn\\na.\\nbelow 250,000;\\nb.\\nbetween 260,000 and 300,000?\\n32. The sample mean and sample standard deviation on your economics ex-\\namination were 60 and 20, respectively; the sample mean and sample\\nstandard deviation on your statistics examination were 55 and 10, re-\\nspectively. You scored 70 on the economics exam and 62 on the statistics\\nexam. Assuming that the two histograms of test scores are approximately\\nnormal histograms,\\na.\\non which exam was your percentile score highest?\\nb.\\napproximate the percentage of the scores on the economics exam\\nthat were below your score.\\nc.\\napproximate the percentage of the scores on the statistics exam that\\nwere below your score.\\n33. Value at risk (VAR) has become a key concept in ﬁnancial calculations.\\nThe VAR of an investment is deﬁned as that value v such that there is\\nonly a 1 percent chance that the loss from the investment will exceed v.\\na.\\nIf the gain from an investment is a normal random variable with\\nmean 10 and variance 49, determine the value at risk. (If X is the\\ngain, then −X is the loss.)\\nb.\\nAmong a set of investments whose gains are all normally distributed\\nshow that the one having the smallest VAR is the one having the\\nlargest value of μ−2.33σ, where μ and σ 2 are the mean and variance\\nof the gain from the investment.\\n34. The annual rainfall in Cincinnati is normally distributed with mean\\n40.14 inches and standard deviation 8.7 inches.\\na.\\nWhat is the probability this year’s rainfall will exceed 42 inches?\\nb.\\nWhat is the probability that the sum of the next 2 years’ rainfall will\\nexceed 84 inches?\\nc.\\nWhat is the probability that the sum of the next 3 years’ rainfall will\\nexceed 126 inches?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 229}, page_content='Problems\\n217\\nd.\\nFor parts (b) and (c), what independence assumptions are you mak-\\ning?\\n35. The height of adult women in the United States is normally distributed\\nwith mean 64.5 inches and standard deviation 2.4 inches. Find the prob-\\nability that a randomly chosen woman is\\na.\\nless than 63 inches tall;\\nb.\\nless than 70 inches tall;\\nc.\\nbetween 63 and 70 inches tall.\\nd.\\nAlice is 72 inches tall. What percentage of women is shorter than\\nAlice?\\ne.\\nFind the probability that the average of the heights of two randomly\\nchosen women exceeds 66 inches.\\nf.\\nRepeat part (e) for four randomly chosen women.\\n36. An IQ test produces scores that are normally distributed with mean value\\n100 and standard deviation 14.2. The top 1 percent of all scores are in\\nwhat range?\\n37. The time (in hours) required to repair a machine is an exponentially\\ndistributed random variable with parameter λ = 1.\\na.\\nWhat is the probability that a repair time exceeds 2 hours?\\nb.\\nWhat is the conditional probability that a repair takes at least 3\\nhours, given that its duration exceeds 2 hours?\\n38. The number of years a radio functions is exponentially distributed with\\nparameter λ = 1\\n8. If Jones buys a used radio, what is the probability that\\nit will be working after an additional 10 years?\\n39. Jones ﬁgures that the total number of thousands of miles that a used\\nauto can be driven before it would need to be junked is an exponential\\nrandom variable with parameter 1\\n20. Smith has a used car that he claims\\nhas been driven only 10,000 miles. If Jones purchases the car, what is the\\nprobability that she would get at least 20,000 additional miles out of it?\\nRepeat under the assumption that the lifetime mileage of the car is not\\nexponentially distributed but rather is (in thousands of miles) uniformly\\ndistributed over (0, 40).\\n40.9 Let X1,X2,...,Xn denote the ﬁrst n interarrival times of a Poisson pro-\\ncess and set Sn = \\nn\\ni=1 Xi.\\na.\\nWhat is the interpretation of Sn?\\nb.\\nArgue that the two events {Sn ≤t} and {N(t) ≥n} are identical.\\nc.\\nUse part (b) to show that\\nP{Sn ≤t} = 1 −\\nn−1\\n\\x06\\nj=0\\ne−λt(λt) j/j!\\n9From optional sections.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 230}, page_content='218 CHAPTER 5: Special random variables\\nd.\\nBy differentiating the distribution function of Sn given in part (c),\\nconclude that Sn is a gamma random variable with parameters n\\nand λ. (This result also follows from Corollary 5.7.2.)\\n41.10 Earthquakes occur in a given region in accordance with a Poisson pro-\\ncess with rate 5 per year.\\na.\\nWhat is the probability there will be at least two earthquakes in the\\nﬁrst half of 2015?\\nb.\\nAssuming that the event in part (a) occurs, what is the probability\\nthat there will be no earthquakes during the ﬁrst 9 months of 2016?\\nc.\\nAssuming that the event in part (a) occurs, what is the probability\\nthat there will be at least four earthquakes over the ﬁrst 9 months of\\nthe year 2015?\\n42.11 When shooting at a target in a two-dimensional plane, suppose that the\\nhorizontal miss distance is normally distributed with mean 0 and vari-\\nance 4 and is independent of the vertical miss distance, which is also\\nnormally distributed with mean 0 and variance 4. Let D denote the dis-\\ntance between the point at which the shot lands and the target. Find E[D].\\n43. If X is a chi-square random variable with 6 degrees of freedom, ﬁnd\\na.\\nP{X ≤6};\\nb.\\nP{3 ≤X ≤9}.\\n44. If X and Y are independent chi-square random variables with 3 and 6\\ndegrees of freedom, respectively, determine the probability that X + Y\\nwill exceed 10.\\n45. Show that \\t(1/2) = √π (Hint: Evaluate\\n\\x17 ∞\\n0 e−xx−1/2 dx by letting x =\\ny2/2, dx = y dy.)\\n46. If T has a t-distribution with 8 degrees of freedom, ﬁnd (a) P{T ≥1},\\n(b) P{T ≤2}, and (c) P{−1 < T < 1}.\\n47. If Tn has a t-distribution with n degrees of freedom, show that T 2\\nn has an\\nF-distribution with 1 and n degrees of freedom.\\n48. Let \\x08 be the standard normal distribution function. If, for constants a\\nand b >0\\nP{X ≤x} = \\x08\\n\\x02x −a\\nb\\n\\x03\\ncharacterize the distribution of X.\\n49.12 Suppose that Y has a Pareto distribution with minimal parameter α and\\nindex parameter λ.\\na.\\nFind E[Y] when λ > 1, and show that E[Y] = ∞when λ ≤1.\\nb.\\nFind Var(Y) when λ > 2.\\n10From optional sections.\\n11From optional sections.\\n12From optional sections.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 231}, page_content='Problems\\n219\\n50.13 Suppose that Y = αeX, where X is exponential with rate λ. Use the lack\\nof memory property of the exponential to argue that the conditional dis-\\ntribution of Y given that Y > y0 > α is Pareto with parameters y0 and λ.\\n13From optional sections.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 232}, page_content='CHAPTER 6\\nDistributions of sampling statistics\\n6.1\\nIntroduction\\nThe science of statistics deals with drawing conclusions from observed data.\\nFor instance, a typical situation in a technological study arises when one is\\nconfronted with a large collection, or population, of items that have measurable\\nvalues associated with them. By suitably sampling from this collection, and then\\nanalyzing the sampled items, one hopes to be able to draw some conclusions\\nabout the collection as a whole.\\nTo use sample data to make inferences about an entire population, it is neces-\\nsary to make some assumptions about the relationship between the two. One\\nsuch assumption, which is often quite reasonable, is that there is an underly-\\ning (population) probability distribution such that the measurable values of\\nthe items in the population can be thought of as being independent random\\nvariables having this distribution. If the sample data are then chosen in a ran-\\ndom fashion, then it is reasonable to suppose that they too are independent\\nvalues from the distribution.\\nDeﬁnition. If X1,...,Xn are independent random variables having a common\\ndistribution F, then we say that they constitute a sample (sometimes called a\\nrandom sample) from the distribution F.\\nIn most applications, the population distribution F will not be completely\\nspeciﬁed and one will attempt to use the data to make inferences about F.\\nSometimes it will be supposed that F is speciﬁed up to some unknown pa-\\nrameters (for instance, one might suppose that F was a normal distribution\\nfunction having an unknown mean and variance, or that it is a Poisson dis-\\ntribution function whose mean is not given), and at other times it might be\\nassumed that almost nothing is known about F (except maybe for assuming\\nthat it is a continuous, or a discrete, distribution). Problems in which the form\\nof the underlying distribution is speciﬁed up to a set of unknown parameters\\nare called parametric inference problems, whereas those in which nothing is\\nassumed about the form of F are called nonparametric inference problems.\\nExample 6.1.a. Suppose that a new process has just been installed to produce\\ncomputer chips, and suppose that the successive chips produced by this new\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00015-6\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n221'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 233}, page_content='222 CHAPTER 6: Distributions of sampling statistics\\nprocess will have useful lifetimes that are independent with a common un-\\nknown distribution F. Physical reasons sometimes suggest the parametric form\\nof the distribution F; for instance, it may lead us to believe that F is a normal\\ndistribution, or that F is an exponential distribution. In such cases, we are con-\\nfronted with a parametrical statistical problem in which we would want to use\\nthe observed data to estimate the parameters of F. For instance, if F were as-\\nsumed to be a normal distribution, then we would want to estimate its mean\\nand variance; if F were assumed to be exponential, we would want to estimate\\nits mean. In other situations, there might not be any physical justiﬁcation for\\nsupposing that F has any particular form; in this case the problem of making\\ninferences about F would constitute a nonparametric inference problem.\\n■\\nIn this chapter, we will be concerned with the probability distributions of cer-\\ntain statistics that arise from a sample, where a statistic is a random variable\\nwhose value is determined by the sample data. Two important statistics that\\nwe will discuss are the sample mean and the sample variance. In Section 6.2,\\nwe consider the sample mean and derive its expectation and variance. We note\\nthat when the sample size is at least moderately large, the distribution of the\\nsample mean is approximately normal. This follows from the central limit the-\\norem, one of the most important theoretical results in probability, which is\\ndiscussed in Section 6.3. In Section 6.4, we introduce the sample variance and\\ndetermine its expected value. In Section 6.5, we suppose that the population\\ndistribution is normal and present the joint distribution of the sample mean\\nand the sample variance. In Section 6.6, we suppose that we are sampling from\\na ﬁnite population of elements and explain what it means for the sample to be\\na “random sample.” When the population size is large in relation to the sam-\\nple size, we often treat it as if it were of inﬁnite size; this is illustrated and its\\nconsequences are discussed.\\n6.2\\nThe sample mean\\nConsider a population of elements, each of which has a numerical value at-\\ntached to it. For instance, the population might consist of the adults of a\\nspeciﬁed community and the value attached to each adult might be his or\\nher annual income, or height, or age, and so on. We often suppose that the\\nvalue associated with any member of the population can be regarded as be-\\ning the value of a random variable having expectation μ and variance σ 2. The\\nquantities μ and σ 2 are called the population mean and the population variance,\\nrespectively. Let X1,X2,...,Xn be a sample of values from this population. The\\nsample mean is deﬁned by\\nX = X1 + ··· + Xn\\nn\\nSince the value of the sample mean X is determined by the values of the ran-\\ndom variables in the sample, it follows that X is also a random variable. Its'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 234}, page_content='6.2 The sample mean 223\\nexpected value and variance are obtained as follows:\\nE[X] = E\\n\\x02X1 + ··· + Xn\\nn\\n\\x03\\n= 1\\nn(E[X1] + ··· + E[Xn])\\n= μ\\nand\\nVar(X) = Var\\n\\x04X1 + ··· + Xn\\nn\\n\\x05\\n= 1\\nn2 [Var(X1) + ··· + Var(Xn)]\\nby independence\\n= nσ 2\\nn2\\n= σ 2\\nn\\nwhere μ and σ 2 are the population mean and variance, respectively. Hence,\\nthe expected value of the sample mean is the population mean μ whereas its\\nvariance is 1/n times the population variance. As a result, we can conclude that\\nX is also centered about the population mean μ, but its spread becomes more\\nand more reduced as the sample size increases. Figure 6.1 plots the probability\\ndensity function of the sample mean from a standard normal population for a\\nvariety of sample sizes.\\nFIGURE 6.1\\nDensities of sample means from a standard normal population.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 235}, page_content='224 CHAPTER 6: Distributions of sampling statistics\\n6.3\\nThe central limit theorem\\nIn this section, we will consider one of the most remarkable results in probabil-\\nity — namely, the central limit theorem. Loosely speaking, this theorem asserts\\nthat the sum of a large number of independent random variables has a dis-\\ntribution that is approximately normal. Hence, it not only provides a simple\\nmethod for computing approximate probabilities for sums of independent\\nrandom variables, but it also helps explain the remarkable fact that the em-\\npirical frequencies of so many natural populations exhibit a bell-shaped (that\\nis, a normal) curve.\\nIn its simplest form, the central limit theorem is as follows:\\nTheorem 6.3.1 (The central limit theorem). Let X1,X2,...,Xn be a sequence\\nof independent and identically distributed random variables each having mean\\nμ and variance σ 2. Then for n large, the distribution of\\nX1 + ··· + Xn\\nis approximately normal with mean nμ and variance nσ 2.\\nIt follows from the central limit theorem that\\nX1 + ··· + Xn −nμ\\nσ√n\\nis approximately a standard normal random variable; thus, for n large,\\nP\\n\\x06X1 + ··· + Xn −nμ\\nσ√n\\n< x\\n\\x07\\n≈P{Z < x}\\nwhere Z is a standard normal random variable.\\nExample 6.3.a. An insurance company has 25,000 automobile policy holders.\\nIf the yearly claim of a policy holder is a random variable with mean 320 and\\nstandard deviation 540, approximate the probability that the total yearly claim\\nexceeds 8.3 million.\\nSolution. Let X denote the total yearly claim. Number the policy holders, and\\nlet Xi denote the yearly claim of policy holder i. With n = 25,000, we have\\nfrom the central limit theorem that X = \\x08n\\ni=1 Xi will have approximately a\\nnormal distribution with mean 320 × 25,000 = 8 × 106 and standard deviation\\n540√25,000 = 8.5381 × 104. Therefore,\\nP{X > 8.3 × 106} = P\\n\\x06 X −8 × 106\\n8.5381 × 104 > 8.3 × 106 −8 × 106\\n8.5381 × 104\\n\\x07\\n= P\\n\\x06 X −8 × 106\\n8.5381 × 104 >\\n.3 × 106\\n8.5381 × 104\\n\\x07'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 236}, page_content='6.3 The central limit theorem\\n225\\n≈P{Z > 3.51}\\nwhere Z is a standard normal\\n≈.00023\\nThus, there are only 2.3 chances out of 10,000 that the total yearly claim will\\nexceed 8.3 million.\\nOf course, we could have used R to calculate the desired probability:\\n> 1 −pnorm(8.3 ∗(10)6,320 ∗25000,540 ∗sqrt(25000))\\n[1] 0.0002210042\\n■\\nExample 6.3.b. Civil engineers believe that W, the amount of weight (in units\\nof 1000 pounds) that a certain span of a bridge can withstand without struc-\\ntural damage resulting, is normally distributed with mean 400 and standard\\ndeviation 40. Suppose that the weight (again, in units of 1000 pounds) of a car\\nis a random variable with mean 3 and standard deviation .3. How many cars\\nwould have to be on the bridge span for the probability of structural damage\\nto exceed .1?\\nSolution. Let Pn denote the probability of structural damage when there are n\\ncars on the bridge. That is,\\nPn = P{X1 + ··· + Xn ≥W}\\n= P{X1 + ··· + Xn −W ≥0}\\nwhere Xi is the weight of the ith car, i = 1,...,n. Now it follows from the\\ncentral limit theorem that \\x08n\\ni=1 Xi is approximately normal with mean 3n and\\nvariance .09n. Hence, since W is independent of the Xi,i = 1,...,n, and is also\\nnormal, it follows that \\x08n\\ni=1 Xi −W is approximately normal, with mean and\\nvariance given by\\nE\\n\\t n\\n\\n1\\nXi −W\\n\\x0b\\n= 3n −400\\nVar\\n\\x0c n\\n\\n1\\nXi −W\\n\\r\\n= Var\\n\\x0c n\\n\\n1\\nXi\\n\\r\\n+ Var(W) = .09n + 1,600\\nThus,\\nPn\\n=\\nP\\n\\x06X1 + ··· + Xn −W −(3n −400)\\n√.09n + 1,600\\n≥−(3n −400)\\n√.09n + 1,600\\n\\x07\\n≈\\nP\\n\\x06\\nZ ≥\\n400 −3n\\n√.09n + 1,600\\n\\x07'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 237}, page_content='226 CHAPTER 6: Distributions of sampling statistics\\nwhere Z is a standard normal random variable. Now P{Z ≥1.28} ≈.1, and so\\nif the number of cars n is such that\\n400 −3n\\n√.09n + 1,600 ≤1.28\\nor\\nn ≥117\\nthen there is at least 1 chance in 10 that structural damage will occur.\\n■\\nOne of the most important applications of the central limit theorem is in re-\\ngard to binomial random variables. Since such a random variable X having\\nparameters (n, p) represents the number of successes in n independent trials\\nwhen each trial is a success with probability p, we can express it as\\nX = X1 + ··· + Xn\\nwhere\\nXi =\\n\\x06 1\\nif the ith trial is a success\\n0\\notherwise\\nBecause\\nE[Xi] = p,\\nVar(Xi) = p(1 −p)\\nit follows from the central limit theorem that for n large\\nX −np\\n√np(1 −p)\\nwill approximately be a standard normal random variable [see Figure 6.2,\\nwhich graphically illustrates how the probability mass function of a binomial\\n(n, p) random variable becomes more and more “normal” as n becomes larger\\nand larger].\\nExample 6.3.c. The ideal size of a ﬁrst-year class at a particular college is 150\\nstudents. The college, knowing from past experience that, on the average, only\\n30 percent of those accepted for admission will actually attend, uses a policy\\nof approving the applications of 450 students. Compute the probability that\\nmore than 150 ﬁrst-year students attend this college.\\nSolution. Let X denote the number of students that attend; then assuming that\\neach accepted applicant will independently attend, it follows that X is a bino-\\nmial random variable with parameters n = 450 and p = .3. Since the binomial\\nis a discrete and the normal a continuous distribution, it is best to compute'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 238}, page_content='6.3 The central limit theorem\\n227\\nFIGURE 6.2\\nBinomial probability mass functions converging to the normal density.\\nP{X = i} as P{i −.5 < X < i + .5} when applying the normal approximation\\n(this is called the continuity correction). This yields the approximation\\nP{X > 150.5} = P\\n\\x06X −(450)(.3)\\n√450(.3)(.7) ≥150.5 −(450)(.3)\\n√450(.3)(.7)\\n\\x07\\n≈P{Z > 1.59} = .06\\nHence, only 6 percent of the time do more than 150 of the ﬁrst 450 accepted\\nactually attend.\\n■\\nIt should be noted that we now have two possible approximations to binomial\\nprobabilities: The Poisson approximation, which yields a good approximation\\nwhen n is large and p small, and the normal approximation, which can be\\nshown to be quite good when np(1 −p) is large. [The normal approximation\\nwill, in general, be quite good for values of n satisfying np(1 −p) ≥10.]\\n6.3.1\\nApproximate distribution of the sample mean\\nLet X1,...,Xn be a sample from a population having mean μ and variance σ 2.\\nThe central limit theorem can be used to approximate the distribution of the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 239}, page_content='228 CHAPTER 6: Distributions of sampling statistics\\nsample mean\\nX =\\nn\\n\\ni=1\\nXi/n\\nSince a constant multiple of a normal random variable is also normal, it fol-\\nlows from the central limit theorem that X will be approximately normal when\\nthe sample size n is large. Since the sample mean has expected value μ and\\nstandard deviation σ/√n, it then follows that\\nX −μ\\nσ/√n\\nhas approximately a standard normal distribution.\\nExample 6.3.d. The weights of a population of workers have mean 167 and\\nstandard deviation 27.\\n(a) If a sample of 36 workers is chosen, approximate the probability that the\\nsample mean of their weights lies between 163 and 170.\\n(b) Repeat part (a) when the sample is of size 144.\\nSolution. Let Z be a standard normal random variable.\\n(a) It follows from the central limit theorem that X is approximately normal\\nwith mean 167 and standard deviation 27/\\n√\\n36 = 4.5. Therefore, with Z\\nbeing a standard normal random variable,\\nP{163 < X < 170} = P\\n\\x0e\\n163 −167\\n4.5\\n< X −167\\n4.5\\n< 170 −167\\n4.5\\n\\x0f\\n≈P{−.8889 < Z < .8889}\\n= P{Z < .8889} −P{Z < −.8889}\\n= 2P{Z < .8889} −1\\n≈2 ∗pnorm(.8889) −1\\n= 0.6259432\\n(b) For a sample of size 144, the sample mean will be approximately normal\\nwith mean 167 and standard deviation 27/\\n√\\n144 = 2.25. Therefore,\\nP{163 < X < 170} = P\\n\\x0e\\n163 −167\\n2.25\\n< X −167\\n2.25\\n< 170 −167\\n2.25\\n\\x0f\\n= P\\n\\x0e\\n−1.7778 < X −167\\n4.5\\n< 1.7778\\n\\x0f'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 240}, page_content='6.3 The central limit theorem\\n229\\n≈2P{Z < 1.7778} −1\\n= 2 ∗pnorm(1.7778) −1\\n= 0.9245633\\nThus increasing the sample size from 36 to 144 increases the probability\\nfrom .6259 to .9246.\\n■\\nExample 6.3.e. An astronomer wants to measure the distance from her ob-\\nservatory to a distant star. However, due to atmospheric disturbances, any\\nmeasurement will not yield the exact distance d. As a result, the astronomer\\nhas decided to make a series of measurements and then use their average value\\nas an estimate of the actual distance. If the astronomer believes that the val-\\nues of the successive measurements are independent random variables with a\\nmean of d light-years and a standard deviation of 2 light-years, how many mea-\\nsurements need she make to be at least 95 percent certain that her estimate is\\naccurate to within ± .5 light-years?\\nSolution. If the astronomer makes n measurements, then X, the sample mean\\nof these measurements, will be approximately a normal random variable with\\nmean d and standard deviation 2/√n. Thus, the probability that it will lie be-\\ntween d ± .5 is obtained as follows:\\nP{−.5 < X −d < .5} = P\\n\\x0e\\n−.5\\n2/√n < X −d\\n2/√n <\\n.5\\n2/√n\\n\\x0f\\n≈P{−√n/4 < Z < √n/4}\\n= 2P{Z < √n/4} −1\\nwhere Z is a standard normal random variable.\\nThus, the astronomer should make n measurements, where n is such that\\n2P{Z < √n/4} −1 ≥.95\\nor, equivalently,\\nP{Z < √n/4} ≥.975\\nSince P{Z < 1.96} = .975, it follows that n should be chosen so that\\n√n/4 ≥1.96\\nThat is, at least 62 observations are necessary.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 241}, page_content='230 CHAPTER 6: Distributions of sampling statistics\\nFIGURE 6.3\\nDensities of the average of n exponential random variables having mean 1.\\n6.3.2\\nHow large a sample is needed?\\nThe central limit theorem leaves open the question of how large the sample size\\nn needs to be for the normal approximation to be valid, and indeed the answer\\ndepends on the population distribution of the sample data. For instance, if the\\nunderlying population distribution is normal, then the sample mean X will\\nalso be normal regardless of the sample size. A general rule of thumb is that one\\ncan be conﬁdent of the normal approximation whenever the sample size n is at\\nleast 30. That is, practically speaking, no matter how nonnormal the underlying\\npopulation distribution is, the sample mean of a sample of size at least 30 will\\nbe approximately normal. In most cases, the normal approximation is valid for\\nmuch smaller sample sizes. Indeed, a sample of size 5 will often sufﬁce for the\\napproximation to be valid. Figure 6.3 presents the distribution of the sample\\nmeans from an exponential population distribution for samples of sizes n =\\n1,5,10.\\n6.4\\nThe sample variance\\nLet X1,...,Xn be a random sample from a distribution with mean μ and vari-\\nance σ 2. Let X be the sample mean, and recall the following deﬁnition from\\nSection 2.3.2.\\nDeﬁnition. The statistic S2, deﬁned by\\nS2 =\\nn\\x08\\ni=1\\n(Xi −X)2\\nn −1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 242}, page_content='6.5 Sampling distributions from a normal population\\n231\\nis called the sample variance. S =\\n√\\nS2 is called the sample standard deviation.\\nTo compute E[S2], we use an identity that was proven in Section 2.3.2: For any\\nnumbers x1,...,xn\\nn\\n\\ni=1\\n(xi −x)2 =\\nn\\n\\ni=1\\nx2\\ni −nx2\\nwhere x = \\x08n\\ni=1 xi/n. It follows from this identity that\\n(n −1)S2 =\\nn\\n\\ni=1\\nX2\\ni −nX2\\nTaking expectations of both sides of the preceding yields, upon using the fact\\nthat for any random variable W,E[W 2] = Var(W) + (E[W])2,\\n(n −1)E[S2] = E\\n\\t n\\n\\ni=1\\nX2\\ni\\n\\x0b\\n−nE[X2]\\n= nE[X2\\n1] −nE[X2]\\n= nVar(X1) + n(E[X1])2 −nVar(X) −n(E[X])2\\n= nσ 2 + nμ2 −n(σ 2/n) −nμ2\\n= (n −1)σ 2\\nor\\nE[S2] = σ 2\\nThat is, the expected value of the sample variance S2 is equal to the population\\nvariance σ 2.\\n6.5\\nSampling distributions from a normal population\\nLet X1,X2,...,Xn be a sample from a normal population having mean μ and\\nvariance σ 2. That is, they are independent and Xi ∼N(μ,σ 2), i = 1,...,n. Also\\nlet\\nX =\\nn\\n\\ni=1\\nXi/n\\nand\\nS2 =\\nn\\x08\\ni=1\\n(Xi −X)2\\nn −1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 243}, page_content='232 CHAPTER 6: Distributions of sampling statistics\\ndenote the sample mean and sample variance, respectively. We would like to\\ncompute their distributions.\\n6.5.1\\nDistribution of the sample mean\\nSince the sum of independent normal random variables is normally dis-\\ntributed, it follows that X is normal with mean\\nE[X] =\\nn\\n\\ni=1\\nE[Xi]\\nn\\n= μ\\nand variance\\nVar(X) = 1\\nn2\\nn\\n\\ni=1\\nVar(Xi) = σ 2/n\\nThat is, X, the average of the sample, is normal with a mean equal to the popu-\\nlation mean but with a variance reduced by a factor of 1/n. It follows from this\\nthat\\nX −μ\\nσ/√n\\nis a standard normal random variable.\\n6.5.2\\nJoint distribution of X and S2\\nIn this section, we not only obtain the distribution of the sample variance S2,\\nbut we also discover a fundamental fact about normal samples — namely, that\\nX and S2 are independent with (n −1)S2/σ 2 having a chi-square distribution\\nwith n −1 degrees of freedom.\\nTo start, for numbers x1,...,xn, let yi = xi −μ, i = 1,...,n. Then as y = x −μ,\\nit follows from the identity\\nn\\n\\ni=1\\n(yi −y)2 =\\nn\\n\\ni=1\\ny2\\ni −ny2\\nthat\\nn\\n\\ni=1\\n(xi −x)2 =\\nn\\n\\ni=1\\n(xi −μ)2 −n(x −μ)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 244}, page_content='6.5 Sampling distributions from a normal population\\n233\\nNow, if X1,...,Xn is a sample from a normal population having mean μ and\\nvariance σ 2, then we obtain from the preceding identity that\\nn\\x08\\ni=1\\n(Xi −μ)2\\nσ 2\\n=\\nn\\x08\\ni=1\\n(Xi −X)2\\nσ 2\\n+ n(X −μ)2\\nσ 2\\nor, equivalently,\\nn\\n\\ni=1\\n\\x04Xi −μ\\nσ\\n\\x052\\n=\\nn\\x08\\ni=1\\n(Xi −X)2\\nσ 2\\n+\\n\\t√n(X −μ)\\nσ\\n\\x0b2\\n(6.5.1)\\nBecause (Xi −μ)/σ,i = 1,...,n are independent standard normals, it follows\\nthat the left side of Equation (6.5.1) is a chi-square random variable with n de-\\ngrees of freedom. Also, as shown in Section 6.5.1, √n(X −μ)/σ is a standard\\nnormal random variable and so its square is a chi-square random variable with\\n1 degree of freedom. Thus Equation (6.5.1) equates a chi-square random vari-\\nable having n degrees of freedom to the sum of two random variables, one of\\nwhich is chi-square with 1 degree of freedom. But it has been established that\\nthe sum of two independent chi-square random variables is also chi-square\\nwith a degree of freedom equal to the sum of the two degrees of freedom.\\nThus, it would seem that there is a reasonable possibility that the two terms\\non the right side of Equation (6.5.1) are independent, with \\x08n\\ni=1(Xi −X)2/σ 2\\nhaving a chi-square distribution with n−1 degrees of freedom. Since this result\\ncan indeed be established, we have the following fundamental result.\\nTheorem 6.5.1. If X1,...,Xn is a sample from a normal population having\\nmean μ and variance σ 2, then X and S2 are independent random variables,\\nwith X being normal with mean μ and variance σ 2/n and (n −1)S2/σ 2 being\\nchi-square with n −1 degrees of freedom.\\nTheorem 6.5.1 not only provides the distributions of X and S2 for a normal\\npopulation but also establishes the important fact that they are independent.\\nIn fact, it turns out that this independence of X and S2 is a unique property of\\nthe normal distribution. Its importance will become evident in the following\\nchapters.\\nExample 6.5.a. The time it takes a central processing unit to process a certain\\ntype of job is normally distributed with mean 20 seconds and standard devia-\\ntion 3 seconds. If a sample of 15 such jobs is observed, what is the probability\\nthat the sample variance will exceed 12?\\nSolution. Since the sample is of size n = 15 and σ 2 = 9, write\\nP{S2 > 12} = P\\n\\x0614S2\\n9\\n> 14\\n9 .12\\n\\x07'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 245}, page_content='234 CHAPTER 6: Distributions of sampling statistics\\nUsing R this gives the solution\\n> 1 −pchisq(56/3,14)\\n[1] 0.1780811\\n■\\nThe following corollary of Theorem 6.5.1 will be quite useful in the following\\nchapters.\\nCorollary 6.5.2. Let X1,...,Xn be a sample from a normal population with\\nmean μ. If X denotes the sample mean and S the sample standard deviation,\\nthen\\n√n(X −μ)\\nS\\n∼tn−1\\nThat is, √n(X −μ)/S has a t-distribution with n −1 degrees of freedom.\\nProof. Recall that a t-random variable with n degrees of freedom is deﬁned as\\nthe distribution of\\nZ\\n\\x10\\nχ2n/n\\nwhere Z is a standard normal random variable that is independent of χ2\\nn, a\\nchi-square random variable with n degrees of freedom. Because Theorem 6.5.1\\ngives that √n(X −μ)/σ is a standard normal that is independent of (n −\\n1)S2/σ 2, which is chi-square with n −1 degrees of freedom, we can conclude\\nthat\\n√n(X −μ)/σ\\n\\x10\\nS2/σ 2\\n= √n(X −μ)\\nS\\nis a t-random variable with n −1 degrees of freedom.\\n■\\n6.6\\nSampling from a ﬁnite population\\nConsider a population of N elements, and suppose that p is the proportion of\\nthe population that has a certain characteristic of interest; that is, Np elements\\nhave this characteristic, and N(1−p) do not. A sample of size n from this pop-\\nulation is said to be a random sample if it is chosen in such a manner that each\\nof the\\n\\x11N\\nn\\n\\x12\\npopulation subsets of size n is equally likely to be the sample. For\\ninstance, if the population consists of the three elements a, b, c, then a random\\nsample of size 2 is one that is chosen so that each of the subsets {a,b}, {a,c},\\nand {b,c} is equally likely to be the sample. A random subset can be chosen'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 246}, page_content='6.6 Sampling from a ﬁnite population 235\\nsequentially by letting its ﬁrst element be equally likely to be any of the N el-\\nements of the population, then letting its second element be equally likely to\\nbe any of the remaining N −1 elements of the population, and so on.\\nSuppose now that a random sample of size n has been chosen from a popula-\\ntion of size N. For i = 1,...,n, let\\nXi =\\n\\x06 1\\nif the ith member of the sample has the characteristic\\n0\\notherwise\\nConsider now the sum of the Xi; that is, consider\\nX = X1 + X2 + ··· + Xn\\nBecause the term Xi contributes 1 to the sum if the ith member of the sample\\nhas the characteristic and 0 otherwise, it follows that X is equal to the num-\\nber of members of the sample that possess the characteristic. In addition, the\\nsample mean\\nX = X/n =\\nn\\n\\ni=1\\nXi/n\\nis equal to the proportion of the members of the sample that possess the char-\\nacteristic.\\nLet us now consider the probabilities associated with the statistics X and X.\\nTo begin, note that since each of the N members of the population is equally\\nlikely to be the ith member of the sample, it follows that\\nP{Xi = 1} = Np\\nN = p\\nAlso,\\nP{Xi = 0} = 1 −P{Xi = 1} = 1 −p\\nThat is, each Xi is equal to either 1 or 0 with respective probabilities p and\\n1 −p.\\nIt should be noted that the random variables X1,X2,...,Xn are not indepen-\\ndent. For instance, since the second selection is equally likely to be any of the N\\nmembers of the population, of which Np have the characteristic, it follows that\\nthe probability that the second selection has the characteristic is Np/N = p.\\nThat is, without any knowledge of the outcome of the ﬁrst selection,\\nP{X2 = 1} = p\\nHowever, the conditional probability that X2 = 1, given that the ﬁrst selection\\nhas the characteristic, is'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 247}, page_content='236 CHAPTER 6: Distributions of sampling statistics\\nP{X2 = 1|X1 = 1} = Np −1\\nN −1\\nwhich is seen by noting that if the ﬁrst selection has the characteristic, then the\\nsecond selection is equally likely to be any of the remaining N −1 elements, of\\nwhich Np −1 have the characteristic. Similarly, the probability that the second\\nselection has the characteristic given that the ﬁrst one does not is\\nP{X2 = 1|X1 = 0} =\\nNp\\nN −1\\nThus, knowing whether or not the ﬁrst element of the random sample has\\nthe characteristic changes the probability for the next element. However, when\\nthe population size N is large in relation to the sample size n, this change will\\nbe very slight. For instance, if N = 1000, p = .4, then\\nP{X2 = 1|X1 = 1} = 399\\n999 = .3994\\nwhich is very close to the unconditional probability that X2 = 1; namely,\\nP{X2 = 1} = .4\\nSimilarly, the probability that the second element of the sample has the char-\\nacteristic given that the ﬁrst does not is\\nP{X2 = 1|X1 = 0} = 400\\n999 = .4004\\nwhich is again very close to .4.\\nIndeed, it can be shown that when the population size N is large with respect\\nto the sample size n, then X1,X2,...,Xn are approximately independent. Now\\nif we think of each Xi as representing the result of a trial that is a success if Xi\\nequals 1 and a failure otherwise, it follows that X = \\x08n\\ni=1 Xi can be thought of\\nas representing the total number of successes in n trials. Hence, if the Xi were\\nindependent, then X would be a binomial random variable with parameters n\\nand p. In other words, when the population size N is large in relation to the\\nsample size n, then the distribution of the number of members of the sam-\\nple that possess the characteristic is approximately that of a binomial random\\nvariable with parameters n and p.\\nRemark\\nOf course, X is a hypergeometric random variable (Section 5.4); and so the\\npreceding shows that a hypergeometric can be approximated by a binomial\\nrandom variable when the number chosen is small in relation to the total num-\\nber of elements.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 248}, page_content='6.6 Sampling from a ﬁnite population 237\\nFor the remainder of this text, we will suppose that the underlying population\\nis large in relation to the sample size and we will take the distribution of X to\\nbe binomial.\\nBy using the formulas given in Section 5.4 for the mean and standard deviation\\nof a binomial random variable, we see that\\nE[X] = np\\nand\\nSD(X) =\\n\\x10\\nnp(1 −p)\\nMoreover X = X/n, the proportion of the sample that has the characteristic,\\nhas mean and variance given by\\nE[X] = E[X]/n = p\\nand\\nVar(X) = Var(X)/n2 = p(1 −p)/n\\nExample 6.6.a. Suppose that 45 percent of the population favors a certain\\ncandidate in an upcoming election. If a random sample of size 200 is chosen,\\nﬁnd\\n(a) the expected value and standard deviation of the number of members of\\nthe sample that favor the candidate;\\n(b) the probability that more than half the members of the sample favor the\\ncandidate.\\nSolution.\\n(a) The expected value and standard deviation of the proportion\\nthat favor the candidate are\\nE[X] = 200(.45) = 90,\\nSD(X) =\\n\\x10\\n200(.45)(1 −.45) = 7.0356\\n(b) Because X is binomial with parameters 200 and .45, use R to obtain\\nP(X ≥101) =\\n> 1 −pbinom(100,200,.45)\\n[1] 0.06807525\\n■\\nEven when each element of the population has more than two possible values,\\nit still remains true that if the population size is large in relation to the sam-\\nple size, then the sample data can be regarded as being independent random\\nvariables from the population distribution.\\nExample 6.6.b. According to the U.S. Department of Agriculture’s World Live-\\nstock Situation, the country with the greatest per capita consumption of pork is\\nDenmark. In 2013, the amount of pork consumed by a person residing in Den-\\nmark had a mean value of 147 pounds with a standard deviation of 62 pounds.\\nIf a random sample of 25 Danes is chosen, approximate the probability that'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 249}, page_content='238 CHAPTER 6: Distributions of sampling statistics\\nthe average amount of pork consumed by the members of this group in 2013\\nexceeded 150 pounds.\\nSolution. If we let Xi be the amount consumed by the ith member of the\\nsample, i = 1,...,25, then the desired probability is\\nP\\n\\x06X1 + ··· + X25\\n25\\n> 150\\n\\x07\\n= P{X > 150}\\nwhere X is the sample mean of the 25 sample values. Since we can regard\\nthe Xi as being independent random variables with mean 147 and standard\\ndeviation 62, it follows from the central limit theorem that their sample mean\\nwill be approximately normal with mean 147 and standard deviation 62/5.\\nThus, with Z being a standard normal random variable, we have\\nP{X > 150} = P\\n\\x0e\\nX −147\\n12.4\\n> 150 −147\\n12.4\\n\\x0f\\nP( ¯X > 150) ≈P\\n\\x04\\nZ > 150 −147\\n12.4\\n\\x05\\n> 1 −pnorm(3/12.4)\\n[1] 0.4044151\\n■\\nProblems\\n1. Suppose that X1, X2, X3 are independent with the common probability\\nmass function\\nP{Xi = 0} = .2,\\nP{Xi = 1} = .3,\\nP{Xi = 3} = .5,\\ni = 1,2,3\\na.\\nPlot the probability mass function of X2 = X1 + X2\\n2\\n.\\nb.\\nDetermine E[X2] and Var(X2).\\nc.\\nPlot the probability mass function of X3 = X1 + X2 + X3\\n3\\n.\\nd.\\nDetermine E[X3] and Var(X3).\\n2. If 10 fair dice are rolled, approximate the probability that the sum of\\nthe values obtained (which ranges from 10 to 60) is between 30 and 40\\ninclusive.\\n3. Approximate the probability that the sum of 16 independent uniform (0,\\n1) random variables exceeds 10.\\n4. A roulette wheel has 38 slots, numbered 0, 00, and 1 through 36. If you\\nbet 1 on a speciﬁed number, you either win 35 if the roulette ball lands\\non that number or lose 1 if it does not. If you continually make such bets,\\napproximate the probability that\\na.\\nyou are winning after 34 bets;'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 250}, page_content='Problems 239\\nb.\\nyou are winning after 1000 bets;\\nc.\\nyou are winning after 100,000 bets.\\nAssume that each roll of the roulette ball is equally likely to land on any\\nof the 38 numbers.\\n5. A highway department has enough salt to handle a total of 80 inches of\\nsnowfall. Suppose the daily amount of snow has a mean of 1.5 inches\\nand a standard deviation of .3 inch.\\na.\\nApproximate the probability that the salt on hand will sufﬁce for the\\nnext 50 days.\\nb.\\nWhat assumption did you make in solving part (a)?\\nc.\\nDo you think this assumption is justiﬁed? Explain brieﬂy.\\n6. Fifty numbers are rounded off to the nearest integer and then summed.\\nIf the individual roundoff errors are uniformly distributed between −.5\\nand .5, what is the approximate probability that the resultant sum differs\\nfrom the exact sum by more than 3?\\n7. A six-sided die, in which each side is equally likely to appear, is repeatedly\\nrolled until the total of all rolls exceeds 400. Approximate the probability\\nthat this will require more than 140 rolls.\\n8. The amount of time that a certain type of battery functions is a random\\nvariable with mean 5 weeks and standard deviation 1.5 weeks. Upon\\nfailure, it is immediately replaced by a new battery. Approximate the\\nprobability that 13 or more batteries will be needed in a year.\\n9. The lifetime of a certain electrical part is a random variable with mean\\n100 hours and standard deviation 20 hours. If 16 such parts are tested,\\nﬁnd the probability that the sample mean is\\na.\\nless than 104;\\nb.\\nbetween 98 and 104 hours.\\n10. A tobacco company claims that the amount of nicotine in its cigarettes\\nis a random variable with mean 2.2 mg and standard deviation .3 mg.\\nHowever, the sample mean nicotine content of 100 randomly chosen\\ncigarettes was 3.1 mg. What is the approximate probability that the sam-\\nple mean would have been as high or higher than 3.1 if the company’s\\nclaims were true?\\n11. The lifetime (in hours) of a type of electric bulb has expected value 500\\nand standard deviation 80. Approximate the probability that the sample\\nmean of n such bulbs is greater than 525 when\\na.\\nn = 4;\\nb.\\nn = 16;\\nc.\\nn = 36;\\nd.\\nn = 64.\\n12. An instructor knows from past experience that student exam scores have\\nmean 77 and standard deviation 15. At present the instructor is teaching\\ntwo separate classes — one of size 25 and the other of size 64.\\na.\\nApproximate the probability that the average test score in the class\\nof size 25 lies between 72 and 82.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 251}, page_content='240 CHAPTER 6: Distributions of sampling statistics\\nb.\\nRepeat part (a) for a class of size 64.\\nc.\\nWhat is the approximate probability that the average test score in\\nthe class of size 25 is higher than that of the class of size 64?\\nd.\\nSuppose the average scores in the two classes are 76 and 83. Which\\nclass, the one of size 25 or the one of size 64, do you think was more\\nlikely to have averaged 83?\\n13. If X is binomial with parameters n=150, p =.6, compute the exact value\\nof P{X ≤80} and compare with its normal approximation both (a) mak-\\ning use of and (b) not making use of the continuity correction.\\n14. Teams 1,2,3,4 are all scheduled to play with each of the other teams 10\\ntimes. Whenever team i plays team j, team i is the winner with probabil-\\nity Pi,j, and team j is the winner with probability Pj,i = 1 −Pi,j. If\\nP1,2 = .6,\\nP1,3 = .7,\\nP1,4 = .75,\\nP2,3 = .6,\\nP2,4 = .70,\\nP3,4 = .5,\\na.\\napproximate the probability that team 1 wins at least 20 games.\\nSuppose we want to approximate the probability that team 2 wins at\\nleast as many games as does team 1. To do so, let X be the number\\nof games that team 2 wins against team 1, let Y be the total number\\nof games that team 2 wins against teams 3 and 4, and let Z be the\\ntotal number of games that team 1 wins against teams 3 and 4.\\nb.\\nAre X,Y,Z independent?\\nc.\\nExpress the event that team 2 wins at least as many games as does\\nteam 1 in terms of the random variables X,Y,Z.\\nd.\\nApproximate the probability that team 2 wins at least as many games\\nas team 1.\\n15. A club basketball team will play a 60-game season. Thirty-two of these\\ngames are against class A teams and 28 are against class B teams. The out-\\ncomes of all the games are independent. The team will win each game\\nagainst a class A opponent with probability .5, and it will win each game\\nagainst a class B opponent with probability .7. Let X denote its total num-\\nber of victories in the season.\\na.\\nIs X a binomial random variable?\\nb.\\nLet XA and XB denote, respectively, the number of victories against\\nclass A and class B teams. What are the distributions of XA and XB?\\nc.\\nWhat is the relationship between XA, XB, and X?\\nd.\\nApproximate the probability that the team wins 40 or more games.\\n16. Argue, based on the central limit theorem, that a Poisson random vari-\\nable having mean λ will approximately have a normal distribution with\\nmean and variance both equal to λ when λ is large. If X is Poisson with\\nmean 100, compute the exact probability that X is less than or equal to\\n116 and compare it with its normal approximation both when a conti-\\nnuity correction is utilized and when it is not. The convergence of the\\nPoisson to the normal is indicated in Figure 6.4.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 252}, page_content='Problems 241\\nFIGURE 6.4\\nPoisson probability mass functions.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 253}, page_content='242 CHAPTER 6: Distributions of sampling statistics\\n17. Use the text disk to compute P{X ≤10} when X is a binomial random\\nvariable with parameters n = 100, p = .1. Now compare this with its (a)\\nPoisson and (b) normal approximation. In using the normal approxi-\\nmation, write the desired probability as P{X < 10.5} so as to utilize the\\ncontinuity correction.\\n18. The temperature at which a thermostat goes off is normally distributed\\nwith variance σ 2. If the thermostat is to be tested ﬁve times, ﬁnd\\na.\\nP{S2/σ 2 ≤1.8}\\nb.\\nP{.85 ≤S2/σ 2 ≤1.15}\\nwhere S2 is the sample variance of the ﬁve data values.\\n19. In Problem 18, how large a sample would be necessary to ensure that the\\nprobability in part (a) is at least .95?\\n20. Consider two independent samples — the ﬁrst of size 10 from a normal\\npopulation having variance 4 and the second of size 5 from a normal\\npopulation having variance 2. Compute the probability that the sample\\nvariance from the second sample exceeds the one from the ﬁrst. (Hint:\\nRelate it to the F-distribution.)\\n21. Twelve percent of the population is left-handed. Find the probability that\\nthere are between 10 and 14 left-handers in a random sample of 100\\nmembers of this population. That is, ﬁnd P{10 ≤X ≤14}, where X is the\\nnumber of left-handers in the sample.\\n22. Fifty-two percent of the residents of a certain city are in favor of teaching\\nevolution in high school. Find or approximate the probability that at\\nleast 50 percent of a random sample of size n is in favor of teaching\\nevolution, when\\na.\\nn = 10;\\nb.\\nn = 100;\\nc.\\nn = 1000;\\nd.\\nn = 10,000.\\n23. The following table gives the percentages of individuals of a given city,\\ncategorized by gender, that follow certain negative health practices. Sup-\\npose a random sample of 300 men is chosen. Approximate the probabil-\\nity that\\na.\\nat least 150 of them rarely eat breakfast;\\nb.\\nfewer than 100 of them smoke.\\nSleeps 6 Hours\\nor Less per\\nNight\\nSmoker\\nRarely Eats\\nBreakfast\\nIs 20 Percent\\nor More\\nOverweight\\nMen\\n22.7\\n28.4\\n45.4\\n29.6\\nWomen\\n21.4\\n22.8\\n42.0\\n25.6\\nSource: U.S. National Center for Health Statistics, Health Promotion and Disease Prevention.\\n24. (Use the table from Problem 23.) Suppose a random sample of 300\\nwomen is chosen. Approximate the probability that'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 254}, page_content='Problems 243\\na.\\nat least 60 of them are overweight by 20 percent or more;\\nb.\\nfewer than 50 of them sleep 6 hours or less nightly.\\n25. (Use the table from Problem 23.) Suppose random samples of 300\\nwomen and of 300 men are chosen. Approximate the probability that\\nmore women than men rarely eat breakfast.\\n26. The following table uses data concerning the percentages of teenage male\\nand female full-time workers whose annual salaries fall in different salary\\ngroupings. Suppose random samples of 1000 men and 1000 women were\\nchosen. Use the table to approximate the probability that\\na.\\nat least half of the women earned less than $20,000;\\nb.\\nmore than half of the men earned $20,000 or more;\\nc.\\nmore than half of the women and more than half of the men earned\\n$20,000 or more;\\nd.\\n250 or fewer of the women earned at least $25,000;\\ne.\\nat least 200 of the men earned $50,000 or more;\\nf.\\nmore women than men earned between $20,000 and $24,999.\\nEarnings Range\\nPercentage of\\nWomen\\nPercentage of Men\\n$4999 or less\\n2.8\\n1.8\\n$5000 to $9999\\n10.4\\n4.7\\n$10,000 to $19,999\\n41.0\\n23.1\\n$20,000 to $24,999\\n16.5\\n13.4\\n$25,000 to $49,999\\n26.3\\n42.1\\n$50,000 and over\\n3.0\\n14.9\\nSource: U.S. Department of Commerce, Bureau of the Census.\\n27. Today, roughly 10.5 percent of the labor force belong to a union. If ﬁve\\nworkers are randomly chosen, what is the probability that none of them\\nbelong to a union? Compare your answer to what it would have been in\\n1983 when 20.1 percent of the workforce belonged to a union.\\n28. The sample mean and sample standard deviation of all San Francisco\\nstudent scores on the most recent Scholastic Aptitude Test examination\\nin mathematics were 517 and 120. Approximate the probability that a\\nrandom sample of 144 students would have an average score exceeding\\na.\\n507;\\nb.\\n517;\\nc.\\n537;\\nd.\\n550.\\n29. The average salary of newly graduated students with bachelor’s degrees\\nin chemical engineering is $53,600, with a standard deviation of $3200.\\nApproximate the probability that the average salary of a sample of 12\\nrecently graduated chemical engineers exceeds $55,000.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 255}, page_content='244 CHAPTER 6: Distributions of sampling statistics\\n30. A certain component is critical to the operation of an electrical system\\nand must be replaced immediately upon failure. If the mean lifetime of\\nthis type of component is 100 hours and its standard deviation is 30\\nhours, how many of the components must be in stock so that the proba-\\nbility that the system is in continual operation for the next 2000 hours is\\nat least .95?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 256}, page_content='CHAPTER 7\\nParameter estimation\\n7.1\\nIntroduction\\nLet X1,...,Xn be a random sample from a distribution Fθ that is speciﬁed up\\nto a vector of unknown parameters θ. For instance, the sample could be from\\na Poisson distribution whose mean value is unknown; or it could be from a\\nnormal distribution having an unknown mean and variance. Whereas in prob-\\nability theory it is usual to suppose that all of the parameters of a distribution\\nare known, the opposite is true in statistics, where a central problem is to use\\nthe observed data to make inferences about the unknown parameters.\\nIn Section 7.2, we present the maximum likelihood method for determining es-\\ntimators of unknown parameters. The estimates so obtained are called point\\nestimates, because they specify a single quantity as an estimate of θ. In Sec-\\ntion 7.3, we consider the problem of obtaining interval estimates. In this case,\\nrather than specifying a certain value as our estimate of θ, we specify an interval\\nin which we estimate that θ lies. Additionally, we consider the question of how\\nmuch conﬁdence we can attach to such an interval estimate. We illustrate by\\nshowing how to obtain an interval estimate of the unknown mean of a normal\\ndistribution whose variance is speciﬁed. We then consider a variety of interval\\nestimation problems. In Section 7.3.1, we present an interval estimate of the\\nmean of a normal distribution whose variance is unknown. In Section 7.3.2,\\nwe obtain an interval estimate of the variance of a normal distribution. In Sec-\\ntion 7.4, we determine an interval estimate for the difference of two normal\\nmeans, both when their variances are assumed to be known and when they\\nare assumed to be unknown (although in the latter case we suppose that the\\nunknown variances are equal). In Sections 7.5 and the optional Section 7.6, we\\npresent interval estimates of the mean of a Bernoulli random variable and the\\nmean of an exponential random variable.\\nIn the optional Section 7.7, we return to the general problem of obtaining\\npoint estimates of unknown parameters and show how to evaluate an estima-\\ntor by considering its mean square error. The bias of an estimator is discussed,\\nand its relationship to the mean square error is explored.\\nIn the optional Section 7.8, we consider the problem of determining an esti-\\nmate of an unknown parameter when there is some prior information avail-\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00016-8\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n245'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 257}, page_content='246 CHAPTER 7: Parameter estimation\\nable. This is the Bayesian approach, which supposes that prior to observing the\\ndata, information about θ is always available to the decision maker, and that\\nthis information can be expressed in terms of a probability distribution on θ.\\nIn such a situation, we show how to compute the Bayes estimator, which is the\\nestimator whose expected squared distance from θ is minimal.\\n7.2\\nMaximum likelihood estimators\\nAny statistic used to estimate the value of an unknown parameter θ is called\\nan estimator of θ. The observed value of the estimator is called the estimate. For\\ninstance, as we shall see, the usual estimator of the mean of a normal popula-\\ntion, based on a sample X1,...,Xn from that population, is the sample mean\\nX = \\x02\\ni Xi/n. If a sample of size 3 yields the data X1 =2, X2 =3, X3 =4, then\\nthe estimate of the population mean, resulting from the estimator X, is the\\nvalue 3.\\nSuppose that the random variables X1,...,Xn, whose joint distribution is as-\\nsumed given except for an unknown parameter θ, are to be observed. The\\nproblem of interest is to use the observed values to estimate θ. For example,\\nthe Xi’s might be independent, exponential random variables each having the\\nsame unknown mean θ. In this case, the joint density function of the random\\nvariables would be given by\\nf (x1,x2,...,xn)\\n= fX1(x1)fX2(x2)···fXn(xn)\\n= 1\\nθ e−x1/θ 1\\nθ e−x2/θ ··· 1\\nθ e−xn/θ,\\n0 < xi < ∞,i = 1,...,n\\n= 1\\nθn exp\\n\\x03\\n−\\nn\\n\\x04\\n1\\nxi/θ\\n\\x05\\n,\\n0 < xi < ∞,i = 1,...,n\\nand the objective would be to estimate θ from the observed data X1,X2,...,Xn.\\nA particular type of estimator, known as the maximum likelihood estima-\\ntor, is widely used in statistics. It is obtained by reasoning as follows. Let\\nf (x1,...,xn|θ) denote the joint probability mass function of the random vari-\\nables X1,X2,...,Xn when they are discrete, and let it be their joint proba-\\nbility density function when they are jointly continuous random variables.\\nBecause θ is assumed unknown, we also write f as a function of θ. Now since\\nf (x1,...,xn|θ) represents the likelihood that the values x1,x2,...,xn will be\\nobserved when θ is the true value of the parameter, it would seem that a rea-\\nsonable estimate of θ would be that value yielding the largest likelihood of the\\nobserved values. In other words, the maximum likelihood estimate ˆθ is deﬁned\\nto be that value of θ maximizing f (x1,...,xn|θ) where x1,...,xn are the ob-'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 258}, page_content='7.2 Maximum likelihood estimators\\n247\\nserved values. The function f (x1,...,xn|θ) is often referred to as the likelihood\\nfunction of θ.\\nIn determining the maximizing value of θ, it is often useful to use the fact that\\nf (x1,...,xn|θ) and log[f (x1,...,xn|θ)] have their maximum at the same value\\nof θ. Hence, we may also obtain ˆθ by maximizing log[f (x1,...,xn|θ)].\\nExample 7.2.a (Maximum Likelihood Estimator of a Bernoulli Parameter).\\nSuppose that n independent trials, each of which is a success with probabil-\\nity p, are performed. What is the maximum likelihood estimator of p?\\nSolution. The data consist of the values of X1,...,Xn where\\nXi =\\n\\x03\\n1\\nif trial i is a success\\n0\\notherwise\\nNow\\nP{Xi = 1} = p = 1 −P{Xi = 0}\\nwhich can be succinctly expressed as\\nP{Xi = x} = px(1 −p)1−x,\\nx = 0,1\\nHence, by the assumed independence of the trials, the likelihood (that is, the\\njoint probability mass function) of the data is given by\\nf (x1,...,xn|p) = P{X1 = x1,...,Xn = xn|p}\\n= px1(1 −p)1−x1 ···pxn(1 −p)1−xn\\n= p\\x03n\\n1xi(1 −p)n−\\x03n\\n1 xi,\\nxi = 0,1,\\ni = 1,...,n\\nTo determine the value of p that maximizes the likelihood, ﬁrst take logs to\\nobtain\\nlog f (x1,...,xn|p) =\\nn\\n\\x04\\n1\\nxi log p +\\n\\x06\\nn −\\nn\\n\\x04\\n1\\nxi\\n\\x07\\nlog(1 −p)\\nDifferentiation yields\\nd\\ndp log f (x1,...,xn|p) =\\nn\\x02\\n1\\nxi\\np\\n−\\n\\x08\\nn −\\nn\\x02\\n1\\nxi\\n\\t\\n1 −p'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 259}, page_content='248 CHAPTER 7: Parameter estimation\\nUpon equating to zero and solving, we obtain that the maximum likelihood\\nestimate ˆp satisﬁes\\nn\\x02\\n1\\nxi\\nˆp\\n=\\nn −\\nn\\x02\\n1\\nxi\\n1 −ˆp\\nor\\nˆp=\\nn\\x02\\ni=1\\nxi\\nn\\nHence, the maximum likelihood estimator of the unknown mean of a\\nBernoulli distribution is given by\\nd(X1,...,Xn) =\\nn\\x02\\ni=1\\nXi\\nn\\nSince \\x02n\\ni=1 Xi is the number of successful trials, we see that the maximum\\nlikelihood estimator of p is equal to the proportion of the observed trials that\\nresult in successes. For an illustration, suppose that each RAM (random access\\nmemory) chip produced by a certain manufacturer is, independently, of accept-\\nable quality with probability p. Then if out of a sample of 1000 tested 921 are\\nacceptable, it follows that the maximum likelihood estimate of p is .921.\\n■\\nExample 7.2.b. Two proofreaders were given the same manuscript to read. If\\nproofreader 1 found n1 errors, and proofreader 2 found n2 errors, with n1,2 of\\nthese errors being found by both proofreaders, estimate N, the total number of\\nerrors that are in the manuscript.\\nSolution. Before we can estimate N we need to make some assumptions about\\nthe underlying probability model. So let us assume that the results of the\\nproofreaders are independent, and that each error in the manuscript is inde-\\npendently found by proofreader i with probability pi, i = 1,2.\\nTo estimate N, we will start by deriving an estimator of p1. To do so, note\\nthat each of the n2 errors found by reader 2 will, independently, be found by\\nproofreader 1 with probability pi. Because proofreader 1 found n1,2 of those\\nn2 errors, a reasonable estimate of p1 is given by\\nˆp1 = n1,2\\nn2\\nHowever, because proofreader 1 found n1 of the N errors in the manuscript, it\\nis reasonable to suppose that p1 is also approximately equal to n1\\nN . Equating'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 260}, page_content='7.2 Maximum likelihood estimators\\n249\\nthis to ˆp1 gives that\\nn1,2\\nn2\\n≈n1\\nN\\nor\\nN ≈n1n2\\nn1,2\\nBecause the preceding estimate is symmetric in n1 and n2, it follows that it is\\nthe same no matter which proofreader is designated as proofreader 1.\\nAn interesting application of the preceding occurred when two teams of re-\\nsearchers recently announced that they had decoded the human genetic code\\nsequence. As part of their work both teams estimated that the human genome\\nconsisted of approximately 33,000 genes. Because both teams independently\\narrived at the same number, many scientists found this number believable.\\nHowever, most scientists were quite surprised by this relatively small number of\\ngenes; by comparison it is only about twice as many as a fruit ﬂy has. However,\\na closer inspection of the ﬁndings indicated that the two groups only agreed\\non the existence of about 17,000 genes. (That is, 17,000 genes were found by\\nboth teams.) Thus, based on our preceding estimator, we would estimate that\\nthe actual number of genes, rather than being 33,000, is\\nn1n2\\nn1,2\\n= 33,000 × 33,000\\n17,000\\n≈64,000\\n(Because there is some controversy about whether some of the genes claimed\\nto be found are actually genes, 64,000 should probably be taken as an upper\\nbound on the actual number of genes.)\\nThe estimation approach used when there are two proofreaders does not work\\nwhen there are m proofreaders, when m > 2. Because, if for each i, we let ˆpi\\nbe the fraction of the errors found by at least one of the other proofreaders\\nj, (j ̸= i), that are also found by i, and then set that equal to ni\\nN , then the\\nestimate of N, namely ni\\nˆpi , would differ for different values of i. Moreover, with\\nthis approach it is possible that we may have that ˆpi > ˆpj even if proofreader\\ni ﬁnds fewer errors than does proofreader j. For instance, for m = 3, suppose\\nproofreaders 1 and 2 ﬁnd exactly the same set of 10 errors whereas proofreader\\n3 ﬁnds 20 errors with only 1 of them in common with the set of errors found by\\nthe others. Then, because proofreader 1 (and 2) found 10 of the 29 errors found\\nby at least one of the other proofreaders, ˆpi = 10/29, i = 1,2. On the other\\nhand, because proofreader 3 only found 1 of the 10 errors found by the others,\\nˆp3 = 1/10. Therefore, although proofreader 3 found twice the number of errors\\nas did proofreader 1, the estimate of p3 is less than that of p1. To obtain more\\nreasonable estimates, we could take the preceding values of ˆpi, i = 1,...,m,\\nas preliminary estimates of the pi. Now, let nf be the number of errors that'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 261}, page_content='250 CHAPTER 7: Parameter estimation\\nare found by at least one proofreader. Because nf /N is the fraction of errors\\nthat are found by at least one proofreader, this should approximately equal 1−\\n\\nm\\ni=1(1 −pi), the probability that an error is found by at least one proofreader.\\nTherefore, we have\\nnf\\nN ≈1 −\\nm\\n\\x0b\\ni=1\\n(1 −pi)\\nsuggesting that N ≈ˆN, where\\nˆN =\\nnf\\n1 −\\nm\\ni=1(1 −ˆpi)\\n(7.2.1)\\nWith this estimate of N, we can then reset our estimates of the pi by using\\nˆpi = ni\\nˆN\\n,\\ni = 1,...,m\\n(7.2.2)\\nWe can then reestimate N by using the new value (Equation (7.2.1)). (The esti-\\nmation need not stop here; each time we obtain a new estimate ˆN of N we can\\nuse Equation (7.2.2) to obtain new estimates of the pi, which can then be used\\nto obtain a new estimate of N, and so on.)\\n■\\nExample 7.2.c (Maximum Likelihood Estimator of a Poisson Parameter). Sup-\\npose X1,...,Xn are independent Poisson random variables each having mean\\nλ. Determine the maximum likelihood estimator of λ.\\nSolution. The likelihood function is given by\\nf (x1,...,xn|λ) = e−λλx1\\nx1!\\n··· e−λλxn\\nxn!\\n= e−nλλ\\x03n\\n1xi\\nx1!...xn!\\nThus,\\nlog f (x1,...,xn|λ) = −nλ +\\nn\\n\\x04\\n1\\nxi logλ −logc\\nwhere c = \\nn\\ni=1 xi! does not depend on λ. Differentiation yields\\nd\\ndλ log f (x1,...,xn|λ) = −n +\\nn\\x02\\n1\\nxi\\nλ'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 262}, page_content='7.2 Maximum likelihood estimators\\n251\\nBy equating to zero, we obtain that the maximum likelihood estimate ˆλ equals\\nˆλ =\\nn\\x02\\n1\\nxi\\nn\\nand so the maximum likelihood estimator is given by\\nd(X1,...,Xn) =\\nn\\x02\\ni=1\\nXi\\nn\\nFor example, suppose that the number of people who enter a certain retail\\nestablishment in any day is a Poisson random variable having an unknown\\nmean λ, which must be estimated. If after 20 days a total of 857 people\\nhave entered the establishment, then the maximum likelihood estimate of λ\\nis 857/20 = 42.85. That is, we estimate that on average, 42.85 customers will\\nenter the establishment on a given day.\\n■\\nExample 7.2.d. The number of trafﬁc accidents in Berkeley, California, in 10\\nrandomly chosen nonrainy days in 1998 is as follows:\\n4,0,6,5,2,1,2,0,4,3\\nUse these data to estimate the proportion of nonrainy days that had 2 or fewer\\naccidents that year.\\nSolution. Since there are a large number of drivers, each of whom has a small\\nprobability of being involved in an accident in a given day, it seems reason-\\nable to assume that the daily number of trafﬁc accidents is a Poisson random\\nvariable. Since\\nX = 1\\n10\\n10\\n\\x04\\ni=1\\nXi = 2.7\\nit follows that the maximum likelihood estimate of the Poisson mean is 2.7.\\nSince the long-run proportion of nonrainy days that have 2 or fewer accidents\\nis equal to P{X ≤2}, where X is the random number of accidents in a day, it\\nfollows that the desired estimate is\\ne−2.7(1 + 2.7 + (2.7)2/2) = .4936\\nThat is, we estimate that a little less than half of the nonrainy days had 2 or\\nfewer accidents.\\n■\\nExample 7.2.e (Maximum Likelihood Estimator in a Normal Population).\\nSuppose X1,...,Xn are independent, normal random variables each with un-\\nknown mean μ and unknown standard deviation σ. The joint density is given'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 263}, page_content='252 CHAPTER 7: Parameter estimation\\nby\\nf (x1,...,xn|μ,σ) =\\nn\\n\\x0b\\ni=1\\n1\\n√\\n2πσ\\nexp\\n\\x0c−(xi −μ)2\\n2σ 2\\n\\r\\n=\\n\\x0e 1\\n2π\\n\\x0fn/2 1\\nσ n exp\\n⎡\\n⎢⎢⎣\\n−\\nn\\x02\\n1\\n(xi −μ)2\\n2σ 2\\n⎤\\n⎥⎥⎦\\nThe logarithm of the likelihood is thus given by\\nlog f (x1,...,xn|μ,σ) = −n\\n2 log(2π) −nlogσ −\\nn\\x02\\n1\\n(xi −μ)2\\n2σ 2\\nIn order to ﬁnd the value of μ and σ maximizing the foregoing, we compute\\n∂\\n∂μ log f (x1,...,xn|μ,σ) =\\nn\\x02\\ni=1\\n(xi −μ)\\nσ 2\\n∂\\n∂σ log f (x1,...,xn|μ,σ) = −n\\nσ +\\nn\\x02\\n1\\n(xi −μ)2\\nσ 3\\nEquating these equations to zero yields that\\nˆμ =\\nn\\n\\x04\\ni=1\\nxi/n\\nand\\nˆσ =\\n\\x16 n\\n\\x04\\ni=1\\n(xi −ˆμ)2/n\\n\\x171/2\\nHence, the maximum likelihood estimators of μ and σ are given, respectively,\\nby\\nX\\nand\\n\\x16 n\\n\\x04\\ni=1\\n(Xi −X)2/n\\n\\x171/2\\n(7.2.3)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 264}, page_content='7.2 Maximum likelihood estimators\\n253\\nIt should be noted that the maximum likelihood estimator of the standard\\ndeviation σ differs from the sample standard deviation\\nS =\\n\\x16 n\\n\\x04\\ni=1\\n(Xi −X)2/(n −1)\\n\\x171/2\\nin that the denominator in Equation (7.2.3) is √n rather than\\n√\\nn −1. However,\\nfor n of reasonable size, these two estimators of σ will be approximately equal.\\n■\\nExample 7.2.f. Kolmogorov’s law of fragmentation states that the size of an indi-\\nvidual particle in a large collection of particles resulting from the fragmentation\\nof a mineral compound will have an approximate lognormal distribution,\\nwhere a random variable X is said to have a lognormal distribution if log(X)\\nhas a normal distribution. The law, which was ﬁrst noted empirically and then\\nlater given a theoretical basis by Kolmogorov, has been applied to a variety of\\nengineering studies. For instance, it has been used in the analysis of the size of\\nrandomly chosen gold particles from a collection of gold sand. A less obvious\\napplication of the law has been to a study of the stress release in earthquake\\nfault zones (see Lomnitz, C., “Global Tectonics and Earthquake Risk,” Develop-\\nments in Geotectonics, Elsevier, Amsterdam, 1979).\\nSuppose that a sample of 10 grains of metallic sand taken from a large sand\\npile have respective lengths (in millimeters):\\n2.2, 3.4, 1.6, 0.8, 2.7, 3.3, 1.6, 2.8, 2.5, 1.9\\nEstimate the percentage of sand grains in the entire pile whose length is be-\\ntween 2 and 3 mm.\\nSolution. Taking the natural logarithm of these 10 data values, the following\\ntransformed data set results\\n.7885, 1.2238, .4700, −.2231, .9933, 1.1939, .4700, 1.0296, .9163, .6419\\nBecause the sample mean and sample standard deviation of these data are\\nx = .7504,\\ns = .4351\\nit follows that the logarithm of the length of a randomly chosen grain has a\\nnormal distribution with mean approximately equal to .7504 and with stan-\\ndard deviation approximately equal to .4351. Hence, if X is the length of the\\ngrain, then\\nP{2 < X < 3} = P{log(2) < log(X) < log(3)}\\n= P\\n\\x18log(2) −.7504\\n.4351\\n< log(X) −.7504\\n.4351\\n< log(3) −.7504\\n.4351\\n\\x19'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 265}, page_content='254 CHAPTER 7: Parameter estimation\\n≈pnorm(( log(3) −.7504)/.4351)\\n−pnorm(( log(2) −.7504)/.4351)\\n= 0.3405766\\n■\\nThe lognormal distribution is often assumed in situations where the random\\nvariable under interest can be regarded as the product of a large number of\\nindependent and identically distributed random variables. For instance, it is\\ncommonly used in ﬁnance as the distribution of the price of a security at some\\nfuture time. To see why this might be reasonable, suppose that the current price\\nof the security is s and that we are interested in S(t), the price of the secu-\\nrity after an additional time t. For a large value n, let ti = it/n, and consider\\nS(t1),...,S(tn), the prices of the security at the times t1,...,tn. Now, a common\\nassumption in ﬁnance is that the ratios S(ti)/S(ti−1) are approximately inde-\\npendent and identically distributed. Consequently, if we let Xi = S(ti)/S(ti−1),\\nthen writing\\nS(t) = S(tn) = S(t0) · S(t1)\\nS(t0) · S(t2)\\nS(t1) ··· S(tn)\\nS(tn−1)\\n= s\\nn\\n\\x0b\\ni=1\\nXi\\nwe obtain, upon taking logarithms, that\\nlog(S(t)) = log(s) +\\nn\\n\\x04\\ni=1\\nlog(Xi)\\nThus, by the central limit theorem log(S(t)) will approximately have a normal\\ndistribution.\\nThe lognormal distribution has also been shown to be a good ﬁt for such ran-\\ndom variables as length of patient stays in hospitals, and vehicle travel times.\\nIn all of the foregoing examples, the maximum likelihood estimator of the\\npopulation mean turned out to be the sample mean X. To show that this is not\\nalways the situation, consider the following example.\\nExample 7.2.g (Estimating the Mean of a Uniform Distribution). Suppose\\nX1,...,Xn constitute a sample from a uniform distribution on (0, θ), where\\nθ is unknown. Their joint density is thus\\nf (x1,x2,...,xn|θ) =\\n⎧\\n⎨\\n⎩\\n1\\nθn\\n0 < xi < θ,\\ni = 1,...,n\\n0\\notherwise\\nThis density is maximized by choosing θ as small as possible. Since θ must be at\\nleast as large as all of the observed values xi, it follows that the smallest possible'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 266}, page_content='7.2 Maximum likelihood estimators\\n255\\nchoice of θ is equal to max(x1,x2,...,xn). Hence, the maximum likelihood\\nestimator of θ is\\nˆθ = max(X1,X2,...,Xn)\\nIt easily follows from the foregoing that the maximum likelihood estimator of\\nθ/2, the mean of the distribution, is max(X1,X2,...,Xn)/2.\\n■\\n7.2.1\\nEstimating life distributions1\\nLet X denote the age at death of a randomly chosen child born today. That is,\\nX = i if the newborn dies in its ith year, i ≥1. To estimate the probability mass\\nfunction of X, let λi denote the probability that a newborn who has survived\\nhis or her ﬁrst i −1 years dies in year i. That is,\\nλi = P{X = i|X > i −1} =\\nP{X = i}\\nP{X > i −1}\\nAlso, let\\nsi = 1 −λi =\\nP{X > i}\\nP{X > i −1}\\nbe the probability that a newborn who survives her ﬁrst i −1 years also survives\\nyear i. The quantity λi is called the failure rate, and si is called the survival rate,\\nof an individual who is entering his or her ith year. Now,\\ns1s2 ···si = P{X > 1}P{X > 2}P{X > 3}\\nP{X > 1}P {X > 2} ···\\nP{X > i}\\nP{X > i −1}\\n= P{X > i}\\nTherefore,\\nP{X = n} = P{X > n −1}λn = s1 ···sn−1(1 −sn)\\nConsequently, we can estimate the probability mass function of X by estimat-\\ning the quantities si, i = 1,...,n. The value si can be estimated by looking at\\nall individuals in the population who reached age i 1 year ago, and then letting\\nthe estimate ˆsi be the fraction of them who are alive today. We would then use\\nˆs1ˆs2 ··· ˆsn−1\\n\\x1d\\n1 −ˆsn\\n\\x1e\\nas the estimate of P{X = n}. (Note that although we are\\nusing the most recent possible data to estimate the quantities si, our estimate\\nof the probability mass function of the lifetime of a newborn assumes that the\\nsurvival rate of the newborn when it reaches age i will be the same as last year’s\\nsurvival rate of someone of age i.)\\n1Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 267}, page_content='256 CHAPTER 7: Parameter estimation\\nThe use of the survival rate to estimate a life distribution is also of importance\\nin health studies with partial information. For instance, consider a study in\\nwhich a new drug is given to a random sample of 12 lung cancer patients.\\nSuppose that after some time we have the following data on the number of\\nmonths of survival after starting the new drug:\\n4,7∗,9,11∗,12,3,14∗,1,8,7,5,3∗\\nwhere x means that the patient died in month x after starting the drug treat-\\nment, and x∗means that the patient has taken the drug for x months and is\\nstill alive.\\nLet X equal the number of months of survival after beginning the drug treat-\\nment, and let\\nsi = P{X > i|X > i −1} =\\nP{X > i}\\nP{X > i −1}\\nTo estimate si, the probability that a patient who has survived the ﬁrst i −1\\nmonths will also survive month i, we should take the fraction of those patients\\nwho began their ith month of drug taking and survived the month. For in-\\nstance, because 11 of the 12 patients survived month 1, ˆs1 = 11/12. Because\\nall 11 patients who began month 2 survived, ˆs2 = 11/11. Because 10 of the 11\\npatients who began month 3 survived, ˆs3 = 10/11. Because 8 of the 9 patients\\nwho began their fourth month of taking the drug (the 9 being all but the ones\\nlabeled 1, 3, and 3∗) survived month 4, ˆs4 = 8/9. Similar reasoning holds for\\nthe others, giving the following survival rate estimates:\\nˆs1 = 11/12\\nˆs2 = 11/11\\nˆs3 = 10/11\\nˆs4 = 8/9\\nˆs5 = 7/8\\nˆs6 = 7/7\\nˆs7 = 6/7\\nˆs8 = 4/5\\nˆs9 = 3/4\\nˆs10 = 3/3\\nˆs11 = 3/3\\nˆs12 = 1/2\\nˆs13 = 1/1\\nˆs14 = 1/1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 268}, page_content='7.3 Interval estimates\\n257\\nWe can now use \\n j\\ni=1 ˆsi to estimate the probability that a drug taker survives\\nat least j time periods, j = 1,...,14. For instance, our estimate of P{X > 6} is\\n35/54.\\n7.3\\nInterval estimates\\nSuppose that X1,...,Xn is a sample from a normal population having un-\\nknown mean μ and known variance σ 2. It has been shown that X = \\x02n\\ni=1 Xi/n\\nis the maximum likelihood estimator for μ. However, we don’t expect that the\\nsample mean X will exactly equal μ, but rather that it will “be close.” Hence,\\nrather than a point estimate, it is sometimes more valuable to be able to specify\\nan interval for which we have a certain degree of conﬁdence that μ lies within.\\nTo obtain such an interval estimator, we make use of the probability distribu-\\ntion of the point estimator. Let us see how it works for the preceding situation.\\nIn the foregoing, since the point estimator X is normal with mean μ and vari-\\nance σ 2/n, it follows that\\nX −μ\\nσ/√n = √n(X −μ)\\nσ\\nhas a standard normal distribution. Therefore,\\nP\\n\\x03\\n−1.96 < √n(X −μ)\\nσ\\n< 1.96\\n\\x05\\n= .95\\nor, equivalently,\\nP\\n\\x18\\n−1.96 σ\\n√n < X −μ < 1.96 σ\\n√n\\n\\x19\\n= .95\\nMultiplying through by −1 yields the equivalent statement\\nP\\n\\x18\\n−1.96 σ\\n√n < μ −X < 1.96 σ\\n√n\\n\\x19\\n= .95\\nor, equivalently,\\nP\\n\\x18\\nX −1.96 σ\\n√n < μ < X + 1.96 σ\\n√n\\n\\x19\\n= .95\\nThat is, 95 percent of the time the value of the sample average ¯X will be such\\nthat the distance between it and the mean μ will be less than 1.96σ/√n. If we\\nnow observe the sample and it turns out that X = x, then we say that “with 95\\npercent conﬁdence”\\nx −1.96 σ\\n√n < μ < x + 1.96 σ\\n√n\\n(7.3.1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 269}, page_content='258 CHAPTER 7: Parameter estimation\\nThat is, “with 95 percent conﬁdence” we assert that the true mean lies within\\n1.96σ/√n of the observed sample mean. The interval\\n\\x0e\\nx −1.96 σ\\n√n, x + 1.96 σ\\n√n\\n\\x0f\\nis called a 95 percent conﬁdence interval estimate of μ.\\nExample 7.3.a. Suppose that when a signal having value μ is transmitted from\\nlocation A the value received at location B is normally distributed with mean\\nμ and variance 4. That is, if μ is sent, then the value received is μ + N where\\nN, representing noise, is normal with mean 0 and variance 4. To reduce error,\\nsuppose the same value is sent 9 times. If the successive values received are 5,\\n8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5, let us construct a 95 percent conﬁdence interval\\nfor μ.\\nSince\\nx = 81\\n9 = 9\\nit follows, under the assumption that the values received are independent, that\\na 95 percent conﬁdence interval for μ is\\n\\x0e\\n9 −1.96σ\\n3 , 9 + 1.96σ\\n3\\n\\x0f\\n= (7.69,10.31)\\nHence, we are “95 percent conﬁdent” that the true message value lies between\\n7.69 and 10.31.\\n■\\nThe interval in Equation (7.3.1) is called a two-sided conﬁdence interval. Some-\\ntimes, however, we are interested in determining a value so that we can assert\\nwith, say, 95 percent conﬁdence, that μ is at least as large as that value.\\nTo determine such a value, note that if Z is a standard normal random variable\\nthen\\nP{Z < 1.645} = .95\\nAs a result,\\nP\\n\\x03\\n√n(X −μ)\\nσ\\n< 1.645\\n\\x05\\n= .95\\nor\\nP\\n\\x18\\nX −1.645 σ\\n√n < μ\\n\\x19\\n= .95'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 270}, page_content='7.3 Interval estimates\\n259\\nThus, a 95 percent one-sided upper conﬁdence interval for μ is\\n\\x0e\\nx −1.645 σ\\n√n,∞\\n\\x0f\\nwhere x is the observed value of the sample mean.\\nA one-sided lower conﬁdence interval is obtained similarly; when the observed\\nvalue of the sample mean is x, then the 95 percent one-sided lower conﬁdence\\ninterval for μ is\\n\\x0e\\n−∞, x + 1.645 σ\\n√n\\n\\x0f\\nExample 7.3.b. Determine the upper and lower 95 percent conﬁdence interval\\nestimates of μ in Example 7.3.a.\\nSolution. Since\\n1.645 σ\\n√n = 3.29\\n3\\n= 1.097\\nthe 95 percent upper conﬁdence interval is\\n(9 −1.097,∞) = (7.903,∞)\\nand the 95 percent lower conﬁdence interval is\\n(−∞,9 + 1.097) = (−∞,10.097)\\n■\\nWe can also obtain conﬁdence intervals of any speciﬁed level of conﬁdence. To\\ndo so, recall that zα is such that\\nP{Z > zα} = α\\nwhen Z is a standard normal random variable. But this implies (see Figure 7.1)\\nthat for any α\\nP{−zα/2 < Z < zα/2} = 1 −α\\nAs a result, we see that\\nP\\n\\x03\\n−zα/2 < √n(X −μ)\\nσ\\n< zα/2\\n\\x05\\n= 1 −α\\nor\\nP\\n\\x18\\n−zα/2\\nσ\\n√n < X −μ < zα/2\\nσ\\n√n\\n\\x19\\n= 1 −α'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 271}, page_content='260 CHAPTER 7: Parameter estimation\\nFIGURE 7.1\\nP{−zα/2 < Z < zα/2} = 1 −α.\\nor\\nP\\n\\x18\\n−zα/2\\nσ\\n√n < μ −X < zα/2\\nσ\\n√n\\n\\x19\\n= 1 −α\\nThat is,\\nP\\n\\x18\\nX −zα/2\\nσ\\n√n < μ < X + zα/2\\nσ\\n√n\\n\\x19\\n= 1 −α\\nHence, a 100(1 −α) percent two-sided conﬁdence interval for μ is\\n\\x0e\\nx −zα/2\\nσ\\n√n,\\nx + zα/2\\nσ\\n√n\\n\\x0f\\nwhere x is the observed sample mean.\\nSimilarly, knowing that Z = √n (X−μ)\\nσ\\nis a standard normal random variable,\\nalong with the identities\\nP{Z > zα} = α\\nand\\nP{Z < −zα} = α\\nresults in one-sided conﬁdence intervals of any desired level of conﬁdence.\\nSpeciﬁcally, we obtain that\\n\\x0e\\nx −zα\\nσ\\n√n, ∞\\n\\x0f\\nand\\n\\x0e\\n−∞, x + zα\\nσ\\n√n\\n\\x0f\\nare, respectively, 100(1 −α) percent one-sided upper and 100(1 −α) percent\\none-sided lower conﬁdence intervals for μ.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 272}, page_content='7.3 Interval estimates\\n261\\nExample 7.3.c. Use the data of Example 7.3.a to obtain a 99 percent conﬁ-\\ndence interval estimate of μ, along with 99 percent one-sided upper and lower\\nintervals.\\nSolution. Since z.005 = 2.58, and\\n2.58 α\\n√n = 5.16\\n3\\n= 1.72\\nit follows that a 99 percent conﬁdence interval for μ is\\n9 ± 1.72\\nThat is, the 99 percent conﬁdence interval estimate is (7.28, 10.72).\\nAlso, since z.01 = 2.33, a 99 percent upper conﬁdence interval is\\n(9 −2.33(2/3), ∞) = (7.447, ∞)\\nSimilarly, a 99 percent lower conﬁdence interval is\\n(−∞, 9 + 2.33(2/3)) = (−∞, 10.553)\\n■\\nSometimes we are interested in a two-sided conﬁdence interval of a certain\\nlevel, say 1 −α, and the problem is to choose the sample size n so that the\\ninterval is of a certain size. For instance, suppose that we want to compute an\\ninterval of length .1 that we can assert, with 99 percent conﬁdence, contains μ.\\nHow large need n be? To solve this, note that as z.005 = 2.58 it follows that the\\n99 percent conﬁdence interval for μ from a sample of size n is\\n\\x0e\\nx −2.58 σ\\n√n,\\nx + 2.58 σ\\n√n\\n\\x0f\\nHence, its length is\\n5.16 σ\\n√n\\nThus, to make the length of the interval equal to .1, we must choose\\n5.16 σ\\n√n = .1\\nor\\nn = (51.6σ)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 273}, page_content='262 CHAPTER 7: Parameter estimation\\nRemark\\nThe interpretation of “a 100(1 −α) percent conﬁdence interval” can be con-\\nfusing. It should be noted that we are not asserting that the probability that\\nμ ∈(x −1.96σ/√n, x + 1.96σ/√n) is .95, for there are no random variables\\ninvolved in this assertion. What we are asserting is that the technique utilized\\nto obtain this interval is such that 95 percent of the time that it is employed\\nit will result in an interval in which μ lies. In other words, before the data are\\nobserved we can assert that with probability .95 the interval that will be ob-\\ntained will contain μ, whereas after the data are obtained we can only assert\\nthat the resultant interval indeed contains μ “with conﬁdence .95.”\\nExample 7.3.d. From past experience it is known that the weights of salmon\\ngrown at a commercial hatchery are normal with a mean that varies from sea-\\nson to season but with a standard deviation that remains ﬁxed at 0.3 pound.\\nIf we want to be 95 percent certain that our estimate of the present season’s\\nmean weight of a salmon is correct to within ±0.1 pound, how large a sample\\nis needed?\\nSolution. A 95 percent conﬁdence interval estimate for the unknown mean μ,\\nbased on a sample of size n, is\\nμ ∈\\n\\x0e\\nx −1.96 σ\\n√n, x + 1.96 σ\\n√n\\n\\x0f\\nBecause the estimate x is within 1.96(σ/√n) = .588/√n of any point in the\\ninterval, it follows that we can be 95 percent certain that x is within 0.1 of μ\\nprovided that\\n.588\\n√n ≤0.1\\nThat is, provided that\\n√n ≥5.88\\nor\\nn ≥34.57\\nThat is, a sample size of 35 or larger will sufﬁce.\\n■\\n7.3.1\\nConﬁdence interval for a normal mean when\\nthe variance is unknown\\nSuppose now that X1,...,Xn is a sample from a normal distribution with un-\\nknown mean μ and unknown variance σ 2, and that we wish to construct a'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 274}, page_content='7.3 Interval estimates\\n263\\nFIGURE 7.2\\nt-density function.\\n100(1 −α) percent conﬁdence interval for μ. Since σ is unknown, we can no\\nlonger base our interval on the fact that √n(X −μ)/σ is a standard normal\\nrandom variable. However, by letting S2 = \\x02n\\ni=1(Xi −X)2/(n −1) denote the\\nsample variance, then from Corollary 6.5.2 it follows that\\n√n (X −μ)\\nS\\nis a t-random variable with n−1 degrees of freedom. Hence, from the symmetry\\nof the t-density function (see Figure 7.2), we have that for any α ∈(0,1/2),\\nP\\n\\x03\\n−tα/2,n−1 < √n (X −μ)\\nS\\n< tα/2,n−1\\n\\x05\\n= 1 −α\\nor, equivalently,\\nP{−tα/2,n−1\\nS\\n√n < ¯X −μ < tα/2,n−1\\nS\\n√n} = 1 −α\\nMultiplying all sides of the preceding by −1 and then adding ¯X yields that\\nP\\n\\x18\\nX −tα/2,n−1\\nS\\n√n < μ < X + tα/2,n−1\\nS\\n√n\\n\\x19\\n= 1 −α\\nThus, if it is observed that X = x and S = s, then we can say that “with 100(1 −\\nα) percent conﬁdence”\\nμ ∈\\n\\x0e\\nx −tα/2,n−1\\ns\\n√n, x + tα/2,n−1\\ns\\n√n\\n\\x0f'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 275}, page_content='264 CHAPTER 7: Parameter estimation\\nExample 7.3.e. Let us again consider Example 7.3.a but let us now suppose\\nthat when the value μ is transmitted at location A then the value received at\\nlocation B is normal with mean μ and variance σ 2 but with σ 2 being unknown.\\nIf 9 successive values are, as in Example 7.3.a, 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5,\\ncompute a 95 percent conﬁdence interval for μ.\\nSolution. A simple calculation yields that\\nx = 9\\nand\\ns2 =\\n\\x02x2\\ni −9(x)2\\n8\\n= 9.5\\nor\\ns = 3.082\\nHence, as t.025,8 = 2.306, a 95 percent conﬁdence interval for μ is\\n\\x0c\\n9 −2.306(3.082)\\n3\\n, 9 + 2.306(3.082)\\n3\\n\\r\\n= (6.63,11.37)\\na larger interval than obtained in Example 7.3.a. The reason why the interval\\njust obtained is larger than the one in Example 7.3.a is twofold. The primary\\nreason is that we have a larger estimated variance than in Example 7.3.a. That\\nis, in Example 7.3.a we assumed that σ 2 was known to equal 4, whereas in this\\nexample we assumed it to be unknown and our estimate of it turned out to\\nbe 9.5, which resulted in a larger conﬁdence interval. In fact, the conﬁdence\\ninterval would have been larger than in Example 7.3.a even if our estimate of\\nσ 2 was again 4 because by having to estimate the variance we need to utilize the\\nt-distribution, which has a greater variance and thus a larger spread than the\\nstandard normal (which can be used when σ 2 is assumed known). For instance,\\nif it had turned out that x = 9 and s2 = 4, then our conﬁdence interval would\\nhave been\\n(9 −2.306 · 2\\n3,9 + 2.306 · 2\\n3) = (7.46,10.54)\\nwhich is larger than that obtained in Example 7.3.a.\\n■\\nRemarks\\n(a) The conﬁdence interval for μ when σ is known is based on the fact that\\n√n(X −μ)/σ has a standard normal distribution. When σ is unknown,\\nthe foregoing approach is to estimate it by S and then use the fact that\\n√n(X −μ)/S has a t-distribution with n −1 degrees of freedom.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 276}, page_content='7.3 Interval estimates\\n265\\n(b) The length of a 100(1−2α) percent conﬁdence interval for μ is not always\\nlarger when the variance is unknown. For the length of such an interval\\nis 2zασ/√n when σ is known, whereas it is 2tα,n−1S/√n when σ is un-\\nknown; and it is certainly possible that the sample standard deviation S\\ncan turn out to be much smaller than σ. However, it can be shown that\\nthe mean length of the interval is longer when σ is unknown. That is, it\\ncan be shown that\\ntα,n−1E[S] ≥zασ\\nIndeed, E[S] is evaluated in Chapter 14 and it is shown, for instance, that\\nE[S] =\\n\\x03\\n.94 σ\\nwhen n = 5\\n.97 σ\\nwhen n = 9\\nSince\\nz.025 = 1.96,\\nt.025,4 = 2.78,\\nt.025,8 = 2.31\\nthe length of a 95 percent conﬁdence interval from a sample of size 5 is\\n2×1.96σ/\\n√\\n5 = 1.75σ when σ is known, whereas its expected length is\\n2 × 2.78 × .94σ/\\n√\\n5 = 2.34σ when σ is unknown — an increase of 33.7\\npercent. If the sample is of size 9, then the two values to compare are 1.31\\nσ and 1.49 σ — a gain of 13.7 percent.\\n■\\nA one-sided upper conﬁdence interval can be obtained by noting that\\nP\\n\\x03\\n√n(X −μ)\\nS\\n< tα,n−1\\n\\x05\\n= 1 −α\\nor\\nP\\n\\x18\\nX −μ < S\\n√ntα, n−1\\n\\x19\\n= 1 −α\\nor\\nP\\n\\x18\\nμ > X −S\\n√ntα, n−1\\n\\x19\\n= 1 −α\\nHence, if it is observed that X = x, S = s, then we can assert “with 100(1 −α)\\npercent conﬁdence” that\\nμ ∈\\n\\x0e\\nx −s\\n√ntα,n−1, ∞\\n\\x0f'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 277}, page_content='266 CHAPTER 7: Parameter estimation\\nSimilarly, a 100(1 −α) lower conﬁdence interval would be\\nμ ∈\\n\\x0e\\n−∞, x + s\\n√ntα, n−1\\n\\x0f\\nWe can use R to obtain conﬁdence intervals. To obtain a 100(1 −α) percent\\nconﬁdence interval for the data values x1,...,xn, input\\n> d = c(x1,...,xn)\\n> l = mean(d) −qt(1 −α/2,n −1) ∗sqrt(var(d)/n)\\n> u = mean(d) + qt(1 −α/2,n −1) ∗sqrt(var(d)/n)\\n> c(l,u)\\nand press return. For instance, suppose we want to use R to compute a 99 per-\\ncent conﬁdence interval for the mean of a normal data set when the variance is\\nunknown. Let the data be 5,9,12,8.6,7.2,13,9.5. Then, write the following R\\ncommands:\\n> d = c(5,9,12,8.6,7.2,13,9.5)\\n> l = mean(d) −qt(.995,6) ∗sqrt(var(d)/7)\\n> u = mean(d) + qt(.995,6) ∗sqrt(var(d)/7)\\n> c(l,u)\\nPress return and R yields\\n[1] 5.37346331037313 12.9979652610554\\nWe obtain upper and lower conﬁdence intervals similarly.\\nExample 7.3.f. Determine a 95 percent conﬁdence interval for the average rest-\\ning pulse of the members of a health club if a random selection of 15 members\\nof the club yielded the data 54, 63, 58, 72, 49, 92, 70, 73, 69, 104, 48, 66, 80,\\n64, 77. Also determine a 95 percent lower conﬁdence interval for this mean.\\nSolution. We use R to obtain the two-sided 95 percent conﬁdence interval, and\\nthe 95 percent lower conﬁdence interval as follows:\\n>d = c(54,63,58,72,49,92,70,73,69,104,48,66,80,64,77)\\n>l = mean(d) −qt(.975,14) ∗sqrt(var(d)/15)\\n>u = mean(d) + qt(.975,14) ∗sqrt(var(d)/15)\\n>c(l,u)\\n[1] 60.8669366732691, 77.6663966600643\\n>ub = mean(d) + qt(.95,14) ∗sqrt(var(d)/15)\\n[1] 76.16457'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 278}, page_content='7.3 Interval estimates\\n267\\nThus, the 95 percent two-sided conﬁdence interval for μ is (60.8669366732691,\\n77.6663966600643), while the 95 percent lower conﬁdence interval is\\n(−∞, 76.16457).\\n■\\nOur derivations of the 100(1 −α) percent conﬁdence intervals for the popula-\\ntion mean μ have assumed that the population distribution is normal. How-\\never, even when this is not the case, if the sample size is reasonably large then\\nthe intervals obtained will still be approximate 100(1 −α) percent conﬁdence\\nintervals for μ. This is true because, by the central limit theorem, √n(X −μ)/σ\\nwill have approximately a normal distribution, and √n(X −μ)/S will have\\napproximately a t-distribution.\\nExample 7.3.g. Simulation provides a powerful method for evaluating single\\nand multidimensional integrals. For instance, let f be a function of an r-valued\\nvector (y1,...,yr), and suppose that we want to estimate the quantity θ, de-\\nﬁned by\\nθ =\\n\\x1f 1\\n0\\n\\x1f 1\\n0\\n···\\n\\x1f 1\\n0\\nf (y1,y2,...,yr)dy1dy2,...,dyr\\nTo accomplish this, note that if U1,U2,...,Ur are independent uniform ran-\\ndom variables on (0, 1), then\\nθ = E[f (U1,U2,...,Ur)]\\nNow, the values of independent uniform (0, 1) random variables can be ap-\\nproximated on a computer (by so-called pseudo random numbers); if we generate\\na vector of r of them, and evaluate f at this vector, then the value obtained, call\\nit X1, will be a random variable with mean θ. If we now repeat this process,\\nthen we obtain another value, call it X2, which will have the same distribution\\nas X1. Continuing on, we can generate a sequence X1,X2,...,Xn of indepen-\\ndent and identically distributed random variables with mean θ; we then use\\ntheir observed values to estimate θ. This method of approximating integrals is\\ncalled Monte Carlo simulation.\\nFor instance, suppose we wanted to estimate the one-dimensional integral\\nθ =\\n\\x1f 1\\n0\\n \\n1 −y2 dy = E[\\n!\\n1 −U2]\\nwhere U is a uniform (0, 1) random variable. To do so, let U1,...,U100 be\\nindependent uniform (0, 1) random variables, and set\\nXi =\\n \\n1 −U2\\ni ,\\ni = 1,...,100\\nIn this way, we have generated a sample of 100 random variables having mean\\nθ. Suppose that the computer-generated values of U1,...,U100, resulting in'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 279}, page_content='268 CHAPTER 7: Parameter estimation\\nX1,...,X100 having sample mean .786 and sample standard deviation .03.\\nConsequently, since t.025,99 = 1.985, it follows that a 95 percent conﬁdence in-\\nterval for θ would be given by\\n.786 ± 1.985(.003)\\nAs a result, we could assert, with 95 percent conﬁdence, that θ (which can be\\nshown to equal π/4) is between .780 and .792.\\n■\\n7.3.2\\nPrediction intervals\\nSuppose that X1,...,Xn,Xn+1 is a sample from a normal distribution with\\nunknown mean μ and unknown variance σ 2. Suppose further that the values\\nof X1,...,Xn are to be observed and that we want to use them to predict the\\nvalue of Xn+1. To begin, note that if the mean μ were known, then it would\\nbe the natural predictor for Xn+1. As it is not known, it seems natural to use\\nits current estimator after observing X1,...,Xn, namely the average of these\\nobserved values, as the predicted value of Xn+1. That is, we should use the\\nobserved value of ¯Xn = \\x02n\\ni=1 Xi/n, as the predicted value of Xn+1.\\nSuppose now that we want to determine an interval in which we predict, with\\na certain degree of conﬁdence, that Xn+1 will lie. To obtain such a prediction\\ninterval, note that as ¯Xn is normal with mean μ and variance σ 2/n, and is inde-\\npendent of Xn+1 which is normal with mean μ and variance σ 2, it follows that\\nXn+1 −¯Xn is normal with mean 0 and variance sigma2/n + σ 2. Consequently,\\nXn+1 −¯Xn\\nσ√1 + 1/n\\nis a standard normal random variable.\\nBecause this is independent of S2\\nn = \\x02n\\ni=1(Xi −¯Xn)2/(n −1), it follows from\\nthe same argument used to establish Corollary 6.5.2, that replacing σ by its\\nestimator Sn in the preceding expression will yield a t-random variable with\\nn −1 degrees of freedom. That is,\\nXn+1 −¯Xn\\nSn\\n√1 + 1/n\\nis a t-random variable with n −1 degrees of freedom. Hence, for any α ∈\\n(0,1/2),\\nP{−tα/2,n−1 < Xn+1 −¯Xn\\nSn\\n√1 + 1/n < tα/2,n−1} = 1 −α\\nwhich is equivalent to\\nP{ ¯Xn −tα/2,n−1 Sn\\n!\\n1 + 1/n < Xn+1 < ¯Xn + tα/2,n−1 Sn\\n!\\n1 + 1/n}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 280}, page_content='7.3 Interval estimates\\n269\\nHence, if the observed values of ¯Xn and Sn are, respectively, ¯xn and sn, then\\nwe can predict, with 100(1 −α) percent conﬁdence, that Xn+1 will lie between\\n¯xn −tα/2,n−1sn\\n√1 + 1/n and ¯xn + tα/2,n−1sn\\n√1 + 1/n . That is, with 100(1 −α)\\npercent conﬁdence, we can predict that\\nXn+1 ∈\\n\\x08\\n¯xn −tα/2,n−1sn\\n!\\n1 + 1/n, ¯xn + tα/2,n−1sn\\n!\\n1 + 1/n\\n\\t\\nExample 7.3.h. The following are the number of steps walked in each of the\\nlast 7 days\\n6822\\n5333\\n7420\\n7432\\n6252\\n7005\\n6752\\nAssuming that the daily number of steps can be thought of as being inde-\\npendent realizations from a normal distribution, give a prediction interval\\nthat, with 95 percent conﬁdence, will contain the number of steps that will\\nbe walked tomorrow.\\nSolution. A simple calculation gives that the sample mean and sample vari-\\nance of the 7 data values are\\n¯X7 = 6716.57\\nS7 = 733.97\\nBecause t.025,6 =2.447, and 2.4447 · 733.97√1+1/7=1920.03, we can predict,\\nwith 95 percent conﬁdence, that tomorrow’s number of steps will be between\\n6716.57 −1920.03 and 6716.57 + 1920.03. That is, with 95 percent conﬁdence,\\nX8 will lie in the interval (4796.54, 8636.60).\\n7.3.3\\nConﬁdence intervals for the variance of a\\nnormal distribution\\nIf X1,...,Xn is a sample from a normal distribution having unknown param-\\neters μ and σ 2, then we can construct a conﬁdence interval for σ 2 by using the\\nfact that\\n(n −1) S2\\nσ 2 ∼χ2\\nn−1\\nHence,\\nP\\n\\x18\\nχ2\\n1−α/2,n−1 ≤(n −1) S2\\nσ 2 ≤χ2\\nα/2,n−1\\n\\x19\\n= 1 −α\\nor, equivalently,\\nP\\n\\x03\\n(n −1)S2\\nχ2\\nα/2,n−1\\n≤σ 2 ≤(n −1)S2\\nχ2\\n1−α/2,n−1\\n\\x05\\n= 1 −α'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 281}, page_content='270 CHAPTER 7: Parameter estimation\\nHence when S2 = s2, a 100(1 −α) percent conﬁdence interval for σ 2 is\\n\\x06\\n(n −1)s2\\nχ2\\nα/2,n−1\\n, (n −1)s2\\nχ2\\n1−α/2,n−1\\n\\x07\\nExample 7.3.i. A standardized procedure is expected to produce washers with\\nvery small deviation in their thicknesses. Suppose that 10 such washers were\\nchosen and measured. If the thicknesses of these washers were, in inches,\\n.123\\n.133\\n.124\\n.125\\n.126\\n.128\\n.120\\n.124\\n.130\\n.126\\nwhat is a 90 percent conﬁdence interval for the standard deviation of the thick-\\nness of a washer produced by this procedure?\\nSolution. Using R, we have\\n>d = c(.123,.124,.126,.120,.130,.133,.125,.128,.124,.126)\\n>l = 9 ∗var(d)/ qchisq(.95,9)\\n>u = 9 ∗var(d)/ qchisq(.05,9)\\n>c(l,u)\\n[1]7.26403231164731e −06 3.69611516361794e −05\\nIt follows that, with conﬁdence .90,\\nσ 2 ∈(7.2640 × 10−6, 36.9612 × 10−6)\\nTaking square roots yields, with conﬁdence .90,\\nσ ∈(.00269, .00608)\\n■\\nOne-sided conﬁdence intervals for σ 2 are obtained by similar reasoning and\\nare presented in Table 7.1, which sums up the results of this section.\\n7.4\\nEstimating the difference in means of two normal\\npopulations\\nLet X1,X2,...,Xn be a sample of size n from a normal population having\\nmean μ1 and variance σ 2\\n1 and let Y1,...,Ym be a sample of size m from a dif-\\nferent normal population having mean μ2 and variance σ 2\\n2 and suppose that\\nthe two samples are independent of each other. We are interested in estimating\\nμ1 −μ2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 282}, page_content='7.4 Estimating the difference in means of two normal populations\\n271\\nTable 7.1 100(1 −α) Percent Conﬁdence Intervals\\nX1,...,Xn ∼N (μ,σ 2)\\nX=\\nn\\n\\x04\\ni=1\\nXi/n,\\nS =\\n\"\\n#\\n#\\n$\\nn\\n\\x04\\ni=1\\n(Xi −X)2/(n −1)\\n.\\nAssump-\\ntion\\nParame-\\nter\\nConﬁdence\\nInterval\\nLower Interval\\nUpper Interval\\nσ2 known\\nμ\\nX±zα/2\\nσ\\n√n\\n\\x0e\\n−∞, X+zα\\nσ\\n√n\\n\\x0f\\n\\x0e\\nX+zα\\nσ\\n√n , ∞\\n\\x0f\\nσ2 unknown\\nμ\\nX±tα/2, n−1\\nS\\n√n\\n\\x0e\\n−∞, X+tα, n−1\\nS\\n√n\\n\\x0f\\n\\x0e\\nX−tα, n−1\\nS\\n√n , ∞\\n\\x0f\\nμ unknown\\nσ2\\n⎛\\n⎝(n −1)S2\\nχ2\\nα/2, n−1\\n,\\n(n −1)S2\\nχ2\\n1−α/2, n−1\\n⎞\\n⎠\\n⎛\\n⎝0, (n −1)S2\\nχ2\\n1−α, n−1\\n⎞\\n⎠\\n⎛\\n⎝(n −1)S2\\nχ2\\nα, n−1\\n, ∞\\n⎞\\n⎠\\nSince X = \\x02n\\ni=1 Xi/n and Y = \\x02m\\ni=1 Yi/m are the maximum likelihood esti-\\nmators of μ1 and μ2 it seems intuitive (and can be proven) that X −Y is the\\nmaximum likelihood estimator of μ1 −μ2.\\nTo obtain a conﬁdence interval estimator, we need the distribution of X −Y.\\nBecause\\nX ∼N (μ1,σ 2\\n1 /n)\\nY ∼N (μ2,σ 2\\n2 /m)\\nit follows from the fact that the sum of independent normal random variables\\nis also normal, that\\nX −Y ∼N\\n\\x06\\nμ1 −μ2, σ 2\\n1\\nn + σ 2\\n2\\nm\\n\\x07\\nHence, assuming σ 2\\n1 and σ 2\\n2 are known, we have that\\nX −Y −(μ1 −μ2)\\n)\\nσ 2\\n1\\nn + σ 2\\n2\\nm\\n∼N (0,1)\\n(7.4.1)\\nand so\\nP\\n⎧\\n⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎩\\n−zα/2 < X −Y −(μ1 −μ2)\\n)\\nσ 2\\n1\\nn + σ 2\\n2\\nm\\n< zα/2\\n⎫\\n⎪⎪⎪⎪⎬\\n⎪⎪⎪⎪⎭\\n= 1 −α'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 283}, page_content='272 CHAPTER 7: Parameter estimation\\nor, equivalently,\\nP\\n⎧\\n⎨\\n⎩X −Y −zα/2\\n)\\nσ 2\\n1\\nn + σ 2\\n2\\nm < μ1 −μ2 < X −Y + zα/2\\n)\\nσ 2\\n1\\nn + σ 2\\n2\\nm\\n⎫\\n⎬\\n⎭= 1 −α\\nHence, if X and Y are observed to equal x and y, respectively, then a 100(1 −α)\\ntwo-sided conﬁdence interval estimate for μ1 −μ2 is\\nμ1 −μ2 ∈\\n⎛\\n⎝x −y −zα/2\\n)\\nσ 2\\n1\\nn + σ 2\\n2\\nm , x −y + zα/2\\n)\\nσ 2\\n1\\nn + σ 2\\n2\\nm\\n⎞\\n⎠\\nOne-sided conﬁdence intervals for μ1 −μ2 are obtained in a similar fashion,\\nand we leave it for the reader to verify that a 100(1 −α) percent one-sided\\ninterval is given by\\nμ1 −μ2 ∈\\n\\x0e\\n−∞, x −y + zα\\n \\nσ 2\\n1 /n + σ 2\\n2 /m\\n\\x0f\\nExample 7.4.a. Two different types of electrical cable insulation have recently\\nbeen tested to determine the voltage level at which failures tend to occur. When\\nspecimens were subjected to an increasing voltage stress in a laboratory exper-\\niment, failures for the two types of cable insulation occurred at the following\\nvoltages:\\nType A\\nType B\\n36\\n54\\n52\\n60\\n44\\n52\\n64\\n44\\n41\\n37\\n38\\n48\\n53\\n51\\n68\\n46\\n38\\n44\\n66\\n70\\n36\\n35\\n52\\n62\\n34\\n44\\nSuppose that it is known that the amount of voltage that cables having type\\nA insulation can withstand is normally distributed with unknown mean μA\\nand known variance σ 2\\nA =40, whereas the corresponding distribution for type\\nB insulation is normal with unknown mean μB and known variance σ 2\\nB = 100.\\nDetermine a 95 percent conﬁdence interval for μB −μA. Determine the largest\\nvalue that we can assert, with 95 percent conﬁdence, is less than μB −μA.\\nSolution. Using R, we have\\n>x = c(52,64,38,68,66,52,60,44,48,46,70,62)\\n>y = c(36,44,41,53,38,36,34,54,52,37,51,44,35,44)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 284}, page_content='7.4 Estimating the difference in means of two normal populations\\n273\\n>l = mean(x) −mean(y) −qnorm(.975) ∗sqrt(40/14 + 100/12)\\n>u = mean(x) −mean(y) + qnorm(.975) ∗sqrt(40/14 + 100/12)\\n>c(l,u)\\n[1]6.491114 19.604124\\nThus, with 95 percent conﬁdence, we can assert that μB −μA ∈(6.491114,\\n19.604124). To obtain the lower conﬁdence bound, continue with R to obtain\\n>mean(x) −mean(y) + qnorm(.95) ∗sqrt(40/14 + 100/12)\\n[1]7.545227\\nHence, we can be 95 percent conﬁdent that μB −μA ≥7.545227.\\n■\\nLet us suppose now that we again desire an interval estimator of μ1 −μ2 but\\nthat the population variances σ 2\\n1 and σ 2\\n2 are unknown. In this case, it is natural\\nto try to replace σ 2\\n1 and σ 2\\n2 in Equation (7.4.1) by the sample variances\\nS2\\n1 =\\nn\\n\\x04\\ni=1\\n(Xi −X)2\\nn −1\\nS2\\n2 =\\nm\\n\\x04\\ni=1\\n(Yi −Y)2\\nm −1\\nThat is, it is natural to base our interval estimate on something like\\nX −Y −(μ1 −μ2)\\n \\nS2\\n1/n + S2\\n2/m\\nHowever, to utilize the foregoing to obtain a conﬁdence interval, we need its\\ndistribution and it must not depend on any of the unknown parameters σ 2\\n1\\nand σ 2\\n2 . Unfortunately, this distribution is both complicated and does indeed\\ndepend on the unknown parameters σ 2\\n1 and σ 2\\n2 . In fact, it is only in the special\\ncase when σ 2\\n1 = σ 2\\n2 that we will be able to obtain an interval estimator. So let\\nus suppose that the population variances, though unknown, are equal and let\\nσ 2 denote their common value. Now, from Theorem 6.5.1 it follows that\\n(n −1) S2\\n1\\nσ 2 ∼χ2\\nn−1\\nand\\n(m −1) S2\\n2\\nσ 2 ∼χ2\\nm−1\\nAlso, because the samples are independent, it follows that these two chi-square\\nrandom variables are independent. Hence, from the additive property of chi-\\nsquare random variables, which states that the sum of independent chi-square'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 285}, page_content='274 CHAPTER 7: Parameter estimation\\nrandom variables is also chi-square with a degree of freedom equal to the sum\\nof their degrees of freedom, it follows that\\n(n −1) S2\\n1\\nσ 2 + (m −1) S2\\n2\\nσ 2 ∼χ2\\nn+m−2\\n(7.4.2)\\nAlso, since\\nX −Y ∼N\\n\\x0e\\nμ1 −μ2, σ 2\\nn + σ 2\\nm\\n\\x0f\\nwe see that\\nX −Y −(μ1 −μ2)\\n)\\nσ 2\\nn + σ 2\\nm\\n∼N (0,1)\\n(7.4.3)\\nNow it follows from the fundamental result that in normal sampling X and S2\\nare independent (Theorem 6.5.1), that X1, S2\\n1, X2, S2\\n2 are independent random\\nvariables. Hence, using the deﬁnition of a t-random variable (as the ratio of\\ntwo independent random variables, the numerator being a standard normal\\nand the denominator being the square root of a chi-square random variable\\ndivided by its degree of freedom parameter), it follows from Equations (7.4.2)\\nand (7.4.3) that if we let\\nS2\\np = (n −1)S2\\n1 + (m −1)S2\\n2\\nn + m −2\\nthen\\nX −Y −(μ1 −μ2)\\n!\\nσ 2(1/n + 1/m)\\n÷\\n \\nS2p/σ 2 = X −Y −(μ1 −μ2)\\n \\nS2p(1/n + 1/m)\\nhas a t-distribution with n + m −2 degrees of freedom. Consequently,\\nP\\n\\x03\\n−tα/2,n+m−2 ≤X −Y −(μ1 −μ2)\\nSp\\n√1/n + 1/m\\n≤tα/2,n+m−2\\n\\x05\\n= 1 −α\\nTherefore, when the data result in the values X = x, Y = y, Sp = sp, we obtain\\nthe following 100(1 −α) percent conﬁdence interval for μ1 −μ2:\\n\\x08\\nx −y −tα/2,n+m−2 sp\\n!\\n1/n + 1/m,\\nx −y + tα/2,n+m−2 sp\\n!\\n1/n + 1/m\\n\\t\\n(7.4.4)\\nOne-sided conﬁdence intervals are similarly obtained.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 286}, page_content='7.5 Approximate conﬁdence interval for the mean of a Bernoulli random variable\\n275\\nExample 7.4.b. Find a 95 percent two-sided conﬁdence interval for μB −μA\\nfor the data of Example 7.4.a when the variances are assumed equal, but with\\nan unknown value.\\nSolution. Continuing with R gives\\n>sp = sqrt((11 ∗var(x) + 13 ∗var(y))/24)\\n>l = mean(x) −mean(y) −qt(.975,24) ∗sp ∗sqrt(1/12 + 1/14)\\n>l = mean(x) −mean(y) + qt(.975,24) ∗sp ∗sqrt(1/12 + 1/14)\\n>c(l,u)\\n[1]5.830952 20.264286\\n■\\nRemark\\nThe conﬁdence interval given by Equation (7.4.4) was obtained under the as-\\nsumption that the population variances are equal; with σ 2 as their common\\nvalue, it follows that\\nX −Y −(μ1 −μ2)\\n!\\nσ 2/n + σ 2/m\\n= X −Y −(μ1 −μ2)\\nσ√1/n + 1/m\\nhas a standard normal distribution. However, since σ 2 is unknown this result\\ncannot be immediately applied to obtain a conﬁdence interval; σ 2 must ﬁrst\\nbe estimated. To do so, note that both sample variances are estimators of σ 2;\\nmoreover, since S2\\n1 has n −1 degrees of freedom and S2\\n2 has m −1, the appro-\\npriate estimator is to take a weighted average of the two sample variances, with\\nthe weights proportional to these degrees of freedom. That is, the estimator of\\nσ 2 is the pooled estimator\\nS2\\np = (n −1)S2\\n1 + (m −1)S2\\n2\\nn + m −2\\nand the conﬁdence interval is then based on the statistic\\nX −Y −(μ1 −μ2)\\n \\nS2p\\n√1/n + 1/m\\nwhich, by our previous analysis, has a t-distribution with n + m −2 degrees of\\nfreedom. The results of this section are summarized in Table 7.2.\\n7.5\\nApproximate conﬁdence interval for the mean of a\\nBernoulli random variable\\nConsider a population of items, each of which independently meets certain\\nstandards with some unknown probability p. If n of these items are tested to'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 287}, page_content='276 CHAPTER 7: Parameter estimation\\nTable 7.2 100(1 −σ) Percent Conﬁdence Intervals for μ1 −μ2\\nX1,...,Xn ∼N(μ1,σ 2\\n1 )\\nY1,...,Ym ∼N(μ2,σ 2\\n2 )\\nX =\\nn\\n\\x04\\ni=1\\nXi/n,\\nS2\\n1 =\\nn\\x02\\ni=1\\n(Xi −X)2/(n −1)\\nY =\\nm\\n\\x04\\ni=1\\nYi/n,\\nS2\\n2 =\\nm\\n\\x02\\ni=1\\n(Yi −Y)2/(m −1).\\nAssumption\\nConﬁdence Interval\\nσ1, σ2 known\\nX −Y ± zα/2\\n \\nσ 2\\n1 /n + σ 2\\n2 /m\\nσ1, σ2 unknown but equal\\nX −Y ± tα/2, n+m−2\\n)\\x0e 1\\nn + 1\\nm\\n\\x0f (n −1)S2\\n1 + (m −1)S2\\n2\\nn + m −2\\nAssumption\\nLower Conﬁdence Interval\\nσ1, σ2 known\\n(−∞, X −Y + zα\\n \\nσ 2\\n1 /n + σ 2\\n2 /m)\\nσ1, σ2 unknown but equal\\n⎛\\n⎝−∞, X −Y + tα, n+m−2\\n)\\x0e 1\\nn + 1\\nm\\n\\x0f (n −1)S2\\n1 + (m −1)S2\\n2\\nn + m −2\\n⎞\\n⎠\\nNote: Upper conﬁdence intervals for μ1 −μ2 are obtained from lower conﬁdence intervals for μ2 −μ1.\\ndetermine whether they meet the standards, how can we use the resulting data\\nto obtain a conﬁdence interval for p?\\nIf we let X denote the number of the n items that meet the standards, then X is\\na binomial random variable with parameters n and p. Thus, when n is large, it\\nfollows by the normal approximation to the binomial that X is approximately\\nnormally distributed with mean np and variance np(1 −p). Hence,\\nX −np\\n√np(1 −p)\\n·∼N (0,1)\\n(7.5.1)\\nwhere\\n·∼means “is approximately distributed as.” Therefore, for any α ∈(0,1),\\nP\\n\\x18\\n−zα/2 <\\nX −np\\n√np(1 −p) < zα/2\\n\\x19\\n≈1 −α\\nand so if X is observed to equal x, then an approximate 100(1 −α) percent\\nconﬁdence region for p is\\n\\x18\\np : −zα/2 <\\nx −np\\n√np(1 −p) < zα/2\\n\\x19\\nThe foregoing region, however, is not an interval. To obtain a conﬁdence inter-\\nval for p, let ˆp=X/n be the fraction of the items that meet the standards. From'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 288}, page_content='7.5 Approximate conﬁdence interval for the mean of a Bernoulli random variable\\n277\\nExample 7.2.a, ˆp is the maximum likelihood estimator of p, and so should be\\napproximately equal to p. As a result,\\n!\\nn ˆp(1 −ˆp) will be approximately equal\\nto √np(1 −p) and so from Equation (7.5.1) we see that\\nX −np\\n!\\nn ˆp(1 −ˆp)\\n·∼N (0,1)\\nHence, for any α ∈(0,1) we have that\\nP\\n\\x03\\n−zα/2 <\\nX −np\\n!\\nn ˆp(1 −ˆp)\\n< zα/2\\n\\x05\\n≈1 −α\\nor, equivalently,\\nP{−zα/2\\n!\\nn ˆp(1 −ˆp) < np −X < zα/2\\n!\\nn ˆp(1 −ˆp)} ≈1 −α\\nDividing all sides of the preceding inequality by n, and using that ˆp =X/n, the\\npreceding can be written as\\nP{ ˆp−zα/2\\n!\\nˆp(1 −ˆp)/n < p < ˆp+zα/2\\n!\\nˆp(1 −ˆp)/n} ≈1 −α\\nwhich yields an approximate 100(1 −α) percent conﬁdence interval for p.\\nExample 7.5.a. A sample of 100 transistors is randomly chosen from a large\\nbatch and tested to determine if they meet the current standards. If 80 of them\\nmeet the standards, then an approximate 95 percent conﬁdence interval for p,\\nthe fraction of all the transistors that meet the standards, is given by\\n(.8 −1.96\\n!\\n.8(.2)/100, .8 + 1.96\\n!\\n.8(.2)/100) = (.7216, .8784)\\nThat is, with “95 percent conﬁdence,” between 72.16 and 87.84 percent of all\\ntransistors meet the standards.\\n■\\nExample 7.5.b. In August 2013, the New York Times reported that a recent poll\\nindicated that 52 percent of the population was in favor of the job performance\\nof President Obama, with a margin of error of ±4 percent. What does this\\nmean? Can we infer how many people were questioned?\\nSolution. It has become common practice for the news media to present 95\\npercent conﬁdence intervals. Since z.025 = 1.96, a 95 percent conﬁdence interval\\nfor p, the percentage of the population that is in favor of President Obama’s\\njob performance, is given by\\nˆp±1.96\\n!\\nˆp(1 −ˆp)/n = .52 ± 1.96\\n!\\n.52(.48)/n\\nwhere n is the size of the sample. Since the “margin of error” is ±4 percent, it\\nfollows that\\n1.96\\n!\\n.52(.48)/n = .04'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 289}, page_content='278 CHAPTER 7: Parameter estimation\\nor\\nn = (1.96)2(.52)(.48)\\n(.04)2\\n= 599.29\\nThat is, approximately 599 people were sampled, and 52 percent of them re-\\nported favorably on President Obama’s job performance.\\n■\\nWe often want to specify an approximate 100(1 −α) percent conﬁdence inter-\\nval for p that is no greater than some given length, say b. The problem is to\\ndetermine the appropriate sample size n to obtain such an interval. To do so,\\nnote that the length of the approximate 100(1 −α) percent conﬁdence interval\\nfor p from a sample of size n is\\n2zα/2\\n!\\nˆp(1 −ˆp)/n\\nwhich is approximately equal to 2zα/2\\n√p(1 −p)/n. Unfortunately, p is not\\nknown in advance, and so we cannot just set 2zα/2\\n√p(1 −p)/n equal to b to\\ndetermine the necessary sample size n. What we can do, however, is to ﬁrst\\ntake a preliminary sample to obtain a rough estimate of p, and then use this\\nestimate to determine n. That is, we use p∗, the proportion of the preliminary\\nsample that meets the standards, as a preliminary estimate of p; we then deter-\\nmine the total sample size n by solving the equation\\n2zα/2\\n!\\np∗(1 −p∗)/n = b\\nSquaring both sides of the preceding yields that\\n(2zα/2)2p∗(1 −p∗)/n = b2\\nor\\nn = (2zα/2)2p∗(1 −p∗)\\nb2\\nThat is, if k items were initially sampled to obtain the preliminary estimate of\\np, then an additional n −k (or 0 if n ≤k) items should be sampled.\\nExample 7.5.c. A certain manufacturer produces computer chips; each chip is\\nindependently acceptable with some unknown probability p. To obtain an ap-\\nproximate 99 percent conﬁdence interval for p, whose length is approximately\\n.05, an initial sample of 30 chips has been taken. If 26 of these chips are of ac-\\nceptable quality, then the preliminary estimate of p is 26/30. Using this value,\\na 99 percent conﬁdence interval of length approximately .05 would require an\\napproximate sample of size\\nn = 4(z.005)2\\n(.05)2\\n26\\n30\\n\\x0e\\n1 −26\\n30\\n\\x0f\\n= 4(2.58)2\\n(.05)2\\n26\\n30\\n4\\n30 = 1231'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 290}, page_content='7.5 Approximate conﬁdence interval for the mean of a Bernoulli random variable\\n279\\nHence, we should now sample an additional 1201 chips and if, for instance,\\n1040 of them are acceptable, then the ﬁnal 99 percent conﬁdence interval for\\np is\\n\\x06\\n1066\\n1231 −\\n)\\n1066\\n\\x0e\\n1 −1066\\n1231\\n\\x0f z.005\\n1231, 1066\\n1231 +\\n)\\n1066\\n\\x0e\\n1 −1066\\n1231\\n\\x0f z.005\\n1231\\n\\x07\\nor\\np ∈(.84091,.89101)\\n■\\nRemark\\nAs shown, a 100(1−α) percent conﬁdence interval for p will be of approximate\\nlength b when the sample size is\\nn = (2zα/2)2\\nb2\\np(1 −p)\\nNow it is easily shown that the function g(p) = p(1 −p) attains its maximum\\nvalue of 1\\n4, in the interval 0 ≤p ≤1, when p = 1\\n2. Thus an upper bound on n is\\nn ≤(zα/2)2\\nb2\\nand so by choosing a sample whose size is at least as large as (zα/2)2/b2, one\\ncan be assured of obtaining a conﬁdence interval of length no greater than b\\nwithout need of any additional sampling.\\n■\\nOne-sided approximate conﬁdence intervals for p are also easily obtained; Ta-\\nble 7.3 gives the results.\\nTable 7.3 Approximate 100(1 −α) Percent Conﬁdence\\nIntervals for p\\nX Is a Binomial (n,p) Random Variable ˆp = X/n\\n.\\nType of Interval\\nConﬁdence Interval\\nTwo-sided\\nˆp±zα/2\\n!\\nˆp(1 −ˆp)/n\\nOne-sided lower\\n\\x08\\n−∞, ˆp+zα\\n!\\nˆp(1 −ˆp)/n\\n\\t\\nOne-sided upper\\n\\x08\\nˆp−zα\\n!\\nˆp(1 −ˆp)/n, ∞'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 291}, page_content='280 CHAPTER 7: Parameter estimation\\n7.6\\nConﬁdence interval of the mean of the exponential\\ndistribution 2\\nIf X1,X2,...,Xn are independent exponential random variables each having\\nmean θ, then it can be shown that the maximum likelihood estimator of θ\\nis the sample mean \\x02n\\ni=1 Xi/n. To obtain a conﬁdence interval estimator of θ,\\nrecall from Section 5.7 that \\x02n\\ni=1 Xi has a gamma distribution with parameters\\nn, 1/θ. This in turn implies (from the relationship between the gamma and\\nchi-square distribution shown in Section 5.8.1) that\\n2\\nθ\\nn\\n\\x04\\ni=1\\nXi ∼χ2\\n2n\\nHence, for any α ∈(0,1)\\nP\\n\\x03\\nχ2\\n1−α/2, 2n < 2\\nθ\\nn\\n\\x04\\ni=1\\nXi < χ2\\nα/2, 2n\\n\\x05\\n= 1 −α\\nor, equivalently,\\nP\\n⎧\\n⎪⎪⎨\\n⎪⎪⎩\\n2\\nn\\x02\\ni=1\\nXi\\nχ2\\nα/2, 2n\\n< θ <\\n2\\nn\\x02\\ni=1\\nXi\\nχ2\\n1−α/2, 2n\\n⎫\\n⎪⎪⎬\\n⎪⎪⎭\\n= 1 −α\\nHence, a 100(1 −α) percent conﬁdence interval for θ is\\nθ ∈\\n⎛\\n⎜⎜⎝\\n2\\nn\\x02\\ni=1\\nXi\\nχ2\\nα/2, 2n\\n,\\n2\\nn\\x02\\ni=1\\nXi\\nχ2\\n1−α/2, 2n\\n⎞\\n⎟⎟⎠\\nExample 7.6.a. The successive items produced by a certain manufacturer are\\nassumed to have useful lives that (in hours) are independent with a common\\ndensity function\\nf (x) = 1\\nθ e−x/θ,\\n0 < x < ∞\\nIf the sum of the lives of the ﬁrst 10 items is equal to 1740, what is a 95 percent\\nconﬁdence interval for the population mean θ?\\n2Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 292}, page_content='7.7 Evaluating a point estimator\\n281\\nSolution. R gives\\n>l = 2 ∗1740/qchisq(1 −.05/2,20)\\n>u = 2 ∗1740/qchisq(.05/2,20)\\n>c(l,u)\\n[1]101.8449 362.8486\\n■\\n7.7\\nEvaluating a point estimator3\\nLet X = (X1,...,Xn) be a sample from a population whose distribution is spec-\\niﬁed up to an unknown parameter θ, and let d = d(X) be an estimator of θ.\\nHow are we to determine its worth as an estimator of θ? One way is to consider\\nthe square of the difference between d(X) and θ. However, since (d(X) −θ)2 is\\na random variable, let us agree to consider r(d,θ), the mean square error of the\\nestimator d, which is deﬁned by\\nr(d,θ) = E[(d(X) −θ)2]\\nas an indication of the worth of d as an estimator of θ.\\nIt would be nice if there were a single estimator d that minimized r(d,θ) for all\\npossible values of θ. However, except in trivial situations, this will never be the\\ncase. For example, consider the estimator d∗deﬁned by\\nd∗(X1,...,Xn) = 4\\nThat is, no matter what the outcome of the sample data, the estimator d∗\\nchooses 4 as its estimate of θ. While this seems like a silly estimator (since\\nit makes no use of the data), it is, however, true that when θ actually equals 4,\\nthe mean square error of this estimator is 0. Thus, the mean square error of any\\nestimator different from d∗must, in most situations, be larger than the mean\\nsquare error of d∗when θ = 4.\\nAlthough minimum mean square estimators rarely exist, it is sometimes pos-\\nsible to ﬁnd an estimator having the smallest mean square error among all\\nestimators that satisfy a certain property. One such property is that of unbi-\\nasedness.\\nDeﬁnition. Let d = d(X) be an estimator of the parameter θ. Then\\nbθ(d) = E[d(X)] −θ\\nis called the bias of d as an estimator of θ. If bθ(d) = 0 for all θ, then we say\\nthat d is an unbiased estimator of θ. In other words, an estimator is unbiased if\\n3Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 293}, page_content='282 CHAPTER 7: Parameter estimation\\nits expected value always equals the value of the parameter it is attempting to\\nestimate.\\nExample 7.7.a. Let X1,X2,...,Xn be a random sample from a distribution\\nhaving unknown mean θ. Then\\nd1(X1,X2,...,Xn) = X1\\nand\\nd2(X1,X2,...,Xn) = X1 + X2 + ··· + Xn\\nn\\nare both unbiased estimators of θ since\\nE[X1] = E\\n\\x0cX1 + X2 + ··· + Xn\\nn\\n\\r\\n= θ\\nMore generally, d3(X1,X2,...,Xn) = \\x02n\\ni=1 λiXi is an unbiased estimator of θ\\nwhenever \\x02n\\ni=1 λi = 1. This follows since\\nE\\n\\x16 n\\n\\x04\\ni=1\\nλiXi\\n\\x17\\n=\\nn\\n\\x04\\ni=1\\nE[λiXi]\\n=\\nn\\n\\x04\\ni=1\\nλiE(Xi)\\n= θ\\nn\\n\\x04\\ni=1\\nλi\\n= θ\\n■\\nIf d(X1,...,Xn) is an unbiased estimator, then its mean square error is given\\nby\\nr(d,θ) = E[(d(X) −θ)2]\\n= E[(d(X) −E[d(X)])2]\\nsince d is unbiased\\n= Var(d(X))\\nThus the mean square error of an unbiased estimator is equal to its variance.\\nExample 7.7.b (Combining Independent Unbiased Estimators). Let d1 and d2\\ndenote independent unbiased estimators of θ, having known variances σ 2\\n1 and\\nσ 2\\n2 . That is, for i = 1,2,\\nE[di] = θ,\\nVar(di) = σ 2\\ni'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 294}, page_content='7.7 Evaluating a point estimator\\n283\\nAny estimator of the form\\nd = λd1 + (1 −λ)d2\\nwill also be unbiased. To determine the value of λ that results in d having the\\nsmallest possible mean square error, note that\\nr(d,θ) = Var(d)\\n= λ2 Var(d1) + (1 −λ)2 Var(d2)\\nby the independence of d1and d2\\n= λ2σ 2\\n1 + (1 −λ)2σ 2\\n2\\nDifferentiation yields that\\nd\\ndλr(d,θ) = 2λσ 2\\n1 −2(1 −λ)σ 2\\n2\\nTo determine the value of λ that minimizes r(d,θ) — call it ˆλ — set this equal\\nto 0 and solve for λ to obtain\\n2ˆλσ 2\\n1 = 2(1 −ˆλ)σ 2\\n2\\nor\\nˆλ =\\nσ 2\\n2\\nσ 2\\n1 + σ 2\\n2\\n=\\n1/σ 2\\n1\\n1/σ 2\\n1 + 1/σ 2\\n2\\nIn words, the optimal weight to give an estimator is inversely proportional to\\nits variance (when all the estimators are unbiased and independent).\\nFor an application of the foregoing, suppose that a conservation organization\\nwants to determine the acidity content of a certain lake. To determine this\\nquantity, they draw some water from the lake and then send samples of this\\nwater to n different laboratories. These laboratories will then, independently,\\ntest for acidity content by using their respective titration equipment, which is of\\ndiffering precision. Speciﬁcally, suppose that di, the result of a titration test at\\nlaboratory i, is a random variable having mean θ, the true acidity of the sample\\nwater, and variance σ 2\\ni ,i = 1,...,n. If the quantities σ 2\\ni ,i = 1,...,n are known\\nto the conservation organization, then they should estimate the acidity of the\\nsampled water from the lake by\\nd =\\nn\\x02\\ni=1\\ndi/σ 2\\ni\\nn\\x02\\ni=1\\n1/σ 2\\ni'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 295}, page_content='284 CHAPTER 7: Parameter estimation\\nThe mean square error of d is as follows:\\nr(d,θ) = Var(d)\\nsince d is unbiased\\n=\\n\\x06 n\\n\\x04\\ni=1\\n1/σ 2\\ni\\n\\x07−2\\nn\\n\\x04\\ni=1\\n\\x06\\n1\\nσ 2\\ni\\n\\x072\\nσ 2\\ni\\n=\\n1\\nn\\x02\\ni=1\\n1/σ 2\\ni\\n■\\nA generalization of the result that the mean square error of an unbiased esti-\\nmator is equal to its variance is that the mean square error of any estimator is\\nequal to its variance plus the square of its bias. This follows since\\nr(d,θ) = E[(d(X) −θ)2]\\n= E[(d −E[d] + E[d] −θ)2]\\n= E[(d −E[d])2 + (E[d] −θ)2 + 2(E[d] −θ)(d −E[d])]\\n= E[(d −E[d])2] + E[(E[d] −θ)2]\\n+ 2E[(E[d] −θ)(d −E[d])]\\n= E[(d −E[d])2] + (E[d] −θ)2 + 2(E[d] −θ)E[d −E[d]]\\nsince E[d] −θ is constant\\n= E[(d −E[d])2] + (E[d] −θ)2\\nThe last equality follows since\\nE[d −E[d]] = 0\\nHence\\nr(d,θ) = Var(d) + b2\\nθ(d)\\nExample 7.7.c. Let X1,...,Xn denote a sample from a uniform (0,θ) distribu-\\ntion, where θ is assumed unknown. Since\\nE[Xi] = θ\\n2\\na “natural” estimator to consider is the unbiased estimator\\nd1 = d1(X) =\\n2\\nn\\x02\\ni=1\\nXi\\nn\\nSince E[d1] = θ, it follows that\\nr(d1,θ) = Var(d1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 296}, page_content='7.7 Evaluating a point estimator\\n285\\n= 4\\nn Var(Xi)\\n= 4\\nn\\nθ2\\n12\\nsince Var(Xi) = θ2\\n12\\n= θ2\\n3n\\nA second possible estimator of θ is the maximum likelihood estimator, which,\\nas shown in Example 7.2.d, is given by\\nd2 = d2(X) = max\\ni\\nXi\\nTo compute the mean square error of d2 as an estimator of θ, we need to ﬁrst\\ncompute its mean (so as to determine its bias) and variance. To do so, note that\\nthe distribution function of d2 is as follows:\\nF2(x) ≡P{d2(X) ≤x}\\n= P{max\\ni\\nXi ≤x}\\n= P{Xi ≤x for all i = 1,...,n}\\n=\\nn\\n\\x0b\\ni=1\\nP{Xi ≤x}\\nby independence\\n=\\n\\x08x\\nθ\\n\\tn\\nx ≤θ\\nHence, upon differentiating, we obtain that the density function of d2 is\\nf2(x) = nxn−1\\nθn\\n,x ≤θ\\nTherefore,\\nE[d2] =\\n\\x1f ∞\\n0\\nx f2(x)dx = n\\nθn\\n\\x1f θ\\n0\\nxn dx =\\nn\\nn + 1 θ\\n(7.7.1)\\nAlso\\nE[d2\\n2] = n\\nθn\\n\\x1f θ\\n0\\nxn+1 dx =\\nn\\nn + 2 θ2\\nand so\\nVar(d2) =\\nn\\nn + 2θ2 −\\n\\x0e\\nn\\nn + 1θ\\n\\x0f2\\n(7.7.2)\\n= nθ2\\n\\x0c\\n1\\nn + 2 −\\nn\\n(n + 1)2\\n\\r\\n=\\nnθ2\\n(n + 2)(n + 1)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 297}, page_content='286 CHAPTER 7: Parameter estimation\\nHence\\nr(d2,θ) = (E(d2) −θ)2 + Var(d2)\\n(7.7.3)\\n=\\nθ2\\n(n + 1)2 +\\nnθ2\\n(n + 2)(n + 1)2\\n=\\nθ2\\n(n + 1)2\\n\\x0c\\n1 +\\nn\\nn + 2\\n\\r\\n=\\n2θ2\\n(n + 1)(n + 2)\\nSince\\n2θ2\\n(n + 1)(n + 2) ≤θ2\\n3n\\nn = 1,2,...\\nit follows that d2 is a superior estimator of θ than is d1.\\nEquation (7.7.1) suggests the use of even another estimator — namely, the\\nunbiased estimator (1 + 1/n)d2(X) = (1 + 1/n)maxi Xi. However, rather than\\nconsidering this estimator directly, let us consider all estimators of the form\\ndc(X) = c max\\ni\\nXi = c d2(X)\\nwhere c is a given constant. The mean square error of this estimator is\\nr(dc(X),θ) = Var(dc(X)) + (E[dc(X)] −θ)2\\n= c2 Var(d2(X)) + (cE[d2(X)] −θ)2\\n=\\nc2nθ2\\n(n + 2)(n + 1)2 + θ2\\n\\x0e c n\\nn + 1 −1\\n\\x0f2\\nby Equations (7.7.2) and (7.7.1)\\n(7.7.4)\\nTo determine the constant c resulting in minimal mean square error, we differ-\\nentiate to obtain\\nd\\ndcr(dc(X),θ) =\\n2c nθ2\\n(n + 2)(n + 1)2 + 2θ2n\\nn + 1\\n\\x0e c n\\nn + 1 −1\\n\\x0f\\nEquating this to 0 shows that the best constant c — call it c∗— is such that\\nc∗\\nn + 2 + c∗n −(n + 1) = 0\\nor\\nc∗= (n + 1)(n + 2)\\nn2 + 2n + 1\\n= n + 2\\nn + 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 298}, page_content='7.8 The Bayes estimator\\n287\\nSubstituting this value of c into Equation (7.7.4) yields that\\nr\\n\\x0en + 2\\nn + 1 max\\ni\\nXi,θ\\n\\x0f\\n= (n + 2)nθ2\\n(n + 1)4\\n+ θ2\\n\\x0en(n + 2)\\n(n + 1)2 −1\\n\\x0f2\\n= (n + 2)nθ2\\n(n + 1)4\\n+\\nθ2\\n(n + 1)4\\n=\\nθ2\\n(n + 1)2\\nA comparison with Equation (7.7.3) shows that the (biased) estimator (n +\\n2)/(n + 1)maxi Xi has about half the mean square error of the maximum like-\\nlihood estimator maxi Xi.\\n■\\n7.8\\nThe Bayes estimator4\\nIn certain situations it seems reasonable to regard an unknown parameter θ\\nas being the value of a random variable from a given probability distribution.\\nThis usually arises when, prior to the observance of the outcomes of the data\\nX1,...,Xn, we have some information about the value of θ and this informa-\\ntion is expressible in terms of a probability distribution (called appropriately\\nthe prior distribution of θ). For instance, suppose that from past experience we\\nknow that θ is equally likely to be near any value in the interval (0, 1). Hence,\\nwe could reasonably assume that θ is chosen from a uniform distribution on\\n(0, 1).\\nSuppose now that our prior feelings about θ are that it can be regarded as\\nbeing the value of a continuous random variable having probability density\\nfunction p(θ); and suppose that we are about to observe the value of a sample\\nwhose distribution depends on θ. Speciﬁcally, suppose that f (x|θ) represents\\nthe likelihood — that is, it is the probability mass function in the discrete case\\nor the probability density function in the continuous case — that a data value\\nis equal to x when θ is the value of the parameter. If the observed data values\\nare Xi = xi,i = 1,...,n, then the updated, or conditional, probability density\\nfunction of θ is as follows:\\nf (θ|x1,...,xn) = f (θ,x1,...,xn)\\nf (x1,...,xn)\\n=\\np(θ)f (x1,...,xn|θ)\\n0\\nf (x1,...,xn|θ)p(θ)dθ\\nThe conditional density function f (θ|x1,...,xn) is called the posterior density\\nfunction. (Thus, before observing the data, one’s feelings about θ are expressed\\n4Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 299}, page_content='288 CHAPTER 7: Parameter estimation\\nin terms of the prior distribution, whereas once the data are observed, this prior\\ndistribution is updated to yield the posterior distribution.)\\nNow we have shown that whenever we are given the probability distribution\\nof a random variable, the best estimate of the value of that random variable,\\nin the sense of minimizing the expected squared error, is its mean. Therefore,\\nit follows that the best estimate of θ, given the data values Xi = xi,i = 1,...,n,\\nis the mean of the posterior distribution f (θ|x1,...,xn). This estimator, called\\nthe Bayes estimator, is written as E[θ|X1,...,Xn]. That is, if Xi = xi, i = 1,...,n,\\nthen the value of the Bayes estimator is\\nE[θ|X1 = x1,...,Xn = xn] =\\n\\x1f\\nθf (θ|x1,...,xn)dθ\\nExample 7.8.a. Suppose that X1,...,Xn are independent Bernoulli random\\nvariables, each having probability mass function given by\\nf (x|θ) = θx(1 −θ)1−x,\\nx = 0,1\\nwhere θ is unknown. Further, suppose that θ is chosen from a uniform distri-\\nbution on (0, 1). Compute the Bayes estimator of θ.\\nSolution. We must compute E[θ|X1,...,Xn]. Since the prior density of θ is\\nthe uniform density\\np(θ) = 1,\\n0 < θ < 1\\nwe have that the conditional density of θ given X1,...,Xn is given by\\nf (θ|x1,...,xn) = f (x1,...,xn,θ)\\nf (x1,...,xn)\\n=\\nf (x1,...,xn|θ)p(θ)\\n0 1\\n0 f (x1,...,xn|θ)p(θ)dθ\\n=\\nθ\\x03n\\n1xi(1 −θ)n−\\x03n\\n1 xi\\n0 1\\n0 θ\\x03n\\n1xi(1 −θ)n−\\x03n\\n1 xi dθ\\nNow it can be shown that for integral values m and r\\n\\x1f 1\\n0\\nθm(1 −θ)r dθ =\\nm!r!\\n(m + r + 1)!\\n(7.8.1)\\nHence, upon letting x = \\x02n\\ni=1 xi\\nf (θ|x1,...,xn) = (n + 1)! θx(1 −θ)n−x\\nx! (n −x)!\\n(7.8.2)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 300}, page_content='7.8 The Bayes estimator\\n289\\nTherefore,\\nE[θ|x1,...,xn] = (n + 1)!\\nx!(n −x)!\\n\\x1f 1\\n0\\nθ1+x(1 −θ)n−x dθ\\n= (n + 1)!\\nx!(n −x)!\\n(1 + x)!(n −x)!\\n(n + 2)!\\nfrom Equation (7.8.1)\\n= x + 1\\nn + 2\\nThus, the Bayes estimator is given by\\nE[θ|X1,...,Xn] =\\nn\\x02\\ni=1\\nXi + 1\\nn + 2\\nAs an illustration, if 10 independent trials, each of which results in a success\\nwith probability θ, result in 6 successes, then assuming a uniform (0, 1) prior\\ndistribution on θ, the Bayes estimator of θ is 7/12 (as opposed, for instance, to\\nthe maximum likelihood estimator of 6/10).\\n■\\nRemark\\nThe conditional distribution of θ given that Xi = xi,i = 1,...,n, whose den-\\nsity function is given by Equation (7.8.2), is called the beta distribution with\\nparameters \\x02n\\ni=1 xi + 1, n −\\x02n\\ni=1 xi + 1.\\n■\\nExample 7.8.b. Suppose X1,...,Xn are independent normal random vari-\\nables, each having unknown mean θ and known variance σ 2\\n0 . If θ is itself\\nselected from a normal population having known mean μ and known vari-\\nance σ 2, what is the Bayes estimator of θ?\\nSolution. In order to determine E[θ|X1,...,Xn], the Bayes estimator, we need\\nﬁrst determine the conditional density of θ given the values of X1,...,Xn. Now\\nf (θ|x1,...,xn) = f (x1,...,xn|θ)p(θ)\\nf (x1,...,xn)\\nwhere\\nf (x1,...,xn|θ) =\\n1\\n(2π)n/2σ n\\n0\\nexp\\n\\x03\\n−\\nn\\n\\x04\\ni=1\\n(xi −θ)2/2σ 2\\n0\\n\\x05\\np(θ) =\\n1\\n√\\n2πσ\\nexp{−(θ −μ)2/2σ 2}\\nand\\nf (x1,...,xn) =\\n\\x1f ∞\\n−∞\\nf (x1,...,xn|θ)p(θ)dθ'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 301}, page_content='290 CHAPTER 7: Parameter estimation\\nWith the help of a little algebra, it can now be shown that this conditional\\ndensity is a normal density with mean\\nE[θ|X1,...,Xn] =\\nnσ 2\\nnσ 2 + σ 2\\n0\\nX +\\nσ 2\\n0\\nnσ 2 + σ 2\\n0\\nμ\\n(7.8.3)\\n=\\nn\\nσ 2\\n0\\nn\\nσ 2\\n0\\n+ 1\\nσ 2\\nX +\\n1\\nσ 2\\nn\\nσ 2\\n0\\n+ 1\\nσ 2\\nμ\\nand variance\\nVar(θ|X1,...,Xn) =\\nσ 2\\n0 σ 2\\nnσ 2 + σ 2\\n0\\nWriting the Bayes estimator as we did in Equation (7.8.3) is informative, for it\\nshows that it is a weighted average of X, the sample mean, and μ, the a priori\\nmean. In fact, the weights given to these two quantities are in proportion to the\\ninverses of σ 2\\n0 /n (the conditional variance of the sample mean X given θ) and\\nσ 2 (the variance of the prior distribution).\\n■\\nRemark: On choosing a normal prior\\nAs illustrated by Example 7.8.b, it is computationally very convenient to choose\\na normal prior for the unknown mean θ of a normal distribution — for then\\nthe Bayes estimator is simply given by Equation (7.8.3). This raises the question\\nof how one should go about determining whether there is a normal prior that\\nreasonably represents one’s prior feelings about the unknown mean.\\nTo begin, it seems reasonable to determine the value — call it μ — that you\\na priori feel is most likely to be near θ. That is, we start with the mode (which\\nequals the mean when the distribution is normal) of the prior distribution. We\\nshould then try to ascertain whether or not we believe that the prior distribu-\\ntion is symmetric about μ. That is, for each a > 0 do we believe that it is just as\\nlikely that θ will lie between μ −a and μ as it is that it will be between μ and\\nμ + a? If the answer is positive, then we accept, as a working hypothesis, that\\nour prior feelings about θ can be expressed in terms of a prior distribution that\\nis normal with mean μ. To determine σ, the standard deviation of the normal\\nprior, think of an interval centered about μ that you a priori feel is 90 percent\\ncertain to contain θ. For instance, suppose you feel 90 percent (no more and\\nno less) certain that θ will lie between μ −a and μ + a. Then, since a normal\\nrandom variable θ with mean μ and variance σ 2 is such that\\nP\\n\\x18\\n−1.645 < θ −μ\\nσ\\n< 1.645\\n\\x19\\n= .90'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 302}, page_content='7.8 The Bayes estimator\\n291\\nor\\nP{μ −1.645σ < θ < μ + 1.645σ} = .90\\nit seems reasonable to take\\n1.645σ = a\\nor\\nσ =\\na\\n1.645\\nThus, if your prior feelings can indeed be reasonably described by a normal\\ndistribution, then that distribution would have mean μ and standard deviation\\nσ = a/1.645. As a test of whether this distribution indeed ﬁts your prior feelings\\nyou might ask yourself such questions as whether you are 95 percent certain\\nthat θ will fall between μ−1.96σ and μ+1.96σ, or whether you are 99 percent\\ncertain that θ will fall between μ −2.58σ and μ + 2.58σ, where these intervals\\nare determined by the equalities\\nP\\n\\x18\\n−1.96 < θ −μ\\nσ\\n< 1.96\\n\\x19\\n= .95\\nP\\n\\x18\\n−2.58 < θ −μ\\nσ\\n< 2.58\\n\\x19\\n= .99\\nwhich hold when θ is normal with mean μ and variance σ 2.\\nExample 7.8.c. Consider the likelihood function f (x1,...,xn|θ) and suppose\\nthat θ is uniformly distributed over some interval (a,b). The posterior density\\nof θ given X1,...,Xn equals\\nf (θ|x1,...,xn) =\\nf (x1,...,xn|θ)p(θ)\\n0 b\\na f (x1,...,xn|θ)p(θ)dθ\\n=\\nf (x1,...,xn|θ)\\n0 b\\na f (x1,...,xn|θ)dθ\\na < θ < b\\nNow the mode of a density f (θ) was deﬁned to be that value of θ that maximizes\\nf (θ). By the foregoing, it follows that the mode of the density f (θ|x1,...,xn) is\\nthat value of θ maximizing f (x1,...,xn|θ); that is, it is just the maximum like-\\nlihood estimate of θ [when it is constrained to be in (a,b)]. In other words, the\\nmaximum likelihood estimate equals the mode of the posterior distribution\\nwhen a uniform prior distribution is assumed.\\n■\\nIf, rather than a point estimate, we desire an interval in which θ lies with a\\nspeciﬁed probability — say 1−α — we can accomplish this by choosing values\\na and b such that\\n\\x1f b\\na\\nf (θ|x1,...,xn)dθ = 1 −α'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 303}, page_content='292 CHAPTER 7: Parameter estimation\\nExample 7.8.d. Suppose that if a signal of value s is sent from location A, then\\nthe signal value received at location B is normally distributed with mean s and\\nvariance 60. Suppose also that the value of a signal sent at location A is, a priori,\\nknown to be normally distributed with mean 50 and variance 100. If the value\\nreceived at location B is equal to 40, determine an interval that will contain the\\nactual value sent with probability .90.\\nSolution. It follows from Example 7.8.b that the conditional distribution of S,\\nthe signal value sent, given that 40 is the value received, is normal with mean\\nand variance given by\\nE[S|data] =\\n1/60\\n1/60 + 1/10040 +\\n1/100\\n1/60 + 1/10050 = 43.75\\nVar(S|data) =\\n1\\n1/60 + 1/100 = 37.5\\nHence, given that the value received is 40, (S −43.75)/\\n√\\n37.5 has a standard\\nnormal distribution and so\\nP\\n\\x18\\n−1.645 < S −43.75\\n√\\n37.5\\n< 1.645|data\\n\\x19\\n= .90\\nor\\nP{43.75 −1.645\\n√\\n37.5 < S < 43.75 + 1.645\\n√\\n37.5|data} = .95\\nThat is, with probability .90, the true signal sent lies within the interval (33.68,\\n53.82).\\n■\\nProblems\\n1. Let X1,...,Xn be a sample from the distribution whose density function\\nis\\nf (x) =\\n\\x03\\ne−(x−θ)\\nx ≥θ\\n0\\notherwise\\nDetermine the maximum likelihood estimator of θ.\\n2. Determine the maximum likelihood estimator of θ when X1,...,Xn is a\\nsample with density function\\nf (x) = 1\\n2e−|x−θ|,\\n−∞< x < ∞\\n3. Let X1,...,Xn be a sample from a normal μ,σ 2 population. Determine\\nthe maximum likelihood estimator of σ 2 when μ is known. What is the\\nexpected value of this estimator?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 304}, page_content='Problems 293\\n4. Determine the maximum likelihood estimates of a and λ when X1,...,Xn\\nis a sample from the Pareto density function\\nf (x) =\\n\\x03\\nλaλx−(λ+1),\\nif x ≥a\\n0,\\nif x < a\\n5. Suppose that X1,...,Xn are normal with mean μ1; Y1,...,Yn are normal\\nwith mean μ2; and W1,...,Wn are normal with mean μ1 + μ2. Assuming\\nthat all 3n random variables are independent with a common variance,\\nﬁnd the maximum likelihood estimators of μ1 and μ2.\\n6. River ﬂoods are often measured by their discharges (in units of feet cubed\\nper second). The value v is said to be the value of a 100-year ﬂood if\\nP{D ≥v} = .01\\nwhere D is the discharge of the largest ﬂood in a randomly chosen year.\\nThe following table gives the ﬂood discharges of the largest ﬂoods of\\nthe Blackstone River in Woonsocket, Rhode Island, in each of the years\\nfrom 1929 to 1965. Assuming that these discharges follow a lognormal\\ndistribution, estimate the value of a 100-year ﬂood.\\nTable 7.4\\nAnnual Floods of the\\nBlackstone River (1929–1965).\\nYear\\nFlood Discharge\\n(ft3/s)\\n1929\\n4,570\\n1930\\n1,970\\n1931\\n8,220\\n1932\\n4,530\\n1933\\n5,780\\n1934\\n6,560\\n1935\\n7,500\\n1936\\n15,000\\n1937\\n6,340\\n1938\\n15,100\\n1939\\n3,840\\n1940\\n5,860\\n1941\\n4,480\\n1942\\n5,330\\n1943\\n5,310\\n1944\\n3,830\\n1945\\n3,410\\ncontinued on next page'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 305}, page_content='294 CHAPTER 7: Parameter estimation\\nTable 7.4 (continued)\\nYear\\nFlood Discharge\\n(ft3/s)\\n1946\\n3830\\n1947\\n3150\\n1948\\n5810\\n1949\\n2030\\n1950\\n3620\\n1951\\n4920\\n1952\\n4090\\n1953\\n5570\\n1954\\n9400\\n1955\\n32,900\\n1956\\n8710\\n1957\\n3850\\n1958\\n4970\\n1959\\n5398\\n1960\\n4780\\n1961\\n4020\\n1962\\n5790\\n1963\\n4510\\n1964\\n5520\\n1965\\n5300\\n7. Recall that X is said to have a lognormal distribution with parameters μ\\nand σ 2 if log(X) is normal with mean μ and variance σ 2. Suppose X is\\nsuch a lognormal random variable.\\na.\\nFind E[X].\\nb.\\nFind Var(X).\\nHint: Make use of the formula for the moment generating function\\nof a normal random variable.\\nc.\\nThe following are, in minutes, travel times to work over a sequence\\nof 10 days.\\n42,28,53,57,67,39,35,50,44,39\\nAssuming an underlying lognormal distribution, use the data to es-\\ntimate the mean travel time.\\n8. An electric scale gives a reading equal to the true weight plus a random\\nerror that is normally distributed with mean 0 and standard deviation\\nσ = .1 mg. Suppose that the results of ﬁve successive weighings of the\\nsame object are as follows: 3.142, 3.163, 3.155, 3.150, 3.141.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 306}, page_content='Problems 295\\na.\\nDetermine a 95 percent conﬁdence interval estimate of the true\\nweight.\\nb.\\nDetermine a 99 percent conﬁdence interval estimate of the true\\nweight.\\n9. The PCB concentration of a ﬁsh caught in Lake Michigan was measured\\nby a technique that is known to result in an error of measurement that\\nis normally distributed with a standard deviation of .08 ppm (parts per\\nmillion). Suppose the results of 10 independent measurements of this\\nﬁsh are\\n11.2, 12.4, 10.8, 11.6, 12.5, 10.1, 11.0, 12.2, 12.4, 10.6\\na.\\nGive a 95 percent conﬁdence interval for the PCB level of this ﬁsh.\\nb.\\nGive a 95 percent lower conﬁdence interval.\\nc.\\nGive a 95 percent upper conﬁdence interval.\\n10. The standard deviation of test scores on a certain achievement test is 11.3.\\nIf a random sample of 81 students had a sample mean score of 74.6,\\nﬁnd a 90 percent conﬁdence interval estimate for the average score of all\\nstudents.\\n11. Let X1,...,Xn,Xn+1 be a sample from a normal population having an\\nunknown mean μ and variance 1. Let ¯Xn = \\x02n\\ni=1 Xi/n be the average of\\nthe ﬁrst n of them.\\na.\\nWhat is the distribution of Xn+1 −¯Xn?\\nb.\\nIf ¯Xn = 4, give an interval that, with 90 percent conﬁdence, will con-\\ntain the value of Xn+1.\\n12. If X1,...,Xn is a sample from a normal population whose mean μ is\\nunknown but whose variance σ 2 is known, show that (−∞,X+zασ/√n)\\nis a 100(1 −α) percent lower conﬁdence interval for μ.\\n13. A sample of 20 cigarettes is tested to determine nicotine content and\\nthe average value observed was 1.2 mg. Compute a 99 percent two-sided\\nconﬁdence interval for the mean nicotine content of a cigarette if it is\\nknown that the standard deviation of a cigarette’s nicotine content is σ =\\n.2 mg.\\n14. In Problem 13, suppose that the population variance is not known in\\nadvance of the experiment. If the sample variance is .04, compute a 99\\npercent two-sided conﬁdence interval for the mean nicotine content.\\n15. In Problem 14, compute a value c for which we can assert “with 99 per-\\ncent conﬁdence” that c is larger than the mean nicotine content of a\\ncigarette.\\n16. Suppose that when sampling from a normal population having an un-\\nknown mean μ and unknown variance σ 2, we wish to determine a\\nsample size n so as to guarantee that the resulting 100(1 −α) percent\\nconﬁdence interval for μ will be of size no greater than A, for given val-\\nues α and A. Explain how we can approximately do this by a double\\nsampling scheme that ﬁrst takes a subsample of size 30 and then chooses\\nthe total sample size by using the results of the ﬁrst subsample.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 307}, page_content='296 CHAPTER 7: Parameter estimation\\n17. The following data resulted from 24 independent measurements of the\\nmelting point of lead.\\n330◦C\\n322◦C\\n345◦C\\n328.6◦C\\n331◦C\\n342◦C\\n342.4◦C\\n340.4◦C\\n329.7◦C\\n334◦C\\n326.5◦C\\n325.8◦C\\n337.5◦C\\n327.3◦C\\n322.6◦C\\n341◦C\\n340◦C\\n333◦C\\n343.3◦C\\n331◦C\\n341◦C\\n329.5◦C\\n332.3◦C\\n340◦C\\nAssuming that the measurements can be regarded as constituting a nor-\\nmal sample whose mean is the true melting point of lead, determine a\\n95 percent two-sided conﬁdence interval for this value. Also determine a\\n99 percent two-sided conﬁdence interval.\\n18. The following are scores on IQ tests of a random sample of 18 students\\nat a large eastern university.\\n130, 122, 119, 142, 136, 127, 120, 152, 141,\\n132, 127, 118, 150, 141, 133, 137, 129, 142\\na.\\nConstruct a 95 percent conﬁdence interval estimate of the average\\nIQ score of all students at the university.\\nb.\\nConstruct a 95 percent lower conﬁdence interval estimate.\\nc.\\nConstruct a 95 percent upper conﬁdence interval estimate.\\n19. Suppose that a random sample of nine recently sold houses in a certain\\ncity resulted in a sample mean price of $222,000, with a sample standard\\ndeviation of $22,000. Give a 95 percent upper conﬁdence interval for the\\nmean price of all recently sold houses in this city.\\n20. A company self-insures its large ﬂeet of cars against collisions. To deter-\\nmine its mean repair cost per collision, it has randomly chosen a sample\\nof 16 accidents. If the average repair cost in these accidents is $2200 with\\na sample standard deviation of $800, ﬁnd a 90 percent conﬁdence inter-\\nval estimate of the mean cost per collision.\\n21. A standardized test is given annually to all sixth-grade students in the\\nstate of Washington. To determine the average score of students in her\\ndistrict, a school supervisor selects a random sample of 100 students. If\\nthe sample mean of these students’ scores is 320 and the sample stan-\\ndard deviation is 16, give a 95 percent conﬁdence interval estimate of the\\naverage score of students in that supervisor’s district.\\n22. Each of 20 science students independently measured the melting point\\nof lead. The sample mean and sample standard deviation of these mea-\\nsurements were (in degrees centigrade) 330.2 and 15.4, respectively. Con-\\nstruct (a) a 95 percent and (b) a 99 percent conﬁdence interval estimate\\nof the true melting point of lead.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 308}, page_content='Problems\\n297\\n23. A random sample of 300 CitiBank VISA cardholder accounts indicated a\\nsample mean debt of $1220 with a sample standard deviation of $840.\\nConstruct a 95 percent conﬁdence interval estimate of the average debt\\nof all cardholders.\\n24. In Problem 23, ﬁnd the smallest value v that “with 90 percent conﬁ-\\ndence,” exceeds the average debt per cardholder.\\n25. Verify the formula given in Table 7.1 for the 100(1 −α) percent lower\\nconﬁdence interval for μ when σ is unknown.\\n26. The following are the daily number of steps taken by a certain individual\\nin 20 weekdays.\\n2,100\\n1,984\\n2,072\\n1,898\\n1,950\\n1,992\\n2,096\\n2,103\\n2,043\\n2,218\\n2,244\\n2,206\\n2,210\\n2,152 1,962\\n2,007\\n2,018\\n2,106\\n1,938 1,956\\nAssuming that the daily number of steps is normally distributed, con-\\nstruct (a) a 95 percent and (b) a 99 percent two-sided conﬁdence interval\\nfor the mean number of steps. (c) Determine the largest value v that,\\n“with 95 percent conﬁdence,” will be less than the mean range.\\n27. Studies were conducted in Los Angeles to determine the carbon monox-\\nide concentration near freeways. The basic technique used was to capture\\nair samples in special bags and to then determine the carbon monoxide\\nconcentration by using a spectrophotometer. The measurements in ppm\\n(parts per million) over a sampled period during the year were 102.2,\\n98.4, 104.1, 101, 102.2, 100.4, 98.6, 88.2, 78.8, 83, 84.7, 94.8, 105.1,\\n106.2, 111.2, 108.3, 105.2, 103.2, 99, 98.8. Compute a 95 percent two-\\nsided conﬁdence interval for the mean carbon monoxide concentration.\\n28. A set of 10 determinations, by a method devised by the chemist Karl\\nFischer, of the percentage of water in a methanol solution yielded the\\nfollowing data.\\n.50, .55, .53, .56, .54,\\n.57, .52, .60, .55, .58\\nAssuming normality, use these data to give a 95 percent conﬁdence inter-\\nval for the actual percentage.\\n29. Suppose that U1,U2,... is a sequence of independent uniform (0,1) ran-\\ndom variables, and deﬁne N by\\nN = min{n : U1 + ··· + Un > 1}\\nThat is, N is the number of uniform (0, 1) random variables that need\\nto be summed to exceed 1. Use random numbers to determine the value\\nof 36 random variables having the same distribution as N, and then use'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 309}, page_content='298 CHAPTER 7: Parameter estimation\\nthese data to obtain a 95 percent conﬁdence interval estimate of E[N].\\nBased on this interval, guess the exact value of E[N].\\n30. An important issue for a retailer is to decide when to reorder stock from\\na supplier. A common policy used to make the decision is of a type called\\ns,S: The retailer orders at the end of a period if the on-hand stock is less\\nthan s, and orders enough to bring the stock up to S. The appropriate\\nvalues of s and S depend on different cost parameters, such as inventory\\nholding costs and the proﬁt per item sold, as well as the distribution of\\nthe demand during a period. Consequently, it is important for the retailer\\nto collect data relating to the parameters of the demand distribution.\\nSuppose that the following data give the numbers of a certain type of\\nitem sold in each of 30 weeks.\\n14,8,12,9,5,22,15,12,16,7,10,9,15,15,12,\\n9,11,16,8,7,15,13,9,5,18,14,10,13,7,11\\nAssuming that the numbers sold each week are independent random\\nvariables from a common distribution, use the data to obtain a 95 per-\\ncent conﬁdence interval for the mean number sold in a week.\\n31. A random sample of 16 professors at a large private university yielded\\na sample mean annual salary of $90,450 with a sample standard devia-\\ntion of $9400. Determine a 95 percent conﬁdence interval of the average\\nsalary of all professors at that university.\\n32. Let X1,...,Xn+1 be a sample from a population with mean μ and vari-\\nance σ 2. As noted in the text, the natural predictor of Xn+1 based on the\\ndata values X1,...,Xn is ¯Xn = \\x02n\\ni=1 Xi/n. Determine the mean square\\nerror of this predictor. That is, ﬁnd E[(Xn+1 −¯Xn)2].\\n33. National Safety Council data show that the number of accidental deaths\\ndue to drowning in the United States in the years from 1990 to 1993\\nwere (in units of one thousand) 5.2, 4.6, 4.3, 4.8. Use these data to give\\nan interval that will, with 95 percent conﬁdence, contain the number of\\nsuch deaths in 1994.\\n34. The daily dissolved oxygen concentration for a water stream has been\\nrecorded over 30 days. If the sample average of the 30 values is 2.5\\nmg/liter and the sample standard deviation is 2.12 mg/liter, determine\\na value which, with 90 percent conﬁdence, exceeds the mean daily con-\\ncentration.\\n35. Verify the formulas given in Table 7.1 for the 100(1 −α) percent lower\\nand upper conﬁdence intervals for σ 2.\\n36. The capacities (in ampere-hours) of 10 batteries were recorded as follows:\\n140,136,150,144,148,152,138,141,143,151\\na.\\nEstimate the population variance σ 2.\\nb.\\nCompute a 99 percent two-sided conﬁdence interval for σ 2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 310}, page_content='Problems 299\\nc.\\nCompute a value v that enables us to state, with 90 percent conﬁ-\\ndence, that σ 2 is less than v.\\n37. Find a 95 percent two-sided conﬁdence interval for the variance of the\\ndiameter of a rivet based on the data given here.\\n6.68\\n6.66 6.62 6.72\\n6.76\\n6.67\\n6.70\\n6.72\\n6.78\\n6.66 6.76 6.72\\n6.76\\n6.70\\n6.76 6.76\\n6.74\\n6.74\\n6.81\\n6.66\\n6.64\\n6.79 6.72 6.82\\n6.81\\n6.77 6.60 6.72\\n6.74\\n6.70\\n6.64 6.78\\n6.70\\n6.70\\n6.75 6.79\\nAssume a normal population.\\n38. The following are independent samples from two normal populations,\\nboth of which have the same standard deviation σ.\\n16,17,19,20,18\\nand\\n3,4,8\\nUse them to estimate σ.\\n39. The amount of beryllium in a substance is often determined by the use\\nof a photometric ﬁltration method. If the weight of the beryllium is μ,\\nthen the value given by the photometric ﬁltration method is normally\\ndistributed with mean μ and standard deviation σ. A total of eight in-\\ndependent measurements of 3.180 mg of beryllium gave the following\\nresults.\\n3.166,3.192,3.175,3.180,3.182,3.171,3.184,3.177\\nUse the preceding data to\\na.\\nestimate σ;\\nb.\\nﬁnd a 90 percent conﬁdence interval estimate of σ.\\n40. If X1,...,Xn is a sample from a normal population, explain how to\\nobtain a 100(1 −α) percent conﬁdence interval for the population vari-\\nance σ 2 when the population mean μ is known. Explain in what sense\\nknowledge of μ improves the interval estimator compared with when it\\nis unknown.\\nRepeat Problem 38 if it is known that the mean burning time is 53.6\\nseconds.\\n41. A civil engineer wishes to measure the compressive strength of two differ-\\nent types of concrete. A random sample of 10 specimens of the ﬁrst type\\nyielded the following data (in psi)\\nType 1:\\n3,250,\\n3,268, 4,302, 3,184, 3,266\\n3,297, 3,332, 3,502, 3,064,\\n3,116'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 311}, page_content='300 CHAPTER 7: Parameter estimation\\nwhereas a sample of 10 specimens of the second yielded the data\\nType 2:\\n3,094,\\n3,106, 3,004, 3,066, 2,984,\\n3,124,\\n3,316,\\n3,212,\\n3,380,\\n3,018\\nIf we assume that the samples are normal with a common variance, de-\\ntermine\\na.\\na 95 percent two-sided conﬁdence interval for μ1 −μ2, the difference\\nin means;\\nb.\\na 95 percent one-sided upper conﬁdence interval for μ1 −μ2;\\nc.\\na 95 percent one-sided lower conﬁdence interval for μ1 −μ2.\\n42. Independent random samples are taken from the output of two machines\\non a production line. The weight of each item is of interest. From the\\nﬁrst machine, a sample of size 36 is taken, with sample mean weight\\nof 120 grams and a sample variance of 4. From the second machine, a\\nsample of size 64 is taken, with a sample mean weight of 130 grams and a\\nsample variance of 5. It is assumed that the weights of items from the ﬁrst\\nmachine are normally distributed with mean μ1 and variance σ 2 and that\\nthe weights of items from the second machine are normally distributed\\nwith mean μ2 and variance σ 2 (that is, the variances are assumed to be\\nequal). Find a 99 percent conﬁdence interval for μ1 −μ2, the difference\\nin population means.\\n43. Do Problem 42 when it is known in advance that the population vari-\\nances are 4 and 5.\\n44. The following are the daily numbers of company website visits resulting\\nfrom advertisements on two different types of media.\\nType I\\nType II\\n481\\n572 526 537\\n506\\n561\\n511\\n582\\n527\\n501\\n556\\n605\\n661\\n487 542 558\\n501\\n524\\n491\\n578\\nFind a 99 percent conﬁdence interval for the mean difference in daily\\nvisits assuming normality with unknown but equal variances.\\n45. If X1,...,Xn is a sample from a normal population having known mean\\nμ1 and unknown variance σ 2\\n1 , and Y1,...,Ym is an independent sample\\nfrom a normal population having known mean μ2 and unknown vari-\\nance σ 2\\n2 , determine a 100(1 −α) percent conﬁdence interval for σ 2\\n1 /σ 2\\n2 .\\n46. Two analysts took repeated readings on the hardness of city water. As-\\nsuming that the readings of analyst i constitute a sample from a normal\\npopulation having variance σ 2\\ni ,i = 1,2, compute a 95 percent two-sided\\nconﬁdence interval for σ 2\\n1 /σ 2\\n2 when the data are as follows:'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 312}, page_content='Problems 301\\nCoded Measures of Hardness\\nAnalyst 1\\nAnalyst 2\\n.46\\n.82\\n.62\\n.61\\n.37\\n.89\\n.40\\n.51\\n.44\\n.33\\n.58\\n.48\\n.48\\n.23\\n.53\\n.25\\n.67\\n.88\\n47. A problem of interest in baseball is whether a sacriﬁce bunt is a good\\nstrategy when there is a man on ﬁrst base and no outs. Assuming that the\\nbunter will be out but will be successful in advancing the man on base,\\nwe could compare the probability of scoring a run with a player on ﬁrst\\nbase and no outs with the probability of scoring a run with a player on\\nsecond base and one out.\\nThe following data resulted from a study of randomly chosen Major\\nLeague Baseball games played in 1959 and 1960.\\na.\\nGive a 95 percent conﬁdence interval estimate for the probability of\\nscoring at least one run when there is a man on ﬁrst and no outs.\\nb.\\nGive a 95 percent conﬁdence interval estimate for the probability of\\nscoring at least one run when there is a man on second and one out.\\nBase Occupied\\nNumber of\\nOuts\\nNumber of\\nCases in Which\\n0 Runs Are\\nScored\\nTotal Number\\nof Cases\\nFirst\\n0\\n1044\\n1728\\nSecond\\n1\\n401\\n657\\n48. A random sample of 1200 engineers included 48 Hispanic Americans, 80\\nAfrican Americans, and 204 females. Determine 90 percent conﬁdence\\nintervals for the proportion of all engineers who are\\na.\\nfemale;\\nb.\\nHispanic Americans or African Americans.\\n49. To estimate p, the proportion of all newborn babies that are male, the\\ngender of 10,000 newborn babies was noted. If 5106 of them were male,\\ndetermine (a) a 90 percent and (b) a 99 percent conﬁdence interval esti-\\nmate of p.\\n50. An airline is interested in determining the proportion of its customers\\nwho are ﬂying for reasons of business. If they want to be 90 percent cer-'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 313}, page_content='302 CHAPTER 7: Parameter estimation\\ntain that their estimate will be correct to within 2 percent, how large a\\nrandom sample should they select?\\n51. A recent newspaper poll indicated that Candidate A is favored over Can-\\ndidate B by a 53 to 47 percentage, with a margin of error of ±4 percent.\\nThe newspaper then stated that since the 6-point gap is larger than the\\nmargin of error, its readers can be certain that Candidate A is the current\\nchoice. Is this reasoning correct?\\n52. A market research ﬁrm is interested in determining the proportion of\\nhouseholds that are watching a particular sporting event. To accomplish\\nthis task, they plan on using a telephone poll of randomly chosen house-\\nholds. How large a sample is needed if they want to be 90 percent certain\\nthat their estimate is correct to within ±.02?\\n53. In a recent study, 79 of 140 meteorites were observed to enter the at-\\nmosphere with a velocity of less than 25 miles per second. If we take\\nˆp=79/140 as an estimate of the probability that an arbitrary meteorite\\nthat enters the atmosphere will have a speed less than 25 miles per sec-\\nond, what can we say, with 99 percent conﬁdence, about the maximum\\nerror of our estimate?\\n54. A random sample of 100 items from a production line revealed 17 of\\nthem to be defective. Compute a 95 percent two-sided conﬁdence interval\\nfor the probability that an item produced is defective. Determine also a\\n99 percent upper conﬁdence interval for this value. What assumptions\\nare you making?\\n55. Of 100 randomly detected cases of individuals having lung cancer, 67\\ndied within 5 years of detection.\\na.\\nEstimate the probability that a person contracting lung cancer will\\ndie within 5 years.\\nb.\\nHow large an additional sample would be required to be 95 percent\\nconﬁdent that the error in estimating the probability in part (a) is\\nless than .02?\\n56. Derive 100(1 −α) percent lower and upper conﬁdence intervals for p,\\nwhen the data consist of the values of n independent Bernoulli random\\nvariables with parameter p.\\n57. Suppose the lifetimes of batteries are exponentially distributed with\\nmean θ. If the average of a sample of 10 batteries is 36 hours, determine\\na 95 percent two-sided conﬁdence interval for θ.\\n58. Determine 100(1 −α) percent one-sided upper and lower conﬁdence in-\\ntervals for θ in Problem 57.\\n59. Let X1,X2,...,Xn denote a sample from a population whose mean value\\nθ is unknown. Use the results of Example 7.7.b to argue that among all\\nunbiased estimators of θ of the form \\x02n\\ni=1 λiXi,\\x02n\\ni=1 λi = 1, the one\\nwith minimal mean square error has λi ≡1/n, i = 1,...,n.\\n60. Consider two independent samples from normal populations having the\\nsame variance σ 2, of respective sizes n and m. That is, X1,...,Xn and\\nY1,...,Ym are independent samples from normal populations each hav-'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 314}, page_content='Problems\\n303\\ning variance σ 2. Let S2\\nx and S2\\ny denote the respective sample variances.\\nThus both S2\\nx and S2\\ny are unbiased estimators of σ 2. Show by using the\\nresults of Example 7.7.b along with the fact that\\nVar(χ2\\nk ) = 2k\\nwhere χ2\\nk is chi-square with k degrees of freedom, that the minimum\\nmean square estimator of σ 2 of the form λS2\\nx + (1 −λ)S2\\ny is\\nS2\\np =\\n(n −1)S2\\nx + (m −1)S2\\ny\\nn + m −2\\nThis is called the pooled estimator of σ 2.\\n61. Consider two estimators d1 and d2 of a parameter θ. If E[d1] = θ,\\nVar(d1) = 6 and E[d2] = θ + 2, Var(d2) = 2, which estimator should be\\npreferred?\\n62. Suppose that the number of accidents occurring daily in a certain plant\\nhas a Poisson distribution with an unknown mean λ. Based on previous\\nexperience in similar industrial plants, suppose that a statistician’s initial\\nfeelings about the possible value of λ can be expressed by an exponential\\ndistribution with parameter 1. That is, the prior density is\\np(λ) = e−λ,\\n0 < λ < ∞\\nDetermine the Bayes estimate of λ if there are a total of 83 accidents over\\nthe next 10 days. What is the maximum likelihood estimate?\\n63. The functional lifetimes in hours of computer chips produced by a cer-\\ntain semiconductor ﬁrm are exponentially distributed with mean 1/λ.\\nSuppose that the prior distribution on λ is the gamma distribution with\\ndensity function\\ng(x) = e−xx2\\n2\\n,\\n0 < x < ∞\\nIf the average life of the ﬁrst 20 chips tested is 4.6 hours, compute the\\nBayes estimate of λ.\\n64. Each item produced will, independently, be defective with probability p.\\nIf the prior distribution on p is uniform on (0, 1), compute the posterior\\nprobability that p is less than .2 given\\na.\\na total of 2 defectives out of a sample of size 10;\\nb.\\na total of 1 defective out of a sample of size 10;\\nc.\\na total of 10 defectives out of a sample of size 10.\\n65. The breaking strength of a certain type of cloth is to be measured for 10\\nspecimens. The underlying distribution is normal with unknown mean θ\\nbut with a standard deviation equal to 3 psi. Suppose also that based on'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 315}, page_content='304 CHAPTER 7: Parameter estimation\\nprevious experience we feel that the unknown mean has a prior distribu-\\ntion that is normally distributed with mean 200 and standard deviation\\n2. If the average breaking strength of a sample of 20 specimens is 182 psi,\\ndetermine a region that contains θ with probability .95.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 316}, page_content='CHAPTER 8\\nHypothesis testing\\n8.1\\nIntroduction\\nAs in the previous chapter, let us suppose that a random sample from a popu-\\nlation distribution, speciﬁed except for a vector of unknown parameters, is to\\nbe observed. However, rather than wishing to explicitly estimate the unknown\\nparameters, let us now suppose that we are primarily concerned with using\\nthe resulting sample to test some particular hypothesis concerning them. As an\\nillustration, suppose that a construction ﬁrm has just purchased a large sup-\\nply of cables that have been guaranteed to have an average breaking strength\\nof at least 7000 pounds per square inch (PSI). To verify this claim, the ﬁrm\\nhas decided to take a random sample of 10 of these cables to determine their\\nbreaking strengths. They will then use the result of this experiment to ascer-\\ntain whether or not they accept the cable manufacturer’s hypothesis that the\\npopulation mean is at least 7000 PSI.\\nA statistical hypothesis is usually a statement about a set of parameters of\\na population distribution. It is called a hypothesis because it is not known\\nwhether or not it is true. A primary problem is to develop a procedure for de-\\ntermining whether or not the values of a random sample from this population\\nare consistent with the hypothesis. For instance, consider a particular normally\\ndistributed population having an unknown mean value θ and known variance\\n1. The statement “θ is less than 1” is a statistical hypothesis that we could try to\\ntest by observing a random sample from this population. If the random sam-\\nple is deemed to be consistent with the hypothesis under consideration, we\\nsay that the hypothesis has been “accepted”; otherwise we say that it has been\\n“rejected.”\\nNote that in accepting a given hypothesis we are not actually claiming that it\\nis true but rather we are saying that the resulting data appear to be consistent\\nwith it. For instance, in the case of a normal (θ, 1) population, if a resulting\\nsample of size 10 has an average value of 1.25, then although such a result\\ncannot be regarded as being evidence in favor of the hypothesis “θ < 1,” it is\\nnot inconsistent with this hypothesis, which would thus be accepted. On the\\nother hand, if the sample of size 10 has an average value of 3, then even though\\na sample value that large is possible when θ < 1, it is so unlikely that it seems\\ninconsistent with this hypothesis, which would thus be rejected.\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00017-X\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n305'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 317}, page_content='306 CHAPTER 8: Hypothesis testing\\n8.2\\nSigniﬁcance levels\\nConsider a population having distribution Fθ, where θ is unknown, and sup-\\npose we want to test a speciﬁc hypothesis about θ. We shall denote this hy-\\npothesis by H0 and call it the null hypothesis. For example, if Fθ is a normal\\ndistribution function with mean θ and variance equal to 1, then two possible\\nnull hypotheses about θ are\\n(a) H0 : θ = 1\\n(b) H0 : θ ≤1\\nThus the ﬁrst of these hypotheses states that the population is normal with\\nmean 1 and variance 1, whereas the second states that it is normal with variance\\n1 and a mean less than or equal to 1. Note that the null hypothesis in (a),\\nwhen true, completely speciﬁes the population distribution, whereas the null\\nhypothesis in (b) does not. A hypothesis that, when true, completely speciﬁes\\nthe population distribution is called a simple hypothesis; one that does not is\\ncalled a composite hypothesis.\\nSuppose now that in order to test a speciﬁc null hypothesis H0, a population\\nsample of size n — say X1,...,Xn — is to be observed. Based on these n values,\\nwe must decide whether or not to accept H0. A test for H0 can be speciﬁed by\\ndeﬁning a region C in n-dimensional space with the proviso that the hypothesis\\nis to be rejected if the random sample X1,...,Xn turns out to lie in C and\\naccepted otherwise. The region C is called the critical region. In other words, the\\nstatistical test determined by the critical region C is the one that\\naccepts\\nH0\\nif\\n(X1,X2,...,Xn) /∈C\\nand\\nrejects\\nH0\\nif\\n(X1,...,Xn) ∈C\\nFor instance, a common test of the hypothesis that θ, the mean of a normal\\npopulation with variance 1, is equal to 1 has a critical region given by\\nC = {(X1,...,Xn) : | ¯X −1| > 1.96/√n}\\n(8.2.1)\\nThus, this test calls for rejection of the null hypothesis that θ = 1 when the\\nsample average differs from 1 by more than 1.96 divided by the square root of\\nthe sample size.\\nIt is important to note when developing a procedure for testing a given null\\nhypothesis H0 that, in any test, two different types of errors can result. The\\nﬁrst of these, called a type I error, is said to result if the test incorrectly calls for\\nrejecting H0 when it is indeed correct. The second, called a type II error, results\\nif the test calls for accepting H0 when it is false.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 318}, page_content='8.3 Tests concerning the mean of a normal population\\n307\\nNow, as was previously mentioned, the objective of a statistical test of H0 is\\nnot to explicitly determine whether or not H0 is true but rather to determine\\nif its validity is consistent with the resultant data. Hence, with this objective it\\nseems reasonable that H0 should only be rejected if the resultant data are very\\nunlikely when H0 is true. The classical way of accomplishing this is to specify a\\nvalue α and then require the test to have the property that whenever H0 is true\\nits probability of being rejected is never greater than α. The value α, called the\\nlevel of signiﬁcance of the test, is usually set in advance, with commonly chosen\\nvalues being α = .1,.05,.005. In other words, the classical approach to testing\\nH0 is to ﬁx a signiﬁcance level α and then require that the test have the property\\nthat the probability of a type I error occurring can never be greater than α.\\nSuppose now that we are interested in testing a certain hypothesis concerning\\nθ, an unknown parameter of the population. Speciﬁcally, for a given set of\\nparameter values w, suppose we are interested in testing\\nH0 : θ ∈w\\nA common approach to developing a test of H0, say at level of signiﬁcance α, is\\nto start by determining a point estimator of θ — say d(X). The hypothesis is\\nthen rejected if d(X) is “far away” from the region w. However, to determine\\nhow “far away” it need be to justify rejection of H0, we need to determine the\\nprobability distribution of d(X) when H0 is true since this will usually enable\\nus to determine the appropriate critical region so as to make the test have the\\nrequired signiﬁcance level α. For example, the test of the hypothesis that the\\nmean of a normal (θ,1) population is equal to 1, given by Equation (8.2.1),\\ncalls for rejection when the point estimate of θ — that is, the sample average\\n— is farther than 1.96/√n away from 1. As we will see in the next section, the\\nvalue 1.96/√n was chosen to meet a level of signiﬁcance of α = .05.\\n8.3\\nTests concerning the mean of a normal population\\n8.3.1\\nCase of known variance\\nSuppose that X1,...,Xn is a sample of size n from a normal distribution having\\nan unknown mean μ and a known variance σ 2 and suppose we are interested\\nin testing the null hypothesis\\nH0 : μ = μ0\\nagainst the alternative hypothesis\\nH1 : μ ̸= μ0\\nwhere μ0 is some speciﬁed constant.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 319}, page_content='308 CHAPTER 8: Hypothesis testing\\nSince X = \\x02n\\ni=1 Xi/n is a natural point estimator of μ, it seems reasonable to\\naccept H0 if X is not too far from μ0. That is, the critical region of the test would\\nbe of the form\\nC = {X1,...,Xn : |X −μ0| > c}\\n(8.3.1)\\nfor some suitably chosen value c.\\nIf we desire that the test has signiﬁcance level α, then we must determine the\\ncritical value c in Equation (8.3.1) that will make the type I error equal to α.\\nThat is, c must be such that\\nPμ0{|X −μ0| > c} = α\\n(8.3.2)\\nwhere we write Pμ0 to mean that the preceding probability is to be computed\\nunder the assumption that μ = μ0. However, when μ = μ0, X will be normally\\ndistributed with mean μ0 and variance σ 2/n and so Z, deﬁned by\\nZ ≡\\n¯X −μ0\\nσ/√n =\\n√n( ¯X −μ0)\\nσ\\nwill have a standard normal distribution. Now Equation (8.3.2) is equivalent to\\nP\\n\\x03\\n|Z| > c√n\\nσ\\n\\x04\\n= α\\nor, equivalently,\\n2P\\n\\x03\\nZ > c√n\\nσ\\n\\x04\\n= α\\nwhere Z is a standard normal random variable. However, we know that\\nP{Z > zα/2} = α/2\\nand so\\nc√n\\nσ\\n= zα/2\\nor\\nc = zα/2σ\\n√n\\nThus, the signiﬁcance level α test is to reject H0 if |X −μ0| > zα/2σ/√n and\\naccept otherwise; or, equivalently, to\\nreject\\nH0\\nif\\n√n\\nσ |X −μ0| > zα/2\\naccept\\nH0\\nif\\n√n\\nσ |X −μ0| ≤zα/2\\n(8.3.3)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 320}, page_content='8.3 Tests concerning the mean of a normal population\\n309\\nFIGURE 8.1\\nThis can be pictorially represented as shown in Figure 8.1, where we have su-\\nperimposed the standard normal density function [which is the density of the\\ntest statistic √n(X −μ0)/σ when H0 is true].\\nExample 8.3.a. It is known that if a signal of value μ is sent from location\\nA, then the value received at location B is normally distributed with mean μ\\nand standard deviation 2. That is, the random noise added to the signal is an\\nN(0,4) random variable. There is reason for the people at location B to suspect\\nthat the signal value μ = 8 will be sent today. Test this hypothesis if the same\\nsignal value is independently sent ﬁve times and the average value received at\\nlocation B is X = 9.5.\\nSolution. Suppose we are testing at the 5 percent level of signiﬁcance. To begin,\\nwe compute the test statistic\\n√n\\nσ |X −μ0| =\\n√\\n5\\n2 (1.5) = 1.68\\nSince this value is less than z.025 = 1.96, the hypothesis is accepted. In other\\nwords, the data are not inconsistent with the null hypothesis in the sense that a\\nsample average as far from the value 8 as observed would be expected, when the\\ntrue mean is 8, over 5 percent of the time. Note, however, that if a less stringent\\nsigniﬁcance level were chosen — say α = .1 — then the null hypothesis would\\nhave been rejected. This follows since z.05 = 1.645, which is less than 1.68.\\nHence, if we would have chosen a test that had a 10 percent chance of rejecting\\nH0 when H0 was true, then the null hypothesis would have been rejected.\\nThe “correct” level of signiﬁcance to use in a given situation depends on the\\nindividual circumstances involved in that situation. For instance, if rejecting a\\nnull hypothesis H0 would result in large costs that would thus be lost if H0\\nwere indeed true, then we might elect to be quite conservative and so choose\\na signiﬁcance level of .05 or .01. Also, if we initially feel strongly that H0 was\\ncorrect, then we would require very stringent data evidence to the contrary for\\nus to reject H0. (That is, we would set a very low signiﬁcance level in this situa-\\ntion.)\\n■\\nThe test given by Equation (8.3.3) can be described as follows: For any observed\\nvalue of the test statistic √n|X −μ0|/σ, call it v, the test calls for rejection of'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 321}, page_content='310 CHAPTER 8: Hypothesis testing\\nthe null hypothesis if the probability that the test statistic would be as large as\\nv when H0 is true is less than or equal to the signiﬁcance level α. From this,\\nit follows that we can determine whether or not to accept the null hypothesis\\nby computing, ﬁrst, the value of the test statistic and, second, the probability\\nthat a standard normal would (in absolute value) exceed that quantity. This\\nprobability — called the p-value of the test — gives the critical signiﬁcance\\nlevel in the sense that H0 will be accepted if the signiﬁcance level α is less than\\nthe p-value and rejected if it is greater than or equal.\\nIn practice, the signiﬁcance level is often not set in advance but rather the data\\nare looked at to determine the resultant p-value. Sometimes, this critical signif-\\nicance level is clearly much larger than any we would want to use, and so the\\nnull hypothesis can be readily accepted. At other times the p-value is so small\\nthat it is clear that the hypothesis should be rejected.\\nExample 8.3.b. In Example 8.3.a, suppose that the average of the 5 values\\nreceived is X = 8.5. In this case,\\n√n\\nσ |X −μ0| =\\n√\\n5\\n4 = .559\\nSince\\nP{|Z| > .559} = 2P{Z > .559}\\n= 2 × .288 = .576\\nit follows that the p-value is .576 and thus the null hypothesis H0 that the\\nsignal sent has value 8 would be accepted at any signiﬁcance level α < .576.\\nSince we would clearly never want to test a null hypothesis using a signiﬁcance\\nlevel as large as .576, H0 would be accepted.\\nOn the other hand, if the average of the data values were 11.5, then the p-value\\nof the test that the mean is equal to 8 would be\\nP{|Z| > 1.75\\n√\\n5} = P {|Z| > 3.913}\\n≈.00005\\nFor such a small p-value, the hypothesis that the value 8 was sent is rejected.\\n■\\nWe have not yet talked about the probability of a type II error — that is, the\\nprobability of accepting the null hypothesis when the true mean μ is unequal\\nto μ0. This probability will depend on the value of μ, and so let us deﬁne β(μ)\\nby\\nβ(μ) = Pμ{acceptance of H0}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 322}, page_content='8.3 Tests concerning the mean of a normal population\\n311\\n= Pμ\\n\\x05\\x06\\x06\\x06\\x06\\x06\\nX −μ0\\nσ/√n\\n\\x06\\x06\\x06\\x06\\x06 ≤zα/2\\n\\x07\\n= Pμ\\n\\x05\\n−zα/2 ≤X −μ0\\nσ/√n ≤zα/2\\n\\x07\\nThe function β(μ) is called the operating characteristic (or OC) curve and repre-\\nsents the probability that H0 will be accepted when the true mean is μ.\\nTo compute this probability, we use the fact that X is normal with mean μ and\\nvariance σ 2/n and so\\nZ ≡X −μ\\nσ/√n ∼(0,1)\\nHence,\\nβ(μ) = Pμ\\n\\x05\\n−zα/2 ≤X −μ0\\nσ/√n ≤zα/2\\n\\x07\\n= Pμ\\n\\x05\\n−zα/2 −\\nμ\\nσ/√n ≤X −μ0 −μ\\nσ/√n\\n≤zα/2 −\\nμ\\nσ/√n\\n\\x07\\n= Pμ\\n\\x03\\n−zα/2 −\\nμ\\nσ/√n ≤Z −\\nμ0\\nσ/√n ≤zα/2 −\\nμ\\nσ/√n\\n\\x04\\n= P\\n\\x03μ0 −μ\\nσ/√n −zα/2 ≤Z ≤μ0 −μ\\nσ/√n + zα/2\\n\\x04\\n= \\x06\\n\\x08μ0 −μ\\nσ/√n + zα/2\\n\\t\\n−\\x06\\n\\x08μ0 −μ\\nσ/√n −zα/2\\n\\t\\n(8.3.4)\\nwhere \\x06 is the standard normal distribution function.\\nFor a ﬁxed signiﬁcance level α, the OC curve given by Equation (8.3.4) is sym-\\nmetric about μ0 and indeed will depend on μ only through √n|μ −μ0|/σ.\\nThis curve with the abscissa changed from μ to d = √n|μ −μ0|/σ is presented\\nin Figure 8.2 when α = .05.\\nExample 8.3.c. For the problem presented in Example 8.3.a, let us determine\\nthe probability of accepting the null hypothesis that μ = 8 when the actual\\nvalue sent is 10. To do so, we compute\\n√n\\nσ (μ0 −μ) = −\\n√\\n5\\n2 × 2 = −\\n√\\n5\\nAs z.025 = 1.96, the desired probability is, from Equation (8.3.4),\\n\\x06(−\\n√\\n5 + 1.96) −\\x06(−\\n√\\n5 −1.96)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 323}, page_content='312 CHAPTER 8: Hypothesis testing\\nFIGURE 8.2\\nThe OC curve for the two-sided normal test for signiﬁcance level α = .05.\\n= 1 −\\x06(\\n√\\n5 −1.96) −[1 −\\x06(\\n√\\n5 + 1.96)]\\n= \\x06(4.196) −\\x06(.276)\\n= .392\\n■\\nRemark\\nThe function 1 −β(μ) is called the power-function of the test. Thus, for a\\ngiven value μ, the power of the test is equal to the probability of rejection\\nwhen μ is the true value.\\n■\\nThe operating characteristic function is useful in determining how large the\\nrandom sample need be to meet certain speciﬁcations concerning type II errors.\\nFor instance, suppose that we desire to determine the sample size n necessary\\nto ensure that the probability of accepting H0 : μ = μ0 when the true mean is\\nactually μ1 is approximately β. That is, we want n to be such that\\nβ(μ1) ≈β\\nBut from Equation (8.3.4), this is equivalent to\\n\\x06\\n\\x08√n(μ0 −μ1)\\nσ\\n+ zα/2\\n\\t\\n−\\x06\\n\\x08√n(μ0 −μ1)\\nσ\\n−zα/2\\n\\t\\n≈β\\n(8.3.5)\\nAlthough the foregoing cannot be analytically solved for n, a solution can be\\nobtained by using the standard normal distribution table. In addition, an ap-\\nproximation for n can be derived from Equation (8.3.5) as follows. To start,\\nsuppose that μ1 > μ0. Then, because this implies that\\n√n(μ0 −μ1)\\nσ\\n−zα/2 ≤−zα/2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 324}, page_content='8.3 Tests concerning the mean of a normal population\\n313\\nit follows, since \\x06 is an increasing function, that\\n\\x06\\n\\x08√n(μ0 −μ1)\\nσ\\n−zα/2\\n\\t\\n≤\\x06(−zα/2) = P{Z ≤−zα/2} = P{Z ≥zα/2} = α/2\\nHence, we can take\\n\\x06\\n\\x08√n(μ0 −μ1)\\nσ\\n−zα/2\\n\\t\\n≈0\\nand so from Equation (8.3.5)\\nβ ≈\\x06\\n\\x08√n(μ0 −μ1)\\nσ\\n+ zα/2\\n\\t\\n(8.3.6)\\nor, since\\nβ = P{Z > zβ} = P{Z < −zβ} = \\x06(−zβ)\\nwe obtain from Equation (8.3.6) that\\n−zβ ≈(μ0 −μ1)\\n√n\\nσ\\n+ zα/2\\nor\\nn ≈(zα/2 + zβ)2σ 2\\n(μ1 −μ0)2\\n(8.3.7)\\nIn fact, the same approximation would result when μ1 < μ0 (the details are left\\nas an exercise) and so Equation (8.3.7) is in all cases a reasonable approxima-\\ntion to the sample size necessary to ensure that the type II error at the value\\nμ = μ1 is approximately equal to β.\\nExample 8.3.d. For the problem of Example 8.3.a, how many signals need be\\nsent so that the .05 level test of H0 : μ = 8 has at least a 75 percent probability\\nof rejection when μ = 9.2?\\nSolution. Since z.025 = 1.96, z.25 = .67, the approximation from Equation\\n(8.3.7) yields\\nn ≈(1.96 + .67)2\\n(1.2)2\\n4 = 19.21\\nHence a sample of size 20 is needed. From Equation (8.3.4), we see that with\\nn = 20\\nβ(9.2) = \\x06\\n\\n−1.2\\n√\\n20\\n2\\n+ 1.96\\n\\x0b\\n−\\x06\\n\\n−1.2\\n√\\n20\\n2\\n−1.96\\n\\x0b\\n= \\x06(−.723) −\\x06(−4.643)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 325}, page_content='314 CHAPTER 8: Hypothesis testing\\n≈1 −\\x06(.723)\\n≈.235\\nTherefore, if the message is sent 20 times, then there is a 76.5 percent chance\\nthat the null hypothesis μ = 8 will be rejected when the true mean is 9.2.\\n■\\n8.3.1.1\\nOne-sided tests\\nIn testing the null hypothesis that μ = μ0, we have chosen a test that calls for\\nrejection when X is far from μ0. That is, a very small value of X or a very\\nlarge value appears to make it unlikely that μ (which X is estimating) could\\nequal μ0. However, what happens when the only alternative to μ being equal\\nto μ0 is for μ to be greater than μ0? That is, what happens when the alternative\\nhypothesis to H0 : μ = μ0 is H1 : μ > μ0? Clearly, in this latter case we would\\nnot want to reject H0 when X is small (since a small X is more likely when H0\\nis true than when H1 is true). Thus, in testing\\nH0 : μ = μ0\\nversus\\nH1 : μ > μ0\\n(8.3.8)\\nwe should reject H0 when X, the point estimate of μ0, is much greater than μ0.\\nThat is, the critical region should be of the following form:\\nC = {(X1,...,Xn) : X −μ0 > c}\\nSince the probability of rejection should equal α when H0 is true (that is, when\\nμ = μ0), we require that c be such that\\nPμ0{X −μ0 > c} = α\\n(8.3.9)\\nBut since\\nZ =\\n¯X −μ0\\nσ/√n =\\n√n( ¯X −μ0)\\nσ\\nhas a standard normal distribution when H0 is true, Equation (8.3.9) is equiva-\\nlent to\\nP\\n\\x03\\nZ > c√n\\nσ\\n\\x04\\n= α\\nwhen Z is a standard normal. But since\\nP{Z > zα} = α\\nwe see that\\nc = zασ\\n√n'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 326}, page_content='8.3 Tests concerning the mean of a normal population\\n315\\nHence, the test of the Hypothesis 8.3.8 is to reject H0 if X −μ0 > zασ/√n, and\\naccept otherwise; or, equivalently, to\\naccept\\nH0\\nif\\n√n\\nσ (X −μ0) ≤zα\\nreject\\nH0\\nif\\n√n\\nσ (X −μ0) > zα\\n(8.3.10)\\nThis is called a one-sided critical region (since it calls for rejection only when X\\nis large). Correspondingly, the hypothesis testing problem\\nH0 : μ = μ0\\nH1 : μ > μ0\\nis called a one-sided testing problem (in contrast to the two-sided problem that\\nresults when the alternative hypothesis is H1 : μ ̸= μ0).\\nTo compute the p-value in the one-sided test, Equation (8.3.10), we ﬁrst use the\\ndata to determine the value of the statistic √n(X −μ0)/σ. The p-value is then\\nequal to the probability that a standard normal would be at least as large as\\nthis value.\\nExample 8.3.e. Suppose in Example 8.3.a that we know in advance that the\\nsignal value is at least as large as 8. What can be concluded in this case?\\nSolution. To see if the data are consistent with the hypothesis that the mean\\nis 8, we test\\nH0 : μ = 8\\nagainst the one-sided alternative\\nH1 : μ > 8\\nThe value of the test statistic is √n(X −μ0)/σ =\\n√\\n5(9.5 −8)/2 = 1.68, and the\\np-value is the probability that a standard normal would exceed 1.68, namely,\\np-value = 1 −\\x06(1.68) = .0465\\nThe test would call for rejection at all signiﬁcance levels greater than or equal\\nto .0465. It would, for instance, reject the null hypothesis at the α = .05 level\\nof signiﬁcance.\\n■\\nThe operating characteristic function of the one-sided test, Equation (8.3.10),\\nβ(μ) = Pμ{accepting H0}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 327}, page_content='316 CHAPTER 8: Hypothesis testing\\ncan be obtained as follows:\\nβ(μ) = Pμ\\n\\x03\\nX ≤μ0 + zα\\nσ\\n√n\\n\\x04\\n= P\\n\\x03√n( ¯X −μ0)\\nσ\\n≤\\n√n(μ0 −μ)\\nσ\\n+ zα\\n\\x04\\n= P\\n\\x03\\nZ ≤\\n√n(μ0 −μ)\\nσ\\n+ zα\\n\\x04\\n,\\nZ ∼(0,1)\\nwhere the last equation follows since √n(X −μ)/σ has a standard normal\\ndistribution. Hence we can write\\nβ(μ) = \\x06\\n\\x08√n(μ0 −μ)\\nσ\\n+ zα\\n\\t\\nSince \\x06, being a distribution function, is increasing in its argument, it follows\\nthat β(μ) decreases in μ, which is intuitively pleasing since it certainly seems\\nreasonable that the larger the true mean μ, the less likely it should be to con-\\nclude that μ ≤μ0. Also since \\x06(zα) = 1 −α, it follows that\\nβ(μ0) = 1 −α\\nThe test given by Equation (8.3.10), which was designed to test H0 : μ = μ0 ver-\\nsus H1 : μ > μ0, can also be used to test, at level of signiﬁcance α, the one-sided\\nhypothesis\\nH0 : μ ≤μ0\\nversus\\nH1 : μ > μ0\\nTo verify that it remains a level α test, we need to show that the probability of\\nrejection is never greater than α when H0 is true. That is, we must verify that\\n1 −β(μ) ≤α\\nfor all μ ≤μ0\\nor\\nβ(μ) ≥1 −α\\nfor all μ ≤μ0\\nBut it has previously been shown that for the test given by Equation (8.3.10),\\nβ(μ) decreases in μ and β(μ0) = 1 −α. This gives that\\nβ(μ) ≥β(μ0) = 1 −α\\nfor all μ ≤μ0\\nwhich shows that the test given by Equation (8.3.10) remains a level α test for\\nH0 : μ ≤μ0 against the alternative hypothesis H1 : μ > μ0.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 328}, page_content='8.3 Tests concerning the mean of a normal population\\n317\\nRemark\\nWe can also test the one-sided hypothesis\\nH0 : μ = μ0\\n(or μ ≥μ0)\\nversus\\nH1 : μ < μ0\\nat signiﬁcance level α by\\naccepting\\nH0\\nif\\n√n\\nσ (X −μ0) ≥−zα\\nrejecting\\nH0\\notherwise\\nThis test can alternatively be performed by ﬁrst computing the value of the test\\nstatistic √n(X −μ0)/σ. The p-value would then equal the probability that a\\nstandard normal would be less than this value, and the hypothesis would be\\nrejected at any signiﬁcance level greater than or equal to this p-value.\\nExample 8.3.f. All cigarettes presently on the market have an average nicotine\\ncontent of at least 1.6 mg per cigarette. A ﬁrm that produces cigarettes claims\\nthat it has discovered a new way to cure tobacco leaves that will result in the av-\\nerage nicotine content of a cigarette being less than 1.6 mg. To test this claim, a\\nsample of 20 of the ﬁrm’s cigarettes were analyzed. If it is known that the stan-\\ndard deviation of a cigarette’s nicotine content is .8 mg, what conclusions can\\nbe drawn, at the 5 percent level of signiﬁcance, if the average nicotine content\\nof the 20 cigarettes is 1.54?\\nNote: The above raises the question of how we would know in advance that\\nthe standard deviation is .8. One possibility is that the variation in a cigarette’s\\nnicotine content is due to variability in the amount of tobacco in each cigarette\\nand not on the method of curing that is used. Hence, the standard deviation\\ncan be known from previous experience.\\nSolution. We must ﬁrst decide on the appropriate null hypothesis. As was pre-\\nviously noted, our approach to testing is not symmetric with respect to the null\\nand the alternative hypotheses since we consider only tests having the property\\nthat their probability of rejecting the null hypothesis when it is true will never\\nexceed the signiﬁcance level α. Thus, whereas rejection of the null hypothesis\\nis a strong statement about the data not being consistent with this hypothesis,\\nan analogous statement cannot be made when the null hypothesis is accepted.\\nHence, since in the preceding example we would like to endorse the producer’s\\nclaims only when there is substantial evidence for it, we should take this claim\\nas the alternative hypothesis. That is, we should test\\nH0 : μ ≥1.6\\nversus\\nH1 : μ < 1.6\\nNow, the value of the test statistic is\\n√n(X −μ0)/σ =\\n√\\n20(1.54 −1.6)/.8 = −.336'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 329}, page_content='318 CHAPTER 8: Hypothesis testing\\nand so the p-value is given by\\np-value = P{Z < −.336},\\nZ ∼N(0,1)\\n= .368\\nSince this value is greater than .05, the foregoing data do not enable us to reject,\\nat the .05 percent level of signiﬁcance, the hypothesis that the mean nicotine\\ncontent exceeds 1.6 mg. In other words, the evidence, although supporting the\\ncigarette producer’s claim, is not strong enough to prove that claim.\\n■\\nRemarks\\n(a) There is a direct analogy between conﬁdence interval estimation and hy-\\npothesis testing. For instance, for a normal population having mean μ and\\nknown variance σ 2, we have shown in Section 7.3 that a 100(1 −α) percent\\nconﬁdence interval for μ is given by\\nμ ∈\\n\\x08\\nx −zα/2\\nσ\\n√n, x + zα/2\\nσ\\n√n\\n\\t\\nwhere x is the observed sample mean. More formally, the preceding conﬁdence\\ninterval statement is equivalent to\\nP\\n\\x03\\nμ ∈\\n\\x08\\nX −zα/2\\nσ\\n√n, X + zα/2\\nσ\\n√n\\n\\t\\x04\\n= 1 −α\\nHence, if μ = μ0, then the probability that μ0 will fall in the interval\\n\\x08\\nX −zα/2\\nσ\\n√n, X + zα/2\\nσ\\n√n\\n\\t\\nis 1 −α, implying that a signiﬁcance level α test of H0 : μ = μ0 versus H1 : μ ̸=\\nμ0 is to reject H0 when\\nμ0 /∈\\n\\x08\\nX −zα/2\\nσ\\n√n, X + zα/2\\nσ\\n√n\\n\\t\\nSimilarly, since a 100(1−α) percent one-sided conﬁdence interval for μ is given\\nby\\nμ ∈\\n\\x08\\nX −zα\\nσ\\n√n,∞\\n\\t\\nit follows that an α-level signiﬁcance test of H0 : μ ≤μ0 versus H1 : μ > μ0 is to\\nreject H0 when μ0 /∈(X −zασ/√n,∞) — that is, when μ0 < X −zασ/√n.\\n(b) A remark on robustness A test that performs well even when the under-\\nlying assumptions on which it is based are violated is said to be robust. For'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 330}, page_content='8.3 Tests concerning the mean of a normal population\\n319\\nTable 8.1 X1,...,Xn Is a Sample from a (μ,σ 2) Population, σ 2 Is Known,\\nX =\\nn\\x02\\ni=1\\nXi/n.\\nH0\\nH1\\nTest Statistic TS\\nSigniﬁcance\\nLevel α Test\\np-Value if TS = t\\nμ = μ0\\nμ ̸= μ0\\n√n(X −μ0)/σ\\nReject if |T S| > zα/2\\n2P {Z ≥|t|}\\nμ ≤μ0\\nμ > μ0\\n√n(X −μ0)/σ\\nReject if T S > zα\\nP {Z ≥t}\\nμ ≥μ0\\nμ < μ0\\n√n(X −μ0)/σ\\nReject if T S < −zα\\nP {Z ≤t}\\nZ is a standard normal random variable.\\ninstance, the tests of Sections 8.3.1 and 8.3.1.1 were derived under the as-\\nsumption that the underlying population distribution is normal with known\\nvariance σ 2. However, in deriving these tests, this assumption was used only to\\nconclude that X also has a normal distribution. But, by the central limit the-\\norem, it follows that for a reasonably large sample size, X will approximately\\nhave a normal distribution no matter what the underlying distribution. Thus\\nwe can conclude that these tests will be relatively robust for any population\\ndistribution with variance σ 2.\\nTable 8.1 summarizes the tests of this subsection.\\n8.3.2\\nCase of unknown variance: the t-test\\nUp to now we have supposed that the only unknown parameter of the normal\\npopulation distribution is its mean. However, the more common situation is\\none where the mean μ and variance σ 2 are both unknown. Let us suppose this\\nto be the case and again consider a test of the hypothesis that the mean is equal\\nto some speciﬁed value μ0. That is, consider a test of\\nH0 : μ = μ0\\nversus the alternative\\nH1 : μ ̸= μ0\\nIt should be noted that the null hypothesis is not a simple hypothesis since it\\ndoes not specify the value of σ 2.\\nAs before, it seems reasonable to reject H0 when the sample mean X is far from\\nμ0. However, how far away it need be to justify rejection will depend on the\\nvariance σ 2. Recall that when the value of σ 2 was known, the test called for\\nrejecting H0 when |X −μ0| exceeded zα/2σ/√n or, equivalently, when\\n\\x06\\x06\\x06\\x06\\n√n( ¯X −μ0)\\nσ\\n\\x06\\x06\\x06\\x06 > zα/2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 331}, page_content='320 CHAPTER 8: Hypothesis testing\\nNow when σ 2 is no longer known, it seems reasonable to estimate it by\\nS2 =\\nn\\x02\\ni=1\\n(Xi −X)2\\nn −1\\nand then to reject H0 when\\n\\x06\\x06\\x06\\x06\\x06\\nX −μ0\\nS/√n\\n\\x06\\x06\\x06\\x06\\x06\\nis large.\\nTo determine how large a value of the statistic\\n\\x06\\x06\\x06\\x06\\x06\\n√n(X −μ0)\\nS\\n\\x06\\x06\\x06\\x06\\x06\\nto require for rejection, in order that the resulting test have signiﬁcance level\\nα, we must determine the probability distribution of this statistic when H0 is\\ntrue. However, as shown in Section 6.5, the statistic T , deﬁned by\\nT =\\n√n(X −μ0)\\nS\\nhas, when μ = μ0, a t-distribution with n −1 degrees of freedom. Hence,\\nPμ0\\n\\x05\\n−tα/2,n−1 ≤\\n√n(X −μ0)\\nS\\n≤tα/2,n−1\\n\\x07\\n= 1 −α\\n(8.3.11)\\nwhere tα/2,n−1 is the 100 α/2 upper percentile value of the t-distribution with\\nn −1 degrees of freedom. (That is, P{Tn−1 ≥tα/2,n−1} = P{Tn−1 ≤−tα/2,n−1} =\\nα/2 when Tn−1 has a t-distribution with n−1 degrees of freedom.) From Equa-\\ntion (8.3.11) we see that the appropriate signiﬁcance level α test of\\nH0 : μ = μ0\\nversus\\nH1 : μ ̸= μ0\\nis, when σ 2 is unknown, to\\naccept\\nH0\\nif\\n\\x06\\x06\\x06\\x06\\x06\\n√n(X −μ0)\\nS\\n\\x06\\x06\\x06\\x06\\x06 ≤tα/2, n−1\\nreject\\nH0\\nif\\n\\x06\\x06\\x06\\x06\\x06\\n√n(X −μ0)\\nS\\n\\x06\\x06\\x06\\x06\\x06 > tα/2, n−1\\n(8.3.12)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 332}, page_content='8.3 Tests concerning the mean of a normal population\\n321\\nFIGURE 8.3\\nThe two-sided t-test.\\nThe test deﬁned by Equation (8.3.12) is called a two-sided t-test. It is pictorially\\nillustrated in Figure 8.3.\\nIf we let t denote the observed value of the test statistic T = √n(X−μ0)/S, then\\nthe p-value of the test is the probability that |T | would exceed |t| when H0 is\\ntrue. That is, the p-value is the probability that the absolute value of a t-random\\nvariable with n −1 degrees of freedom would exceed |t|. The test then calls for\\nrejection at all signiﬁcance levels higher than the p-value and acceptance at all\\nlower signiﬁcance levels.\\nExample 8.3.g. Among a clinic’s patients having blood cholesterol levels rang-\\ning in the medium to high range (at least 220 milliliters per deciliter of serum),\\nvolunteers were recruited to test a new drug designed to reduce blood choles-\\nterol. A group of 50 volunteers was given the drug for 1 month and the changes\\nin their blood cholesterol levels were noted. If the average change was a reduc-\\ntion of 14.8 with a sample standard deviation of 6.4, what conclusions can be\\ndrawn?\\nSolution. Let us start by testing the hypothesis that the change could be due\\nsolely to chance — that is, that the 50 changes constitute a normal sample with\\nmean 0. Because the value of the t-statistic used to test the hypothesis that a\\nnormal mean is equal to 0 is\\nT = √n X/S =\\n√\\n5014.8/6.4 = 16.352\\nit clear that we should reject the hypothesis that the changes were solely due\\nto chance. Unfortunately, however, we are not justiﬁed at this point in con-\\ncluding that the changes were due to the speciﬁc drug used and not to some\\nother possibility. For instance, it is well known that any medication received by\\na patient (whether or not this medication is directly relevant to the patient’s\\nsuffering) often leads to an improvement in the patient’s condition — the so-\\ncalled placebo effect. Also, another possibility that may need to be taken into\\naccount would be the weather conditions during the month of testing, for it is\\ncertainly conceivable that this affects blood cholesterol level. Indeed, it must\\nbe concluded that the foregoing was a very poorly designed experiment, for in\\norder to test whether a speciﬁc treatment has an effect on a disease that may be\\naffected by many things, we should try to design the experiment so as to neu-\\ntralize all other possible causes. The accepted approach for accomplishing this\\nis to divide the volunteers at random into two groups — one group to receive\\nthe drug and the other to receive a placebo (that is, a tablet that looks and tastes'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 333}, page_content='322 CHAPTER 8: Hypothesis testing\\nlike the actual drug but has no physiological effect). The volunteers should not\\nbe told whether they are in the actual or control group, and indeed it is best\\nif even the clinicians do not have this information (the so-called double-blind\\ntest) so as not to allow their own biases to play a role. Since the two groups are\\nchosen at random from among the volunteers, we can now hope that on aver-\\nage all factors affecting the two groups will be the same except that one received\\nthe actual drug and the other a placebo. Hence, any difference in performance\\nbetween the groups can be attributed to the drug.\\n■\\nExample 8.3.h. A public health ofﬁcial claims that the mean home water use is\\n350 gallons a day. To verify this claim, a study of 20 randomly selected homes\\nwas instigated with the result that the average daily water uses of these 20\\nhomes were as follows:\\n340\\n344\\n362\\n375\\n356\\n386\\n354\\n364\\n332\\n402\\n340\\n355\\n362\\n322\\n372\\n324\\n318\\n360\\n338\\n370\\nDo the data contradict the ofﬁcial’s claim?\\nSolution. To determine if the data contradict the ofﬁcial’s claim, we need to\\ntest\\nH0 : μ = 350\\nversus\\nH1 : μ ̸= 350\\nThis can be accomplished by noting ﬁrst that the sample mean and sample\\nstandard deviation of the preceding data set are\\nX = 353.8,\\nS = 21.8478\\nThus, the value of the test statistic is\\nT =\\n√\\n20(3.8)\\n21.8478 = .7778\\nBecause this is less than t.05,19 = 1.730, the null hypothesis is accepted at the 10\\npercent level of signiﬁcance. Indeed, the p-value of the test data is\\np-value = P{|T19| > .7778} = 2P{T19 > .7778} = .4462\\nindicating that the null hypothesis would be accepted at any reasonable signif-\\nicance level, and thus that the data are not inconsistent with the claim of the\\nhealth ofﬁcial.\\n■\\nR can be used to perform the computations in a t-test. To test the null hypoth-\\nesis that a mean μ is equal to μ0 against the alternative that μ ̸= μ0 when the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 334}, page_content='8.3 Tests concerning the mean of a normal population\\n323\\ndata are x1,...,xn, type\\n>x = c(x1,...,xn)\\n>t.test(x,mu = μ0)\\nand hit return. Upon doing so, the output will include both a 95 percent con-\\nﬁdence interval and the p-value. For instance, to use R to solve Example 8.3.h,\\ndo the following:\\n>x = c(340,356,332,362,318,344,386,402,322,360,362,354,340,372,338,\\n375,363,355,324,370)\\n>t.test(x,mu = 350)\\nAfter hitting return after the second line, you will receive the following output:\\nOne Sample t-test\\ndata: x\\nt = 0.77784, df = 19, p-value = 0.4462\\nalternative hypothesis: true mean is not equal to 350\\n95 percent conﬁdence interval:\\n343.5749 364.0251\\nsample estimates:\\nmean of x\\n353.8\\nWe can use a one-sided t-test to test the hypothesis\\nH0 : μ = μ0\\n(orH0 : μ ≤μ0)\\nagainst the one-sided alternative\\nH1 : μ > μ0\\nThe signiﬁcance level α test is to\\naccept\\nH0\\nif\\n√n(X −μ0)\\nS\\n≤tα, n−1\\nreject\\nH0\\nif\\n√n(X −μ0)\\nS\\n> tα, n−1\\n(8.3.13)\\nIf √n(X −μ0)/S = v, then the p-value of the test is the probability that a\\nt-random variable with n −1 degrees of freedom would be at least as large\\nas v.\\nThe signiﬁcance level α test of\\nH0 : μ = μ0\\n(orH0 : μ ≥μ0)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 335}, page_content='324 CHAPTER 8: Hypothesis testing\\nversus the alternative\\nH1 : μ < μ0\\nis to\\naccept\\nH0\\nif\\n√n(X −μ0)\\nS\\n≥−tα, n−1\\nreject\\nH0\\nif\\n√n(X −μ0)\\nS\\n< −tα, n−1\\nThe p-value of this test is the probability that a t-random variable with n −\\n1 degrees of freedom would be less than or equal to the observed value of\\n√n(X −μ0)/S.\\nExample 8.3.i. The manufacturer of a new ﬁberglass tire claims that its average\\nlife will be at least 40,000 miles. To verify this claim a sample of 12 tires is\\ntested, with their lifetimes (in 1000s of miles) being as follows:\\nTire\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\nLife\\n36.1 40.2 33.8\\n38.5 42 35.8\\n37 41 36.8\\n37.2 33 36\\nTest the manufacturer’s claim at the 5 percent level of signiﬁcance.\\nSolution. To determine whether the foregoing data are consistent with the\\nhypothesis that the mean life is at least 40,000 miles, we will test\\nH0 : μ ≥40,000\\nversus\\nH1 : μ < 40,000\\nA computation gives that\\nX = 37.2833,\\nS = 2.7319\\nand so the value of the test statistic is\\nT =\\n√\\n12(37.2833 −40)\\n2.7319\\n= −3.4448\\nSince this is less than −t.05,11 = −1.796, the null hypothesis is rejected at the 5\\npercent level of signiﬁcance. Indeed, the p-value of the test data is\\np-value = P{T11 < −3.4448} = P{T11 > 3.4448} = .0027\\nindicating that the manufacturer’s claim would be rejected at any signiﬁcance\\nlevel greater than .003.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 336}, page_content='8.3 Tests concerning the mean of a normal population\\n325\\nThe preceding could also have been solved by using R. R performs one-sided\\nt-tests by specifying that the alternative hypothesis is that the mean is either\\nless than or greater than μ0. That is, to test\\nH0 : μ = μ0 versus H1 : μ > μ0\\nwhen using data x1,...,xn, just type\\n>x = c(x1,...,xn)\\n>t.test(x,alternative =“greater”,mu = μ0)\\nand hit return. Thus, for Example 8.3.i, we obtain\\n>x = c(36.1,40.2,33.8,38.5,42,35.8,37,41,36.8,37.2,33,36)\\n>t.test(x,alternative = “less”,mu = 40)\\nOne Sample t-test\\ndata: x\\nt = -3.4448, df = 11, p-value = 0.002739\\nalternative hypothesis: true mean is less than 40\\n95 percent conﬁdence interval:\\n-Inf\\n38.69963\\nsample estimates:\\nmean of x\\n37.28333\\nExample 8.3.j. In a single-server queueing system in which customers arrive ac-\\ncording to a Poisson process, the long-run average queueing delay per customer\\ndepends on the service distribution through its mean and variance. Indeed, if\\nμ is the mean service time, and σ 2 is the variance of a service time, then the\\naverage amount of time that a customer spends waiting in queue is given by\\nλ(μ2 + σ 2)\\n2(1 −λμ)\\nprovided that λμ < 1, where λ is the arrival rate. (The average delay is inﬁnite if\\nλμ ≥1.) As can be seen by this formula, the average delay is quite large when\\nμ is only slightly smaller than 1/λ, where, since λ is the arrival rate, 1/λ is the\\naverage time between arrivals.\\nSuppose that the owner of a service station will hire a second server if it can\\nbe shown that the average service time exceeds 8 minutes. The following data\\ngive the service times (in minutes) of 28 customers of this queueing system.\\nDo they indicate that the mean service time is greater than 8 minutes?\\n8.6,9.4,5.0,4.4,3.7,11.4,10.0,7.6,14.4,12.2,11.0,14.4,9.3,10.5,\\n10.3,7.7,8.3,6.4,9.2,5.7,7.9,9.4,9.0,13.3,11.6,10.0,9.5,6.6'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 337}, page_content='326 CHAPTER 8: Hypothesis testing\\nTable 8.2 X1,...,Xn Is a Sample from a (μ,σ 2) Population, σ 2 Is Unknown,\\nX =\\nn\\x02\\ni=1\\nXi/nS2 =\\nn\\x02\\ni=1\\n(Xi −X)2/(n −1).\\nH0\\nH1\\nTest Statistic TS\\nSigniﬁcance Level α Test\\np-Value if\\nTS = t\\nμ = μ0\\nμ ̸= μ0\\n√n(X −μ0)/S\\nReject if |T S| > tα/2, n−1\\n2P {Tn−1 ≥\\n|t|}\\nμ ≤μ0\\nμ > μ0\\n√n(X −μ0)/S\\nReject if T S > tα, n−1\\nP {Tn−1 ≥t}\\nμ ≥μ0\\nμ < μ0\\n√n(X −μ0)/S\\nReject if T S < −tα, n−1\\nP {Tn−1 ≤t}\\nTn−1 is a t-random variable with n −1 degrees of freedom: P{Tn−1 > tα, n−1} = α.\\nSolution. Let us use the preceding data to test the null hypothesis that the\\nmean service time is less than or equal to 8 minutes. A small p-value will then\\nbe strong evidence that the mean service time is greater than 8 minutes. Using\\nR on these data shows that the value of the test statistic is 2.257, with a resulting\\np-value of .016. Such a small p-value is certainly strong evidence that the mean\\nservice time exceeds 8 minutes.\\n■\\nTable 8.2 summarizes the tests of this subsection.\\n8.4\\nTesting the equality of means of two normal\\npopulations\\nA common situation faced by a practicing engineer is one in which she must\\ndecide whether two different approaches lead to the same solution. Often such\\na situation can be modeled as a test of the hypothesis that two normal popula-\\ntions have the same mean value.\\n8.4.1\\nCase of known variances\\nSuppose that X1,...,Xn and Y1,...,Ym are independent samples from normal\\npopulations having unknown means μx and μy but known variances σ 2\\nx and\\nσ 2\\ny . Let us consider the problem of testing the hypothesis\\nH0 : μx = μy\\nversus the alternative\\nH1 : μx ̸= μy\\nSince X is an estimate of μx and Y of μy, it follows that X −Y can be used\\nto estimate μx −μy. Hence, because the null hypothesis can be written as H0 :\\nμx −μy = 0, it seems reasonable to reject it when X −Y is far from zero. That'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 338}, page_content='8.4 Testing the equality of means of two normal populations\\n327\\nis, the form of the test should be to\\nreject\\nH0\\nif\\n|X −Y| > c\\naccept\\nH0\\nif\\n|X −Y| ≤c\\n(8.4.1)\\nfor some suitably chosen value c.\\nTo determine that value of c that would result in the test in Equation (8.4.1)\\nhaving a signiﬁcance level α, we need determine the distribution of X −Y\\nwhen H0 is true. However, as was shown in Section 7.3.2,\\nX −Y ∼\\n\\nμx −μy, σ 2\\nx\\nn +\\nσ 2\\ny\\nm\\n\\x0b\\nwhich implies that\\nX −Y −(μx −μy)\\n\\x0c\\nσ 2\\nx\\nn +\\nσ 2\\ny\\nm\\n∼(0,1)\\n(8.4.2)\\nHence, when H0 is true (and so μx −μy = 0), it follows that\\n(X −Y)\\n\\r\\x0c\\nσ 2x\\nn +\\nσ 2y\\nm\\nhas a standard normal distribution, and thus\\nPH0\\n⎧\\n⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎩\\n−zα/2 ≤\\nX −Y\\n\\x0c\\nσ 2\\nx\\nn +\\nσ 2\\ny\\nm\\n≤zα/2\\n⎫\\n⎪⎪⎪⎪⎬\\n⎪⎪⎪⎪⎭\\n= 1 −α\\n(8.4.3)\\nFrom Equation (8.4.3), we obtain that the signiﬁcance level α test of H0 : μx =\\nμy versus H1 : μx ̸= μy is\\naccept\\nH0\\nif\\n|X −Y|\\n\\x15\\nσ 2x /n + σ 2y /m\\n≤zα/2\\nreject\\nH0\\nif\\n|X −Y|\\n\\x15\\nσ 2x /n + σ 2y /m\\n≥zα/2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 339}, page_content='328 CHAPTER 8: Hypothesis testing\\nTable 8.3 Tire Lives in Units of 100\\nKilometers.\\nTires Tested at A\\nTires Tested at B\\n61.1\\n62.2\\n58.2\\n56.6\\n62.3\\n66.4\\n64\\n56.2\\n59.7\\n57.4\\n66.2\\n58.4\\n57.8\\n57.6\\n61.4\\n65.4\\n62.2\\n63.6\\nExample 8.4.a. Two new methods for producing a tire have been proposed. To\\nascertain which is superior, a tire manufacturer produces a sample of 10 tires\\nusing the ﬁrst method and a sample of 8 using the second. The ﬁrst set is to\\nbe road tested at location A and the second at location B. It is known from\\npast experience that the lifetime of a tire that is road tested at one of these\\nlocations is normally distributed with a mean life due to the tire but with a\\nvariance due (for the most part) to the location. Speciﬁcally, it is known that the\\nlifetimes of tires tested at location A are normal with standard deviation equal\\nto 4000 kilometers, whereas those tested at location B are normal with σ =\\n6000 kilometers. If the manufacturer is interested in testing the hypothesis that\\nthere is no appreciable difference in the mean life of tires produced by either\\nmethod, what conclusion should be drawn at the 5 percent level of signiﬁcance\\nif the resulting data are as given in Table 8.3?\\nSolution. A simple computation shows that the value of the test statistic is\\n.066. For such a small value of the test statistic (which has a standard normal\\ndistribution when H0 is true), it is clear that the null hypothesis is accepted.\\n■\\nIt follows from Equation (8.4.1) that a test of the hypothesis H0 : μx = μy\\n(or H0 : μx ≤μy) against the one-sided alternative H1 : μx > μy would be to\\naccept\\nH0\\nif\\nX −Y ≤zα\\n\\x0c\\nσ 2\\nx\\nn +\\nσ 2\\ny\\nm\\nreject\\nH0\\nif\\nX −Y > zα\\n\\x0c\\nσ 2\\nx\\nn +\\nσ 2\\ny\\nm\\n8.4.2\\nCase of unknown variances\\nSuppose again that X1, ...,Xn and Y1, ...,Ym are independent samples from\\nnormal populations having respective parameters (μx,σ 2\\nx ) and (μy,σ 2\\ny ), but'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 340}, page_content='8.4 Testing the equality of means of two normal populations\\n329\\nnow suppose that all four parameters are unknown. We will once again con-\\nsider a test of\\nH0 : μx = μy\\nversus\\nH1 : μx ̸= μy\\nTo determine a signiﬁcance level α test of H0 we will need to make the ad-\\nditional assumption that the unknown variances σ 2\\nx and σ 2\\ny are equal. Let σ 2\\ndenote their value — that is,\\nσ 2 = σ 2\\nx = σ 2\\ny\\nAs before, we would like to reject H0 when X −Y is “far” from zero. To deter-\\nmine how far from zero it needs to be, let\\nS2\\nx =\\nn\\x02\\ni=1\\n(Xi −X)2\\nn −1\\nS2\\ny =\\nm\\n\\x02\\ni=1\\n(Yi −Y)2\\nm −1\\ndenote the sample variances of the two samples. Then, as was shown in Sec-\\ntion 7.3.2,\\nX −Y −(μx −μy)\\n\\x15\\nS2p(1/n + 1/m)\\n∼tn+m−2\\nwhere S2\\np, the pooled estimator of the common variance σ 2, is given by\\nS2\\np =\\n(n −1)S2\\nx + (m −1)S2\\ny\\nn + m −2\\nHence, when H0 is true, and so μx −μy = 0, the statistic\\nT ≡\\nX −Y\\n\\x15\\nS2p(1/n + 1/m)\\nhas a t-distribution with n + m −2 degrees of freedom. From this, it follows\\nthat we can test the hypothesis that μx = μy as follows:\\naccept\\nH0\\nif\\n|T | ≤tα/2, n+m−2\\nreject\\nH0\\nif\\n|T | > tα/2, n+m−2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 341}, page_content='330 CHAPTER 8: Hypothesis testing\\nFIGURE 8.4\\nDensity of a t-random variable with k degrees of freedom.\\nwhere tα/2, n+m−2 is the 100 α/2 percentile point of a t-random variable with\\nn + m −2 degrees of freedom (see Figure 8.4).\\nAlternatively, the test can be run by determining the p-value. If T is observed\\nto equal v, then the resulting p-value of the test of H0 against H1 is given by\\np-value = P{|Tn+m−2| ≥|v|}\\n= 2P{Tn+m−2 ≥|v|}\\nwhere Tn+m−2 is a t-random variable having n + m −2 degrees of freedom.\\nIf we are interested in testing the one-sided hypothesis\\nH0 : μx ≤μy\\nversus\\nH1 : μx > μy\\nthen H0 will be rejected at large values of T . Thus the signiﬁcance level α test is\\nto\\nreject\\nH0\\nif\\nT ≥tα, n+m−2\\nnot reject\\nH0\\notherwise\\nIf the value of the test statistic T is v, then the p-value is given by\\np-value = P{Tn+m−2 ≥v}\\nR can be used to implement this two-sample t-test. Suppose that x1,...,xn and\\ny1,...,ym are the values of independent samples from two different normal\\npopulations having a common variance. Let μx denote the mean of the ﬁrst\\npopulation, and let μy be the mean of the second. To test H0 : μx = μy against\\nthe alternative H1 : μx ̸= μy, do the following.\\n>x = c(x1,...,xn)\\n>y = c(y1,...,ym)\\n>t.test(x,y,var.equal = TRUE)\\nUpon hitting return, the output will give the p-value as well as a 95 percent con-\\nﬁdence interval for μx −μy. For instance, suppose the x-data are 3,5,7,9 and'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 342}, page_content='8.4 Testing the equality of means of two normal populations\\n331\\nthe y-data are 6,8,12,15; then to test that H0 : μx = μy against the alternative\\nH1 : μx ̸= μy do the following:\\n>x = c(3,5,7,9)\\n>y = c(6,8,12,15)\\n>t.test(x,y,var.equal = TRUE)\\nwith the result\\nTwo Sample t-test\\ndata: x and y\\nt = -1.7756, df = 6, p-value = 0.1261\\nalternative hypothesis: true difference in means is not equal to 0\\n95 percent conﬁdence interval:\\n-10.106849 1.606849\\nsample estimates:\\nmean of x mean of y\\n6.00 10.25\\nNote that R returns not only the p-value but also a 95 percent conﬁdence inter-\\nval for μx −μy.\\nSuppose now that we wanted a one-sided test. Say we wanted to test H0 : μx ≤\\nμy against the alternative H1 : μx > μy. This is accomplished by typing the\\nfollowing\\n>x = c(x1,...,xn)\\n>y = c(y1,...,ym)\\n>t.test(x,y,alternative = “greater”,var.equal = TRUE)\\nTo test H0 : μx ≥μy against the alternative H1 : μx < μy, use\\n>x = c(x1,...,xn)\\n>y = c(y1,...,ym)\\n>t.test(x,y,alternative = “less”,var.equal = TRUE)\\nand hit return.\\nExample 8.4.b. Twenty-two volunteers at a cold research institute caught a\\ncold after having been exposed to various cold viruses. A random selection of\\n10 of these volunteers was given tablets containing 1 gram of vitamin C. These\\ntablets were taken four times a day. The control group consisting of the other\\n12 volunteers was given placebo tablets that looked and tasted exactly the same\\nas the vitamin C tablets. This was continued for each volunteer until a doctor,\\nwho did not know if the volunteer was receiving the vitamin C or the placebo'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 343}, page_content='332 CHAPTER 8: Hypothesis testing\\ntablets, decided that the volunteer was no longer suffering from the cold. The\\nlength of time the cold lasted was then recorded.\\nAt the end of this experiment, the following data resulted.\\nTreated with Vitamin C\\nTreated with Placebo\\n5.5\\n6.5\\n6.0\\n6.0\\n7.0\\n8.5\\n6.0\\n7.0\\n7.5\\n6.5\\n6.0\\n8.0\\n7.5\\n7.5\\n5.5\\n6.5\\n7.0\\n7.5\\n6.5\\n6.0\\n8.5\\n7.0\\nDo the data listed prove that taking 4 grams daily of vitamin C reduces the\\nmean length of time a cold lasts? At what level of signiﬁcance?\\nSolution. To prove the above hypothesis, we would need to reject the null\\nhypothesis in a test of\\nH0 : μc ≥μP\\nversus\\nH1 : μc < μP\\nwhere μc is the mean time a cold lasts when the vitamin C tablets are taken and\\nμp is the mean time when the placebo is taken. Assuming that the variance of\\nthe length of the cold is the same for the vitamin C patients and the placebo\\npatients, we test the above by using R. This yields the following:\\n>x = c(5.5,6,7,6,7.5,6,7.5,5.5,7,6.5)\\n>y = c(6.5,6,8.5,7,6.5,8,7.5,6.5,7.5,6,8.5,7)\\n>t.test(x,y,alternative = “less”,var.equal = TRUE)\\nTwo Sample t-test\\ndata: x and y\\nt = -1.8987, df = 20, p-value = 0.03606\\nalternative hypothesis: true difference in means is less than 0\\n95 percent conﬁdence interval: -Inf -0.06185013\\nsample estimates:\\nmean of x mean of y\\n6.450 7.125'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 344}, page_content='8.4 Testing the equality of means of two normal populations\\n333\\nAs the p-value is .036, the result that vitamin C reduces the length of time of\\na cold is established at the 5 percent level of signiﬁcance. Also, note that aside\\nfrom outputting the p-value, R also gives a one-sided 95 percent conﬁdence\\ninterval for μx −μy.\\n■\\n8.4.3\\nCase of unknown and unequal variances\\nLet us now suppose that the population variances σ 2\\nx and σ 2\\ny are not only un-\\nknown but also cannot be considered to be equal. In this situation, since S2\\nx is\\nthe natural estimator of σ 2\\nx and S2\\ny of σ 2\\ny , it would seem reasonable to base our\\ntest of\\nH0 : μx = μy\\nversus\\nH1 : μx ̸= μy\\non the test statistic\\nX −Y\\n\\x0c\\nS2\\nx\\nn +\\nS2\\ny\\nm\\n(8.4.4)\\nHowever, the foregoing has a complicated distribution, which, even when H0\\nis true, depends on the unknown parameters, and thus cannot be generally\\nemployed. The one situation in which we can utilize the statistic of Equation\\n(8.4.4) is when n and m are both large. In such a case, it can be shown that\\nwhen H0 is true Equation (8.4.4) will have approximately a standard normal\\ndistribution. Hence, when n and m are large an approximate level α test of H0 :\\nμx = μy versus H1 : μx ̸= μy is to\\naccept\\nH0\\nif\\n−zα/2 ≤\\nX −Y\\n\\x0c\\nS2\\nx\\nn +\\nS2\\ny\\nm\\n≤zα/2\\nreject\\notherwise\\nThe problem of determining an exact level α test of the hypothesis that the\\nmeans of two normal populations, having unknown and not necessarily equal\\nvariances, are equal is known as the Behrens-Fisher problem. There is no com-\\npletely satisfactory solution known.\\nTable 8.4 presents the two-sided tests of this section.\\n8.4.4\\nThe paired t-test\\nSuppose we are interested in determining whether the installation of a certain\\nantipollution device will affect a car’s mileage. To test this, a collection of n cars\\nthat do not have this device are gathered. Each car’s mileage per gallon is then'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 345}, page_content='334 CHAPTER 8: Hypothesis testing\\nTable 8.4 X1,...,Xn Is a Sample from a (μ1,σ 2\\n1 ) Population; Y1,...,Ym Is a Sam-\\nple from a (μ2,σ 2\\n2 ) Population,\\nThe Two Population Samples Are Independent to Test\\nH0 : μ1 = μ2 versus H0 : μ1 ̸= μ2.\\nAssump-\\ntion\\nTest Statistic TS\\nSigniﬁcance Level α Test\\np-Value if TS = t\\nσ1, σ2\\nknown\\nX−Y\\n\\x15\\nσ 2\\n1 /n + σ 2\\n2 /m\\nReject if |T S| > zα/2\\n2P {Z ≥|t|}\\nσ1 = σ2\\nX−Y\\n\\x16\\n(n−1)S2\\n1 + (m−1)S2\\n2\\nn + m−2\\n√1/n + 1/m\\nReject if |T S| > tα/2, n + m −2\\n2P {Tn + m −2 ≥|t|}\\nn, m large\\nX −Y\\n\\x15\\nS2\\n1/n + S2\\n2/m\\nReject if |T S| > zα/2\\n2P {Z ≥|t|}\\ndetermined both before and after the device is installed. How can we test the\\nhypothesis that the antipollution control has no effect on gas consumption?\\nThe data can be described by the n pairs (Xi,Yi),i = 1, ...,n, where Xi is the\\ngas consumption of the ith car before installation of the pollution control de-\\nvice, and Yi of the same car after installation. Because each of the n cars will\\nbe inherently different, we cannot treat X1, ...,Xn and Y1, ...,Yn as being in-\\ndependent samples. For example, if we know that X1 is large (say, 40 miles per\\ngallon), we would certainly expect that Y1 would also probably be large. Thus,\\nwe cannot employ the earlier methods presented in this section.\\nOne way in which we can test the hypothesis that the antipollution device does\\nnot affect gas mileage is to let the data consist of each car’s difference in gas\\nmileage. That is, let Wi = Xi −Yi, i = 1, ...,n. Now, if there is no effect from\\nthe device, it should follow that the Wi would have mean 0. Hence, we can test\\nthe hypothesis of no effect by testing\\nH0 : μw = 0\\nversus\\nH1 : μw ̸= 0\\nwhere W1, ...,Wn are assumed to be a sample from a normal population hav-\\ning unknown mean μw and unknown variance σ 2\\nw. But the t-test described in\\nSection 8.3.2 shows that this can be tested by\\naccepting\\nH0\\nif\\n−tα/2, n−1 < √n W\\nSw\\n< tα/2,n−1\\nrejecting\\nH0\\notherwise\\nExample 8.4.c. An industrial safety program was recently instituted in the\\ncomputer chip industry. The average weekly loss (averaged over 1 month) in\\nlabor-hours due to accidents in 10 similar plants both before and after the pro-\\ngram are as follows:'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 346}, page_content='8.4 Testing the equality of means of two normal populations\\n335\\nPlant\\nBefore\\nAfter\\nA −B\\n1\\n30.5\\n23\\n−7.5\\n2\\n18.5\\n21\\n2.5\\n3\\n24.5\\n22\\n−2.5\\n4\\n32\\n28.5\\n−3.5\\n5\\n16\\n14.5\\n−1.5\\n6\\n15\\n15.5\\n.5\\n7\\n23.5\\n24.5\\n1\\n8\\n25.5\\n21\\n−4.5\\n9\\n28\\n23.5\\n−4.5\\n10\\n18\\n16.5\\n−1.5\\nDetermine, at the 5 percent level of signiﬁcance, whether the safety program\\nhas been proven to be effective.\\nSolution. To determine this, we will test\\nH0 : μA −μB ≥0\\nversus\\nH1 : μA −μB < 0\\nbecause this will enable us to see whether the null hypothesis that the safety\\nprogram has not had a beneﬁcial effect is a reasonable possibility. To test this,\\nwe use R to ﬁrst obtain v, the value of the test statistic, and then the p-value,\\nequal to the probability that a t random variable with 9 degrees of freedom\\nwould be less than v.\\n>d = c(−7.5,2.5,−2.5,−3.5,−1.5,.5,1,−4.5,−4.5,−1.5)\\n>v = sqrt(10/var(d)) ∗mean(d)\\n>v\\n[1] −2.265949\\n>pt(v,9)\\n[1]0.02484552\\nThus, v = −2.265949, with resulting\\np-value = P(T9 ≤−2.265949) = 0.02484552\\n(Note that we use the R command pt(v,n) to obtain the probability that a t\\nrandom variable with n degrees of freedom is less than or equal to v.)\\nSince the p-value is less than .05, the hypothesis that the safety program has\\nnot been effective is rejected and so we can conclude that its effectiveness has\\nbeen established (at least for any signiﬁcance level greater than .025).\\n■\\nNote that the paired-sample t-test can be used even though the samples are not\\nindependent and the population variances are unequal.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 347}, page_content='336 CHAPTER 8: Hypothesis testing\\n8.5\\nHypothesis tests concerning the variance of a\\nnormal population\\nLet X1, ...,Xn denote a sample from a normal population having unknown\\nmean μ and unknown variance σ 2, and suppose we desire to test the hypothesis\\nH0 : σ 2 = σ 2\\n0\\nversus the alternative\\nH1 : σ 2 ̸= σ 2\\n0\\nfor some speciﬁed value σ 2\\n0 .\\nTo obtain a test, recall (as was shown in Section 6.5) that (n −1)S2/σ 2 has a\\nchi-square distribution with n −1 degrees of freedom. Hence, when H0 is true\\n(n −1)S2\\nσ 2\\n0\\n∼χ2\\nn−1\\nBecause P{χ2\\nn−1 < χ2\\nα/2,n−1} = 1 −α/2 and P{χ2\\nn−1 < χ2\\n1−α/2,n−1} = α/2, it fol-\\nlows that\\nPH0\\n\\x05\\nχ2\\n1−α/2,n−1 ≤(n −1)S2\\nσ 2\\n0\\n≤χ2\\nα/2,n−1\\n\\x07\\n= 1 −α\\nTherefore, a signiﬁcance level α test is to\\naccept\\nH0\\nif\\nχ2\\n1−α/2,n−1 ≤(n −1)S2\\nσ 2\\n0\\n≤χ2\\nα/2,n−1\\nreject\\nH0\\notherwise\\nThe preceding test can be implemented by ﬁrst computing the value of the test\\nstatistic (n−1)S2/σ 2\\n0 — call it c. Then compute the probability that a chi-square\\nrandom variable with n −1 degrees of freedom would be (a) less than and\\n(b) greater than c. If either of these probabilities is less than α/2, then the\\nhypothesis is rejected. In other words, the p-value of the test data is\\np-value = 2min(P{χ2\\nn−1 < c},1 −P{χ2\\nn−1 < c})\\nThe quantity P{χ2\\nn−1 < c} can be obtained from the R command pchisq(c,n −\\n1). The p-value for a one-sided test is similarly obtained.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 348}, page_content='8.5 Hypothesis tests concerning the variance of a normal population 337\\nExample 8.5.a. A machine that automatically controls the amount of ribbon\\non a tape has recently been installed. This machine will be judged to be effec-\\ntive if the standard deviation σ of the amount of ribbon on a tape is less than\\n.15 cm. If a sample of 20 tapes yields a sample variance of S2 = .025 cm2, are\\nwe justiﬁed in concluding that the machine is ineffective?\\nSolution. We will test the hypothesis that the machine is effective, since a re-\\njection of this hypothesis will then enable us to conclude that it is ineffective.\\nSince we are thus interested in testing\\nH0 : σ 2 ≤.0225\\nversus\\nH1 : σ 2 > .0225\\nit follows that we would want to reject H0 when S2 is large. Hence, the p-value\\nof the preceding test data is the probability that a chi-square random variable\\nwith 19 degrees of freedom would exceed the observed value of 19S2/.0225 =\\n19 × .025/.0225 = 21.111. That is,\\np-value = P{χ2\\n19 > 21.111}\\n= 1 −pchisq(21.111,19) = 0.3307001\\nTherefore, we must conclude that the observed value of S2 = .025 is not large\\nenough to reasonably preclude the possibility that σ 2 ≤.0225, and so the null\\nhypothesis is accepted.\\n■\\n8.5.1\\nTesting for the equality of variances of two\\nnormal populations\\nLet X1,...,Xn and Y1,...,Ym denote independent samples from two normal\\npopulations having respective (unknown) parameters μx, σ 2\\nx and μy, σ 2\\ny and\\nconsider a test of\\nH0 : σ 2\\nx = σ 2\\ny\\nversus\\nH1 : σ 2\\nx ̸= σ 2\\ny\\nIf we let\\nS2\\nx =\\nn\\x02\\ni=1\\n(Xi −X)2\\nn −1\\nS2\\ny =\\nm\\n\\x02\\ni=1\\n(Yi −Y)2\\nm −1\\ndenote the sample variances, then as shown in Section 6.5, (n −1)S2\\nx/σ 2\\nx and\\n(m −1)S2\\ny/σ 2\\ny are independent chi-square random variables with n −1 and'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 349}, page_content='338 CHAPTER 8: Hypothesis testing\\nm −1 degrees of freedom, respectively. Therefore, (S2\\nx/σ 2\\nx )/(S2\\ny/σ 2\\ny ) has an\\nF-distribution with parameters n −1 and m −1. Hence, when H0 is true\\nS2\\nx/S2\\ny ∼Fn−1,m−1\\nand so\\nPH0{F1−α/2,n−1,m−1 ≤S2\\nx/S2\\ny ≤Fα/2,n−1,m−1} = 1 −α\\nThus, a signiﬁcance level α test of H0 against H1 is to\\naccept\\nH0\\nif\\nF1−α/2,n−1,m−1 < S2\\nx/S2\\ny < Fα/2,n−1,m−1\\nreject\\nH0\\notherwise\\nThe preceding test can be effected by ﬁrst determining the value of the test\\nstatistic S2\\nx/S2\\ny, say its value is v, and then computing P{Fn−1,m−1 ≤v} where\\nFn−1,m−1 is an F-random variable with parameters n −1, m −1. If this prob-\\nability is either less than α/2 (which occurs when S2\\nx is signiﬁcantly less than\\nS2\\ny) or greater than 1 −α/2 (which occurs when S2\\nx is signiﬁcantly greater than\\nS2\\ny), then the hypothesis is rejected. In other words, the p-value of the test data\\nis\\np-value = 2min(P{Fn−1,m−1 < v},1 −P{Fn−1,m−1 < v})\\nThe test now calls for rejection whenever the signiﬁcance level α is at least as\\nlarge as the p-value.\\nExample 8.5.b. There are two different choices of a catalyst to stimulate a cer-\\ntain chemical process. To test whether the variance of the yield is the same no\\nmatter which catalyst is used, a sample of 10 batches is produced using the ﬁrst\\ncatalyst, and 12 using the second. If the resulting data are S2\\n1 = .14 and S2\\n2 = .28,\\ncan we reject, at the 5 percent level, the hypothesis of equal variance?\\nSolution. Using the R command pf(v,n,m) to obtain the probability that an F\\nrandom variable with parameters n and m is less than or equal to v, we see that\\nP(F9,11 ≤.5) = pf (.5,9,11) = 0.1537596\\nHence,\\np-value = 2min(.1537596,1 −.1537596) = .3075\\nand so the hypothesis of equal variance cannot be rejected.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 350}, page_content='8.6 Hypothesis tests in Bernoulli populations\\n339\\n8.6\\nHypothesis tests in Bernoulli populations\\nThe binomial distribution is frequently encountered in engineering problems.\\nFor a typical example, consider a production process that manufactures items\\nthat can be classiﬁed in one of two ways — either as acceptable or as defective.\\nAn assumption often made is that each item produced will, independently, be\\ndefective with probability p, and so the number of defects in a sample of n\\nitems will thus have a binomial distribution with parameters (n, p). We will\\nnow consider a test of\\nH0 : p ≤p0\\nversus\\nH1 : p > p0\\nwhere p0 is some speciﬁed value.\\nIf we let X denote the number of defects in the sample of size n, then it is clear\\nthat we wish to reject H0 when X is large. To see how large it needs to be to\\njustify rejection at the α level of signiﬁcance, note that\\nP{X ≥k} =\\nn\\n\\x17\\ni=k\\nP{X = i} =\\nn\\n\\x17\\ni=k\\n\\x08n\\ni\\n\\t\\npi(1 −p)n−i\\nNow it is certainly intuitive (and can be proven) that P{X ≥k} is an increasing\\nfunction of p — that is, the probability that the sample will contain at least k\\nerrors increases in the defect probability p. Using this, we see that when H0 is\\ntrue (and so p ≤p0),\\nP{X ≥k} ≤\\nn\\n\\x17\\ni=k\\n\\x08n\\ni\\n\\t\\npi\\n0(1 −p0)n−i\\nHence, a signiﬁcance level α test of H0 : p ≤p0 versus H1 : p > p0 is to reject H0\\nwhen\\nX ≥k∗\\nwhere k∗is the smallest value of k for which \\x02n\\ni=k\\n\\x18n\\ni\\n\\x19\\npi\\n0(1 −p0)n−i ≤α. That\\nis,\\nk∗= min\\n\\x05\\nk :\\nn\\n\\x17\\ni=k\\n\\x08n\\ni\\n\\t\\npi\\n0(1 −p0)n−i ≤α\\n\\x07\\nThis test can best be performed by ﬁrst determining the value of the test statistic\\n— say, X = x — and then computing the p-value given by\\np-value = P{B(n,p0) ≥x}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 351}, page_content='340 CHAPTER 8: Hypothesis testing\\n=\\nn\\n\\x17\\ni=x\\n\\x08n\\ni\\n\\t\\npi\\n0(1 −p0)n−i\\nExample 8.6.a. A computer chip manufacturer claims that no more than 2 per-\\ncent of the chips it sends out are defective. An electronics company, impressed\\nwith this claim, has purchased a large quantity of such chips. To determine if\\nthe manufacturer’s claim can be taken literally, the company has decided to\\ntest a sample of 300 of these chips. If 10 of these 300 chips are found to be\\ndefective, should the manufacturer’s claim be rejected?\\nSolution. Let us test the claim at the 5 percent level of signiﬁcance. To see if\\nrejection is called for, we need to compute the probability that the sample of\\nsize 300 would have resulted in 10 or more defectives when p is equal to .02.\\n(That is, we compute the p-value.) If this probability is less than or equal to\\n.05, then the manufacturer’s claim should be rejected. Now\\nP.02{X ≥10} = 1 −P.02{X ≤9}\\n= 1 −pbinom(9,300,.02)\\n= 0.08183807\\nand so the manufacturer’s claim cannot be rejected at the 5 percent level of\\nsigniﬁcance.\\n■\\nExample 8.6.b. In an attempt to show that proofreader A is superior to proof-\\nreader B, both proofreaders were given the same manuscript to read. If proof-\\nreader A found 28 errors, and proofreader B found 18, with 10 of these errors\\nbeing found by both, can we conclude that A is the superior proofreader?\\nSolution. To begin note that A found 18 errors that B missed, and that B found\\n8 that A missed. Hence, a total of 26 errors were found by just a single proof-\\nreader. Now, if A and B were equally competent then they would be equally\\nlikely to be the sole ﬁnder of an error found by just one of them. Consequently,\\nif A and B were equally competent then each of the 26 singly found errors\\nwould have been found by A with probability 1/2. Hence, to establish that A\\nis the superior proofreader the result of 18 successes in 26 trials must be strong\\nenough to reject the null hypothesis when testing\\nH0 : p ≤1/2\\nversus\\nH1 : p > 1/2\\nwhere p is a Bernoulli probability that a trial is a success. Because the resultant\\np-value for the data cited is\\np-value = P{Bin(26,.5) ≥18} = 0.03776\\nthe null hypothesis would be rejected at the 5 percent level of signiﬁcance, thus\\nenabling one to conclude (at that level of signiﬁcance) that A is the superior\\nproofreader.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 352}, page_content='8.6 Hypothesis tests in Bernoulli populations 341\\nWhen the sample size n is large, we can derive an approximate signiﬁcance level\\nα test of H0 : p ≤p0 versus H1 : p > p0 by using the normal approximation to\\nthe binomial. It works as follows: Because when n is large X will have approxi-\\nmately a normal distribution with mean and variance\\nE[X] = np,\\nVar(X) = np(1 −p)\\nit follows that\\nX −np\\n√np(1 −p)\\nwill have approximately a standard normal distribution. Therefore, an approx-\\nimate signiﬁcance level α test would be to reject H0 if\\nX −np0\\n√np0(1 −p0) ≥zα\\nEquivalently, one can use the normal approximation to approximate the\\np-value.\\nExample 8.6.c. In Example 8.6.a, np0 = 300(.02) = 6, and √np0(1 −p0) =\\n√\\n5.88. Consequently, the p-value that results from the data X = 10 is\\np-value = P.02{X ≥10}\\n= P.02{X ≥9.5}\\n= P.02\\n\\x03 X −6\\n√\\n5.88\\n≥9.5 −6\\n√\\n5.88\\n\\x04\\n≈P{Z ≥1.443}\\n= .0745\\nThus, whereas the exact p-value is .0818, the normal approximation gives the\\nvalue .0745.\\n■\\nSuppose now that we want to test the null hypothesis that p is equal to some\\nspeciﬁed value; that is, we want to test\\nH0 : p = p0\\nversus\\nH1 : p ̸= p0\\nIf X, a binomial random variable with parameters n and p, is observed to equal\\nx, then a signiﬁcance level α test would reject H0 if the value x was either sig-\\nniﬁcantly larger or signiﬁcantly smaller than what would be expected when p\\nis equal to p0. More precisely, the test would reject H0 if either\\nP{Bin(n,p0) ≥x} ≤α/2\\nor\\nP{Bin(n,p0) ≤x} ≤α/2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 353}, page_content='342 CHAPTER 8: Hypothesis testing\\nIn other words, the p-value when X = x is\\np-value = 2min(P{Bin(n,p0) ≥x},P{Bin(n,p0) ≤x})\\nExample 8.6.d. Historical data indicate that 4 percent of the components pro-\\nduced at a certain manufacturing facility are defective. A particularly acrimo-\\nnious labor dispute has recently been concluded, and management is curious\\nabout whether it will result in any change in this ﬁgure of 4 percent. If a random\\nsample of 500 items indicated 16 defectives (3.2 percent), is this signiﬁcant ev-\\nidence, at the 5 percent level of signiﬁcance, to conclude that a change has\\noccurred?\\nSolution. To be able to conclude that a change has occurred, the data need to\\nbe strong enough to reject the null hypothesis when we are testing\\nH0 : p = .04\\nversus\\nH1 : p ̸= .04\\nwhere p is the probability that an item is defective. The p-value of the observed\\ndata of 16 defectives in 500 items is\\np-value = 2min{P{X ≤16},P{X ≥16}}\\nwhere X is a binomial (500, .04) random variable. Since 500 × .04 = 20, we see\\nthat\\np-value = 2P{X ≤16}\\nSince X has mean 20 and standard deviation √20(.96) ≈4.38, it is clear that\\ntwice the probability that X will be less than or equal to 16 — a value less\\nthan one standard deviation lower than the mean — is not going to be small\\nenough to justify rejection. Indeed, it can be shown that\\np-value = 2P{X ≤16} = 0.43161\\nand so there is not sufﬁcient evidence to reject the hypothesis that the proba-\\nbility of a defective item has remained unchanged.\\n■\\n8.6.1\\nTesting the equality of parameters in two\\nBernoulli populations\\nSuppose there are two distinct methods for producing a certain type of chip;\\nand suppose that chips produced by the ﬁrst method will, independently, be\\ndefective with probability p1, with the corresponding probability being p2 for\\nthose produced by the second method. To test the hypothesis that p1 =p2, a\\nsample of n1 chips is produced using method 1 and n2 using method 2.\\nLet X1 denote the number of defective chips obtained from the ﬁrst sample and\\nX2 for the second. Thus, X1 and X2 are independent binomial random vari-\\nables with respective parameters (n1,p1) and (n2,p2). Suppose that X1+X2 = k'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 354}, page_content='8.6 Hypothesis tests in Bernoulli populations 343\\nand so there have been a total of k defectives. Now, if H0 is true, then each of\\nthe n1 + n2 chips produced will have the same probability of being defective,\\nand so the determination of the k defectives will have the same distribution as\\na random selection of a sample of size k from a population of n1 + n2 items\\nof which n1 are white and n2 are black. In other words, given a total of k de-\\nfectives, the conditional distribution of the number of defective chips obtained\\nfrom method 1 will, when H0 is true, have the following hypergeometric dis-\\ntribution1:\\nPH0{X1 = i|X1 + X2 = k} =\\n\\x08n1\\ni\\n\\t\\x08 n2\\nk −i\\n\\t\\n\\x08n1 + n2\\nk\\n\\t ,\\ni = 0,1,...,k\\n(8.6.1)\\nNow, in testing\\nH0 : p1 = p2\\nversus\\nH1 : p1 ̸= p2\\nit seems reasonable to reject the null hypothesis when the proportion of de-\\nfective chips produced by method 1 is much different from the proportion of\\ndefectives obtained under method 2. Therefore, if there is a total of k defectives,\\nthen we would expect, when H0 is true, that X1/n1 (the proportion of defective\\nchips produced by method 1) would be close to (k −X1)/n2 (the proportion of\\ndefective chips produced by method 2). Because X1/n1 and (k −X1)/n2 will be\\nfarthest apart when X1 is either very small or very large, it thus seems that a rea-\\nsonable signiﬁcance level α test of Equation (8.6.1) is as follows. If X1 +X2 = k,\\nthen one should\\nreject\\nH0\\nif either\\nP{X ≤x1} ≤α/2\\nor\\nP{X ≥x1} ≤α/2\\naccept\\nH0\\notherwise\\nwhere X is a hypergeometric random variable with probability mass function\\nP{X = i} =\\n\\x08n1\\ni\\n\\t\\x08 n2\\nk −i\\n\\t\\n\\x08n1 + n2\\nk\\n\\t\\ni = 0,1,...,k\\n(8.6.2)\\nIn other words, this test will call for rejection if the signiﬁcance level is at least\\nas large as the p-value given by\\np-value = 2min(P{X ≤x1},P {X ≥x1})\\n(8.6.3)\\nThis is called the Fisher-Irwin test.\\n1See Example 5.3.b for a formal veriﬁcation of Equation (8.6.1).'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 355}, page_content='344 CHAPTER 8: Hypothesis testing\\nThe probabilities in Equation (8.6.3) can be obtained by using the R command\\nphyper(x,n,m,k) to obtain the probability that a hypergeometric random vari-\\nable with parameters n,m,k — which represents the number of red balls cho-\\nsen when k balls are randomly chosen from an urn containing n red and m\\nblue balls — is less than or equal to x.\\nExample 8.6.e. Suppose that method 1 resulted in 20 unacceptable transistors\\nout of 100 produced, whereas method 2 resulted in 12 unacceptable transistors\\nout of 100 produced. Can we conclude from this, at the 10 percent level of\\nsigniﬁcance, that the two methods are equivalent?\\nSolution. R yields the solution\\np-value =2min(P(X ≤20), P(X ≥20))\\n=2min(P(X ≤20), 1 −P(X ≤19))\\n=2 ∗min(phyper(20,100,100,32),1 −phyper(19,100,100,32))\\n[1]0.1763396\\nwhere the preceding used the R command min(x,y), which returns the mini-\\nmum of x and y.\\nHence, the hypothesis that the two methods are equivalent cannot be rejected.■\\nThe ideal way to test the hypothesis that the results of two different treatments\\nare identical is to randomly divide a group of people into a set that will receive\\nthe ﬁrst treatment and one that will receive the second. However, such ran-\\ndomization is not always possible. For instance, if we want to study whether\\ndrinking alcohol increases the risk of prostate cancer, we cannot instruct a\\nrandomly chosen sample to drink alcohol. An alternative way to study the hy-\\npothesis is to use an observational study that begins by randomly choosing a set\\nof drinkers and one of nondrinkers. These sets are followed for a period of time\\nand the resulting data are then used to test the hypothesis that members of the\\ntwo groups have the same risk for prostate cancer.\\nOur next sample illustrates another way of performing an observational study.\\nExample 8.6.f. In 1970, the researchers Herbst, Ulfelder, and Poskanzer (H-U-\\nP) suspected that vaginal cancer in young women, a rather rare disease, might\\nbe caused by one’s mother having taken the drug diethylstilbestrol (usually re-\\nferred to as DES) while pregnant. To study this possibility, the researchers could\\nhave performed an observational study by searching for a (treatment) group\\nof women whose mothers took DES when pregnant and a (control) group of\\nwomen whose mothers did not. They could then observe these groups for a\\nperiod of time and use the resulting data to test the hypothesis that the prob-\\nabilities of contracting vaginal cancer are the same for both groups. However,\\nbecause vaginal cancer is so rare (in both groups) such a study would require a'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 356}, page_content='8.7 Tests concerning the mean of a Poisson distribution 345\\nlarge number of individuals in both groups and would probably have to con-\\ntinue for many years to obtain signiﬁcant results. Consequently, H-U-P decided\\non a different type of observational study. They uncovered 8 women between\\nthe ages of 15 and 22 who had vaginal cancer. Each of these women (called\\ncases) was then matched with 4 others, called referents or controls. Each of the\\nreferents of a case was free of the cancer and was born within 5 days in the\\nsame hospital and in the same type of room (either private or public) as the\\ncase. Arguing that if DES had no effect on vaginal cancer then the probability,\\ncall it pc, that the mother of a case took DES would be the same as the proba-\\nbility, call it pr, that the mother of a referent took DES, the researchers H-U-P\\ndecided to test\\nH0 : pc = pr\\nagainst\\nH1 : pc ̸= pr\\nDiscovering that 7 of the 8 cases had mothers who took DES while preg-\\nnant, while none of the 32 referents had mothers who took the drug, the\\nresearchers (see Herbst, A., Ulfelder, H., and Poskanzer, D., “Adenocarcinoma\\nof the Vagina: Association of Maternal Stilbestrol Therapy with Tumor Appear-\\nance in Young Women,” New England Journal of Medicine, 284, 878–881, 1971)\\nconcluded that there was a strong association between DES and vaginal can-\\ncer. (The p-value for these data is approximately 0.)\\n■\\nWhen n1 and n2 are large, an approximate level α test of H0 : p1 = p2, based on\\nthe normal approximation to the binomial, is outlined in Problem 63.\\n8.7\\nTests concerning the mean of a Poisson distribution\\nLet X denote a Poisson random variable having mean λ and consider a test of\\nH0 : λ = λ0\\nversus\\nH1 : λ ̸= λ0\\nIf the observed value of X is X = x, then a level α test would reject H0 if either\\nPλ0{X ≥x} ≤α/2\\nor\\nPλ0{X ≤x} ≤α/2\\n(8.7.1)\\nwhere Pλ0 means that the probability is computed under the assumption that\\nthe Poisson mean is λ0. It follows from Equation (8.7.1) that the p-value is\\ngiven by\\np-value = 2min(Pλ0{X ≥x},Pλ0{X ≤x})\\nThe calculation of the preceding probabilities that a Poisson random variable\\nwith mean λ0 is greater (less) than or equal to x can be obtained by using\\nProgram 5.2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 357}, page_content='346 CHAPTER 8: Hypothesis testing\\nExample 8.7.a. Management’s claim that the mean number of defective com-\\nputer chips produced daily is not greater than 25 is in dispute. Test this hypoth-\\nesis, at the 5 percent level of signiﬁcance, if a sample of 5 days revealed 28, 34,\\n32, 38, and 22 defective chips.\\nSolution. Because each individual computer chip has a very small chance of\\nbeing defective, it is probably reasonable to suppose that the daily number of\\ndefective chips is approximately a Poisson random variable, with mean, say,\\nλ. To see whether or not the manufacturer’s claim is credible, we shall test the\\nhypothesis\\nH0 : λ ≤25\\nversus\\nH1 : λ > 25\\nNow, under H0, the total number of defective chips produced over a 5-day\\nperiod is Poisson distributed (since the sum of independent Poisson random\\nvariables is Poisson) with a mean no greater than 125. Since this number is\\nequal to 154, it follows that the p-value of the data is given by\\np-value = P125{X ≥154}\\n= 1 −P125{X ≤153}\\n= 1 −ppois(153,125)\\n[1] 0.006664794\\nTherefore, the manufacturer’s claim is rejected at the 5 percent (as it would be\\neven at the 1 percent) level of signiﬁcance.\\n■\\n8.7.1\\nTesting the relationship between two Poisson\\nparameters\\nLet X1 and X2 be independent Poisson random variables with respective means\\nλ1 and λ2, and consider a test of\\nH0 : λ2 = cλ1\\nversus\\nH1 : λ2 ̸= cλ1\\nfor a given constant c. Our test of this is a conditional test (similar in spirit\\nto the Fisher-Irwin test of Section 8.6.1), which is based on the fact that the\\nconditional distribution of X1 given the sum of X1 and X2 is binomial. More\\nspeciﬁcally, we have the following proposition.\\nProposition 8.7.1.\\nP{X1 = k|X1 + X2 = n} =\\n\\x08n\\nk\\n\\t\\n[λ1/(λ1 + λ2)]k[λ2/(λ1 + λ2)]n−k\\nProof.\\nP{X1 = k|X1 + X2 = n}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 358}, page_content='8.7 Tests concerning the mean of a Poisson distribution 347\\n= P{X1 = k,X1 + X2 = n}\\nP{X1 + X2 = n}\\n= P{X1 = k,X2 = n −k}\\nP{X1 + X2 = n}\\n= P{X1 = k}P{X2 = n −k}\\nP{X1 + X2 = n}\\nby independence\\n= exp{−λ1}λk\\n1/k!exp{−λ2}λn−k\\n2\\n/(n −k)!\\nexp{−(λ1 + λ2)}(λ1 + λ2)n/n!\\n=\\nn!\\n(n −k)!k![λ1/(λ1 + λ2)]k[λ2/(λ1 + λ2)]n−k\\nwhere the next to last equality follows because the sum of independent Poisson\\nrandom variables is also Poisson.\\n■\\nIt follows from Proposition 8.7.1 that, if H0 is true, then the conditional distri-\\nbution of X1 given that X1 + X2 = n is the binomial distribution with param-\\neters n and p = 1/(1 + c). From this we can conclude that if X1 + X2 = n, then\\nH0 should be rejected if the observed value of X1, call it x1, is such that either\\nP{Bin(n,1/(1 + c)) ≥x1} ≤α/2\\nor\\nP{Bin(n,1/(1 + c)) ≤x1} ≤α/2\\nExample 8.7.b. An industrial concern runs two large plants. If the number of\\naccidents during the past 8 weeks at plant 1 were 16, 18, 9, 22, 17, 19, 24, 8\\nwhile the number of accidents during the last 6 weeks at plant 2 were 22, 18,\\n26, 30, 25, 28, can we conclude, at the 5 percent level of signiﬁcance, that the\\nsafety conditions differ from plant to plant?\\nSolution. Since there is a small probability of an industrial accident in any\\ngiven minute, it would seem that the weekly number of such accidents should\\nhave approximately a Poisson distribution. If we let X1 denote the total number\\nof accidents during an 8-week period at plant 1, and let X2 be the number\\nduring a 6-week period at plant 2, then if the safety conditions did not differ at\\nthe two plants we would have that\\nλ2 = 3\\n4λ1\\nwhere λi ≡E[Xi], i = 1,2. Hence, as X1 = 133, X2 = 149 it follows that the\\np-value of the test of\\nH0 : λ2 = 3\\n4λ1\\nversus\\nH1 : λ2 ̸= 3\\n4λ1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 359}, page_content='348 CHAPTER 8: Hypothesis testing\\nis given by\\np-value = 2min\\n\\x18\\nP\\n\\x1a\\nBin\\n\\x18\\n282, 4\\n7\\n\\x19\\n≥133\\n\\x1b\\n,P\\n\\x1a\\nBin\\n\\x18\\n282, 4\\n7\\n\\x19\\n≤133\\n\\x1b\\x19\\n= 9.408 × 10−4\\nThus, the hypothesis that the safety conditions at the two plants are equivalent\\nis rejected.\\n■\\nProblems\\n1. Consider a trial in which a jury must decide between the hypothesis that\\nthe defendant is guilty and the hypothesis that he or she is innocent.\\na.\\nIn the framework of hypothesis testing and the U.S. legal system,\\nwhich of the hypotheses should be the null hypothesis?\\nb.\\nWhat do you think would be an appropriate signiﬁcance level in this\\nsituation?\\n2. A colony of laboratory mice consists of several thousand mice. The av-\\nerage weight of all the mice is 32 grams with a standard deviation of 4\\ngrams. A laboratory assistant was asked by a scientist to select 25 mice for\\nan experiment. However, before performing the experiment the scientist\\ndecided to weigh the mice as an indicator of whether the assistant’s se-\\nlection constituted a random sample or whether it was made with some\\nunconscious bias (perhaps the mice selected were the ones that were\\nslowest in avoiding the assistant, which might indicate some inferiority\\nabout this group). If the sample mean of the 25 mice was 30.4, would\\nthis be signiﬁcant evidence, at the 5 percent level of signiﬁcance, against\\nthe hypothesis that the selection constituted a random sample?\\n3. A population distribution is known to have standard deviation 20. De-\\ntermine the p-value of a test of the hypothesis that the population mean\\nis equal to 50, if the average of a sample of 64 observations is\\n(a) 52.5; (b) 55.0; (c) 57.5.\\n4. In a certain chemical process, it is very important that a particular solu-\\ntion that is to be used as a reactant have a pH of exactly 8.20. A method\\nfor determining pH that is available for solutions of this type is known\\nto give measurements that are normally distributed with a mean equal to\\nthe actual pH and with a standard deviation of .02. Suppose 10 indepen-\\ndent measurements yielded the following pH values:\\n8.18\\n8.17\\n8.16\\n8.15\\n8.17\\n8.21\\n8.22\\n8.16\\n8.19\\n8.18\\na.\\nWhat conclusion can be drawn at the α = .10 level of signiﬁcance?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 360}, page_content='Problems\\n349\\nb.\\nWhat about at the α = .05 level of signiﬁcance?\\n5. The mean breaking strength of a certain type of ﬁber is required to be\\nat least 200 psi. Past experience indicates that the standard deviation of\\nbreaking strength is 5 psi. If a sample of 8 pieces of ﬁber yielded breakage\\nat the following pressures,\\n210\\n198\\n195\\n202\\n197.4\\n196\\n199\\n195.5\\nwould you conclude, at the 5 percent level of signiﬁcance, that the ﬁber\\nis unacceptable? What about at the 10 percent level of signiﬁcance?\\n6. It is known that the average height of a man residing in the United States\\nis 5 feet 10 inches and the standard deviation is 3 inches. To test the\\nhypothesis that men in your city are “average,” a sample of 20 men have\\nbeen chosen. The heights of the men in the sample follow:\\nMan\\nHeight in\\nInches\\nMan\\n1\\n72\\n70.4\\n11\\n2\\n68.1\\n76\\n12\\n3\\n69.2\\n72.5\\n13\\n4\\n72.8\\n74\\n14\\n5\\n71.2\\n71.8\\n15\\n6\\n72.2\\n69.6\\n16\\n7\\n70.8\\n75.6\\n17\\n8\\n74\\n70.6\\n18\\n9\\n66\\n76.2\\n19\\n10\\n70.3\\n77\\n20\\nWhat do you conclude? Explain what assumptions you are making.\\n7. Suppose in Problem 4 that we wished to design a test so that if the pH\\nwere really equal to 8.20, then this conclusion will be reached with prob-\\nability equal to .95. On the other hand, if the pH differs from 8.20 by\\n.03 (in either direction), we want the probability of picking up such a\\ndifference to exceed .95.\\na.\\nWhat test procedure should be used?\\nb.\\nWhat is the required sample size?\\nc.\\nIf x = 8.31, what is your conclusion?\\nd.\\nIf the actual pH is 8.32, what is the probability of concluding that\\nthe pH is not 8.20, using the foregoing procedure?\\n8. Verify that the approximation in Equation (8.3.7) remains valid even\\nwhen μ1 < μ0.\\n9. A British pharmaceutical company, Glaxo Holdings, has recently devel-\\noped a new drug for migraine headaches. Among the claims Glaxo made'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 361}, page_content='350 CHAPTER 8: Hypothesis testing\\nfor its drug, called somatriptan, was that the mean time it takes for it to\\nenter the bloodstream is less than 10 minutes. To convince the Food and\\nDrug Administration of the validity of this claim, Glaxo conducted an\\nexperiment on a randomly chosen set of migraine sufferers. To prove its\\nclaim, what should they have taken as the null and what as the alternative\\nhypothesis?\\n10. The weights of salmon grown at a commercial hatchery are normally dis-\\ntributed with a standard deviation of 1.2 pounds. The hatchery claims\\nthat the mean weight of this year’s crop is at least 7.6 pounds. Suppose\\na random sample of 16 ﬁsh yielded an average weight of 7.2 pounds. Is\\nthis strong enough evidence to reject the hatchery’s claims at the\\na.\\n5 percent level of signiﬁcance;\\nb.\\n1 percent level of signiﬁcance?\\nc.\\nWhat is the p-value?\\n11. Consider a test of H0 : μ ≤100 versus H1 : μ > 100. Suppose that a sample\\nof size 20 has a sample mean of X = 105. Determine the p-value of this\\noutcome if the population standard deviation is known to equal\\n(a) 5; (b) 10; (c) 15.\\n12. An advertisement for a new toothpaste claims that it reduces cavities of\\nchildren in their cavity-prone years. Cavities per year for this age group\\nare normal with mean 3 and standard deviation 1. A study of 2500 chil-\\ndren who used this toothpaste found an average of 2.95 cavities per child.\\nAssume that the standard deviation of the number of cavities of a child\\nusing this new toothpaste remains equal to 1.\\na.\\nAre these data strong enough, at the 5 percent level of signiﬁcance,\\nto establish the claim of the toothpaste advertisement?\\nb.\\nDo the data convince you to switch to this new toothpaste?\\n13. There is some variability in the amount of phenobarbital in each capsule\\nsold by a manufacturer. However, the manufacturer claims that the mean\\nvalue is 20.0 mg. To test this, a sample of 25 pills yielded a sample mean\\nof 19.7 with a sample standard deviation of 1.3. What inference would\\nyou draw from these data? In particular, are the data strong enough evi-\\ndence to discredit the claim of the manufacturer? Use the 5 percent level\\nof signiﬁcance.\\n14. Twenty years ago, entering male high school students of Central High\\ncould do an average of 24 pushups in 60 seconds. To see whether this\\nremains true today, a random sample of 36 freshmen was chosen. If their\\naverage was 22.5 with a sample standard deviation of 3.1, can we con-\\nclude that the mean is no longer equal to 24? Use the 5 percent level of\\nsigniﬁcance.\\n15. The mean response time of a species of pigs to a stimulus is .8 second.\\nTwenty-eight pigs were given 2 oz of alcohol and then tested. If their\\naverage response time was 1.0 second with a standard deviation of .3\\nsecond, can we conclude that alcohol affects the mean response time?\\nUse the 5 percent level of signiﬁcance.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 362}, page_content='Problems\\n351\\n16. Suppose that team A and team B are to play a National Football League\\ngame and that team A is favored by f points. Let S(A) and S(B) denote\\nthe scores of teams A and B, and let X = S(A)−S(B)−f . That is, X is the\\namount by which team A beats the point spread. It has been claimed that\\nthe distribution of X is normal with mean 0 and standard deviation 14.\\nUse data from randomly chosen football games to test this hypothesis.\\n17. A medical scientist believes that the average basal temperature of (out-\\nwardly) healthy individuals has increased over time and is now greater\\nthan 98.6 degrees Fahrenheit (37 degrees Celsius). To prove this, she has\\nrandomly selected 100 healthy individuals. If their mean temperature is\\n98.74 with a sample standard deviation of 1.1 degrees, does this prove\\nher claim at the 5 percent level? What about at the 1 percent level?\\n18. Use the results of a Sunday’s worth of NFL professional football games to\\ntest the hypothesis that the average number of points scored by winning\\nteams is less than or equal to 28. Use the 5 percent level of signiﬁcance.\\n19. Use the results of a Sunday’s worth of major league baseball scores to test\\nthe hypothesis that the average number of runs scored by winning teams\\nis at least 5.6. Use the 5 percent level of signiﬁcance.\\n20. A car is advertised as having a gas mileage rating of at least 30 miles/gal-\\nlon in highway driving. If the miles per gallon obtained in 10 indepen-\\ndent experiments are 26, 24, 20, 25, 27, 25, 28, 30, 26, 33, should you\\nbelieve the advertisement? What assumptions are you making?\\n21. A producer speciﬁes that the mean lifetime of a certain type of battery is\\nat least 240 hours. A sample of 18 such batteries yielded the following\\ndata.\\n237\\n242\\n232\\n242\\n248\\n230\\n244\\n243\\n254\\n262\\n234\\n220\\n225\\n236\\n232\\n218\\n228\\n240\\nAssuming that the life of the batteries is approximately normally dis-\\ntributed, do the data indicate that the speciﬁcations are not being met?\\n22. Use the data of Example 2.3.i of Chapter 2 to test the null hypothesis\\nthat the average noise level directly outside of Grand Central Station is\\nless than or equal to 80 decibels.\\n23. An oil company claims that the sulfur content of its diesel fuel is at most\\n.15 percent. To check this claim, the sulfur contents of 40 randomly cho-\\nsen samples were determined; the resulting sample mean and sample\\nstandard deviation were .162 and .040. Using the 5 percent level of sig-\\nniﬁcance, can we conclude that the company’s claims are invalid?\\n24. A company supplies plastic sheets for industrial use. A new type of plastic\\nhas been produced and the company would like to claim that the average'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 363}, page_content='352 CHAPTER 8: Hypothesis testing\\nstress resistance of this new product is at least 30.0, where stress resistance\\nis measured in pounds per square inch (psi) necessary to crack the sheet.\\nThe following random sample was drawn off the production line. Based\\non this sample, would the claim clearly be unjustiﬁed?\\n30.1\\n32.7\\n22.5\\n27.5\\n27.7\\n29.8\\n28.9\\n31.4\\n31.2\\n24.3\\n26.4\\n22.8\\n29.1\\n33.4\\n32.5\\n21.7\\nAssume normality and use the 5 percent level of signiﬁcance.\\n25. It is claimed that a certain type of bipolar transistor has a mean value of\\ncurrent gain that is at least 210. A sample of these transistors is tested.\\nIf the sample mean value of current gain is 200 with a sample standard\\ndeviation of 35, would the claim be rejected at the 5 percent level of\\nsigniﬁcance if\\na.\\nthe sample size is 25;\\nb.\\nthe sample size is 64?\\n26. A manufacturer of capacitors claims that the breakdown voltage of these\\ncapacitors has a mean value of at least 100 V. A test of 12 of these capaci-\\ntors yielded the following breakdown voltages:\\n96,98,105,92,111,114,99,103,95,101,106,97\\nDo these results prove the manufacturer’s claim? Do they disprove them?\\n27. A sample of 10 ﬁsh were caught at lake A and their PCB concentrations\\nwere measured using a certain technique. The resulting data in parts per\\nmillion were\\nLake A: 11.5,10.8,11.6,9.4,12.4,11.4,12.2,11,10.6,10.8\\nIn addition, a sample of 8 ﬁsh were caught at lake B and their levels of\\nPCB were measured by a different technique than that used at lake A. The\\nresultant data were\\nLake B: 11.8,12.6,12.2,12.5,11.7,12.1,10.4,12.6\\nIf it is known that the measuring technique used at lake A has a variance\\nof .09 whereas the one used at lake B has a variance of .16, could you\\nreject (at the 5 percent level of signiﬁcance) a claim that the two lakes are\\nequally contaminated?\\n28. A method for measuring the pH level of a solution yields a measurement\\nvalue that is normally distributed with a mean equal to the actual pH\\nof the solution and with a standard deviation equal to .05. An environ-\\nmental pollution scientist claims that two different solutions come from\\nthe same source. If this were so, then the pH level of the solutions would'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 364}, page_content='Problems 353\\nbe equal. To test the plausibility of this claim, 10 independent measure-\\nments were made of the pH level for both solutions, with the following\\ndata resulting.\\nMeasurements of\\nSolution A\\nMeasurements of\\nSolution B\\n6.24\\n6.27\\n6.31\\n6.25\\n6.28\\n6.33\\n6.30\\n6.27\\n6.25\\n6.24\\n6.26\\n6.31\\n6.24\\n6.28\\n6.29\\n6.29\\n6.22\\n6.34\\n6.28\\n6.27\\na.\\nDo the data disprove the scientist’s claim? Use the 5 percent level of\\nsigniﬁcance.\\nb.\\nWhat is the p-value?\\n29. The following are the values of independent samples from two different\\npopulations.\\nSample 1\\n122,114,130,165,144,133,139,142,150\\nSample 2\\n108,125,122,140,132,120,137,128,138\\nLet μ1 and μ2 be the respective means of the two populations. Find the\\np-value of the test of the null hypothesis\\nH0 : μ1 ≤μ2\\nversus the alternative\\nH1 : μ1 > μ2\\nwhen the population standard deviations are σ1 = 10 and\\n(a) σ2 = 5; (b) σ2 = 10; (c) σ2 = 20.\\n30. The data below give the lifetimes in hundreds of hours of samples of\\ntwo types of electronic tubes. Past lifetime data of such tubes have shown\\nthat they can often be modeled as arising from a lognormal distribution.\\nThat is, the logarithms of the data are normally distributed. Assuming\\nthat variance of the logarithms is equal for the two populations, test, at\\nthe 5 percent level of signiﬁcance, the hypothesis that the two population\\ndistributions are identical.\\nType 1\\n32,84,37,42,78,62,59,74\\nType 2\\n39,111,55,106,90,87,85'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 365}, page_content='354 CHAPTER 8: Hypothesis testing\\n31. The viscosity of two different brands of car oil is measured and the fol-\\nlowing data resulted:\\nBrand 1\\n10.62,10.58,10.33,10.72,10.44,10.74\\nBrand 2\\n10.50,10.52,10.58,10.62,10.55,10.51,10.53\\nTest the hypothesis that the mean viscosity of the two brands is equal,\\nassuming that the populations have normal distributions with equal vari-\\nances.\\n32. It is argued that the resistance of wire A is greater than the resistance of\\nwire B. You make tests on each wire with the following results.\\nWire A\\nWire B\\n.140 ohm\\n.135 ohm\\n.138\\n.140\\n.143\\n.136\\n.142\\n.142\\n.144\\n.138\\n.137\\n.140\\nWhat conclusion can you draw at the 10 percent signiﬁcance level? Ex-\\nplain what assumptions you are making.\\nIn Problems 33 through 40, assume that the population distributions are\\nnormal and have equal variances.\\n33. Twenty-ﬁve men between the ages of 25 and 30, who were participating\\nin a well-known heart study carried out in Framingham, Massachusetts,\\nwere randomly selected. Of these, 11 were smokers and 14 were not. The\\nfollowing data refer to readings of their systolic blood pressure.\\nSmokers\\nNonsmokers\\n124\\n130\\n134\\n122\\n136\\n128\\n125\\n129\\n133\\n118\\n127\\n122\\n135\\n116\\n131\\n127\\n133\\n135\\n125\\n120\\n118\\n122\\n120\\n115\\n123'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 366}, page_content='Problems 355\\nUse these data to test the hypothesis that the mean blood pressures of\\nsmokers and nonsmokers are the same.\\n34. In a 1943 experiment (Whitlock and Bliss, “A Bioassay Technique for\\nAnti-helminthics,” Journal of Parasitology, 29, pp. 48–58) 10 albino rats\\nwere used to study the effectiveness of carbon tetrachloride as a treat-\\nment for worms. Each rat received an injection of worm larvae. After 8\\ndays, the rats were randomly divided into two groups of 5 each; each\\nrat in the ﬁrst group received a dose of .032 cc of carbon tetrachloride,\\nwhereas the dosage for each rat in the second group was .063 cc. Two\\ndays later the rats were killed, and the number of adult worms in each rat\\nwas determined. The numbers detected in the group receiving the .032\\ndosage were\\n421,462,400,378,413\\nwhereas they were\\n207,17,412,74,116\\nfor those receiving the .063 dosage. Do the data prove that the larger\\ndosage is more effective than the smaller?\\n35. A professor claims that the average starting salary of industrial engineer-\\ning graduating seniors is greater than that of civil engineering graduates.\\nTo study this claim, samples of 16 industrial engineers and 16 civil engi-\\nneers, all of whom graduated in 2006, were chosen and sample members\\nwere queried about their starting salaries. If the industrial engineers had\\na sample mean salary of $72,700 and a sample standard deviation of\\n$2400, and the civil engineers had a sample mean salary of $71,400 and\\na sample standard deviation of $2200, has the professor’s claim been\\nveriﬁed? Find the appropriate p-value.\\n36. In a certain experimental laboratory, a method A for producing gasoline\\nfrom crude oil is being investigated. Before completing experimentation,\\na new method B is proposed. All other things being equal, it was decided\\nto abandon A in favor of B only if the average yield of the latter was\\nclearly greater. The yield of both processes is assumed to be normally\\ndistributed. However, there has been insufﬁcient time to ascertain their\\ntrue standard deviations, although there appears to be no reason why\\nthey cannot be assumed equal. Cost considerations impose size limits\\non the size of samples that can be obtained. If a 1 percent signiﬁcance\\nlevel is all that is allowed, what would be your recommendation based\\non the following random samples? The numbers represent percent yield\\nof crude oil.\\nA\\n23.2,26.6,24.4,23.5,22.6,25.7,25.5\\nB\\n25.7,27.7,26.2,27.9,25.0,21.4,26.1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 367}, page_content='356 CHAPTER 8: Hypothesis testing\\n37. A study was instituted to learn how the diets of women changed during\\nthe winter and the summer. A random group of 12 women were observed\\nduring the month of July and the percentage of each woman’s calories\\nthat came from fat was determined. Similar observations were made on a\\ndifferent randomly selected group of size 12 during the month of January.\\nThe results were as follows:\\nJuly\\n32.2,27.4,28.6,32.4,40.5,26.2,29.4,25.8,36.6,30.3,28.5,32.0\\nJanuary\\n30.5,28.4,40.2,37.6,36.5,38.8,34.7,29.5,29.7,37.2,41.5,37.0\\nTest the hypothesis that the mean fat percentage intake is the same for\\nboth months. Use the (a) 5 percent level of signiﬁcance and (b) 1 percent\\nlevel of signiﬁcance.\\n38. To learn about the feeding habits of bats, 22 bats were tagged and tracked\\nby radio. Of these 22 bats, 12 were female and 10 were male. The dis-\\ntances ﬂown (in meters) between feedings were noted for each of the 22\\nbats, and the following summary statistics were obtained.\\nFemale Bats\\nMale Bats\\nn = 12\\nm = 10\\nX = 180\\nY = 136\\nSx = 92\\nSy = 86\\nTest the hypothesis that the mean distance ﬂown between feedings is the\\nsame for the populations of both male and of female bats. Use the 5\\npercent level of signiﬁcance.\\n39. The following data summary was obtained from a comparison of the lead\\ncontent of human hair removed from adult individuals that had died\\nbetween 1880 and 1920 with the lead content of present-day adults. The\\ndata are in units of micrograms, equal to one-millionth of a gram.\\n1880–1920\\nToday\\nSample size:\\n30\\n100\\nSample mean:\\n48.5\\n26.6\\nSample standard deviation:\\n14.5\\n12.3\\na.\\nDo the above data establish, at the 1 percent level of signiﬁcance,\\nthat the mean lead content of human hair is less today than it was\\nin the years between 1880 and 1920? Clearly state what the null and\\nalternative hypotheses are.\\nb.\\nWhat is the p-value for the hypothesis test in part (a)?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 368}, page_content='Problems 357\\n40. Sample weights (in pounds) of newborn babies born in two adjacent\\ncounties in western Pennsylvania yielded the following data.\\nn\\n=\\n53,\\nm\\n=\\n44\\nX\\n=\\n6.8,\\nY\\n=\\n7.2\\nS2\\n=\\n5.2,\\nS2\\n=\\n4.9\\nConsider a test of the hypothesis that the mean weight of newborns is\\nthe same in both counties. What is the resulting p-value?\\n41. To verify the hypothesis that blood lead levels tend to be higher for chil-\\ndren whose parents work in a factory that uses lead in the manufacturing\\nprocess, researchers examined lead levels in the blood of 33 children\\nwhose parents worked in a battery manufacturing factory (Morton, D.,\\nSaah, A., Silberg, S., Owens, W., Roberts, M., and Saah, M., “Lead Ab-\\nsorption in Children of Employees in a Lead-Related Industry,” American\\nJournal of Epidemiology, 115, 549–555, 1982). Each of these children was\\nthen matched by another child who was of similar age, lived in a similar\\nneighborhood, had a similar exposure to trafﬁc, but whose parent did\\nnot work with lead. The blood levels of the 33 cases (sample 1) as well as\\nthose of the 33 controls (sample 2) were then used to test the hypothesis\\nthat the average blood levels of these groups are the same. If the resulting\\nsample means and sample standard deviations were\\n¯x1 = .015,\\ns1 = .004,\\n¯x2 = .006,\\ns2 = .006\\nﬁnd the resulting p-value. Assume a common variance.\\n42. Ten pregnant women were given an injection of pitocin to induce labor.\\nTheir systolic blood pressures immediately before and after the injection\\nwere:\\nPatient\\nBefore\\nAfter\\nPatient\\nBefore\\nAfter\\n1\\n134\\n140\\n6\\n140\\n138\\n2\\n122\\n130\\n7\\n118\\n124\\n3\\n132\\n135\\n8\\n127\\n126\\n4\\n130\\n126\\n9\\n125\\n132\\n5\\n128\\n134\\n10\\n142\\n144\\nDo the data indicate that injection of this drug changes blood pressure?\\n43. A question of medical importance is whether jogging leads to a reduc-\\ntion in one’s pulse rate. To test this hypothesis, 8 nonjogging volunteers\\nagreed to begin a 1-month jogging program. After the month their pulse\\nrates were determined and compared with their earlier values. If the data\\nare as follows, can we conclude that jogging has had an effect on the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 369}, page_content='358 CHAPTER 8: Hypothesis testing\\npulse rates?\\nSubject\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nPulse Rate Before\\n74\\n86\\n98\\n102\\n78\\n84\\n79\\n70\\nPulse Rate After\\n70\\n85\\n90\\n110\\n71\\n80\\n69\\n74\\n44. If X1,...,Xn is a sample from a normal population having unknown\\nparameters μ and σ 2, devise a signiﬁcance level α test of\\nH0 = σ 2 ≤σ 2\\n0\\nversus the alternative\\nH1 = σ 2 > σ 2\\n0\\nfor a given positive value σ 2\\n0 .\\n45. In Problem 44, explain how the test would be modiﬁed if the population\\nmean μ were known in advance.\\n46. A gun-like apparatus has recently been designed to replace needles in ad-\\nministering vaccines. The apparatus can be set to inject different amounts\\nof the serum, but because of random ﬂuctuations the actual amount in-\\njected is normally distributed with a mean equal to the setting and with\\nan unknown variance σ 2. It has been decided that the apparatus would\\nbe too dangerous to use if σ exceeds .10. If a random sample of 50 in-\\njections resulted in a sample standard deviation of .08, should use of\\nthe new apparatus be discontinued? Suppose the level of signiﬁcance is\\nα = .10. Comment on the appropriate choice of a signiﬁcance level for\\nthis problem, as well as the appropriate choice of the null hypothesis.\\n47. A pharmaceutical house produces a certain drug item whose weight has\\na standard deviation of .5 milligram. The company’s research team has\\nproposed a new method of producing the drug. However, this entails\\nsome costs and will be adopted only if there is strong evidence that\\nthe standard deviation of the weight of the items will drop to below .4\\nmilligrams. If a sample of 10 items is produced and has the following\\nweights, should the new method be adopted?\\n5.728\\n5.731\\n5.722\\n5.719\\n5.727\\n5.724\\n5.718\\n5.726\\n5.723\\n5.722\\n48. The production of large electrical transformers and capacitators requires\\nthe use of polychlorinated biphenyls (PCBs), which are extremely haz-\\nardous when released into the environment. Two methods have been'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 370}, page_content='Problems 359\\nsuggested to monitor the levels of PCB in ﬁsh near a large plant. It is\\nbelieved that each method will result in a normal random variable that\\ndepends on the method. Test the hypothesis at the α = .10 level of sig-\\nniﬁcance that both methods have the same variance, if a given ﬁsh is\\nchecked 8 times by each method with the following data (in parts per\\nmillion) recorded.\\nMethod 1\\n6.2,5.8,5.7,6.3,5.9,6.1,6.2,5.7\\nMethod 2\\n6.3,5.7,5.9,6.4,5.8,6.2,6.3,5.5\\n49. In Problem 31, test the hypothesis that the populations have the same\\nvariances.\\n50. If X1, ...,Xn is a sample from a normal population with variance σ 2\\nx ,\\nand Y1, ...,Yn is an independent sample from normal population with\\nvariance σ 2\\ny , develop a signiﬁcance level α test of\\nH0 : σ 2\\nx < σ 2\\ny\\nversus\\nH1 : σ 2\\nx > σ 2\\ny\\nWax in Pounds per Unit Area of Sample\\nOutside Surface\\nInside Surface\\nx = .948\\ny = .652\\n\\x02x2\\ni = 91\\n\\x02y2\\ni = 82\\n51. The amount of surface wax on each side of waxed paper bags is believed\\nto be normally distributed. However, there is reason to believe that there\\nis greater variation in the amount on the inner side of the paper than on\\nthe outside. A sample of 75 observations of the amount of wax on each\\nside of these bags is obtained and the following data recorded.\\nConduct a test to determine whether or not the variability of the amount\\nof wax on the inner surface is greater than the variability of the amount\\non the outer surface (α = .05).\\n52. In a famous experiment to determine the efﬁcacy of aspirin in preventing\\nheart attacks, 22,000 healthy middle-aged men were randomly divided\\ninto two equal groups, one of which was given a daily dose of aspirin\\nand the other a placebo that looked and tasted identical to the aspirin.\\nThe experiment was halted at a time when 104 men in the aspirin group\\nand 189 in the control group had had heart attacks. Use these data to test\\nthe hypothesis that the taking of aspirin does not change the probability\\nof having a heart attack.\\n53. In the study of Problem 52, it also resulted that 119 from the aspirin\\ngroup and 98 from the control group suffered strokes. Are these numbers\\nsigniﬁcant to show that taking aspirin changes the probability of having\\na stroke?\\n54. A standard drug is known to be effective in 72 percent of the cases in\\nwhich it is used to treat a certain infection. A new drug has been devel-\\noped and testing has found it to be effective in 42 cases out of 50. Is this'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 371}, page_content='360 CHAPTER 8: Hypothesis testing\\nstrong enough evidence to prove that the new drug is more effective than\\nthe old one? Find the relevant p-value.\\n55. Three independent news services are running a poll to determine if over\\nhalf the population supports an initiative concerning limitations on driv-\\ning automobiles in the downtown area. Each wants to see if the evidence\\nindicates that over half the population is in favor. As a result, all three\\nservices will be testing\\nH0 : p ≤.5\\nversus\\nH1 : p > .5\\nwhere p is the proportion of the population in favor of the initiative.\\na.\\nSuppose the ﬁrst news organization samples 100 people, of which\\n56 are in favor of the initiative. Is this strong enough evidence, at\\nthe 5 percent level of signiﬁcance, to reject the null hypothesis and\\nso establish that over half the population favors the initiative?\\nb.\\nSuppose the second news organization samples 120 people, of\\nwhich 68 are in favor of the initiative. Is this strong enough evidence,\\nat the 5 percent level of signiﬁcance, to reject the null hypothesis?\\nc.\\nSuppose the third news organization samples 110 people, of which\\n62 are in favor of the initiative. Is this strong enough evidence, at the\\n5 percent level of signiﬁcance, to reject the null hypothesis?\\nd.\\nSuppose the news organizations combine their samples, to come up\\nwith a sample of 330 people, of which 186 support the initiative. Is\\nthis strong enough evidence, at the 5 percent level of signiﬁcance, to\\nreject the null hypothesis?\\n56. It has been a long held belief that the proportion of California births of\\nAfrican American mothers that result in twins is about 1.32 percent. (The\\ntwinning rate appears to be inﬂuenced by the ethnicity of the mother;\\nclaims are that it is 1.05 for Caucasian Americans, and 0.72 percent for\\nAsian Americans.) A public health scientist believes that this number is\\nno longer correct and has decided to test the null hypothesis that the\\nproportion is 1.32 percent by gathering data on the next 1000 recorded\\nbirthing events, where twin births are regarded as a single birthing event,\\nin California.\\na.\\nWhat is the minimal number of twin births that would have to be\\nobserved in order to reject the null hypothesis at the 5 percent level\\nof signiﬁcance?\\nb.\\nWhat is the probability the null hypothesis will be rejected if the\\nactual twinning rate is 1.80?\\n57. An ambulance service claims that at least 45 percent of its calls involve\\nlife-threatening emergencies. To check this claim, a random sample of\\n200 calls was selected from the service’s ﬁles. If 70 of these calls involved\\nlife-threatening emergencies, is the service’s claim believable at the\\na.\\n5 percent level of signiﬁcance;\\nb.\\n1 percent level of signiﬁcance?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 372}, page_content='Problems 361\\n58. A standard drug is known to be effective in 75 percent of the cases in\\nwhich it is used to treat a certain infection. A new drug has been devel-\\noped and has been found to be effective in 42 cases out of 50. Based on\\nthis, would you accept, at the 5 percent level of signiﬁcance, the hypoth-\\nesis that the two drugs are of equal effectiveness? What is the p-value?\\n59. Do Problem 58 by using a test based on the normal approximation to\\nthe binomial.\\n60. In a study of the effect of two chemotherapy treatments on the survival of\\npatients with multiple myeloma, each of 156 patients was equally likely\\nto be given either one of the two treatments. As reported by Lipsitz, Dear,\\nLaird, and Molenberghs in a 1998 paper in Biometrics, the result of this\\nwas that 39 of the 72 patients given the ﬁrst treatment and 44 of the 84\\npatients given the second treatment survived for over 5 years.\\na.\\nUse these data to test the null hypothesis that the two treatments are\\nequally effective.\\nb.\\nIs the fact that 72 of the patients received one of the treatments while\\n84 received the other consistent with the claim that the determi-\\nnation of the treatment to be given to each patient was made in a\\ntotally random fashion?\\n61. Let X1 denote a binomial random variable with parameters (n1,p1) and\\nX2 an independent binomial random variable with parameters (n2,p2).\\nDevelop a test, using the same approach as in the Fisher-Irwin test, of\\nH0 : p1 ≤p2\\nversus the alternative\\nH1 : p1 > p2\\n62. Let X be a hypergeometric random variable with probability mass func-\\ntion\\nP{X = i} =\\n\\x18n1\\ni\\n\\x19\\x18 n2\\nk−i\\n\\x19\\n\\x18n1+n2\\nk\\n\\x19\\nVerify that\\nP{X = i + 1}\\nP{X = i}\\n=\\n(n1 −i)(k −i)\\n(i + 1)(n2 −k + i + 1)\\n63. Let X1 and X2 be binomial random variables with respective parameters\\nn1, p1 and n2, p2. Show that when n1 and n2 are large, an approximate\\nlevel α test of H0 : p1 = p2 versus H1 : p1 ̸= p2 is as follows:\\nreject\\nH0\\nif\\n|X1/n1 −X2/n2|\\n\\x0c\\nX1 + X2\\nn1 + n2\\n\\x08\\n1 −X1 + X2\\nn1 + n2\\n\\t\\x08 1\\nn1\\n+ 1\\nn2\\n\\t > zα/2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 373}, page_content='362 CHAPTER 8: Hypothesis testing\\nHint: a. Argue ﬁrst that when n1 and n2 are large\\nX1\\nn1\\n−X2\\nn2\\n−(p1 −p2)\\n\\x16p1(1 −p1)\\nn1\\n+ p2(1 −p2)\\nn2\\n˙∼N(0,1)\\nwhere ˙∼means “approximately has the distribution.”\\nb.\\nNow argue that when H0 is true and so p1 = p2, their common value\\ncan be best estimated by (X1 + X2)/(n1 + n2).\\n64. Use the approximate test given in Problem 63 on the data of Problem 60.\\n65. Patients suffering from cancer must often decide whether to have their\\ntumors treated with surgery or with radiation. A factor in their decision\\nis the 5-year survival rates for these treatments. Surprisingly, it has been\\nfound that patients’ decisions often seem to be affected by whether they\\nare told the 5-year survival rates or the 5-year death rates (even though\\nthe information content is identical). For instance, in an experiment a\\ngroup of 200 male prostate cancer patients were randomly divided into\\ntwo groups of size 100 each. Each member of the ﬁrst group was told\\nthat the 5-year survival rate for those electing surgery was 77 percent,\\nwhereas each member of the second group was told that the 5-year death\\nrate for those electing surgery was 23 percent. Both groups were given the\\nsame information about radiation therapy. If it resulted that 24 members\\nof the ﬁrst group and 12 of the second group elected to have surgery,\\nwhat conclusions would you draw?\\n66. The following data refer to Larry Bird’s results when shooting a pair of\\nfree throws in basketball. During two consecutive seasons in the National\\nBasketball Association, Bird shot a pair of free throws on 338 occasions.\\nOn 251 occasions he made both shots; on 34 occasions he made the ﬁrst\\nshot but missed the second one; on 48 occasions he missed the ﬁrst shot\\nbut made the second one; on 5 occasions he missed both shots.\\na.\\nUse these data to test the hypothesis that Bird’s probability of mak-\\ning the ﬁrst shot is equal to his probability of making the second\\nshot.\\nb.\\nUse these data to test the hypothesis that Bird’s probability of mak-\\ning the second shot is the same regardless of whether he made or\\nmissed the ﬁrst one.\\n67. In the 1970s, the U.S. Veterans Administration (Murphy, 1977) con-\\nducted an experiment comparing coronary artery bypass surgery with\\nmedical drug therapy as treatments for coronary artery disease. The ex-\\nperiment involved 596 patients, of whom 286 were randomly assigned\\nto receive surgery, with the remaining 310 assigned to drug therapy. A to-\\ntal of 252 of those receiving surgery, and a total of 270 of those receiving\\ndrug therapy were still alive 3 years after treatment. Use these data to test\\nthe hypothesis that the survival probabilities are equal.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 374}, page_content='Problems 363\\n68. Test the hypothesis, at the 5 percent level of signiﬁcance, that the yearly\\nnumber of earthquakes felt on a certain island has mean 52 if the read-\\nings for the past 8 years are 46, 62, 60, 58, 47, 50, 59, 49. Assume an\\nunderlying Poisson distribution and give an explanation to justify this\\nassumption.\\n69. In 1995, the Fermi Laboratory announced the discovery of the top quark,\\nthe last of six quarks predicted by the “standard model of physics.” The\\nevidence for its existence was statistical in nature and involved signals\\ncreated when antiprotons and protons were forced to collide. In a Phys-\\nical Review Letters paper documenting the evidence, Abe, Akimoto, and\\nAkopian (known in physics circle as the three A’s) based their conclusion\\non a theoretical analysis that indicated that the number of decay events\\nin a certain time interval would have a Poisson distribution with a mean\\nequal to 6.7 if a top quark did not exist and with a larger mean if it did\\nexist. In a careful analysis of the data the three A’s showed that the actual\\ncount was 27. Is this strong enough evidence to prove the hypothesis that\\nthe mean of the Poisson distribution was greater than 6.7?\\n70. For the following data, sample 1 is from a Poisson distribution with\\nmean λ1 and sample 2 is from a Poisson distribution with mean λ2. Test\\nthe hypothesis that λ1 = λ2.\\nSample 1\\n24,32,29,33,40,28,34,36\\nSample 2\\n42,36,41\\n71. A scientist looking into the effect of smoking on heart disease has cho-\\nsen a large random sample of smokers and of nonsmokers. She plans\\nto study these two groups for 5 years to see if the number of heart at-\\ntacks among the members of the smokers’ group is signiﬁcantly greater\\nthan the number among the nonsmokers. Such a result, the scientist feels,\\nshould be strong evidence of an association between smoking and heart\\nattacks. Given that\\na.\\nolder people are at greater risk of heart disease than are younger\\npeople; and\\nb.\\nas a group, smokers tend to be somewhat older than nonsmokers,\\nwould the scientist be justiﬁed in her conclusion? Explain how the experi-\\nmental design can be improved so that meaningful conclusions can be\\ndrawn.\\n72. A researcher wants to analyze the average yearly increase in a stock over a\\n20-year period. To do so, she plans to randomly choose 100 stocks from\\nthe listing of current stocks, discarding any that were not in existence 20\\nyears ago. She will then compare the current price of each stock with its\\nprice 20 years ago to determine its percentage increase. Do you think this\\nis a valid method to study the average increase in the price of a stock?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 375}, page_content='CHAPTER 9\\nRegression\\n9.1\\nIntroduction\\nMany engineering and scientiﬁc problems are concerned with determining a\\nrelationship between a set of variables. For instance, in a chemical process, we\\nmight be interested in the relationship between the output of the process, the\\ntemperature at which it occurs, and the amount of catalyst employed. Knowl-\\nedge of such a relationship would enable us to predict the output for various\\nvalues of temperature and amount of catalyst.\\nIn many situations, there is a single response variable Y, also called the dependent\\nvariable, which depends on the value of a set of input, also called independent,\\nvariables x1,...,xr. The simplest type of relationship between the dependent\\nvariable Y and the input variables x1,...,xr is a linear relationship. That is, for\\nsome constants β0,β1,...,βr the equation\\nY = β0 + β1x1 + ··· + βrxr\\n(9.1.1)\\nwould hold. If this was the relationship between Y and the xi,i = 1,...,r, then\\nit would be possible (once the βi were learned) to exactly predict the response\\nfor any set of input values. However, in practice, such precision is almost never\\nattainable, and the most that one can expect is that Equation (9.1.1) would be\\nvalid subject to random error. By this we mean that the explicit relationship is\\nY = β0 + β1x1 + ··· + βrxr + e\\n(9.1.2)\\nwhere e, representing the random error, is assumed to be a random variable\\nhaving mean 0. Indeed, another way of expressing Equation (9.1.2) is as fol-\\nlows:\\nE[Y|x] = β0 + β1x1 + ··· + βrxr\\nwhere x = (x1,...,xr) is the set of independent variables, and E[Y|x] is the\\nexpected response given the inputs x.\\nEquation (9.1.2) is called a linear regression equation. We say that it describes the\\nregression of Y on the set of independent variables x1,...,xr. The quantities\\nβ0,β1,...,βr are called the regression coefﬁcients, and must usually be estimated\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00018-1\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n365'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 376}, page_content='366 CHAPTER 9: Regression\\nfrom a set of data. A regression equation containing a single independent vari-\\nable — that is, one in which r = 1 — is called a simple regression equation,\\nwhereas one containing many independent variables is called a multiple regres-\\nsion equation.\\nThus, a simple linear regression model supposes a linear relationship between\\nthe mean response and the value of a single independent variable. It can be\\nexpressed as\\nY = α + βx + e\\nwhere x is the value of the independent variable, also called the input level,\\nY is the response, and e, representing the random error, is a random variable\\nhaving mean 0.\\nExample 9.1.a. Consider the following 10 data pairs (xi,yi),i = 1,...,10, re-\\nlating y, the percent yield of a laboratory experiment, to x, the temperature at\\nwhich the experiment was run.\\ni\\nxi\\nyi\\ni\\nxi\\nyi\\n1\\n100\\n45\\n6\\n150\\n68\\n2\\n110\\n52\\n7\\n160\\n75\\n3\\n120\\n54\\n8\\n170\\n76\\n4\\n130\\n63\\n9\\n180\\n92\\n5\\n140\\n62\\n10\\n190\\n88\\nA plot of yi versus xi — called a scatter diagram — is given in Figure 9.1. As\\nthis scatter diagram appears to reﬂect, subject to random error, a linear rela-\\nFIGURE 9.1\\nScatter plot.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 377}, page_content='9.2 Least squares estimators of the regression parameters\\n367\\ntion between y and x, it seems that a simple linear regression model would be\\nappropriate.\\n■\\n9.2\\nLeast squares estimators of the\\nregression parameters\\nSuppose that the responses Yi corresponding to the input values xi,i = 1,...,n\\nare to be observed and used to estimate α and β in a simple linear regression\\nmodel. To determine estimators of α and β we reason as follows: If A is the\\nestimator of α and B of β, then the estimator of the response corresponding\\nto the input variable xi would be A + Bxi. Since the actual response is Yi, the\\nsquared difference is (Yi −A −Bxi)2, and so if A and B are the estimators\\nof α and β, then the sum of the squared differences between the estimated\\nresponses and the actual response values — call it SS — is given by\\nSS =\\nn\\n\\x02\\ni=1\\n(Yi −A −Bxi)2\\nThe method of least squares chooses as estimators of α and β the values of A\\nand B that minimize SS. To determine these estimators, we differentiate SS ﬁrst\\nwith respect to A and then to B as follows:\\n∂SS\\n∂A = −2\\nn\\n\\x02\\ni=1\\n(Yi −A −Bxi)\\n∂SS\\n∂B = −2\\nn\\n\\x02\\ni=1\\nxi(Yi −A −Bxi)\\nSetting these partial derivatives equal to zero yields the following equations for\\nthe minimizing values A and B:\\nn\\n\\x02\\ni=1\\nYi = nA + B\\nn\\n\\x02\\ni=1\\nxi\\n(9.2.1)\\nn\\n\\x02\\ni=1\\nxiYi = A\\nn\\n\\x02\\ni=1\\nxi + B\\nn\\n\\x02\\ni=1\\nx2\\ni\\nThe Equations (9.2.1) are known as the normal equations. If we let\\nY =\\n\\x02\\ni\\nYi/n,\\nx =\\n\\x02\\ni\\nxi/n\\nthen we can write the ﬁrst normal equation as\\nA = Y −B x\\n(9.2.2)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 378}, page_content='368 CHAPTER 9: Regression\\nSubstituting this value of A into the second normal equation yields\\n\\x02\\ni\\nxiYi = (Y −B x)nx + B\\n\\x02\\ni\\nx2\\ni\\nor\\nB\\n\\x03\\x02\\ni\\nx2\\ni −nx2\\n\\x04\\n=\\n\\x02\\ni\\nxiYi −nxY\\nor\\nB =\\n\\x02\\ni\\nxiYi −nxY\\n\\x02\\ni\\nx2\\ni −nx2\\nHence, using Equation (9.2.2) and the fact that nY = \\x05n\\ni=1 Yi, we have proven\\nthe following proposition.\\nProposition 9.2.1. The least squares estimators of β and α corresponding to\\nthe data set xi,Yi,i = 1,...,n are, respectively,\\nB =\\nn\\x05\\ni=1\\nxiYi −x\\nn\\x05\\ni=1\\nYi\\nn\\x05\\ni=1\\nx2\\ni −nx2\\nA = Y −B x\\nThe straight line A + Bx is called the estimated regression line.\\nR can be used to obtain the estimated regression line for data pairs (x1,y1),...,\\n(xn,yn). Type\\n>x = c(x1,...,xn)\\n>y = c(y1,...,yn)\\n>ﬁt = lm(y ∼x)\\n>ﬁt\\nThe third line deﬁnes the model as lm(y ∼x), which is the linear model that\\nassumes that y is a linear function of x plus a random error, and that we are\\nnaming the model “ﬁt”. The next line asks R to give the estimated regression\\nline for the model “ﬁt”. Here is an example:\\n>x = c(1,2,3,4,5,6,7)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 379}, page_content='9.2 Least squares estimators of the regression parameters\\n369\\n>y = c(3,2,5,6,4,8,9)\\n>ﬁt = lm(y ∼x)\\n>ﬁt\\nAfter hitting return after the last line, one obtains the following output:\\nCall:\\nlm(formula = y ∼x)\\nCoefﬁcients:\\n(Intercept)\\nx\\n1.143\\n1.036\\nThe preceding says that the estimated regression line is y = 1.143 + 1.036x.\\nTo obtain a plot of the estimated regression line, ﬁrst type > plot(x,y) and\\nhit return. This will give the scatter diagram. Then plot > abline(ﬁt) and hit\\nreturn. The result is the estimated regression line added to the scatter diagram.\\nExample 9.2.a. The raw material used in the production of a certain synthetic\\nﬁber is stored in a location without a humidity control. Measurements of the\\nrelative humidity in the storage location and the moisture content of a sam-\\nple of the raw material were taken over 15 days with the following data (in\\npercentages) resulting.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 380}, page_content='370 CHAPTER 9: Regression\\nRelative\\nhumidity\\n46\\n53 29\\n61\\n36\\n39 47 49 52\\n38 55 32 57\\n54 44\\nMoisture\\ncontent\\n12\\n15\\n7\\n17\\n10\\n11\\n11\\n12 14\\n9\\n16\\n8\\n18\\n14 12\\nCalling the model “moisture” and using R yields\\n>x = c(46,53,29,61,36,39,47,49,52,38,55,32,57,54,44)\\n>y = c(12,15,7,17,10,11,11,12,14,9,16,8,18,14,12)\\n>moisture = lm(y ∼x)\\n>moisture\\nCall:\\nlm(formula = y ∼x)\\nCoefﬁcients:\\n(Intercept)\\nx\\n-2.5105\\n0.3232\\nTo plot the scatter diagram and resulting regression line, now input\\n>plot(x,y)\\n>abline(moisture)\\nto obtain'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 381}, page_content='9.3 Distribution of the estimators\\n371\\n9.3\\nDistribution of the estimators\\nTo specify the distribution of the estimators A and B, it is necessary to make\\nadditional assumptions about the random errors aside from just assuming that\\ntheir mean is 0. The usual approach is to assume that the random errors are\\nindependent normal random variables having mean 0 and variance σ 2. That\\nis, we suppose that if Yi is the response corresponding to the input value xi,\\nthen Y1,...,Yn are independent and\\nYi ∼N(α + βxi,σ 2)\\nNote that the foregoing supposes that the variance of the random error does\\nnot depend on the input value but rather is a constant. This value σ 2 is not\\nassumed to be known but rather must be estimated from the data.\\nSince the least squares estimator B of β can be expressed as\\nB =\\n\\x05\\ni\\n(xi −x)Yi\\n\\x05\\ni\\nx2\\ni −nx2\\n(9.3.1)\\nwe see that it is a linear combination of the independent normal random\\nvariables Yi,i = 1,...,n and so is itself normally distributed. Using Equation\\n(9.3.1), the mean and variance of B are computed as follows:\\nE[B] =\\n\\x05\\ni\\n(xi −x)E[Yi]\\n\\x05\\ni\\nx2\\ni −nx2\\n=\\n\\x05\\ni\\n(xi −x)(α + βxi)\\n\\x05\\ni\\nx2\\ni −nx2\\n=\\nα \\x05\\ni\\n(xi −x) + β \\x05\\ni\\nxi(xi −x)\\n\\x05\\ni\\nx2\\ni −nx2\\n= β\\n\\x06\\x05\\ni\\nx2\\ni −x \\x05\\ni\\nxi\\n\\x07\\n\\x05\\ni\\nx2\\ni −nx2\\nsince\\n\\x02\\ni\\n(xi −x) = 0\\n= β'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 382}, page_content='372 CHAPTER 9: Regression\\nThus E[B] = β and so B is an unbiased estimator of β. We will now compute\\nthe variance of B.\\nVar(B) =\\nVar\\n\\x08 n\\x05\\ni=1\\n(xi −x)Yi\\n\\t\\n\\x08 n\\x05\\ni=1\\nx2\\ni −nx2\\n\\t2\\n=\\nn\\x05\\ni=1\\n(xi −x)2 Var(Yi)\\n\\x08 n\\x05\\ni=1\\nx2\\ni −nx2\\n\\t2\\nby independence\\n=\\nσ 2\\nn\\n\\x02\\ni=1\\n(xi −x)2\\n\\x08 n\\x05\\ni=1\\nx2\\ni −nx2\\n\\t2\\n=\\nσ 2\\nn\\x05\\ni=1\\nx2\\ni −nx2\\n(9.3.2)\\nwhere the ﬁnal equality results from the use of the identity\\nn\\n\\x02\\ni=1\\n(xi −x)2 =\\nn\\n\\x02\\ni=1\\nx2\\ni −nx2\\nUsing Equation (9.3.1) along with the relationship\\nA =\\nn\\n\\x02\\ni=1\\nYi\\nn −B x\\nshows that A can also be expressed as a linear combination of the independent\\nnormal random variables Yi,i=1,...,n and is thus also normally distributed.\\nIts mean is obtained from\\nE[A] =\\nn\\n\\x02\\ni=1\\nE[Yi]\\nn\\n−xE[B]\\n=\\nn\\n\\x02\\ni=1\\n(α + βxi)\\nn\\n−xβ\\n= α + βx −xβ\\n= α'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 383}, page_content='9.3 Distribution of the estimators\\n373\\nThus A is also an unbiased estimator. The variance of A is computed by ﬁrst\\nexpressing A as a linear combination of the Yi. The result (whose details are\\nleft as an exercise) is that\\nVar(A) =\\nσ 2\\nn\\x05\\ni=1\\nx2\\ni\\nn\\n\\x08 n\\x05\\ni=1\\nx2\\ni −nx2\\n\\t\\n(9.3.3)\\nThe quantities Yi −A −Bxi,i = 1,...,n, which represent the differences be-\\ntween the actual responses (that is, the Yi) and their least squares estimators\\n(that is, A + Bxi) are called the residuals. The sum of squares of the residuals\\nSSR =\\nn\\n\\x02\\ni=1\\n(Yi −A −Bxi)2\\ncan be utilized to estimate the unknown error variance σ 2. Indeed, it can be\\nshown that\\nSSR\\nσ 2 ∼χ2\\nn−2\\nThat is, SSR/σ 2 has a chi-square distribution with n −2 degrees of freedom,\\nwhich implies that\\nE\\n\\nSSR\\nσ 2\\n\\x0b\\n= n −2\\nor\\nE\\n\\n SSR\\nn −2\\n\\x0b\\n= σ 2\\nThus SSR/(n −2) is an unbiased estimator of σ 2. In addition, it can be shown\\nthat SSR is independent of the pair A and B.\\nRemarks\\nA plausibility argument as to why SSR/σ 2 might have a chi-square distribution\\nwith n −2 degrees of freedom and be independent of A and B runs as follows.\\nBecause the Yi are independent normal random variables, it follows that (Yi −\\nE[Yi])/√Var(Yi),i = 1,...,n are independent standard normals and so\\nn\\n\\x02\\ni=1\\n(Yi −E[Yi])2\\nVar(Yi)\\n=\\nn\\n\\x02\\ni=1\\n(Yi −α −βxi)2\\nσ 2\\n∼χ2\\nn'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 384}, page_content='374 CHAPTER 9: Regression\\nNow if we substitute the estimators A and B for α and β, then 2 degrees of\\nfreedom are lost, and so it is not an altogether surprising result that SSR/σ 2\\nhas a chi-square distribution with n −2 degrees of freedom.\\nThe fact that SSR is independent of A and B is quite similar to the funda-\\nmental result that in normal sampling X and S2 are independent. Indeed this\\nlatter result states that if Y1,...,Yn is a normal sample with population mean\\nμ and variance σ2, then if in the sum of squares \\x05n\\ni=1(Yi −μ)2/σ 2, which\\nhas a chi-square distribution with n degrees of freedom, one substitutes the\\nestimator Y for μ to obtain the new sum of squares \\x05\\ni(Yi −Y)2/σ 2, then\\nthis quantity [equal to (n −1)S2/σ 2] will be independent of Y and will have\\na chi-square distribution with n −1 degrees of freedom. Since SSR/σ 2 is ob-\\ntained by substituting the estimators A and B for α and β in the sum of squares\\n\\x05n\\ni=1(Yi −α −βxi)2/σ 2, it is not unreasonable to expect that this quantity\\nmight be independent of A and B.\\nWhen the Yi are normal random variables, the least squares estimators are also\\nthe maximum likelihood estimators. To verify this remark, note that the joint\\ndensity of Y1,...,Yn is given by\\nf Y1,...,Yn(y1,...,yn) =\\nn\\n\\x0c\\ni=1\\nf Yi(yi)\\n=\\nn\\n\\x0c\\ni=1\\n1\\n√\\n2πσ\\ne−( yi−α−βxi)2/2σ 2\\n=\\n1\\n(2π)n/2σ n e−\\x05n\\ni=1( yi−α−βxi)2/2σ 2\\nConsequently, the maximum likelihood estimators of α and β are precisely the\\nvalues of α and β that minimize \\x05n\\ni=1(yi −α −βxi)2. That is, they are the least\\nsquares estimators.\\nNotation\\nIf we let\\nSxY =\\nn\\n\\x02\\ni=1\\n(xi −x)(Yi −Y) =\\nn\\n\\x02\\ni=1\\nxiYi −nxY\\nSxx =\\nn\\n\\x02\\ni=1\\n(xi −x)2 =\\nn\\n\\x02\\ni=1\\nx2\\ni −nx2\\nSYY =\\nn\\n\\x02\\ni=1\\n(Yi −Y)2 =\\nn\\n\\x02\\ni=1\\nY 2\\ni −nY 2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 385}, page_content='9.3 Distribution of the estimators\\n375\\nthen the least squares estimators can be expressed as\\nB = SxY\\nSxx\\nA = Y −B x\\nThe following computational identity for SSR, the sum of squares of the resid-\\nuals, can be established.\\nComputational identity for SSR\\nSSR = SxxSYY −S2\\nxY\\nSxx\\n(9.3.4)\\nThe following proposition sums up the results of this section.\\nProposition 9.3.1. Suppose that the responses Yi,i=1,...,n are independent\\nnormal random variables with means α + βxi and common variance σ 2. The\\nleast squares estimators of β and α\\nB = SxY\\nSxx\\n,\\nA = Y −B x\\nare distributed as follows:\\nA ∼N\\n⎛\\n⎜⎝α,\\nσ 2 \\x05\\ni\\nx2\\ni\\nnSxx\\n⎞\\n⎟⎠\\nB ∼N(β, σ 2/Sxx)\\nIn addition, if we let\\nSSR =\\n\\x02\\ni\\n(Yi −A −Bxi)2\\ndenote the sum of squares of the residuals, then\\nSSR\\nσ 2 ∼χ2\\nn−2\\nand SSR is independent of the least squares estimators A and B. Also, SSR can\\nbe computed from\\nSSR = SxxSYY −(SxY )2\\nSxx'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 386}, page_content='376 CHAPTER 9: Regression\\nR can be used to compute the quantities SxY , Sxx, SYY , and SSR as follows:\\n>x = c(x1,...,xn)\\n>y = c(y1,...,yn)\\n>Sxy = sum(x ∗y) −n ∗mean(x) ∗mean(y)\\n>Sxx = sum(x ∗x) −n ∗mean(x)2\\n>Syy = sum(y ∗y) −n ∗mean(y)2\\n>SSR = (Sxx ∗Syy −Sxy2)/Sxx\\nExample 9.3.a. The following data relate x, the moisture of a wet mix of a\\ncertain product, to Y, the density of the ﬁnished product.\\nxi\\nyi\\n5\\n7.4\\n6\\n9.3\\n7\\n10.6\\n10\\n15.4\\n12\\n18.1\\n15\\n22.2\\n18\\n24.1\\n20\\n24.8\\nFit a linear curve to these data. Also determine SSR.\\nSolution. Using R yields, after inputting the data and deﬁning the quantities,\\nSxy, Sxx, Syy, and SSR,\\n>x = c(5,6,7,10,12,15,18,20)\\n>y = c(7.4,9.3,10.6,15.4,18.1,22.2,24.1,24.8)\\n>Sxy = sum(x ∗y) −8 ∗mean(x) ∗mean(y)\\n>Sxx = sum(x ∗x) −8 ∗mean(x)2\\n>Syy = sum(y ∗y) −8 ∗mean(y)2\\n>SSR = (Sxx ∗Syy −Sxy2)/Sxx\\n>SSR\\n[1]9.469758\\nThe R lines\\n>model = lm(y ∼x)\\n>model\\n>plot(x,y)\\n>abline(model)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 387}, page_content='9.4 Statistical inferences about the regression parameters\\n377\\nFIGURE 9.2\\nExample 9.3.a.\\nnow give the estimated regression line\\ny = 2.463 + 1.206x\\nas well as its plot given by Figure 9.2.\\n■\\n9.4\\nStatistical inferences about the\\nregression parameters\\nUsing Proposition 9.3.1, it is a simple matter to devise hypothesis tests and\\nconﬁdence intervals for the regression parameters.\\n9.4.1\\nInferences concerning β\\nAn important hypothesis to consider regarding the simple linear regression\\nmodel\\nY = α + βx + e\\nis the hypothesis that β = 0. Its importance derives from the fact that it is\\nequivalent to stating that the mean response does not depend on the input,\\nor, equivalently, that there is no regression on the input variable. To test\\nH0 : β = 0\\nversus\\nH1 : β ̸= 0'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 388}, page_content='378 CHAPTER 9: Regression\\nnote that, from Proposition 9.3.1,\\nB −β\\n\\x13\\nσ 2/Sxx\\n=\\n\\x13\\nSxx\\n(B −β)\\nσ\\n∼N(0,1)\\n(9.4.1)\\nand is independent of\\nSSR\\nσ 2 ∼χ2\\nn−2\\nHence, from the deﬁnition of a t-random variable it follows that\\n√Sxx(B −β)/σ\\n\\x14\\nSSR\\nσ 2(n−2)\\n=\\n\\x15\\n(n −2)Sxx\\nSSR\\n(B −β) ∼tn−2\\n(9.4.2)\\nThat is, √(n −2)Sxx/SSR(B −β) has a t-distribution with n −2 degrees of free-\\ndom. Therefore, if H0 is true (and so β = 0), then\\n\\x15\\n(n −2)Sxx\\nSSR\\nB ∼tn−2\\nwhich gives rise to the following test of H0 .\\nHypothesis test of H0: β = 0\\nA signiﬁcance level γ test of H0 is to\\nreject\\nH0\\nif\\n\\x16(n −2)Sxx\\nSSR\\n|B| > tγ/2,n−2\\naccept\\nH0\\notherwise\\nThis test can be performed by ﬁrst computing the value of the test statistic\\n√(n −2)Sxx/SSR|B| — call its value v — and then rejecting H0 if the desired\\nsigniﬁcance level is at least as large as\\np-value = P{|Tn−2| > v}\\n= 2P{Tn−2 > v}\\nwhere Tn−2 is a t-random variable with n −2 degrees of freedom.\\nR can be used to test the preceding hypothesis. For instance, if “ﬁt” is the\\nname of the model, then the R command summary(ﬁt) will yield the desired\\np-values.\\nExample 9.4.a. An individual claims that the fuel consumption of his auto-\\nmobile does not depend on how fast the car is driven. To test the plausibility\\nof this hypothesis, the car was tested at various speeds between 45 and 70 miles'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 389}, page_content='9.4 Statistical inferences about the regression parameters\\n379\\nper hour. The miles per gallon attained at each of these speeds was determined,\\nwith the following data resulting:\\nSpeed\\nMiles per Gallon\\n45\\n24.2\\n50\\n25.0\\n55\\n23.3\\n60\\n22.0\\n65\\n21.5\\n70\\n20.6\\n75\\n19.8\\nDo these data refute the claim that the mileage per gallon of gas is unaffected\\nby the speed at which the car is being driven?\\nSolution. Suppose that a simple linear regression model\\nY = α + βx + e\\nrelates Y, the miles per gallon of the car, to x, the speed at which it is being\\ndriven. Now, the claim being made is that the regression coefﬁcient β is equal\\nto 0.\\nR is used to test this as follows. Calling the model “miles”\\n>x = c(45,50,55,60,65,70,75)\\n>y = c(24.2,25,23.3,22,21.5,20.6,19.8)\\n>miles = lm(y ∼x)\\n>miles\\nCall:\\nlm(formula = y ∼x)\\nCoefﬁcients:\\n(Intercept)\\nx\\n32.54\\n-0.17\\nNow type\\n> summary(miles)\\nto obtain the following output.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 390}, page_content='380 CHAPTER 9: Regression\\nThe summary output ﬁrst gives the seven residuals yi −(A + Bxi),i = 1,...,7.\\nThe following lines give the estimates 32.54286 and −0.17 for the intercept α\\nand the slope β. It then gives the standard error of these estimates. Then it gives\\nthe value of the t-statistic for testing that these quantities are 0. For instance,\\nthe t-value for testing that β = 0 is −8.138. It then gives the p-value of the test,\\nequal in this case to 1.69 × 10−6 for testing that α = 0, and equal to 0.000455\\nfor testing that β = 0, indicating that both hypotheses would be rejected at any\\npractical signiﬁcance level. The Residual standard error is the value of\\n\\x14\\nSSR\\nn−2.\\nBecause SSR/(n −2) is the estimator of σ 2, the Residual standard error is the\\nestimate of σ.\\n■\\nA conﬁdence interval estimator for β is easily obtained from Equation (9.4.2).\\nIndeed, it follows from Equation (9.4.2) that for any a,0 < a < 1,\\nP\\n\\x17\\n−ta/2,n−2 <\\n\\x15\\n(n −2)Sxx\\nSSR\\n(B −β) < ta/2,n−2\\n\\x18\\n= 1 −a\\nor, equivalently,\\nP\\n\\x17\\nB −\\n\\x15\\nSSR\\n(n −2)Sxx\\nta/2,n−2 < β < B +\\n\\x15\\nSSR\\n(n −2)Sxx\\nta/2,n−2\\n\\x18\\n= 1 −a\\nwhich yields the following.\\nConﬁdence interval for β\\nA 100(1 −a) percent conﬁdence interval estimator of β is\\n\\x03\\nB −\\n\\x15\\nSSR\\n(n −2)Sxx\\nta/2,n−2,B +\\n\\x15\\nSSR\\n(n −2)Sxx\\nta/2,n−2\\n\\x04'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 391}, page_content='9.4 Statistical inferences about the regression parameters\\n381\\nRemark\\nThe result that\\nB −β\\n\\x13\\nσ 2/Sxx\\n∼N(0,1)\\ncannot be immediately applied to make inferences about β since it involves\\nthe unknown parameter σ 2. Instead, what we do is use the preceding statistic\\nwith σ 2 replaced by its estimator SSR/(n −2), which has the effect of changing\\nthe distribution of the statistic from the standard normal to the t-distribution\\nwith n −2 degrees of freedom.\\nExample 9.4.b. We can also use R to obtain conﬁdence intervals for α and β. If\\nthe name of the model is “name”, then the R command conﬁnt(name, level=\\nm) returns a 100m percent conﬁdence interval for α and β of model “name”.\\nFor instance, suppose we wanted a 95 percent conﬁdence interval for the pa-\\nrameters of the model we called miles. If we type >conﬁnt(miles, level=0.95),\\nthen R returns 95 percent conﬁdence intervals for α and β.\\n>conﬁnt(miles, level = 0.95)\\n2.5% 97.5%\\n(Intercept )29.2766922 35.8090220\\nx −0.2236954\\n−0.1163046\\nThus, the 95 percent conﬁdence intervals for α and β are, respectively,\\n(29.2767, 35.8090) and (−0.2237, −0.1163).\\n■\\n9.4.1.1\\nRegression to the mean\\nThe term regression was originally employed by Francis Galton while describ-\\ning the laws of inheritance. Galton believed that these laws caused population\\nextremes to “regress toward the mean.” By this he meant that children of in-\\ndividuals having extreme values of a certain characteristic would tend to have\\nless extreme values of this characteristic than their parent.\\nIf we assume a linear regression relationship between the characteristic of the\\noffspring (Y) and that of the parent (x), then a regression to the mean will\\noccur when the regression parameter β is between 0 and 1. That is, if\\nE[Y] = α + βx\\nand 0 < β < 1, then E[Y] will be smaller than x when x is large and greater\\nthan x when x is small. That this statement is true can be easily checked either\\nalgebraically or by plotting the two straight lines\\ny = α + βx'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 392}, page_content='382 CHAPTER 9: Regression\\nFIGURE 9.3\\nScatter diagram of son’s height versus father’s height.\\nand\\ny = x\\nA plot indicates that, when 0 < β < 1, the line y = α + βx is above the line\\ny = x for small values of x and is below it for large values of x.\\nExample 9.4.c. To illustrate Galton’s thesis of regression to the mean, the\\nBritish statistician Karl Pearson plotted the heights of 10 randomly chosen sons\\nversus that of their fathers. The resulting data (in inches) were as follows.\\nFathers’ height\\n60\\n62\\n64\\n65\\n66\\n67\\n68\\n70\\n72\\n74\\nSons’ height\\n63.6 65.2 66\\n65.5 66.9 67.1\\n67.4 68.3 70.1\\n70\\nA scatter diagram representing these data is presented in Figure 9.3.\\nNote that whereas the data appear to indicate that taller fathers tend to have\\ntaller sons, it also appears to indicate that the sons of fathers who are either\\nextremely short or extremely tall tend to be more “average” than their fathers\\n— that is, there is a “regression toward the mean.”\\nWe will determine whether the preceding data are strong enough to prove that\\nthere is a regression toward the mean by taking this statement as the alternative\\nhypothesis. That is, we will use the above data to test\\nH0 : β ≥1\\nversus\\nH1 : β < 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 393}, page_content='9.4 Statistical inferences about the regression parameters\\n383\\nwhich is equivalent to a test of\\nH0 : β = 1\\nversus\\nH1 : β < 1\\nIt now follows from Equation (9.4.2) that when β = 1, the test statistic\\nT S =\\n\\x13\\n8Sxx/SSR(B −1)\\nhas a t-distribution with 8 degrees of freedom. If the value of the test statistic is\\nT S = v then the p-value of the test of the null hypothesis H0 : β ≥1 is\\np-value = P(T8 ≤v)\\nwhere T8 is a t-random variable with 8 degrees of freedom. We can use R to\\nboth compute the value of the test statistic and the resulting p-value. To obtain\\nthe value of TS, we use the summary command to obtain\\n>x = c(60,62,64,65,66,67,68,70,72,74)\\n>y = c(63.6,65.2,66,65.5,66.9,67.1,67.4,68.3,70.1,70)\\n>height = lm(y ∼x)\\n>summary(height)\\nWhen we now press enter, among other things we are given a row with the\\nvalues for the coefﬁcient of x:\\nx 0.46457 0.03298 14.08 6.27e −07\\nThis value following x gives that the estimate of β is B = 0.46457. The next\\ncolumn gives the standard error of this estimate, and the following one gives\\nthe value of\\n\\x14\\n8Sxx\\nSSR B, which is the test statistic for testing that β = 0. Hence, the\\nvalue of TS, the test statistic when testing H0 : β = 1, is T S =\\n\\x14\\n8Sxx\\nSSR (B −1) =\\n14.08 −14.08/.46457. R now gives\\n>v = 14.08 ∗(1 −1/.46457)\\n>v\\n[1] −16.2276\\n>pt(v,8)\\n[1]1.045569e −07\\nThus, the value of the test statistic is −16.2276, and the resulting p-value is\\n1.045569 × 10−7. Thus, the null hypothesis that β ≥1 is rejected at almost any\\nsigniﬁcance level, thus establishing a regression toward the mean. (See Fig-\\nure 9.4, which inserts the line y = x on the scatter diagram.)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 394}, page_content='384 CHAPTER 9: Regression\\nFIGURE 9.4\\nExample 9.4.c for x small, y > x. For x large, y < x.\\nA modern biological explanation for the regression to the mean phenomenon\\nwould roughly go along the lines of noting that as an offspring obtains a ran-\\ndom selection of one-half of its parents’ genes, it follows that the offspring of,\\nsay, a very tall parent would, by chance, tend to have fewer “tall” genes than its\\nparent.\\nWhile the most important applications of the regression to the mean phe-\\nnomenon concern the relationship between the biological characteristics of\\nan offspring and that of its parents, this phenomenon also arises in situations\\nwhere we have two sets of data referring to the same variables.\\n■\\nExample 9.4.d. The data of Table 9.1 relate the number of motor vehicle\\ndeaths occurring in 12 counties in the northwestern United States in the years\\n1988 and 1989.\\nA glance at Figure 9.5 indicates that in 1989 there was, for the most part, a\\nreduction in the number of deaths in those counties that had a large number\\nof motor deaths in 1988. Similarly, there appears to have been an increase\\nin those counties that had a low value in 1988. Thus, we would expect that a\\nregression to the mean is in effect. In fact, R yields that the estimated regression\\nequation is\\ny = 74.589 + .276x\\nshowing that the estimated value of β indeed appears to be less than 1.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 395}, page_content='9.4 Statistical inferences about the regression parameters\\n385\\nTable 9.1 Motor Vehicle Deaths, Northwestern\\nUnited States, 1988 and 1989.\\nCounty\\nDeaths in\\n1988\\nDeaths in\\n1989\\n1\\n121\\n104\\n2\\n96\\n91\\n3\\n85\\n101\\n4\\n113\\n110\\n5\\n102\\n117\\n6\\n118\\n108\\n7\\n90\\n96\\n8\\n84\\n102\\n9\\n107\\n114\\n10\\n112\\n96\\n11\\n95\\n488\\n12\\n101\\n106\\nFIGURE 9.5\\nScatter diagram of 1989 deaths versus 1988 deaths.\\nOne must be careful when considering the reason behind the regression to the\\nmean phenomenon in the preceding data. For instance, it might be natural to\\nsuppose that those counties that had a large number of deaths caused by motor\\nvehicles in 1988 would have made a large effort — perhaps by improving the\\nsafety of their roads or by making people more aware of the potential dangers\\nof unsafe driving — to reduce this number. In addition, we might suppose'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 396}, page_content='386 CHAPTER 9: Regression\\nthat those counties that had the fewest number of deaths in 1988 might have\\n“rested on their laurels” and not made much of an effort to further improve\\ntheir numbers — and as a result had an increase in the number of casualties\\nthe following year.\\nWhile this supposition might be correct, it is important to realize that a regres-\\nsion to the mean would probably have occurred even if none of the counties\\nhad done anything out of the ordinary. Indeed, it could very well be the case\\nthat those counties having large numbers of casualties in 1988 were just very\\nunlucky in that year and thus a decrease in the next year was just a return to\\na more normal result for them. (For an analogy, if 9 heads result when 10 fair\\ncoins are ﬂipped then it is quite likely that another ﬂip of these 10 coins will\\nresult in fewer than 9 heads.) Similarly, those counties having few deaths in\\n1988 might have been “lucky” that year and a more normal result in 1989\\nwould thus lead to an increase.\\nThe mistaken belief that regression to the mean is due to some outside inﬂu-\\nence when it is in reality just due to “chance” occurs frequently enough that it\\nis often referred to as the regression fallacy.\\n■\\n9.4.2\\nInferences concerning α\\nThe determination of conﬁdence intervals and hypothesis tests for α is accom-\\nplished in exactly the same manner as was done for β. Speciﬁcally, Proposi-\\ntion 9.3.1 can be used to show that\\n\\x15\\nn(n −2)Sxx\\nSSR\\n\\x05\\ni x2\\ni\\n(A −α) ∼tn−2\\n(9.4.3)\\nwhich leads to the following conﬁdence interval estimator of α.\\nConﬁdence interval estimator of α\\nThe 100(1 −a) percent conﬁdence interval for α is the interval\\nA ± tα/2,n−2\\n\\x15\\nSSR\\n\\x05\\ni x2\\ni\\nn(n −2)Sxx\\nHypothesis tests concerning α are easily obtained from Equation (9.4.3), and\\ntheir development is left as an exercise.\\n9.4.3\\nInferences concerning the mean response α + βx0\\nIt is often of interest to use the data pairs (xi,Yi),i = 1,...,n, to estimate α +\\nβ x0, the mean response for a given input level x0. If it is a point estimator\\nthat is desired, then the natural estimator is A + B x0, which is an unbiased'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 397}, page_content='9.4 Statistical inferences about the regression parameters\\n387\\nestimator since\\nE[A + B x0] = E[A] + x0E[B] = α + β x0\\nHowever, if we desire a conﬁdence interval, or are interested in testing some\\nhypothesis about this mean response, then it is necessary to ﬁrst determine the\\nprobability distribution of the estimator A + B x0. We now do so.\\nUsing the expression for B given by Equation (9.3.1) yields that\\nB = c\\nn\\n\\x02\\ni=1\\n(xi −x)Yi\\nwhere\\nc =\\n1\\nn\\x05\\ni=1\\nx2\\ni −nx2\\n= 1\\nSxx\\nSince\\nA = Y −B x\\nwe see that\\nA + B x0 =\\nn\\x05\\ni=1\\nYi\\nn\\n−B(x −x0)\\n=\\nn\\n\\x02\\ni=1\\nYi\\n\\n1\\nn −c(xi −x)(x −x0)\\n\\x0b\\nSince the Yi are independent normal random variables, the foregoing equation\\nshows that A + B x0 can be expressed as a linear combination of independent\\nnormal random variables and is thus itself normally distributed. Because we\\nalready know its mean, we need only compute its variance, which is accom-\\nplished as follows:\\nVar(A + B x0) =\\nn\\n\\x02\\ni=1\\n\\n1\\nn −c(xi −x)(x −x0)\\n\\x0b2\\nVar(Yi)\\n= σ 2\\nn\\n\\x02\\ni=1\\n\\n 1\\nn2 + c2(x −x0)2(xi −x)2 −2c(xi −x)(x −x0)\\nn\\n\\x0b\\n= σ 2\\n\\x19\\n1\\nn + c2(x −x0)2\\nn\\n\\x02\\ni=1\\n(xi −x)2 −2c(x −x0)\\nn\\n\\x02\\ni=1\\n(xi −x)\\nn\\n\\x1a'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 398}, page_content='388 CHAPTER 9: Regression\\n= σ 2\\n\\n1\\nn + (x −x0)2\\nSxx\\n\\x0b\\nwhere the last equality followed from\\nn\\n\\x02\\ni=1\\n(xi −x)2 =\\nn\\n\\x02\\ni=1\\nx2\\ni −nx2 = 1/c = Sxx,\\nn\\n\\x02\\ni=1\\n(xi −x) = 0\\nHence, we have shown that\\nA + B x0 ∼N\\n\\x08\\nα + β x0,σ 2\\n\\n1\\nn + (x0 −x)2\\nSxx\\n\\x0b\\t\\n(9.4.4)\\nIn addition, because A + B x0 is independent of\\nSSR/σ 2 ∼χ2\\nn−2\\nit follows that\\nA + B x0 −(α + β x0)\\n\\x15\\n1\\nn + (x0 −x)2\\nSxx\\n\\x16 SSR\\nn −2\\n∼tn−2\\n(9.4.5)\\nEquation (9.4.5) can now be used to obtain the following conﬁdence interval\\nestimator of α + β x0.\\nConﬁdence interval estimator of α + βx0\\nWith 100(1 −a) percent conﬁdence, α + β x0 will lie within\\nA + B x0 ±\\n\\x15\\n1\\nn + (x0 −x)2\\nSxx\\n\\x16\\nSSR\\nn −2ta/2, n−2\\nExample 9.4.e. Using the data of Example 9.4.c, determine a 95 percent con-\\nﬁdence interval for the average height of all males whose fathers are 68 inches\\ntall.\\nSolution. Using R yields\\n>x = c(60,62,64,65,66,67,68,70,72,74)\\n>y = c(63.6,65.2,66,65.5,66.9,67.1,67.4,68.3,70.1,70)\\n>Sxy = sum(x ∗y) −10 ∗mean(x) ∗mean(y)\\n>Sxx = sum(x ∗x) −10 ∗mean(x)2\\n>Syy = sum(y ∗y) −10 ∗mean(y)2\\n>Syy'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 399}, page_content='9.4 Statistical inferences about the regression parameters\\n389\\n[1]38.529\\n>SSR = (Sxx ∗Syy −Sxy2)/Sxx\\n>SSR\\n[1]1.493578\\n>B = Sxy/Sxx\\n>A = mean(y) −B ∗mean(x)\\n>a = sqrt((1/10 + (68 −mean(x))2/Sxx) ∗SSR/8)\\n>l = (A + B ∗68) −a ∗qt(.975,8)\\n>u = (A + B ∗68) + a ∗qt(.975,8)\\n>c(l,u)\\n[1]67.23944 67.89552\\nThus, the 95 percent conﬁdence interval is\\nα + βx0 ∈(67.23944, 67.89552)\\n(We had R give the values of SYY and SSR for later use.)\\n■\\n9.4.4\\nPrediction interval of a future response\\nIt is often the case that it is more important to estimate the actual value of\\na future response rather than its mean value. For instance, if an experiment\\nis to be performed at temperature level x0, then we would probably be more\\ninterested in predicting Y(x0), the yield from this experiment, than we would\\nbe in estimating the expected yield — E[Y(x0)] = α +β x0. (On the other hand,\\nif a series of experiments were to be performed at input level x0, then we would\\nprobably want to estimate α + β x0, the mean yield.)\\nSuppose ﬁrst that we are interested in a single value (as opposed to an interval)\\nto use as a predictor of Y(x0), the response at level x0. Now, it is clear that the\\nbest predictor of Y(x0) is its mean value α + β x0. [Actually, this is not so im-\\nmediately obvious since one could argue that the best predictor of a random\\nvariable is (1) its mean — which minimizes the expected square of the differ-\\nence between the predictor and the actual value; or (2) its median — which\\nminimizes the expected absolute difference between the predictor and the ac-\\ntual value; or (3) its mode — which is the most likely value to occur. However,\\nas the mean, median, and mode of a normal random variable are all equal —\\nand the response is, by assumption, normally distributed — there is no doubt\\nin this situation.] Since α and β are not known, it seems reasonable to use their\\nestimators A and B and thus use A + B x0 as the predictor of a new response at\\ninput level x0.\\nLet us now suppose that rather than being concerned with determining a single\\nvalue to predict a response, we are interested in ﬁnding a prediction interval'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 400}, page_content='390 CHAPTER 9: Regression\\nthat, with a given degree of conﬁdence, will contain the response. To obtain\\nsuch an interval, let Y denote the future response whose input level is x0 and\\nconsider the probability distribution of the response minus its predicted value\\n— that is, the distribution of Y −A −B x0. Now,\\nY ∼N(α + β x0,σ 2)\\nand, as was shown in Section 9.4.3,\\nA + B x0 ∼N\\n\\x08\\nα + β x0,σ 2\\n\\n1\\nn + (x0 −x)2\\nSxx\\n\\x0b\\t\\nHence, because Y is independent of the earlier data values Y1,Y2,...,Yn that\\nwere used to determine A and B, it follows that Y is independent of A + B x0\\nand so\\nY −A −B x0 ∼N\\n\\x08\\n0,σ 2\\n\\n1 + 1\\nn + (x0 −x)2\\nSxx\\n\\x0b\\t\\nor, equivalently,\\nY −A −B x0\\nσ\\n\\x15\\nn + 1\\nn\\n+ (x0 −x)2\\nSxx\\n∼N(0,1)\\n(9.4.6)\\nNow, using once again the result that SSR is independent of A and B (and also\\nof Y) and\\nSSR\\nσ 2 ∼χ2\\nn−2\\nwe obtain, by the usual argument, upon replacing σ 2 in Equation (9.4.6) by its\\nestimator SSR/(n −2) that\\nY −A −B x0\\n\\x15\\nn + 1\\nn\\n+ (x0 −x)2\\nSxx\\n\\x16 SSR\\nn −2\\n∼tn−2\\nand so, for any value a,0 < a < 1,\\nP{−ta/2,n−2 <\\nY −A −B x0\\n\\x15\\nn + 1\\nn\\n+ (x0 −x)2\\nSxx\\n\\x16 SSR\\nn −2\\n< ta/2,n−2} = 1 −a\\nThat is, we have just established the following.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 401}, page_content='9.4 Statistical inferences about the regression parameters\\n391\\nPrediction interval for a response at the input level x0\\nBased on the response values Yi corresponding to the input values xi,i =\\n1,2,...,n: With 100(1 −a) percent conﬁdence, the response Y at the input\\nlevel x0 will be contained in the interval\\nA + B x0 ± ta/2,n−2\\n\\x15\\nn + 1\\nn\\n+ (x0 −x)2\\nSxx\\n\\x0b SSR\\nn −2\\nExample 9.4.f. In Example 9.4.c, suppose we want an interval that we can “be\\n95 percent certain” will contain the height of a given male whose father is 68\\ninches tall. A simple computation now yields the prediction interval\\nY(68) ∈67.568 ± 1.050\\nor, with 95 percent conﬁdence, the person’s height will be between 66.518 and\\n68.618.\\n■\\nRemarks\\n(a) There is often some confusion about the difference between a conﬁdence\\nand a prediction interval. A conﬁdence interval is an interval that does contain,\\nwith a given degree of conﬁdence, a ﬁxed parameter of interest. A prediction\\ninterval, on the other hand, is an interval that will contain, again with a given\\ndegree of conﬁdence, a random variable of interest.\\nInferences About\\nUse the Distributional Result\\nβ\\n\\x16(n −2)Sxx\\nSSr\\n(B −β) ∼tn−2\\nα\\n\\x1b\\n\\x1c\\n\\x1c\\n\\x1c\\n\\x1d\\nn(n −2)Sxx\\n\\x02\\ni\\nx2\\ni SSR\\n(A −α) ∼tn−2\\nInferences About\\nUse the Distributional Result\\nα + β x0\\nA + B x0 −α −β x0\\n\\x15\\x08 1\\nn + (x0 −x)2\\nSxx\\n\\t\\x08 SSR\\nn −2\\n\\t ∼tn−2\\nY(x0)\\nY(x0) −A −B x0\\n\\x15\\x08\\n1 + 1\\nn + (x0 −x)2\\nSxx\\n\\t\\x08 SSR\\nn −2\\n\\t ∼tn−2\\n(b) One should not make predictions about responses at input levels that are\\nfar from those used to obtain the estimated regression line. For instance, the\\ndata of Example 9.4.c should not be used to predict the height of a male whose\\nfather is 42 inches tall.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 402}, page_content='392 CHAPTER 9: Regression\\n9.4.5\\nSummary of distributional results\\nWe now summarize the distributional results of this section.\\nModel: Y = α + βx + e,\\ne ∼N(0,σ 2)\\nData: (xi,Yi),\\ni = 1,2,...,n\\n9.5\\nThe coefﬁcient of determination and the sample\\ncorrelation coefﬁcient\\nSuppose we wanted to measure the amount of variation in the set of response\\nvalues Y1,...,Yn corresponding to the set of input values x1,...,xn. A standard\\nmeasure in statistics of the amount of variation in a set of values Y1,...,Yn is\\ngiven by the quantity\\nSYY =\\nn\\n\\x02\\ni=1\\n(Yi −Y)2\\nFor instance, if all the Yi are equal — and thus are all equal to Y — then SYY\\nwould equal 0.\\nThe variation in the values of the Yi arises from two factors. First, because the\\ninput values xi are different, the response variables Yi all have different mean\\nvalues, which will result in some variation in their values. Second, the variation\\nalso arises from the fact that even when the differences in the input values are\\ntaken into account, each of the response variables Yi has variance σ 2 and thus\\nwill not exactly equal the predicted value at its input xi.\\nLet us consider now the question as to how much of the variation in the values\\nof the response variables is due to the different input values, and how much is\\ndue to the inherent variance of the responses even when the input values are\\ntaken into account. To answer this question, note that the quantity\\nSSR =\\nn\\n\\x02\\ni=1\\n(Yi −A −Bxi)2\\nmeasures the remaining amount of variation in the response values after the\\ndifferent input values have been taken into account.\\nThus,\\nSYY −SSR'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 403}, page_content='9.5 The coefﬁcient of determination and the sample correlation coefﬁcient\\n393\\nrepresents the amount of variation in the response variables that is explained by\\nthe different input values, and so the quantity R2 deﬁned by\\nR2 = SYY −SSR\\nSYY\\n= 1 −SSR\\nSYY\\nrepresents the proportion of the variation in the response variables that is ex-\\nplained by the different input values. R2 is called the coefﬁcient of determination.\\nThe coefﬁcient of determination R2 will have a value between 0 and 1. A value\\nof R2 near 1 indicates that most of the variation of the response data is ex-\\nplained by the different input values, whereas a value of R2 near 0 indicates\\nthat little of the variation is explained by the different input values.\\nExample 9.5.a. In Example 9.4.c, which relates the height of a son to that of\\nhis father, the output from R yielded that\\nSYY = 38.529, SSR = 1.493578\\nThus,\\nR2 = 1 −1.493578\\n38.529\\n= 0.961235\\nWe can also obtain the value of R2 by using the R command “summary”. That\\nis, with “height” being the name given (height = lm(y ∼x)), entering\\n> summary(height)\\nwould yield, among other things,\\nMultiple R-squared: 0.9612\\nand that is the value of R2.\\nIn other words, 96 percent of the variation of the heights of the 10 individuals\\nis explained by the heights of their fathers. The remaining (unexplained) 4\\npercent of the variation is due to the variance of a son’s height even when the\\nfather’s height is taken into account. (That is, it is due to σ 2, the variance of the\\nerror random variable.)\\n■\\nThe value of R2 is often used as an indicator of how well the regression model\\nﬁts the data, with a value near 1 indicating a good ﬁt, and one near 0 indicating\\na poor ﬁt. In other words, if the regression model is able to explain most of the\\nvariation in the response data, then it is considered to ﬁt the data well.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 404}, page_content='394 CHAPTER 9: Regression\\nRecall that in Section 2.6 we deﬁned the sample correlation coefﬁcient r of the\\nset of data pairs (xi,Yi),i = 1,...,n, by\\nr =\\nn\\x05\\ni=1\\n(xi −x)(Yi −Y)\\n\\x15\\nn\\x05\\ni=1\\n(xi −x)2\\nn\\x05\\ni=1\\n(Yi −Y)2\\nIt was noted that r provided a measure of the degree to which high values of\\nx are paired with high values of Y and low values of x with low values of Y. A\\nvalue of r near +1 indicated that large x values were strongly associated with\\nlarge Y values and small x values were strongly associated with small Y values,\\nwhereas a value near −1 indicated that large x values were strongly associated\\nwith small Y values and small x values with large Y values.\\nIn the notation of this chapter,\\nr =\\nSxY\\n√SxxSYY\\nUpon using identity (9.3.4):\\nSSR = SxxSYY −S2\\nxY\\nSxx\\nwe see that\\nr2 =\\nS2\\nxY\\nSxxSYY\\n= SxxSYY −SSRSxx\\nSxxSYY\\n= 1 −SSR\\nSYY\\n= R2\\nThat is,\\n|r| =\\n\\x13\\nR2\\nand so, except for its sign indicating whether it is positive or negative, the\\nsample correlation coefﬁcient is equal to the square root of the coefﬁcient of\\ndetermination. The sign of r is the same as that of B.\\nThe above gives additional meaning to the sample correlation coefﬁcient. For\\ninstance, if a data set has its sample correlation coefﬁcient r equal to .9, then\\nthis implies that a simple linear regression model for these data explains 81'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 405}, page_content='9.6 Analysis of residuals: assessing the model\\n395\\npercent (since R2 = .92 = .81) of the variation in the response values. That is,\\n81 percent of the variation in the response values is explained by the different\\ninput values.\\n9.6\\nAnalysis of residuals: assessing the model\\nThe initial step for ascertaining whether or not the simple linear regression\\nmodel\\nY = α + βx + e,\\ne ∼\\n(0,σ 2)\\nis appropriate in a given situation is to investigate the scatter diagram. Indeed,\\nthis is often sufﬁcient to convince one that the regression model is or is not cor-\\nrect. When the scatter diagram does not by itself rule out the preceding model,\\nthen the least squares estimators A and B should be computed and the resid-\\nual Yi −(A + Bxi),i = 1,...,n analyzed. The analysis begins by normalizing,\\nor standardizing, the residuals by dividing them by √SSR/(n −2), the estimate\\nof the standard deviation of the Yi. The resulting quantities\\nYi −(A + Bxi)\\n√SSR/(n −2) ,\\ni = 1,...,n\\nare called the standardized residuals.\\nWhen the simple linear regression model is correct, the standardized residuals\\nare approximately independent standard normal random variables, and thus\\nshould be randomly distributed about 0 with about 95 percent of their values\\nbeing between −2 and +2 (since P{−1.96 < Z < 1.96} = .95). In addition, a\\nplot of the standardized residuals should not indicate any distinct pattern. In-\\ndeed, any indication of a distinct pattern should make one suspicious about\\nthe validity of the assumed simple linear regression model.\\nFigure 9.6 presents three different scatter diagrams and their associated stan-\\ndardized residuals. The ﬁrst of these, as indicated both by its scatter diagram\\nand the random nature of its standardized residuals, appears to ﬁt the straight-\\nline model quite well. The second residual plot shows a discernible pattern, in\\nthat the residuals appear to be ﬁrst decreasing and then increasing as the input\\nlevel increases. This often means that higher-order (than just linear) terms are\\nneeded to describe the relationship between the input and response. Indeed,\\nthis is also indicated by the scatter diagram in this case. The third standardized\\nresidual plot also shows a pattern, in that the absolute value of the residuals,\\nand thus their squares, appear to be increasing, as the input level increases. This\\noften indicates that the variance of the response is not constant but, rather, in-\\ncreases with the input level.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 406}, page_content='396 CHAPTER 9: Regression\\nFIGURE 9.6\\n9.7\\nTransforming to linearity\\nIn many situations, it is clear that the mean response is not a linear function of\\nthe input level. In such cases, if the form of the relationship can be determined\\nit is sometimes possible, by a change of variables, to transform it into a linear\\nform. For instance, in certain applications it is known that W(t), the amplitude\\nof a signal a time t after its origination, is approximately related to t by the\\nfunctional form\\nW(t) ≈ce−dt\\nOn taking logarithms, this can be expressed as\\nlogW(t) ≈logc −dt\\nTemperature\\nPercentage\\n5◦\\n.061\\n10◦\\n.113\\n20◦\\n.192\\n30◦\\n.259\\n40◦\\n.339\\n50◦\\n.401\\n60◦\\n.461\\n80◦\\n.551'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 407}, page_content='9.7 Transforming to linearity 397\\nFIGURE 9.6\\n(continued)\\nIf we now let\\nY = logW(t)\\nα = logc\\nβ = −d\\nthen the foregoing can be modeled as a regression of the form\\nY = α + βt + e'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 408}, page_content='398 CHAPTER 9: Regression\\nFIGURE 9.7\\nExample 9.7.a.\\nThe regression parameters α and β would then be estimated by the usual least\\nsquares approach and the original functional relationships can be predicted\\nfrom\\nW(t) ≈e A+Bt\\nExample 9.7.a. The following table gives the percentages of a chemical that\\nwere used up when an experiment was run at various temperatures (in degrees\\nCelsius). Use it to estimate the percentage of the chemical that would be used\\nup if the experiment were to be run at 350 degrees.\\nSolution. Let P(x) be the percentage of the chemical that is used up when the\\nexperiment is run at 10x degrees. Even though a plot of P(x) looks roughly\\nlinear (see Figure 9.7), we can improve upon the ﬁt by considering a nonlinear\\nrelationship between x and P(x). Speciﬁcally, let us consider a relationship of\\nthe form\\n1 −P(x) ≈c(1 −d)x\\nThat is, let us suppose that the percentage of the chemical that survives an\\nexperiment run at temperature x approximately decreases at an exponential\\nrate when x increases. Taking logs, the preceding can be written as\\nlog(1 −P(x)) ≈log(c) + x log(1 −d)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 409}, page_content='9.7 Transforming to linearity 399\\nThus, setting\\nY = −log(1 −P)\\nα = −logc\\nβ = −log(1 −d)\\nwe obtain the usual regression equation\\nY = α + βx + e\\nTo see whether the data support this model, we can plot −log(1 −P) versus x.\\nThe transformed data are presented in Table 9.2 and the graph in Figure 9.8.\\nTable 9.2\\nTemperature\\n−log(1 −P )\\n5◦\\n.063\\n10◦\\n.120\\n20◦\\n.213\\n30◦\\n.300\\n40◦\\n.414\\n50◦\\n.512\\n60◦\\n.618\\n80◦\\n.801\\nFIGURE 9.8'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 410}, page_content='400 CHAPTER 9: Regression\\nTable 9.3\\nx\\nP\\nˆP\\nP −ˆP\\n5\\n.061\\n.063\\n−.002\\n10\\n.113\\n.109\\n.040\\n20\\n.192\\n.193\\n−.001\\n30\\n.259\\n.269\\n−.010\\n40\\n.339\\n.339\\n.000\\n50\\n.401\\n.401\\n.000\\n60\\n.461\\n.458\\n.003\\n80\\n.551\\n.556\\n−.005\\nUsing R yields that the least square estimates of α and β are\\nA = .0154\\nB = .0099\\nTransforming this back into the original variable gives that the estimates of c\\nand d are\\nˆc = e−A = .9847\\n1 −ˆd = e−B = .9901\\nand so the estimated functional relationship is\\nˆP = 1 −.9847(.9901)x\\nThe residuals P −ˆP are presented in Table 9.3.\\n■\\n9.8\\nWeighted least squares\\nIn the regression model\\nY = α + βx + e\\nit often turns out that the variance of a response is not constant but rather\\ndepends on its input level. If these variances are known — at least up to a\\nproportionality constant — then the regression parameters α and β should be\\nestimated by minimizing a weighted sum of squares. Speciﬁcally, if\\nVar(Yi) = σ 2\\nwi\\nthen the estimators A and B should be chosen to minimize\\n\\x02\\ni\\n[Yi −(A + Bxi)]2\\nVar(Yi)\\n= 1\\nσ 2\\n\\x02\\ni\\nwi(Yi −A −Bxi)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 411}, page_content='9.8 Weighted least squares\\n401\\nOn taking partial derivatives with respect to A and B and setting them equal to\\n0, we obtain the following equations for the minimizing A and B.\\n\\x02\\ni\\nwiYi = A\\n\\x02\\ni\\nwi + B\\n\\x02\\ni\\nwixi\\n(9.8.1)\\n\\x02\\ni\\nwixiYi = A\\n\\x02\\ni\\nwixi + B\\n\\x02\\ni\\nwix2\\ni\\nThese equations are easily solved to yield the least squares estimators.\\nExample 9.8.a. To develop a feel as to why the estimators should be obtained\\nby minimizing the weighted sum of squares rather than the ordinary sum of\\nsquares, consider the following situation. Suppose that X1,...,Xn are indepen-\\ndent normal random variables each having mean μ and variance σ 2. Suppose\\nfurther that the Xi are not directly observable but rather only Y1 and Y2, deﬁned\\nby\\nY1 = X1 + ··· + Xk,\\nY2 = Xk+1 + ··· + Xn,\\nk < n\\nare directly observable. Based on Y1 and Y2, how should we estimate μ?\\nWhereas the best estimator of μ is clearly X = \\x05n\\ni=1 Xi/n = (Y1 + Y2)/n, let us\\nsee what the ordinary least squares estimator would be. Since\\nE[Y1] = kμ,\\nE[Y2] = (n −k)μ\\nthe least squares estimator of μ would be that value of μ that minimizes\\n(Y1 −kμ)2 + (Y2 −[n −k]μ)2\\nOn differentiating and setting equal to zero, we see that the least squares esti-\\nmator of μ — call it ˆμ — is such that\\n−2k(Y1 −k ˆμ) −2(n −k)[Y2 −(n −k) ˆμ] = 0\\nor\\n[k2 + (n −k)2] ˆμ = kY1 + (n −k)Y2\\nor\\nˆμ = kY1 + (n −k)Y2\\nk2 + (n −k)2\\nThus we see that while the ordinary least squares estimator is an unbiased esti-\\nmator of μ — since\\nE[ ˆμ] = kE[Y1] + (n −k)E[Y2]\\nk2 + (n −k)2\\n= k2μ + (n −k)2μ\\nk2 + (n −k)2\\n= μ,'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 412}, page_content='402 CHAPTER 9: Regression\\nit is not the best estimator X.\\nNow let us determine the estimator produced by minimizing the weighted sum\\nof squares. That is, let us determine the value of μ — call it μw — that mini-\\nmizes\\n(Y1 −kμ)2\\nVar(Y1)\\n+ [Y2 −(n −k)μ]2\\nVar(Y2)\\nSince\\nVar(Y1) = kσ 2,\\nVar(Y2) = (n −k)σ 2\\nthis is equivalent to choosing μ to minimize\\n(Y1 −kμ)2\\nk\\n+ [Y2 −(n −k)μ]2\\nn −k\\nUpon differentiating and then equating to 0, we see that μw, the minimizing\\nvalue, satisﬁes\\n−2k(Y1 −kμw)\\nk\\n−2(n −k)[Y2 −(n −k)μw]\\nn −k\\n= 0\\nor\\nY1 + Y2 = nμw\\nor\\nμw = Y1 + Y2\\nn\\nThat is, the weighted least squares estimator is indeed the preferred estimator\\n(Y1 + Y2)/n = X.\\n■\\nRemarks\\n(a) Assuming normally distributed data, the weighted least squares estima-\\ntors are precisely the maximum likelihood estimators. This follows because\\nthe joint density of the data Y1,...,Yn is\\nf Y1,...,Yn(y1,...,yn) =\\nn\\n\\x0c\\ni=1\\n1\\n√\\n2π(σ/√wi)\\ne−( yi−α−βxi)2/(2σ 2/wi)\\n=\\n√w1 ...wn\\n(2π)n/2σ n e−\\x05n\\ni=1 wi( yi−α−βxi)2/2σ 2\\nConsequently, the maximum likelihood estimators of α and β are precisely the\\nvalues of α and β that minimize the weighted sum of squares \\x05n\\ni=1 wi(yi −α −\\nβxi)2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 413}, page_content='9.8 Weighted least squares\\n403\\n(b) The weighted sum of squares can also be seen as the relevant quantity to\\nbe minimized by multiplying the regression equation\\nY = α + βx + e\\nby √w. This results in the equation\\nY√w = α√w + βx√w + e√w\\nNow, in this latter equation the error term e√w has mean 0 and constant vari-\\nance. Hence, the natural least squares estimators of α and β would be the values\\nof A and B that minimize\\n\\x02\\ni\\n(Yi\\n√wi −A√wi −Bxi\\n√wi)2 =\\n\\x02\\ni\\nwi(Yi −A −Bxi)2\\n(c) The weighted least squares approach puts the greatest emphasis on those\\ndata pairs having the greatest weights (and thus the smallest variance in their\\nerror term).\\n■\\nAt this point it might appear that the weighted least squares approach is not\\nparticularly useful since it requires a knowledge, up to a constant, of the vari-\\nance of a response at an arbitrary input level. However, by analyzing the model\\nthat generates the data, it is often possible to determine these values. This will\\nbe indicated by the following two examples.\\nExample 9.8.b. The following data represent travel times in a downtown area\\nof a certain city. The independent, or input, variable is the distance to be trav-\\neled.\\nDistance (miles)\\n.5\\n1\\n1.5\\n2\\n3\\n4\\n5\\n6\\n8\\n10\\nTravel time (minutes) 15.0 15.1 16.5 19.9 27.7 29.7 26.7 35.9 42 49.4\\nAssuming a linear relationship of the form\\nY = α + βx + e\\nbetween Y, the travel time, and x, the distance, how should we estimate α\\nand β? To utilize the weighted least squares approach we need to know, up to\\na multiplicative constant, the variance of Y as a function of x. We will now\\npresent an argument that Var(Y) should be proportional to x.\\nSolution. Let d denote the length of a city block. Thus a trip of distance x will\\nconsist of x/d blocks. If we let Yi,i = 1,...,x/d, denote the time it takes to\\ntraverse block i, then the total travel time can be expressed as\\nY = Y1 + Y2 + ··· + Yx/d'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 414}, page_content='404 CHAPTER 9: Regression\\nFIGURE 9.9\\nExample 9.8.b.\\nNow in many applications it is probably reasonable to suppose that the Yi are\\nindependent random variables with a common variance, and thus,\\nVar(Y) = Var(Y1) + ··· + Var(Yx/d)\\n= (x/d)Var(Y1)\\nsince Var(Yi) = Var(Y1)\\n= xσ 2,\\nwhere σ 2 = Var(Y1)/d\\nThus, it would seem that the estimators A and B should be chosen so as to\\nminimize\\n\\x02\\ni\\n(Yi −A −Bxi)2\\nxi\\nUsing the preceding data with the weights wi = 1/xi, the least squares Equa-\\ntions (9.8.1) are\\n104.22 = 5.34A + 10B\\n277.9 = 10A + 41B\\nwhich yield the solution\\nA = 12.561,\\nB = 3.714\\nA graph of the estimated regression line 12.561 + 3.714x along with the data\\npoints is presented in Figure 9.9. As a qualitative check of our solution, note'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 415}, page_content='9.8 Weighted least squares\\n405\\nthat the regression line ﬁts the data pairs best when the input levels are small,\\nwhich is as it should be since the weights are inversely proportional to the\\ninputs.\\n■\\nExample 9.8.c. Consider the relationship between Y, the number of accidents\\non a heavily traveled highway, and x, the number of cars traveling on the high-\\nway. After a little thought it would probably seem to most that the linear model\\nY = α + βx + e\\nwould be appropriate. However, as there does not appear to be any a priori\\nreason why Var(Y) should not depend on the input level x, it is not clear that\\nwe would be justiﬁed in using the ordinary least squares approach to estimate\\nα and β. Indeed, we will now argue that a weighted least squares approach\\nwith weights 1/x should be employed — that is, we should choose A and B to\\nminimize\\n\\x02\\ni\\n(Yi −A −Bxi)2\\nxi\\nThe rationale behind this claim is that it seems reasonable to suppose that Y\\nhas approximately a Poisson distribution. This is so since we can imagine that\\neach of the x cars will have a small probability of causing an accident and\\nso, for large x, the number of accidents should be approximately a Poisson\\nrandom variable. Since the variance of a Poisson random variable is equal to\\nits mean, we see that\\nVar(Y) ≃E[Y]\\nsince Y is approximately Poisson\\n= α + βx\\n≃βx\\nfor largex\\n■\\nRemarks\\n(a) Another technique that is often employed when the variance of the re-\\nsponse depends on the input level is to attempt to stabilize the variance by\\nan appropriate transformation. For example, if Y is a Poisson random variable\\nwith mean λ, then it can be shown [see Remark (b)] that\\n√\\nY has approximate\\nvariance .25 no matter what the value of λ. Based on this fact, one might try to\\nmodel E[\\n√\\nY] as a linear function of the input. That is, one might consider the\\nmodel\\n√\\nY = α + βx + e\\n(b) Proof that Var(\\n√\\nY) ≈.25 when Y is Poisson with mean λ. Consider the\\nTaylor series expansion of g(y)=√y about the value λ. By ignoring all terms'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 416}, page_content='406 CHAPTER 9: Regression\\nbeyond the second derivative term, we obtain that\\ng(y) ≈g(λ) + g′(λ)(y −λ) + g′′(λ)(y −λ)2\\n2\\n(9.8.2)\\nSince\\ng′(λ) = 1\\n2λ−1/2,\\ng′′(λ) = −1\\n4λ−3/2\\nwe obtain, on evaluating Equation (9.8.2) at y = Y, that\\n√\\nY ≈\\n√\\nλ + 1\\n2λ−1/2(Y −λ) −1\\n8λ−3/2(Y −λ)2\\nTaking expectations, and using the results that\\nE[Y −λ] = 0,\\nE[(Y −λ)2] = Var(Y) = λ\\nyields that\\nE[\\n√\\nY] ≈\\n√\\nλ −\\n1\\n8\\n√\\nλ\\nHence\\n(E[\\n√\\nY])2 ≈λ +\\n1\\n64λ −1\\n4\\n≈λ −1\\n4\\nand so\\nVar(\\n√\\nY) = E[Y] −(E[\\n√\\nY])2\\n≈λ −\\n\\x08\\nλ −1\\n4\\n\\t\\n= 1\\n4\\n9.9\\nPolynomial regression\\nIn situations where the functional relationship between the response Y and\\nthe independent variable x cannot be adequately approximated by a linear\\nrelationship, it is sometimes possible to obtain a reasonable ﬁt by considering a\\npolynomial relationship. That is, we might try to ﬁt to the data set a functional\\nrelationship of the form\\nY = β0 + β1x + β2x2 + ··· + βrxr + e'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 417}, page_content='9.9 Polynomial regression\\n407\\nwhere β0,β1,...,βr are regression coefﬁcients that would have to be estimated.\\nIf the data set consists of the n pairs (xi,Yi),i = 1,...,n, then the least squares\\nestimators of β0,...,βr — call them B0,...,Br — are those values that mini-\\nmize\\nn\\n\\x02\\ni=1\\n(Yi −B0 −B1xi −B2x2\\ni −··· −Brxr\\ni )2\\nTo determine these estimators, we take partial derivatives with respect to\\nB0 ...Br of the foregoing sum of squares, and then set these equal to 0 so\\nas to determine the minimizing values. On doing so, and then rearranging the\\nresulting equations, we obtain that the least squares estimators B0,B1,...,Br\\nsatisfy the following set of r + 1 linear equations called the normal equations.\\nn\\n\\x02\\ni=1\\nYi = B0n + B1\\nn\\n\\x02\\ni=1\\nxi + B2\\nn\\n\\x02\\ni=1\\nx2\\ni + ··· + Br\\nn\\n\\x02\\ni=1\\nxr\\ni\\nn\\n\\x02\\ni=1\\nxiYi = B0\\nn\\n\\x02\\ni=1\\nxi + B1\\nn\\n\\x02\\ni=1\\nx2\\ni + B2\\nn\\n\\x02\\ni=1\\nx3\\ni + ··· + Br\\nn\\n\\x02\\ni=1\\nxr+1\\ni\\nn\\n\\x02\\ni=1\\nx2\\ni Yi = B0\\nn\\n\\x02\\ni=1\\nx2\\ni + B1\\nn\\n\\x02\\ni=1\\nx3\\ni + ··· + Br\\nn\\n\\x02\\ni=1\\nxr+2\\ni\\n...\\n...\\n...\\nn\\n\\x02\\ni=1\\nxr\\ni Yi = B0\\nn\\n\\x02\\ni=1\\nxr\\ni + B1\\nn\\n\\x02\\ni=1\\nxr+1\\ni\\n+ ··· + Br\\nn\\n\\x02\\ni=1\\nx2r\\ni\\nIn ﬁtting a polynomial to a set of data pairs, it is often possible to deter-\\nmine the necessary degree of the polynomial by a study of the scatter diagram.\\nWe emphasize that one should always use the lowest possible degree that ap-\\npears to adequately describe the data. [Thus, for instance, whereas it is usually\\npossible to ﬁnd a polynomial of degree n that passes through all the n pairs\\n(xi,Yi),i = 1,...,n, it would be hard to ascribe much conﬁdence to such a ﬁt.]\\nEven more so than in linear regression, it is extremely risky to use a polynomial\\nﬁt to predict the value of a response at an input level x0 that is far away from the\\ninput levels xi,i =1,...,n used in ﬁnding the polynomial ﬁt. (For one thing,\\nthe polynomial ﬁt may be valid only in a region around the xi,i = 1,...,n and\\nnot including x0.)\\nExample 9.9.a. Fit a polynomial to the following data.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 418}, page_content='408 CHAPTER 9: Regression\\nFIGURE 9.10\\nx\\nY\\n1 20.6\\n2 30.8\\n3 55\\n4 71.4\\n5 97.3\\n6 131.8\\n7 156.3\\n8 197.3\\n9 238.7\\n10 291.7\\nSolution. A plot of these data (see Figure 9.10) indicates that a quadratic rela-\\ntionship\\nY = β0 + β1x + β2x2 + e\\nmight hold. Since\\n\\x02\\ni\\nxi = 55,\\n\\x02\\ni\\nx2\\ni = 385,\\n\\x02\\ni\\nx3\\ni = 3025,\\n\\x02\\ni\\nx4\\ni = 25,333\\n\\x02\\ni\\nYi = 1291.1,\\n\\x02\\ni\\nxiYi = 9549.3,\\n\\x02\\ni\\nx2\\ni Yi = 77,758.9\\nthe least squares estimates are the solution of the following set of equations.\\n1291.1 = 10B0 + 55B1 + 385B2\\n(9.9.1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 419}, page_content='9.9 Polynomial regression\\n409\\nFIGURE 9.11\\n9549.3 = 55B0 + 385B1 + 3025B2\\n77,758.9 = 385B0 + 3025B1 + 25,333B2\\nSolving these equations (see the remark following this example) yields that the\\nleast squares estimates are\\nB0 = 12.59326,\\nB1 = 6.326172,\\nB2 = 2.122818\\nThus, the estimated quadratic regression equation is\\nY = 12.59 + 6.33x + 2.12x2\\nThis equation, along with the data, is plotted in Figure 9.11.\\n■\\nRemark\\nIn matrix notation Equation (9.9.1) can be written as\\n⎡\\n⎣\\n1291.1\\n9549.3\\n77758.9\\n⎤\\n⎦=\\n⎡\\n⎣\\n10\\n55\\n385\\n55\\n385\\n3025\\n385\\n3025\\n25,333\\n⎤\\n⎦\\n⎡\\n⎣\\nB0\\nB1\\nB2\\n⎤\\n⎦\\nwhich has the solution\\n⎡\\n⎣\\nB0\\nB1\\nB2\\n⎤\\n⎦=\\n⎡\\n⎣\\n10\\n55\\n385\\n55\\n385\\n3025\\n385\\n3025\\n25,333\\n⎤\\n⎦\\n−1 ⎡\\n⎣\\n1291.1\\n9549.3\\n77,758.9\\n⎤\\n⎦'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 420}, page_content='410 CHAPTER 9: Regression\\n9.10\\nMultiple linear regression1\\nIn the majority of applications, the response of an experiment can be predicted\\nmore adequately not on the basis of a single independent input variable but\\non a collection of such variables. Indeed, a typical situation is one in which\\nthere are a set of, say, k input variables and the response Y is related to them\\nby the relation\\nY = β0 + β1x1 + ··· + βkxk + e\\nwhere xj,j = 1,...,k is the level of the jth input variable and e is a random\\nerror that we shall assume is normally distributed with mean 0 and (constant)\\nvariance σ 2. The parameters β0,β1,...,βk and σ 2 are assumed to be unknown\\nand must be estimated from the data, which we shall suppose will consist of\\nthe values of Y1,...,Yn where Yi is the response level corresponding to the\\nk input levels xi1,xi2,...,xik. That is, the Yi are related to these input levels\\nthrough\\nE[Yi] = β0 + β1xi1 + β2xi2 + ··· + βkxik\\nIf we let B0,B1,...,Bk denote estimators of β0,...,βk, then the sum of the\\nsquared differences between the Yi and their estimated expected values is\\nn\\n\\x02\\ni=1\\n(Yi −B0 −B1xi1 −B2xi2 −··· −Bkxik)2\\nThe least squares estimators are those values of B0,B1,...,Bk that minimize\\nthe foregoing.\\nTo determine the least squares estimators, we repeatedly take partial derivatives\\nof the preceding sum of squares ﬁrst with respect to B0, then to B1,..., then\\nto Bk. On equating these k + 1 equations to 0, we obtain the following set of\\nequations:\\nn\\n\\x02\\ni=1\\n(Yi −B0 −B1xi1 −B2xi2 −··· −Bkxik) = 0\\nn\\n\\x02\\ni=1\\nxi1(Yi −B0 −B1xi1 −··· −Bkxik) = 0\\nn\\n\\x02\\ni=1\\nxi2(Yi −B0 −B1xi1 −··· −Bkxik) = 0\\n...\\n1Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 421}, page_content='9.10 Multiple linear regression\\n411\\nn\\n\\x02\\ni=1\\nxik(Yi −B0 −B1xi1 −··· −Bixik) = 0\\nRewriting these equations yields that the least squares estimators B0,B1,...,Bk\\nsatisfy the following set of linear equations, called the normal equations:\\nn\\n\\x02\\ni=1\\nYi = nB0 + B1\\nn\\n\\x02\\ni=1\\nxi1 + B2\\nn\\n\\x02\\ni=1\\nxi2 + ··· + Bk\\nn\\n\\x02\\ni=1\\nxik\\n(9.10.1)\\nn\\n\\x02\\ni=1\\nxi1Yi = B0\\nn\\n\\x02\\ni=1\\nxi1 + B1\\nn\\n\\x02\\ni=1\\nx2\\ni1 + B2\\nn\\n\\x02\\ni=1\\nxi1xi2 + ··· + Bk\\nn\\n\\x02\\ni=1\\nxi1xik\\n...\\nk\\n\\x02\\ni=1\\nxikYi = B0\\nn\\n\\x02\\ni=1\\nxik + B1\\nn\\n\\x02\\ni=1\\nxikxi1 + B2\\nn\\n\\x02\\ni=1\\nxikxi2 + ··· + Bk\\nn\\n\\x02\\ni=1\\nx2\\nik\\nBefore solving the normal equations, it is convenient to introduce matrix no-\\ntation. If we let\\nY =\\n⎡\\n⎢⎢⎢⎣\\nY1\\nY2\\n...\\nYn\\n⎤\\n⎥⎥⎥⎦,\\nX =\\n⎡\\n⎢⎢⎢⎣\\n1\\nx11\\nx12\\n···\\nx1k\\n1\\nx21\\nx22\\n···\\nx2k\\n...\\n...\\n...\\n...\\n1\\nxn1\\nxn2\\n···\\nxnk\\n⎤\\n⎥⎥⎥⎦\\nβ =\\n⎡\\n⎢⎢⎢⎣\\nβ0\\nβ1\\n...\\nβk\\n⎤\\n⎥⎥⎥⎦,\\ne =\\n⎡\\n⎢⎢⎢⎣\\ne1\\ne2\\n...\\nen\\n⎤\\n⎥⎥⎥⎦\\nthen Y is an n × 1, X an n × p,β a p × 1, and e an n × 1 matrix where p ≡k + 1.\\nThe multiple regression model can now be written as\\nY = Xβ + e\\nIn addition, if we let\\nB =\\n⎡\\n⎢⎢⎢⎣\\nB0\\nB1\\n...\\nBk\\n⎤\\n⎥⎥⎥⎦'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 422}, page_content='412 CHAPTER 9: Regression\\nbe the matrix of least squares estimators, then the normal Equations (9.10.1)\\ncan be written as\\nX′XB = X′Y\\n(9.10.2)\\nwhere X′ is the transpose of X.\\nTo see that Equation (9.10.2) is equivalent to the normal Equations (9.10.1),\\nnote that\\nX′X =\\n⎡\\n⎢⎢⎢⎢⎢⎣\\n1\\n1\\n···\\n1\\nx11\\nx21\\n···\\nxn1\\nx12\\nx22\\n···\\nxn2\\n...\\n...\\n...\\nx1k\\nx2k\\n···\\nxnk\\n⎤\\n⎥⎥⎥⎥⎥⎦\\n⎡\\n⎢⎢⎢⎣\\n1\\nx11\\nx12\\n···\\nx1k\\n1\\nx21\\nx22\\n···\\nx2k\\n...\\n...\\n...\\n...\\n1\\nxn1\\nxn2\\n···\\nxnk\\n⎤\\n⎥⎥⎥⎦\\n=\\n⎡\\n⎢⎢⎢⎢⎢⎢⎢⎢⎣\\nn\\n\\x05\\ni\\nxi1\\n\\x05\\ni\\nxi2\\n···\\n\\x05\\ni\\nxik\\n\\x05\\ni\\nxi1\\n\\x05\\ni\\nx2\\ni1\\n\\x05\\ni\\nxi1xi2\\n···\\n\\x05\\ni\\nxi1xik\\n...\\n...\\n...\\n...\\n\\x05\\ni\\nxik\\n\\x05\\ni\\nxikxi1\\n\\x05\\ni\\nxikxi2\\n···\\n\\x05\\ni\\nx2\\nik\\n⎤\\n⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nand\\nX′Y =\\n⎡\\n⎢⎢⎢⎢⎢⎢⎢⎣\\n\\x05\\ni\\nYi\\n\\x05\\ni\\nxi1Yi\\n...\\n\\x05\\ni\\nxikYi\\n⎤\\n⎥⎥⎥⎥⎥⎥⎥⎦\\nIt is now easy to see that the matrix equation\\nX′XB = X′Y\\nis equivalent to the set of normal Equations (9.10.1). Assuming that (X′X)−1\\nexists, which is usually the case, we obtain, upon multiplying it by both sides\\nof the foregoing, that the least squares estimators are given by\\nB = (X′X)−1X′Y\\n(9.10.3)\\nR can be used to obtain the least square estimates.\\nExample 9.10.a. The data in Table 9.4 relate the suicide rate to the population\\nsize and the divorce rate at eight different locations.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 423}, page_content='9.10 Multiple linear regression\\n413\\nTable 9.4\\nLocation\\nPopulation in\\nThousands\\nDivorce Rate\\nper 100,000\\nSuicide Rate\\nper 100,000\\nAkron, OH\\n679\\n30.4\\n11.6\\nAnaheim, CA\\n1420\\n34.1\\n16.1\\nBuffalo, NY\\n1349\\n17.2\\n9.3\\nAustin, TX\\n296\\n26.8\\n9.1\\nChicago, IL\\n6975\\n29.1\\n8.4\\nColumbia, SC\\n323\\n18.7\\n7.7\\nDetroit, MI\\n4200\\n32.6\\n11.3\\nGary, IN\\n633\\n32.5\\n8.4\\nFit a multiple linear regression model to these data. That is, ﬁt a model of the\\nform\\nY = β0 + β1x1 + β2x2 + e\\nwhere Y is the suicide rate, x1 is the population, and x2 is the divorce rate.\\nSolution. The multiple linear regression model\\nY = β0 + β1x1 + ··· + βkxk + e\\nis written as\\n> model = lm(y ∼x1 + x2 + ··· + xk)\\nTo use R to obtain the estimated regression line for the data of Example 9.10.a,\\ninput\\n>x1 = c(679,1420,1349,296,6975,323,4200,633)\\n>x2 = c(30.4,34.1,17.2,26.8,29.1,18.7,32.6,32.5)\\n>y = c(11.6,16.1,9.3,9.1,8.4,7.7,11.3,8.4)\\n>model = lm(y ∼x1 + x2)\\n>model\\nThe following output results:\\nCall:\\nlm(formula = y ∼x1 + x2)\\nCoefﬁcients:\\n(Intercept)\\nx1\\nx2\\n3.5073534\\n-0.0002477 0.2609466'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 424}, page_content='414 CHAPTER 9: Regression\\nThus, the estimated regression equation is\\nY = 3.5073534 −0.0002477x1 + 0.2609466x2\\nThe estimated value of β1 indicates that the population does not play a major\\nrole in predicting the suicide rate (at least when the divorce rate is also given).\\nPerhaps the population density, rather than the actual population, would have\\nbeen more useful.\\n■\\nIt follows from Equation (9.10.3) that the least squares estimators B0,B1,...,Bk\\n— the elements of the matrix B — are all linear combinations of the in-\\ndependent normal random variables Y1,...,Yn and so will also be normally\\ndistributed. Indeed in such a situation — namely, when each member of a set\\nof random variables can be expressed as a linear combination of independent\\nnormal random variables — we say that the set of random variables has a joint\\nmultivariate normal distribution.\\nThe least squares estimators turn out to be unbiased. This can be shown as\\nfollows:\\nE[B] = E[(X′X)−1X′Y]\\n= E[(X′X)−1X′(Xβ + e)]\\nsinceY = Xβ + e\\n= E[(X′X)−1X′Xβ + (X′X)−1X′e]\\n= E[β + (X′X)−1X′e]\\n= β + (X′X)−1X′E[e]\\n= β\\nThe variances of the least squares estimators can be obtained from the matrix\\n(X′X)−1. Indeed, the values of this matrix are related to the covariances of the\\nBi’s. Speciﬁcally, the element in the (i + 1)st row, (j + 1)st column of (X′X)−1\\nis equal to Cov(Bi,Bj)/σ 2.\\nTo verify the preceding statement concerning Cov(Bi,Bj), let\\nC = (X′X)−1X′\\nSince X is an n × p matrix and X′ a p × n matrix, it follows that X′X is p × p, as\\nis (X′X)−1, and so C will be a p × n matrix. Let Cij denote the element in row\\ni, column j of this matrix. Now\\n⎡\\n⎢⎢⎢⎢⎢⎢⎢⎣\\nB0\\n...\\nBi−1\\n...\\nBk\\n⎤\\n⎥⎥⎥⎥⎥⎥⎥⎦\\n= B = CY =\\n⎡\\n⎢⎢⎢⎢⎢⎢⎢⎣\\nC11\\n···\\nC1n\\n...\\n...\\nCi1\\n···\\nCin\\n...\\n...\\nCp1\\n···\\nCpn\\n⎤\\n⎥⎥⎥⎥⎥⎥⎥⎦\\n⎡\\n⎢⎢⎢⎢⎢⎣\\nY1\\n...\\nYn\\n⎤\\n⎥⎥⎥⎥⎥⎦'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 425}, page_content='9.10 Multiple linear regression\\n415\\nand so\\nBi−1 =\\nn\\n\\x02\\nl=1\\nCilYl\\nBj−1 =\\nn\\n\\x02\\nr=1\\nCjrYr\\nHence\\nCov(Bi−1,Bj−1) = Cov\\n\\x03 n\\n\\x02\\nl=1\\nCilYl,\\nn\\n\\x02\\nr=1\\nCjrYr\\n\\x04\\n=\\nn\\n\\x02\\nr=1\\nn\\n\\x02\\nl=1\\nCilCjr Cov(Yl,Yr)\\nNow Yl and Yr are independent when l ̸= r, and so\\nCov(Yl,Yr) =\\n$ 0\\nif l ̸= r\\nVar(Yr)\\nif l = r\\nSince Var(Yr) = σ 2, we see that\\nCov(Bi−1,Bj−1) = σ 2\\nn\\n\\x02\\nr=1\\nCirCjr\\n(9.10.4)\\n= σ 2(CC′)ij\\nwhere (CC′)ij is the element in row i, column j of CC′.\\nIf we now let Cov(B) denote the matrix of covariances — that is,\\nCov(B) =\\n⎡\\n⎢⎢⎣\\nCov(B0,B0)\\n···\\nCov(B0,Bk)\\n...\\n...\\nCov(Bk,B0)\\n···\\nCov(Bk,Bk)\\n⎤\\n⎥⎥⎦\\nthen it follows from Equation (9.10.4) that\\nCov(B) = σ 2CC′\\n(9.10.5)\\nNow\\nC′ =\\n%\\n(X′X)−1X′&′\\n= X\\n%\\n(X′X)−1&′\\n= X(X′X)−1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 426}, page_content='416 CHAPTER 9: Regression\\nwhere the last equality follows since (X′X)−1 is symmetric (since X′X is) and so\\nis equal to its transpose. Hence\\nCC′ = (X′X)−1X′X(X′X)−1\\n= (X′X)−1\\nand so we can conclude from Equation (9.10.5) that\\nCov(B) = σ 2(X′X)−1\\n(9.10.6)\\nSince Cov(Bi, Bi) = Var(Bi), it follows that the variances of the least squares\\nestimators are given by σ 2 multiplied by the diagonal elements of (X′X)−1.\\nThe quantity σ 2 can be estimated by using the sum of squares of the residuals.\\nThat is, if we let\\nSSR =\\nn\\n\\x02\\ni=1\\n(Yi −B0 −B1xi1 −B2xi2 −··· −Bkxik)2\\nthen it can be shown that\\nSSr\\nσ 2 ∼χ2\\nn−(k+1)\\nand so\\nE\\n\\nSSR\\nσ 2\\n\\x0b\\n= n −k −1\\nor\\nE[SSR/(n −k −1)] = σ 2\\nThat is, SSR/(n −k −1) is an unbiased estimator of σ 2. In addition, as in the\\ncase of simple linear regression, SSR will be independent of the least squares\\nestimators B0,B1,...,Bk.\\nRemark\\nIf we let ri denote the ith residual\\nri = Yi −B0 −B1xi1 −··· −Bkxik,\\ni = 1,...,n\\nthen\\nr = Y −XB'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 427}, page_content='9.10 Multiple linear regression\\n417\\nwhere\\nr =\\n⎡\\n⎢⎢⎢⎣\\nr1\\nr2\\n...\\nrn\\n⎤\\n⎥⎥⎥⎦\\nHence, we may write\\nSSR =\\nn\\n\\x02\\ni=1\\nr2\\ni\\n(9.10.7)\\n= r′r\\n= (Y −XB)′(Y −XB)\\n= [Y′ −(XB)′](Y −XB)\\n= (Y′ −B′X′)(Y −XB)\\n= Y′Y −Y′XB −B′X′Y + B′X′XB\\n= Y′Y −Y′XB\\nwhere the last equality follows from the normal equations\\nX′XB = X′Y\\nBecause Y′ is 1 × n, X is n × p, and B is p × 1, it follows that Y′XB is a 1 × 1\\nmatrix. That is, Y′XB is a scalar and thus is equal to its transpose, which shows\\nthat\\nY′XB = (Y′XB)′\\n= B′X′Y\\nHence, using Equation (9.10.7) we have proven the following identity:\\nSSR = Y′Y −B′X′Y\\nThe foregoing is a useful computational formula for SSR (though one must be\\ncareful of possible roundoff error when using it).\\nExample 9.10.b. The diameter of a tree at its breast height is inﬂuenced by\\nmany factors. The data in Table 9.5 relate the diameter of a particular type of eu-\\ncalyptus tree to its age, average rainfall at its site, site’s elevation, and the wood’s\\nmean speciﬁc gravity. (The data come from R. G. Skolmen, 1975, “Shrinkage\\nand Speciﬁc Gravity Variation in Robusta Eucalyptus Wood Grown in Hawaii.”\\nUSDA Forest Service PSW-298.)\\nAssuming a linear regression model of the form\\nY = β0 + β1x1 + β2x2 + β3x3 + β4x4 + e'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 428}, page_content='418 CHAPTER 9: Regression\\nTable 9.5\\nAge (years)\\nElevation\\n(1000 ft)\\nRain-\\nfall(inches)\\nSpeciﬁc\\nGravity\\nDiameter at\\nBreast\\nHeight\\n(inches)\\n1\\n44\\n1.3\\n250\\n.63\\n18.1\\n2\\n33\\n2.2\\n115\\n.59\\n19.6\\n3\\n33\\n2.2\\n75\\n.56\\n16.6\\n4\\n32\\n2.6\\n85\\n.55\\n16.4\\n5\\n34\\n2.0\\n100\\n.54\\n16.9\\n6\\n31\\n1.8\\n75\\n.59\\n17.0\\n7\\n33\\n2.2\\n85\\n.56\\n20.0\\n8\\n30\\n3.6\\n75\\n.46\\n16.6\\n9\\n34\\n1.6\\n225\\n.63\\n16.2\\n10\\n34\\n1.5\\n250\\n.60\\n18.5\\n11\\n33\\n2.2\\n255\\n.63\\n18.7\\n12\\n36\\n1.7\\n175\\n.58\\n19.4\\n13\\n33\\n2.2\\n75\\n.55\\n17.6\\n14\\n34\\n1.3\\n85\\n.57\\n18.3\\n15\\n37\\n2.6\\n90\\n.62\\n18.8\\nwhere x1 is the age, x2 is the elevation, x3 is the rainfall, x4 is the speciﬁc gravity,\\nand Y is the tree’s diameter, test the hypothesis that β2 = 0. That is, test the\\nhypothesis that, given the other three factors, the elevation of the tree does not\\naffect its diameter.\\nSolution. We use R, starting with\\n>x1 = c(44,33,33,32,34,31,33,30,34,34,33,36,33,34,37)\\n>x2 = c(1.3,2.2,2.2,2.6,2.0,1.8,2.2,3.6,1.6,1.5,2.2,1.7,2.2,1.3,2.6)\\n>x3 = c(.63,.59,.56,.55,.54,.59,.56,.46,.63,.60,.63,.58,.55,.57,.62)\\n>y = c(18.1,19.6,16.6,16.4,16.9,17.0,20.0,16.6,16.2,18.5,18.7,19.4,17.6,\\n18.3,18.8)\\n>diameter = lm(y ∼x1 + x2 + x3)\\nNow enter\\n> summary(diameter)\\nto obtain the output\\nCall:\\nlm(formula = y ∼x1 + x2 + x3)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 429}, page_content='9.10 Multiple linear regression\\n419\\nThis output tells us that the estimated regression line is\\ny = 11.54873 + 0.05728x1 + 0.08712x2 + 7.33231x3\\nThe p-values of the tests that βi = 0 for i = 0,1,2,3, are, respectively, 0.165,\\n0.682,0.917,0.525, indicating that none of the hypotheses H0 : βi = 0, would\\nbe rejected at any reasonable signiﬁcance level. The Residual standard error,\\ngiven as equal 1.329, is the estimate of σ. The F-statistic on the bottom row is a\\ntest statistic that can be used to test the null hypothesis\\nH0 : βi = 0 for all i = 1,2,3\\nThe resulting p-value of 0.7021 indicates that this null hypothesis cannot be\\nrejected.\\n■\\nRemark\\nThe quantity\\nR2 = 1 −\\nSSR\\n\\x05\\ni\\n(Yi −Y)2\\nwhich measures the amount of reduction in the sum of squares of the residuals\\nwhen using the model\\nY = β0 + β1x1 + ··· + βnxn + e\\nas opposed to the model\\nY = β0 + e\\nis called the coefﬁcient of multiple determination.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 430}, page_content='420 CHAPTER 9: Regression\\n9.10.1\\nPredicting future responses\\nLet us now suppose that a series of experiments is to be performed using the\\ninput levels x1,...,xk. Based on our data, consisting of the prior responses\\nY1,...,Yn, suppose we would like to estimate the mean response. Since the\\nmean response is\\nE[Y|x] = β0 + β1x1 + ··· + βkxk\\na point estimate of it is simply \\x05k\\ni=0 Bixi where x0 ≡1.\\nTo determine a conﬁdence interval estimator, we need the distribution of \\x05k\\ni=0\\nBixi. Because it can be expressed as a linear combination of the independent\\nnormal random variables Yi,i=1,...,n, it follows that it is also normally dis-\\ntributed. Its mean and variance are obtained as follows:\\nE\\n\\x19 k\\n\\x02\\ni=0\\nxiBi\\n\\x1a\\n=\\nk\\n\\x02\\ni=0\\nxiE[Bi]\\n(9.10.8)\\n=\\nk\\n\\x02\\ni=0\\nxiβi\\nsince E[Bi] = βi\\nThat is, it is an unbiased estimator. Also, using the fact that the variance of a\\nrandom variable is equal to the covariance between that random variable and\\nitself, we see that\\nVar\\n\\x03 k\\n\\x02\\ni=0\\nxiBi\\n\\x04\\n= Cov\\n⎛\\n⎝\\nk\\n\\x02\\ni=0\\nxiBi,\\nk\\n\\x02\\nj=0\\nxjBj\\n⎞\\n⎠\\n(9.10.9)\\n=\\nk\\n\\x02\\ni=0\\nk\\n\\x02\\nj=0\\nxixjCov(Bi,Bj)\\nIf we let x denote the matrix\\nx =\\n⎡\\n⎢⎢⎢⎣\\nx0\\nx1\\n...\\nxk\\n⎤\\n⎥⎥⎥⎦\\nthen, recalling that Cov(Bi,Bj)/σ 2 is the element in the (i + 1)st row and (j +\\n1)st column of (X′X)−1, we can express Equation (9.10.9) as\\nVar\\n\\x03 k\\n\\x02\\ni=0\\nxiBi\\n\\x04\\n= x′(X′X)−1xσ 2\\n(9.10.10)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 431}, page_content='9.10 Multiple linear regression\\n421\\nUsing Equations (9.10.8) and (9.10.10), we see that\\nk\\x05\\ni=0\\nxiBi −\\nk\\x05\\ni=0\\nxiβi\\nσ\\n\\x13\\nx′(X′X)−1x\\n∼N(0,1)\\nIf we now replace σ by its estimator √SSR/(n −k −1) we obtain, by the usual\\nargument, that\\nk\\x05\\ni=0\\nxiBi −\\nk\\x05\\ni=0\\nxiβi\\n\\x16\\nSSR\\n(n −k −1)\\n\\x13\\nx′(X′X)−1x\\n∼tn−k−1\\nwhich gives rise to the following conﬁdence interval estimator of \\x05k\\ni=0 xiβi.\\nConﬁdence interval estimate of E[Y|x] = \\x02 k\\ni=0 xiβi, (x 0 ≡1)\\nA 100(1 −a) percent conﬁdence interval estimate of \\x05k\\ni=0 xiβi is given by\\nk\\n\\x02\\ni=0\\nxibi ±\\n\\x16\\nssr\\n(n −k −1)\\n\\x13\\nx′(X′X)−1x\\nta/2,n−k−1\\nwhere b0,...,bk are the values of the least squares estimators B0,B1,...,Bk,\\nand ssr is the value of SSR.\\nExample 9.10.c. A steel company is planning to produce cold reduced sheet\\nsteel consisting of .15 percent copper at an annealing temperature of 1150 (de-\\ngrees F), and is interested in estimating the average (Rockwell 30-T) hardness of\\na sheet. To determine this, they have collected the data shown in Table 9.6 on\\n10 different specimens of sheet steel having different copper contents and an-\\nnealing temperatures. Estimate the average hardness and determine an interval\\nin which it will lie with 95 percent conﬁdence.\\nSolution. We can use R to ﬁnd the desired conﬁdence interval estimator for\\nthe mean hardness. Calling the model “hardness”, start as usual with inputting\\nthe following:\\n>x1 = c(.02,.03,.03,.04,.10,.15,.15,.09,.13,.09)\\n>x2 = (1.05,1.20,1.25,1.30,1.30,1.00,1.10,1.20,1.40,1.40)\\n>y = c(79.2,64.0,55.7,56.3,58.6,84.3,70.4,61.3,51.3,49.8)\\n>hardness = lm(y ∼x1 + x2)\\nTo indicate that we want to determine the desired conﬁdence interval of the\\nmean response at values x1 and x2, we now input these values, calling them'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 432}, page_content='422 CHAPTER 9: Regression\\nTable 9.6\\nHardness\\nCopper Content\\nAnnealing\\nTemperature\\n(units of 1000◦F)\\n79.2\\n.02\\n1.05\\n64.0\\n.03\\n1.20\\n55.7\\n.03\\n1.25\\n56.3\\n.04\\n1.30\\n58.6\\n.10\\n1.30\\n84.3\\n.15\\n1.00\\n70.4\\n.15\\n1.10\\n61.3\\n.09\\n1.20\\n51.3\\n.13\\n1.40\\n49.8\\n.09\\n1.40\\nnewdata, as follows:\\n>newdata=data.frame(x1 = .15,x2 = 1.15)\\nTo obtain the desired 99 percent conﬁdence interval for the mean response at\\nthe newdata values, use the “predict” function as follows:\\n>predict(hardness, newdata, interval=“conﬁdence”, level = 0.99)\\nIf you now press return, R yields the following:\\nﬁt\\nlwr\\nupr\\n1\\n69.86226\\n63.82064\\n75.90388\\nThus, the estimate of the mean hardness is 69.862, and we can say, with 99\\npercent conﬁdence, that the mean lies in the interval (63.82064, 75.90388).\\nThe default conﬁdence level when using R is 0.95. Thus, when we desire a 95\\npercent conﬁdence interval, interval=“conﬁdence”, level = 0.99 can be short-\\nened to interval=“conﬁdence”.\\n■\\nWhen it is only a single experiment that is going to be performed at the input\\nlevels x1,...,xk, we are usually more concerned with predicting the actual re-\\nsponse than its mean value. That is, we are interested in utilizing our data set\\nY1,...,Yn to predict\\nY(x) =\\nk\\n\\x02\\ni=0\\nβixi + e,\\nwhere x0 = 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 433}, page_content='9.10 Multiple linear regression\\n423\\nA point prediction is given by \\x05k\\ni=0 Bixi where Bi is the least squares estimator\\nof βi based on the set of prior responses Y1,...,Yn,i = 1,...,k.\\nTo determine a prediction interval for Y(x), note ﬁrst that since B0,...,Bk are\\nbased on prior responses, it follows that they are independent of Y(x). Hence,\\nit follows that Y(x) −\\x05k\\ni=0 Bixi is normal with mean 0 and variance given by\\nVar\\n\\x19\\nY(x) −\\nk\\n\\x02\\ni=0\\nBixi\\n\\x1a\\n= Var[Y(x)] + Var\\n\\x03 k\\n\\x02\\ni=0\\nBixi\\n\\x04\\nby independence\\n= σ 2 + σ 2x′(X′X)−1x\\nfrom Equation (9.10.10)\\nand so\\nY(x) −\\nk\\x05\\ni=0\\nBixi\\nσ\\n\\x13\\n1 + x′(X′X)−1x\\n∼N(0,1)\\nwhich yields, upon replacing σ by its estimator, that\\nY(x) −\\nk\\x05\\ni=0\\nBixi\\n\\x16\\nSSR\\n(n −k −1)\\n\\x13\\n1 + x′(X′X)−1x\\n∼tn−k−1\\nWe thus have:\\nPrediction Interval for Y(x)\\nWith 100(1 −a) percent conﬁdence Y(x) will lie between\\nk\\n\\x02\\ni=0\\nxibi ±\\n\\x16\\nssr\\n(n −k −1)\\n\\x13\\n1 + x′(X′X)−1x\\nta/2,n−k−1\\nwhere b0,...,bk are the values of the least squares estimators B0,B1,...,Bk,\\nand ssr is the value of SSR.\\nR can be used to obtain prediction intervals for the response y at given values\\nx1,...,xk. One does exactly the same as when a conﬁdence interval for the\\nmean response at those input values is desired, with the command\\n>\\npredict(model name, newdata, interval=“conﬁdence”, level = 0.99)\\nreplaced by\\n>\\npredict(model name, newdata, interval=“prediction”, level = 0.99)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 434}, page_content='424 CHAPTER 9: Regression\\nFor instance, in Example 9.10.c, if we had wanted a 99 percent prediction in-\\nterval for the hardness resulting when x1 = .15,x2 = 1.15, we would do the\\nfollowing\\n>\\nnewdata=data.frame(x1 = .15,x2 = 1.15)\\n>\\npredict(hardness, newdata, interval=“prediction”, level = 0.99)\\nto obtain the following output:\\nﬁt\\nlwr\\nupr\\n1\\n69.86226\\n57.48807\\n82.23645\\nThus, whereas the 99 percent conﬁdence interval for the expected value of the\\nhardness at inputs x1 = .15,x2 = 1.15 is (63.82, 75.9), the 99 percent prediction\\ninterval for the actual value of the hardness at these inputs is the wider interval\\n(57.49,82.24).\\n9.10.2\\nDummy variables for categorical data\\nSuppose that in determining an appropriate multiple regression model for\\npredicting a person’s blood cholesterol level a researcher has decided on the\\nfollowing ﬁve independent variables:\\n1. number of pounds overweight\\n2. number of pounds underweight\\n3. average number of hours of exercise per week\\n4. average number of calories due to saturated fats eaten daily\\n5. whether a smoker or not\\nWhereas each of the ﬁrst four variables takes on values in some interval, the\\nﬁnal variable is a categorical variable that indicates whether the person under\\nconsideration has or does not have a certain characteristic (which, in this case,\\nis the characteristic of being a smoker). To determine which category the person\\nbelongs to, let\\nx5 =\\n$ 1,\\nif person is a smoker\\n0,\\nif person is not a smoker\\nThe researcher can now try to ﬁt the multiple regression model\\nY = β0 + β1x1 + β2x2 + β3x3 + β4x4 + β5x5 + e\\nwhere x1 is the number of pounds the individual is overweight, x2 is the num-\\nber of pounds the individual is underweight, x3 is the average number of hours\\nthe individual exercises per week, x4 is the average number of calories due to'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 435}, page_content='9.11 Logistic regression models for binary output data\\n425\\nsaturated fats that is eaten daily, x5 is as above, and Y is the individual’s choles-\\nterol level. The variable x5 is called a dummy variable, as its only purpose is to\\nindicate whether or not the Y value is determined from data having a particular\\ncharacteristic.\\nOne might wonder at this point why a dummy variable is used rather than\\njust running separate multiple regressions for smokers and for nonsmokers.\\nThe main reason for using a dummy variable is that we can use all the data\\nin a single regression, thus yielding more precise estimates than if we broke\\nthe data into two parts (one for smokers and the other for nonsmokers) and\\nthen used the divided data to run separate regressions. However, it is important\\nto understand what is being assumed when dummy variables are being used.\\nNamely, we are assuming that if Ys stands for the cholesterol level of a smoker,\\nand Yn, the cholesterol level of a nonsmoker, then for speciﬁed values of x1, x2,\\nx3, and x4\\nE[Ys] = β0 + β5 + β1x1 + β2x2 + β3x3 + β4x4\\nand\\nE[Yn] = β0 + β1x1 + β2x2 + β3x3 + β4x4\\nIn other words, in using the model with a dummy variable we are assuming\\nthat if a smoker and nonsmoker had the same values for the four quantitative\\nvariables x1, x2, x3, x4 then the difference between their mean cholesterol levels\\nwould always be a constant, no matter what are the values of x1, x2, x3, x4. Thus,\\nfor instance, the dummy variable model assumes that the amount that one is\\noverweight has the same effect on raising the expected cholesterol level on a\\nsmoker as it does on a nonsmoker. Because this might seem like a questionable\\nassumption, it is typically preferable when the data set is large enough to use\\ntwo regression models rather than combining into one model by the use of a\\ndummy variable.\\nIn situations where there are multiple qualitative characteristics that the re-\\nsearcher feels are relevant it might be necessary to utilize dummy variables, for\\notherwise the data set may become too fragmented to yield reliable estimates\\nof the regression parameters. So, for instance, if the researcher felt that the sex\\nof the person was also a relevant factor, then the researcher could utilize a mul-\\ntiple regression model having two dummy variables: namely x5 and\\nx6 =\\n$ 1,\\nif person is a male\\n0,\\nif person is a female\\n9.11\\nLogistic regression models for binary output data\\nIn this section we consider experiments that result in either a success or a fail-\\nure. We will suppose that these experiments can be performed at various levels,'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 436}, page_content='426 CHAPTER 9: Regression\\nFIGURE 9.12\\nLogistic regression functions.\\nand that an experiment performed at level x will result in a success with prob-\\nability p(x), −∞< x < ∞. If p(x) is of the form\\np(x) =\\nea+bx\\n1 + ea+bx\\nthen the experiments are said to come from a logistic regression model and p(x)\\nis called the logistics regression function. If b > 0, then p(x) = 1/[e−(a+bx) + 1]\\nis an increasing function that converges to 1 as x →∞; if b < 0, then p(x)\\nis a decreasing function that converges to 0 as x →∞. (When b = 0, p(x) is\\nconstant.) Plots of logistics regression functions are given in Figure 9.12. Notice\\nthe s-shape of these curves.\\nWriting p(x) = 1 −[1/(1 + e a+bx)] and differentiating give that\\n∂\\n∂x p(x) =\\nbe a+bx\\n(1 + e a+bx)2 = b p(x)[1 −p(x)]\\nThus the rate of change of p(x) depends on x and is largest at those values of\\nx for which p(x) is near .5. For instance, at the value x such that p(x)=.5, the\\nrate of change is ∂\\n∂x p(x) = .25b, whereas at that value x for which p(x) = .8 the\\nrate of change is .16b.\\nIf we let o(x) be the odds for success when the experiment is run at level x, then\\no(x) =\\np(x)\\n1 −p(x) = ea+bx\\nThus, when b > 0, the odds increase exponentially in the input level x; when\\nb < 0, the odds decrease exponentially in the input level x. Taking logs of the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 437}, page_content=\"9.11 Logistic regression models for binary output data\\n427\\npreceding shows that the log odds, called the logit, is a linear function:\\nlog[o(x)] = a + bx\\nThe parameters a and b of the logistic regression function are assumed to be\\nunknown and need to be estimated. This can be accomplished by using the\\nmaximum likelihood approach. That is, suppose that the experiment is to be\\nperformed at levels x1,...,xk. Let Yi be the result (either 1 if a success or 0 if a\\nfailure) of the experiment when performed at level xi. Then, using the Bernoulli\\ndensity function (that is, the binomial density for a single trial), gives\\nP{Yi = yi} = [p(xi)] yi[1 −p(xi)]1−yi =\\n\\x08\\nea+bxi\\n1 + ea+bxi\\n\\tyi \\x08\\n1\\n1 + ea+bxi\\n\\t1−yi\\n,\\nyi = 0,1\\nThus, the probability that the experiment at level xi results in outcome yi, for\\nall i = 1,...,k, is\\nP{Yi = yi,i = 1,...,k}\\n=\\n\\x0c\\ni\\n\\x08\\nea+bxi\\n1 + ea+bxi\\n\\tyi \\x08\\n1\\n1 + ea+bxi\\n\\t1−yi\\n=\\n\\x0c\\ni\\n'\\nea+bxi(yi\\n1 + ea+bxi\\nTaking logarithms gives that\\nlog(P{Yi = yi,i = 1,...,k}) =\\nk\\n\\x02\\ni=1\\nyi(a + bxi)−\\nk\\n\\x02\\ni=1\\nlog\\n%\\n1 + ea+bxi\\n&\\nThe maximum likelihood estimates can now be obtained by numerically ﬁnd-\\ning the values of a and b that maximize the preceding likelihood. However,\\nbecause the likelihood is nonlinear this requires an iterative approach; conse-\\nquently, one typically resorts to specialized software to obtain the estimates.\\nR can be used to determine the estimates of a and b in the logistics re-\\ngression model. The logistic regression model is written glm(y ∼x). To use\\nR suppose, for instance, that an experiment has been run 7 times, at levels\\nx = (5,9,3,12,22,24,30) and with results (0,1,0,1,0,1,1), with a response 1\\nmeaning that experiment was a success and a response 0 indicating that it was\\na failure. Then the estimated logistics equation is obtained as follows:\\n>x = c(5,9,3,12,22,24,30)\\n>y = c(0,1,0,1,0,1,1)\\n>model = glm(y ∼x)\\n>model\"),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 438}, page_content='428 CHAPTER 9: Regression\\nPressing enter now yields the following output\\nCall: glm(formula= y ∼x)\\nCoefﬁcients:\\n(Intercept)\\nx\\n0.22205\\n0.02329\\nThat is, the estimates of a and b in the logistics regression model\\np(x) =\\nea+bx\\n1 + ea+bx\\nare, respectively, 0.22205 and 0.02329.\\nWhereas the logistic regression model is the most frequently used model when\\nthe response data are binary, other models are often employed. For instance\\nin situations where it is reasonable to suppose that p(x), the probability of a\\npositive response when the input level is x, is an increasing function of x, it is\\noften supposed that p(x) has the form of a speciﬁed probability distribution\\nfunction. Indeed, when b > 0, the logistic regression model is of this form be-\\ncause p(x) is equal to the distribution function of a logistic random variable\\n(Section 5.9) with parameters μ = −a/b, ν = 1/b. Another model of this type\\nis the probit model, which supposes that for some constants, α, β > 0\\np(x) = \\x0b(α + βx) =\\n1\\n√\\n2π\\n) α+βx\\n−∞\\ne−y2/2 dy\\nIn other words p(x) is equal to the probability that a standard normal random\\nvariable is less than α + βx.\\nExample 9.11.a. A common assumption for whether an animal becomes sick\\nwhen exposed to a chemical at dosage level x is to assume a threshold model,\\nwhich supposes that each animal has a random threshold and will become ill if\\nthe dosage level exceeds that threshold. The exponential distribution has some-\\ntimes been used as the threshold distribution. For instance, a model considered\\nin Freedman and Zeisel (“From Mouse to Man: The Quantitative Assessment\\nof Cancer Risks,” Statistical Science, 1988, 3, 1, 3–56) supposes that a mouse\\nexposed to x units of DDT (measured in ppm) will contract cancer of the liver\\nwith probability\\np(x) = 1 −e−ax,\\nx > 0\\nBecause of the lack of memory of the exponential distribution, this is equiva-\\nlent to assuming that if the mouse who is still healthy after receiving a (partial)\\ndosage of level x is as good as it was before receiving any dosage.\\nIt was reported in Freedman and Zeisel that 84 of 111 mice exposed to DDT at\\na level of 250 ppm developed cancer. Therefore, α can be estimated from'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 439}, page_content='Problems 429\\n1 −e−250ˆα = 84\\n111\\nor\\nˆα = −log(27/111)\\n250\\n= .005655\\n■\\nProblems\\n1. The following data relate x, the moisture of a wet mix of a certain prod-\\nuct, to Y, the density of the ﬁnished product.\\nxi\\nYi\\n5\\n7.4\\n6\\n9.3\\n7\\n10.6\\n10\\n15.4\\n12 18.1\\n15 22.2\\n18 24.1\\n20\\n24.8\\na.\\nDraw a scatter diagram.\\nb.\\nFit a linear curve to the data.\\n2. The following data relate the number of units of a good that were ordered\\nas a function of the price of the good at six different locations.\\nNumber ordered\\n88 112 123 136 158 172\\nPrice\\n50\\n40\\n35\\n30\\n20\\n15\\nHow many units do you think would be ordered if the price were 25?\\n3. The corrosion of a certain metallic substance has been studied in dry\\noxygen at 500 degrees centigrade. In this experiment, the gain in weight\\nafter various periods of exposure was used as a measure of the amount of\\noxygen that had reacted with the sample. Here are the data:\\nHours\\nPercent Gain\\n1.0\\n.02\\n2.0\\n.03\\n2.5\\n.035\\n3.0\\n.042\\n3.5\\n.05\\n4.0\\n.054\\na.\\nPlot a scatter diagram.\\nb.\\nFit a linear relation.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 440}, page_content='430 CHAPTER 9: Regression\\nc.\\nPredict the percent weight gain when the metal is exposed for 3.2\\nhours.\\n4. The following data indicate the relationship between x, the speciﬁc grav-\\nity of a wood sample, and Y, its maximum crushing strength in compres-\\nsion parallel to the grain.\\nxi\\nyi(psi)\\nxi\\nyi(psi)\\n.41\\n1850\\n.39\\n1760\\n.46\\n2620\\n.41\\n2500\\n.44\\n2340\\n.44\\n2750\\n.47\\n2690\\n.43\\n2730\\n.42\\n2160\\n.44\\n3120\\na.\\nPlot a scatter diagram. Does a linear relationship seem reasonable?\\nb.\\nEstimate the regression coefﬁcients.\\nc.\\nPredict the maximum crushing strength of a wood sample whose\\nspeciﬁc gravity is .43.\\n5. The following data indicate the gain in reading speed versus the number\\nof weeks in the program of 10 students in a speed-reading program.\\nNumber of\\nWeeks\\nSpeed Gain\\n(wods/min)\\n2\\n21\\n3\\n42\\n8\\n102\\n11\\n130\\n4\\n52\\n5\\n57\\n9\\n105\\n7\\n85\\n5\\n62\\n7\\n90\\na.\\nPlot a scatter diagram to see if a linear relationship is indicated.\\nb.\\nFind the least squares estimates of the regression coefﬁcients.\\nc.\\nEstimate the expected gain of a student who plans to take the pro-\\ngram for 7 weeks.\\n6. Infrared spectroscopy is often used to determine the natural rubber con-\\ntent of mixtures of natural and synthetic rubber. For mixtures of known\\npercentages, the infrared spectroscopy gave the following readings:\\nPercentage\\n0\\n20\\n40\\n60\\n80\\n100\\nReading\\n.734 .885\\n1.050 1.191 1.314 1.432\\nIf a new mixture gives an infrared spectroscopy reading of 1.15, estimate\\nits percentage of natural rubber.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 441}, page_content='Problems 431\\n7. The following table gives the 1996 SAT mean math and verbal scores in\\neach state and the District of Columbia, along with the percentage of\\nthe states’ graduating high school students that took the examination.\\nUse data relating to the ﬁrst 20 locations listed (Alabama to Maine) to\\ndevelop a prediction of the mean student mathematics score in terms of\\nthe percentage of students that take the examination. Then compare your\\npredicted values for the next 5 states (based on the percentage taking the\\nexam in these states) with the actual mean math scores.\\nTable 9.7 SAT Mean Scores by State, 1996 (re-\\ncentered scale).\\n1996\\n% Graduates\\nVerbal\\nMath\\nTaking SAT\\nAlabama. . . .\\n565\\n558\\n8\\nAlaska. . . . .\\n521\\n513\\n47\\nArizona . . . .\\n525\\n521\\n28\\nArkansas . . .\\n566\\n550\\n6\\nCalifornia . . .\\n495\\n511\\n45\\nColorado . . .\\n536\\n538\\n30\\nConnecticut . .\\n507\\n504\\n79\\nDelaware . . .\\n508\\n495\\n66\\nDist. of Columbia\\n489\\n473\\n50\\nFlorida. . . . .\\n498\\n496\\n48\\nGeorgia . . . .\\n484\\n477\\n63\\nHawaii. . . . .\\n485\\n510\\n54\\nIdaho . . . . .\\n543\\n536\\n15\\nIllinois . . . . .\\n564\\n575\\n14\\nIndiana . . . .\\n494\\n494\\n57\\nIowa. . . . . .\\n590\\n600\\n5\\nKansas . . . .\\n579\\n571\\n9\\nKentucky . . .\\n549\\n544\\n12\\nLouisiana . . .\\n559\\n550\\n9\\nMaine . . . . .\\n504\\n498\\n68\\nMaryland . . .\\n507\\n504\\n64\\nMassachusetts.\\n507\\n504\\n80\\nMichigan . . .\\n557\\n565\\n11\\nMinnesota . . .\\n582\\n593\\n9\\nMississippi. . .\\n569\\n557\\n4\\nMissouri . . . .\\n570\\n569\\n9\\nMontana. . . .\\n546\\n547\\n21\\nNebraska . . .\\n567\\n568\\n9\\ncontinued on next page'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 442}, page_content='432 CHAPTER 9: Regression\\nTable 9.7 (continued)\\n1996\\n% Graduates\\nVerbal\\nMath\\nTaking SAT\\nNevada . . . .\\n508\\n507\\n31\\nNew Hampshire\\n520\\n514\\n70\\nNew Jersey . .\\n498\\n505\\n69\\nNew Mexico. .\\n554\\n548\\n12\\nNew York . . .\\n497\\n499\\n73\\nNorth Carolina.\\n490\\n486\\n59\\nNorth Dakota .\\n596\\n599\\n5\\nOhio . . . . .\\n536\\n535\\n24\\nOklahoma. . .\\n566\\n557\\n8\\nOregon . . . .\\n523\\n521\\n50\\nPennsylvania .\\n498\\n492\\n71\\nRhode Island .\\n501\\n491\\n69\\nSouth Carolina.\\n480\\n474\\n57\\nSouth Dakota .\\n574\\n566\\n5\\nTennessee. . .\\n563\\n552\\n14\\nTexas . . . . .\\n495\\n500\\n48\\nUtah . . . . .\\n583\\n575\\n4\\nVermont. . . .\\n506\\n500\\n70\\nVirginia . . . .\\n507\\n496\\n68\\nWashington . .\\n519\\n519\\n47\\nWest Virginia .\\n526\\n506\\n17\\nWisconsin . .\\n577\\n586\\n8\\nWyoming . . .\\n544\\n544\\n11\\nNational Average\\n505\\n508\\n41\\nSource: The College Board.\\n8. Verify Equation (9.3.3), which states that\\nVar(A) =\\nσ 2\\nn\\x05\\ni=1\\nx2\\ni\\nn\\nn\\x05\\ni=1\\n(xi −¯x)2\\n9. In Problem 4,\\na.\\nEstimate the variance of an individual response.\\nb.\\nDetermine a 90 percent conﬁdence interval for the variance.\\n10. Verify that\\nSSR = SxxSYY −S2\\nxY\\nSxx'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 443}, page_content='Problems 433\\n11. The following table relates the number of sunspots that appeared each\\nyear from 1970 to 1983 to the number of auto accident deaths during\\nthat year. Test the hypothesis that the number of auto deaths is not af-\\nfected by the number of sunspots. (The sunspot data are from Jastrow\\nand Thompson, Fundamentals and Frontiers of Astronomy, and the auto\\ndeath data are from General Statistics of the U.S. 1985.)\\nYear\\nSunspots\\nAuto Accident Deaths\\n(1000s)\\n’70\\n165\\n54.6\\n’71\\n89\\n53.3\\n’72\\n55\\n56.3\\n’73\\n34\\n49.6\\n’74\\n9\\n47.1\\n’75\\n30\\n45.9\\n’76\\n59\\n48.5\\n’77\\n83\\n50.1\\n’78\\n109\\n52.4\\n’79\\n127\\n52.5\\n’80\\n153\\n53.2\\n’81\\n112\\n51.4\\n’82\\n80\\n46\\n’83\\n45\\n44.6\\n12. The following data set presents the heights of 12 male law school class-\\nmates whose law school examination scores were roughly equal. It also\\ngives their ﬁrst-year salaries. Each of them went into corporate law. The\\nheight is in inches and the salary in units of $1000.\\nHeight\\nSalary\\n64\\n91\\n65\\n94\\n66\\n88\\n67\\n103\\n69\\n77\\n70\\n96\\n72\\n105\\n72\\n88\\n74\\n122\\n74\\n102\\n75\\n90\\n76\\n114'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 444}, page_content='434 CHAPTER 9: Regression\\na.\\nDo the above data establish the hypothesis that a lawyer’s salary is\\nrelated to his height? Use the 5 percent level of signiﬁcance.\\nb.\\nWhat was the null hypothesis in part (a)?\\n13. Suppose in the simple linear regression model\\nY = α + βx + e\\nthat 0 < β < 1.\\na.\\nShow that if x < α/(1 −β), then\\nx < E[Y] <\\nα\\n1 −β\\nb.\\nShow that if x > α/(1 −β), then\\nx > E[Y] >\\nα\\n1 −β\\nand conclude that E[Y] is always between x and α/(1 −β).\\n14. A study has shown that a good model for the relationship between X and\\nY, the ﬁrst and second year batting averages of a randomly chosen major\\nleague baseball player, is given by the equation\\nY = .159 + .4X + e\\nwhere e is a normal random variable with mean 0. That is, the model is\\na simple linear regression with a regression toward the mean.\\na.\\nIf a player’s batting average is .200 in his ﬁrst year, what would you\\npredict for the second year?\\nb.\\nIf a player’s batting average is .265 in his ﬁrst year, what would you\\npredict for the second year?\\nc.\\nIf a player’s batting average is .310 in his ﬁrst year, what would you\\npredict for the second year?\\n15. Experienced ﬂight instructors have claimed that praise for an exception-\\nally ﬁne landing is typically followed by a poorer landing on the next\\nattempt, whereas criticism of a faulty landing is typically followed by an\\nimproved landing. Should we thus conclude that verbal praise tends to\\nlower performance levels, whereas verbal criticism tends to raise them?\\nOr is some other explanation possible?\\n16. Verify Equation (9.4.3).\\n17. The following data represent the relationship between the number of\\nalignment errors and the number of missing rivets for 10 different air-\\ncraft.\\na.\\nPlot a scatter diagram.\\nb.\\nEstimate the regression coefﬁcients.\\nc.\\nTest the hypothesis that α = 1.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 445}, page_content='Problems 435\\nNumber of Missing\\nRivets = x\\nNumber of Alignment\\nErrors = y\\n13\\n7\\n15\\n7\\n10\\n5\\n22\\n12\\n30\\n15\\n7\\n2\\n25\\n13\\n16\\n9\\n20\\n11\\n15\\n8\\nd.\\nEstimate the expected number of alignment errors of a plane having\\n24 missing rivets.\\ne.\\nCompute a 90 percent conﬁdence interval estimate for the quantity\\nin (d).\\n18. The following are the average scores on the mathematics part of the\\nScholastic Aptitude Test (SAT) for some of the years from 1994 to 2009.\\nYear\\nSAT Score\\n1994\\n504\\n1996\\n508\\n1998\\n512\\n2000\\n514\\n2002\\n516\\n2004\\n518\\n2005\\n520\\n2007\\n515\\n2009\\n515\\nAssuming a simple linear regression model, predict the average scores in\\nthe years 1997, 2006, 2008, and 2010.\\n19.\\na.\\nDraw a scatter diagram of cigarette consumption versus death rate\\nfrom bladder cancer.\\nb.\\nDoes the diagram indicate the possibility of a linear relationship?\\nc.\\nFind the best linear ﬁt.\\nd.\\nIf next year’s average cigarette consumption is 2500, what is your\\nprediction of the death rate from bladder cancer?\\n20.\\na.\\nDraw a scatter diagram relating cigarette use and death rates from\\nlung cancer.\\nb.\\nEstimate the regression parameters α and β.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 446}, page_content='436 CHAPTER 9: Regression\\nc.\\nTest at the .05 level of signiﬁcance the hypothesis that cigarette con-\\nsumption does not affect the death rate from lung cancer.\\nd.\\nWhat is the p-value of the test in part (c)?\\n21.\\na.\\nDraw a scatter diagram of cigarette use versus death rate from kidney\\ncancer.\\nb.\\nEstimate the regression line.\\nc.\\nWhat is the p-value in the test that the slope of the regression line is\\n0?\\nd.\\nDetermine a 90 percent conﬁdence interval for the mean death rate\\nfrom kidney cancer in a state whose citizens smoke an average of\\n3400 cigarettes per year.\\n22.\\na.\\nDraw a scatter diagram of cigarettes smoked versus death rate from\\nleukemia.\\nb.\\nEstimate the regression coefﬁcients.\\nc.\\nTest the hypothesis that there is no regression of the death rate from\\nleukemia on the number of cigarettes used. That is, test that β = 0.\\nd.\\nDetermine a 90 percent prediction interval for the leukemia death\\nrate in a state whose citizens smoke an average of 2500 cigarettes.\\n23.\\na.\\nEstimate the variances in Problems 19 through 22.\\nb.\\nDetermine a 95 percent conﬁdence interval for the variance in the\\ndata relating to lung cancer.\\nc.\\nBreak up the lung cancer data into two parts — the ﬁrst correspond-\\ning to states whose average cigarette consumption is less than 2300,\\nand the second greater. Assume a linear regression model for both\\nsets of data. How would you test the hypothesis that the variance of\\na response is the same for both sets?\\nd.\\nDo the test in part (c) at the .05 level of signiﬁcance.\\n24. Plot the standardized residuals from the data of Problem 1. What does\\nthe plot indicate about the assumptions of the linear regression model?\\n25. It is difﬁcult and time consuming to measure directly the amount of pro-\\ntein in a liver sample. As a result, medical laboratories often make use\\nof the fact that the amount of protein is related to the amount of light\\nthat would be absorbed by the sample. As a result, a spectrometer that\\nemits light is shined on a solution that contains the liver sample and the\\namount of light absorbed is then used to estimate the amount of protein.\\nThe above procedure was tried on ﬁve samples having known amounts\\nof protein, with the following data resulting.\\nLight Absorbed\\nAmount of Protein (mg)\\n.44\\n2\\n.82\\n16\\n1.20\\n30\\n1.61\\n46\\n1.83\\n55'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 447}, page_content='Problems 437\\na.\\nDetermine the coefﬁcient of determination.\\nb.\\nDoes this appear to be a reasonable way of estimating the amount\\nof protein in a liver sample?\\nc.\\nWhat is the estimate of the amount of protein when the light ab-\\nsorbed is 1.5?\\nd.\\nDetermine a prediction interval, in which we can have 90 percent\\nconﬁdence, for the quantity in part (c).\\n26. The determination of the shear strength of spot welds is relatively dif-\\nﬁcult, whereas measuring the weld diameter of spot welds is relatively\\nsimple. As a result, it would be advantageous if shear strength could be\\npredicted from a measurement of weld diameter. The data are as fol-\\nlows:\\nShear Strength (psi)\\nWeld Diameter (.0001 in.)\\n370\\n400\\n780\\n800\\n1210\\n1250\\n1560\\n1600\\n1980\\n2000\\n2450\\n2500\\n3070\\n3100\\n3550\\n3600\\n3940\\n4000\\n3950\\n4000\\na.\\nDraw a scatter diagram.\\nb.\\nFind the least squares estimates of the regression coefﬁcients.\\nc.\\nTest the hypothesis that the slope of the regression line is equal to 1\\nat the .05 level of signiﬁcance.\\nd.\\nEstimate the expected value of shear strength when the weld diame-\\nter is 2500.\\ne.\\nFind a prediction interval such that, with 95 percent conﬁdence, the\\nvalue of shear strength corresponding to a weld diameter of .2250\\ninch will be contained in it.\\nf.\\nPlot the standardized residuals.\\ng.\\nDoes the plot in part (f) support the assumptions of the model?\\n27. The following are the ages and weights of a random sample of 10 high\\nschool male students:'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 448}, page_content='438 CHAPTER 9: Regression\\nAge\\nWeight\\n14\\n129\\n16\\n173\\n18\\n188\\n15\\n121\\n17\\n190\\n16\\n166\\n14\\n133\\n16\\n155\\n15\\n152\\n14\\n115\\nAssuming a simple linear regression model, give an interval that, with\\n95 percent conﬁdence, will contain the average weight of all 17-year-old\\nmale high school students.\\n28. Glass plays a key role in criminal investigations, because criminal activity\\noften results in the breakage of windows and other glass objects. Since\\nglass fragments often lodge in the clothing of the criminal, it is of great\\nimportance to be able to identify such fragments as originating at the\\nscene of the crime. Two physical properties of glass that are useful for\\nidentiﬁcation purposes are its refractive index, which is relatively easy to\\nmeasure, and its density, which is much more difﬁcult to measure. The\\nexact measurement of density is, however, greatly facilitated if one has a\\ngood estimate of this value before setting up the laboratory experiment\\nneeded to determine it exactly. Thus, it would be quite useful if one could\\nuse the refractive index of a glass fragment to estimate its density.\\nThe following data relate the refractive index to the density for 18 pieces\\nof glass.\\nRefractive Index\\nDensity\\nRefractive Index\\nDensity\\n1.5139\\n2.4801\\n1.5161\\n2.4843\\n1.5153\\n2.4819\\n1.5165\\n2.4858\\n1.5155\\n2.4791\\n1.5178\\n2.4950\\n1.5155\\n2.4796\\n1.5181\\n2.4922\\n1.5156\\n2.4773\\n1.5191\\n2.5035\\n1.5157\\n2.4811\\n1.5227\\n2.5086\\n1.5158\\n2.4765\\n1.5227\\n2.5117\\n1.5159\\n2.4781\\n1.5232\\n2.5146\\n1.5160\\n2.4909\\n1.5253\\n2.5187\\na.\\nPredict the density of a piece of glass with a refractive index 1.52.\\nb.\\nDetermine an interval that, with 95 percent conﬁdence, will contain\\nthe density of the glass in part (a).'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 449}, page_content='Problems 439\\n29. The regression model\\nY = βx + e,\\ne ∼N(0,σ 2)\\nis called regression through the origin since it presupposes that the ex-\\npected response corresponding to the input level x=0 is equal to 0.\\nSuppose that (xi,Yi),i = 1,...,n is a data set from this model.\\na.\\nDetermine the least squares estimator B of β.\\nb.\\nWhat is the distribution of B?\\nc.\\nDeﬁne SSR and give its distribution.\\nd.\\nDerive a test of H0 : β = β0 versus H1 : β ̸= β0.\\ne.\\nDetermine a 100(1 −α) percent prediction interval for Y(x0), the\\nresponse at input level x0.\\n30. The following are the body mass index (BMI) and the systolic blood pres-\\nsure of eight randomly chosen men who do not take any blood pressure\\nmedication.\\nBMI\\nSystolic Blood Pressure\\n20.3\\n116\\n22.0\\n110\\n26.4\\n131\\n28.2\\n136\\n31.0\\n144\\n32.6\\n138\\n17.6\\n122\\n19.4\\n115\\nGive an interval that, with 95 percent conﬁdence, will include the systolic\\nblood pressure of a man whose BMI is 26.0.\\n31. The weight and systolic blood pressure of randomly selected males in age\\ngroup 25 to 30 are shown in the following table.\\nSubject\\nWeight\\nSystolic BP\\nSubject\\nWeight\\nSystolic\\nBP\\n1\\n165\\n130\\n11\\n172\\n153\\n2\\n167\\n133\\n12\\n159\\n128\\n3\\n180\\n150\\n13\\n168\\n132\\n4\\n155\\n128\\n14\\n174\\n149\\n5\\n212\\n151\\n15\\n183\\n158\\n6\\n175\\n146\\n16\\n215\\n150\\n7\\n190\\n150\\n17\\n195\\n163\\n8\\n210\\n140\\n18\\n180\\n156\\n9\\n200\\n148\\n19\\n143\\n124\\n10\\n149\\n125\\n20\\n240\\n170'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 450}, page_content='440 CHAPTER 9: Regression\\na.\\nEstimate the regression coefﬁcients.\\nb.\\nDo the data support the claim that systolic blood pressure does not\\ndepend on an individual’s weight?\\nc.\\nIf a large number of males weighing 182 pounds have their blood\\npressures taken, determine an interval that, with 95 percent conﬁ-\\ndence, will contain their average blood pressure.\\nd.\\nAnalyze the standardized residuals.\\ne.\\nDetermine the sample correlation coefﬁcient.\\n32. It has been determined that the relation between stress (S) and the num-\\nber of cycles to failure (N) for a particular type of alloy is given by\\nS = A\\nNm\\nwhere A and m are unknown constants. An experiment is run yielding\\nthe following data.\\nStress\\n(thousand psi)\\nN\\n(million cycles to failure)\\n55.0\\n.223\\n50.5\\n6.75\\n42.5\\n18.1\\n42.0\\n29.1\\n41.0\\n50.5\\n35.7\\n126\\n34.5\\n215\\n33.0\\n445\\n32.0\\n420\\nEstimate A and m.\\n33. In 1957 the Dutch industrial engineer J. R. DeJong proposed the follow-\\ning model for the time it takes to perform a simple manual task as a\\nfunction of the number of times the task has been practiced:\\nT ≈ts−n\\nwhere T is the time, n is the number of times the task has been prac-\\nticed, and t and s are parameters depending on the task and individual.\\nEstimate t and s for the following data set.\\nT\\n22.4 21.3\\n19.7 15.6 15.2\\n13.9 13.7\\nn\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n34. The chlorine residual in a swimming pool at various times after being\\ncleaned is as given:'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 451}, page_content='Problems 441\\nTime (hr)\\nChlorine Residual\\n(pt/million)\\n2\\n1.8\\n4\\n1.5\\n6\\n1.45\\n8\\n1.42\\n10\\n1.38\\n12\\n1.36\\nFit a curve of the form\\nY ≈ae−bx\\nWhat would you predict for the chlorine residual 15 hours after a clean-\\ning?\\n35. The proportion of a given heat rise that has dissipated a time t after the\\nsource is cut off is of the form\\nP = 1 −e−αt\\nfor some unknown constant α. Given the data\\nP\\n.07 .21\\n.32\\n.38 .40\\n.45\\n.51\\nt\\n.1\\n.2\\n.3\\n.4\\n.5\\n.6\\n.7\\nestimate the value of α. Estimate the value of t at which half of the heat\\nrise is dissipated.\\n36. The following data represent the bacterial count of ﬁve individuals at dif-\\nferent times after being inoculated by a vaccine consisting of the bacteria.\\nDays Since Inoculation\\nBacterial Count\\n3\\n121,000\\n6\\n134,000\\n7\\n147,000\\n8\\n210,000\\n9\\n330,000\\na.\\nFit a curve.\\nb.\\nEstimate the bacteria count of a new patient after 8 days.\\n37. The following data yield the amount of hydrogen present (in parts per\\nmillion) in core drillings of ﬁxed size at the following distances (in feet)\\nfrom the base of a vacuum-cast ingot.\\nDistance\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nAmount\\n1.28 1.50\\n1.12 .94 .82\\n.75\\n.60 .72\\n.95 1.20'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 452}, page_content='442 CHAPTER 9: Regression\\na.\\nDraw a scatter diagram.\\nb.\\nFit a curve of the form\\nY = α + βx + γ x2 + e\\nto the data.\\n38. A new drug was tested on mice to determine its effectiveness in reducing\\ncancerous tumors. Tests were run on 10 mice, each having a tumor of\\nsize 4 grams, by varying the amount of the drug used and then determin-\\ning the resulting reduction in the weight of the tumor. The data were as\\nfollows:\\nCoded Amount of Drug\\nTumor Weight Reduction\\n1\\n.50\\n2\\n.90\\n3\\n1.20\\n4\\n1.35\\n5\\n1.50\\n6\\n1.60\\n7\\n1.53\\n8\\n1.38\\n9\\n1.21\\n10\\n.65\\nEstimate the maximum expected tumor reduction and the amount of the\\ndrug that attains it by ﬁtting a quadratic regression equation of the form\\nY = β0 + β1x + β2x2 + e\\n39. The following data represent the relation between the number of cans\\ndamaged in a boxcar shipment of cans and the speed of the boxcar at\\nimpact.\\nSpeed\\nNumber of Cans Damaged\\n3\\n54\\n3\\n62\\n3\\n65\\n5\\n94\\n5\\n122\\n5\\n84\\n6\\n142\\n7\\n139\\n7\\n184\\n8\\n254\\na.\\nAnalyze as a simple linear regression model.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 453}, page_content='Problems\\n443\\nb.\\nPlot the standardized residuals.\\nc.\\nDo the results of part (b) indicate any ﬂaw in the model?\\nd.\\nIf the answer to part (c) is yes, suggest a better model and estimate\\nall resulting parameters .\\n40. Redo Problem 5 under the assumption that the variance of the gain in\\nreading speed is proportional to the number of weeks in the program.\\n41. The following data relate the proportions of coal miners who exhibit\\nsymptoms of pneumoconiosis to the number of years of working in coal\\nmines.\\nYears Working\\nProportion Having Penumoconiosis\\n5\\n0\\n10\\n.0090\\n15\\n.0185\\n20\\n.0672\\n25\\n.1542\\n30\\n.1720\\n35\\n.1840\\n40\\n.2105\\n45\\n.3570\\n50\\n.4545\\nEstimate the probability that a coal miner who has worked for 42 years\\nwill have pneumoconiosis.\\n42. The following data set refers to Example 9.8.c.\\nNumber of Cars\\n(Daily)\\nNumber of\\nAccidents\\n(Monthly)\\n2000\\n15\\n2300\\n27\\n2500\\n20\\n2600\\n21\\n2800\\n31\\n3000\\n16\\n3100\\n22\\n3400\\n23\\n3700\\n40\\n3800\\n39\\n4000\\n27\\n4600\\n43\\n4800\\n53'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 454}, page_content='444 CHAPTER 9: Regression\\na.\\nEstimate the number of accidents in a month when the number of\\ncars using the highway is 3500.\\nb.\\nUse the model\\n√\\nY = α + βx + e\\nand redo part (a).\\n43. The peak discharge of a river is an important parameter for many engi-\\nneering design problems. Estimates of this parameter can be obtained by\\nrelating it to the watershed area (x1) and watershed slope (x2). Estimate\\nthe relationship based on the following data.\\nx1\\nx2\\nPeak Discharge\\n(m2)\\n(ft/ft)\\n(ft3/sec)\\n36\\n.005\\n50\\n37\\n.040\\n40\\n45\\n.004\\n45\\n87\\n.002\\n110\\n450\\n.004\\n490\\n550\\n.001\\n400\\n1200\\n.002\\n650\\n4000\\n.0005\\n1550\\n44. The sediment load in a stream is related to the size of the contributing\\ndrainage area (x1) and the average stream discharge (x2). Estimate this\\nrelationship using the following data.\\nArea\\nDischarge\\nSediment Yield\\n(×103 mi2)\\n(ft3/sec)\\n(millions of tons/yr)\\n8\\n65\\n1.8\\n19\\n625\\n6.4\\n31\\n1450\\n3.3\\n16\\n2400\\n1.4\\n41\\n6700\\n10.8\\n24\\n8500\\n15.0\\n3\\n3500\\n.8\\n3\\n4300\\n.4\\n7\\n12,100\\n1.6\\n45. Fit a multiple linear regression equation to the following data set.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 455}, page_content='Problems 445\\nx1\\nx2\\nx3\\nx4\\ny\\n1\\n11\\n16\\n4\\n275\\n2\\n10\\n9\\n3\\n183\\n3\\n9\\n4\\n2\\n140\\n4\\n8\\n1\\n1\\n82\\n5\\n7\\n2\\n1\\n97\\n6\\n6\\n1\\n−1\\n122\\n7\\n5\\n4\\n−2\\n146\\n8\\n4\\n9\\n−3\\n246\\n9\\n3\\n16\\n−4\\n359\\n10\\n2\\n25\\n−5\\n482\\n46. The following data refer to Stanford heart transplants. It relates the sur-\\nvival time of patients that have received heart transplants to their age\\nwhen the transplant occurred and to a so-called mismatch score that is\\nsupposed to be an indicator of how well the transplanted heart should\\nﬁt the recipient.\\nSurvival Time (in days)\\nMismatch Score\\nAge\\n624\\n1.32\\n51.0\\n46\\n.61\\n42.5\\n64\\n1.89\\n54.6\\n1350\\n.87\\n54.1\\n280\\n1.12\\n49.5\\n10\\n2.76\\n55.3\\n1024\\n1.13\\n43.4\\n39\\n1.38\\n42.8\\n730\\n.96\\n58.4\\n136\\n1.62\\n52.0\\n836\\n1.58\\n45.0\\n60\\n.69\\n64.5\\na.\\nLetting the dependent variable be the logarithm of the survival time,\\nﬁt a regression on the independent variable’s mismatch score and\\nage.\\nb.\\nEstimate the variance of the error term.\\n47.\\na.\\nFit a multiple linear regression equation to the following data set.\\nb.\\nTest the hypothesis that β0 = 0.\\nc.\\nTest the hypothesis that β3 = 0.\\nd.\\nTest the hypothesis that the mean response at the input levels\\nx1 = x2 = x3 = 1 is 8.5.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 456}, page_content='446 CHAPTER 9: Regression\\nx1\\nx2\\nx3\\ny\\n7.1\\n.68\\n4\\n41.53\\n9.9\\n.64\\n1\\n63.75\\n3.6\\n.58\\n1\\n16.38\\n9.3\\n.21\\n3\\n45.54\\n2.3\\n.89\\n5\\n15.52\\n4.6\\n.00\\n8\\n28.55\\n.2\\n.37\\n5\\n5.65\\n5.4\\n.11\\n3\\n25.02\\n8.2\\n.87\\n4\\n52.49\\n7.1\\n.00\\n6\\n38.05\\n4.7\\n.76\\n0\\n30.76\\n5.4\\n.87\\n8\\n39.69\\n1.7\\n.52\\n1\\n17.59\\n1.9\\n.31\\n3\\n13.22\\n9.2\\n.19\\n5\\n50.98\\n48. The tensile strength of a certain synthetic ﬁber is thought to be related\\nto x1, the percentage of cotton in the ﬁber, and x2, the drying time of\\nthe ﬁber. A test of 10 pieces of ﬁber produced under different conditions\\nyielded the following results.\\nY = Tensile\\nStrength\\nx1 =\\nPercentage\\nof Cotton\\nx2 = Drying\\nTime\\n213\\n13\\n2.1\\n220\\n15\\n2.3\\n216\\n14\\n2.2\\n225\\n18\\n2.5\\n235\\n19\\n3.2\\n218\\n20\\n2.4\\n239\\n22\\n3.4\\n243\\n17\\n4.1\\n233\\n16\\n4.0\\n240\\n18\\n4.3\\na.\\nFit a multiple regression equation.\\nb.\\nDetermine a 90 percent conﬁdence interval for the mean tensile\\nstrength of a synthetic ﬁber having 21 percent cotton whose drying\\ntime is 3.6.\\n49. The time to failure of a machine component is related to the operating\\nvoltage (x1), the motor speed in revolutions per minute (x2), and the\\noperating temperature (x3).'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 457}, page_content='Problems 447\\nA designed experiment is run in the research and development labora-\\ntory, and the following data, where y is the time to failure in minutes, are\\nobtained.\\ny\\nx1\\nx2\\nx3\\n2145\\n110\\n750\\n140\\n2155\\n110\\n850\\n180\\n2220\\n110\\n1000\\n140\\n2225\\n110\\n1100\\n180\\n2260\\n120\\n750\\n140\\n2266\\n120\\n850\\n180\\n2334\\n120\\n1000\\n140\\n2340\\n130\\n1000\\n180\\n2212\\n115\\n840\\n150\\n2180\\n115\\n880\\n150\\na.\\nFit a multiple regression model to these data.\\nb.\\nEstimate the error variance.\\nc.\\nDetermine a 95 percent conﬁdence interval for the mean time to\\nfailure when the operating voltage is 125, the motor speed is 900,\\nand the operating temperature is 160.\\n50. Explain why, for the same data, a prediction interval for a future response\\nalways contains the corresponding conﬁdence interval for the mean re-\\nsponse.\\n51. Consider the following data set:\\nx1\\nx2\\ny\\n5.1\\n2\\n55.42\\n5.4\\n8\\n100.21\\n5.9\\n−2\\n27.07\\n6.6\\n12\\n169.95\\n7.5\\n−6\\n−17.93\\n8.6\\n16\\n197.77\\n9.9\\n−10\\n−25.66\\n11.4\\n20\\n264.18\\n13.1\\n−14\\n−53.88\\n15\\n24\\n317.84\\n17.1\\n−18\\n−72.53\\n19.4\\n28\\n385.53\\na.\\nFit a linear relationship between y and x1, x2.\\nb.\\nDetermine the variance of the error term.\\nc.\\nDetermine an interval that, with 95 percent conﬁdence, will contain\\nthe response when the inputs are x1 = 10.2 and x2 = 17.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 458}, page_content='448 CHAPTER 9: Regression\\n52. The cost of producing power per kilowatt hour is a function of the load\\nfactor and the cost of coal in cents per million Btu. The following data\\nwere obtained from 12 mills.\\nLoad\\nFactor (in\\npercent)\\nCost of\\nCoal\\nPower\\nCost\\n84\\n14\\n4.1\\n81\\n16\\n4.4\\n73\\n22\\n5.6\\n74\\n24\\n5.1\\n67\\n20\\n5.0\\n87\\n29\\n5.3\\n77\\n26\\n5.4\\n76\\n15\\n4.8\\n69\\n29\\n6.1\\n82\\n24\\n5.5\\n90\\n25\\n4.7\\n88\\n13\\n3.9\\na.\\nEstimate the relationship.\\nb.\\nTest the hypothesis that the coefﬁcient of the load factor is equal\\nto 0.\\nc.\\nDetermine a 95 percent prediction interval for the power cost when\\nthe load factor is 85 and the coal cost is 20.\\n53. The following data relate the systolic blood pressure to the age (x1) and\\nweight (x2) of a set of individuals of similar body type and lifestyle.\\nAge\\nWeight\\nBlood\\nPressure\\n25\\n162\\n112\\n25\\n184\\n144\\n42\\n166\\n138\\n55\\n150\\n145\\n30\\n192\\n152\\n40\\n155\\n110\\n66\\n184\\n118\\n60\\n202\\n160\\n38\\n174\\n108\\na.\\nTest the hypothesis that, when an individual’s weight is known, age\\ngives no additional information in predicting blood pressure.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 459}, page_content='Problems 449\\nb.\\nDetermine an interval that, with 95 percent conﬁdence, will contain\\nthe average blood pressure of all individuals of the preceding type\\nwho are 45 years old and weigh 180 pounds.\\nc.\\nDetermine an interval that, with 95 percent conﬁdence, will contain\\nthe blood pressure of a given individual of the preceding type who\\nis 45 years old and weighs 180 pounds.\\n54. A recently completed study attempted to relate job satisfaction to income\\n(in 1000s) and seniority for a random sample of 9 municipal workers.\\nThe job satisfaction value given for each worker is his or her own assess-\\nment of such, with a score of 1 being the lowest and 10 being the highest.\\nThe following data resulted.\\nYearly Income\\nYears on the\\nJob\\nJob\\nSatisfaction\\n52\\n8\\n5.6\\n47\\n4\\n6.3\\n59\\n12\\n6.8\\n53\\n9\\n6.7\\n61\\n16\\n7.0\\n64\\n14\\n7.7\\n58\\n10\\n7.0\\n67\\n15\\n8.0\\n71\\n22\\n7.8\\na.\\nEstimate the regression parameters.\\nb.\\nWhat qualitative conclusions can you draw about how job satisfac-\\ntion changes when income remains ﬁxed and the number of years\\nof service increases?\\nc.\\nPredict the job satisfaction of an employee who has spent 5 years on\\nthe job and earns a yearly salary of $56,000.\\n55. Suppose in Problem 54 that job satisfaction was related solely to years\\non the job, with the following data resulting.\\nYears on the Job\\nJob Satisfaction\\n8\\n5.6\\n4\\n6.3\\n12\\n6.8\\n9\\n6.7\\n16\\n7.0\\n14\\n7.7\\n10\\n7.0\\n15\\n8.0\\n22\\n7.8'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 460}, page_content='450 CHAPTER 9: Regression\\na.\\nEstimate the regression parameters α and β.\\nb.\\nWhat is the qualitative relationship between years of service and job\\nsatisfaction? That is, what appears to happen to job satisfaction as\\nservice increases?\\nc.\\nCompare your answer to part (b) with the answer you obtained in\\npart (b) of Problem 54.\\nd.\\nWhat conclusion, if any, can you draw from your answer in part (c)?\\n56. For the logistics regression model, ﬁnd the value x such that p(x) = .5.\\n57. A study of 64 prematurely born infants was interested in the relation be-\\ntween the gestational age (in weeks) of the infant at birth and whether\\nthe infant was breast-feeding at the time of release from the birthing\\nhospital. The following data resulted:\\nGestational Age\\nFrequency\\nNumber Breast-Feeding\\n28\\n6\\n2\\n29\\n5\\n2\\n30\\n9\\n7\\n31\\n9\\n7\\n32\\n20\\n16\\n33\\n15\\n14\\nIn the preceding, the frequency column refers to the number of babies\\nborn after the speciﬁed gestational number of weeks.\\na.\\nExplain how the relationship between gestational age and whether\\nthe infant was breast-feeding can be analyzed via a logistics regres-\\nsion model.\\nb.\\nUse R to estimate the parameters for this model.\\nc.\\nEstimate the probability that a newborn with a gestational age of 29\\nweeks will be breast-feeding.\\n58. Twelve ﬁrst-time heart attack victims were given a test that measures in-\\nternal anger. The following data relates their scores and whether they had\\na second heart attack within 5 years.\\nAnger Score\\nSecond Heart Attack\\n80\\nyes\\n77\\nyes\\n70\\nno\\n68\\nyes\\n64\\nno\\n60\\nyes\\n50\\nyes\\n46\\nno\\n40\\nyes\\n35\\nno\\n30\\nno\\n25\\nyes'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 461}, page_content='Problems 451\\na.\\nExplain how the relationship between a second heart attack and\\none’s anger score can be analyzed via a logistics regression model.\\nb.\\nUse R to estimate the parameters for this model.\\nc.\\nEstimate the probability that a heart attack victim with an anger\\nscore of 55 will have a second attack within 5 years.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 462}, page_content='CHAPTER 10\\nAnalysis of variance\\n10.1\\nIntroduction\\nA large company is considering purchasing, in quantity, one of four different\\ncomputer packages designed to teach a new programming language. Some in-\\nﬂuential people within this company have claimed that these packages are\\nbasically interchangeable in that the one chosen will have little effect on the\\nﬁnal competence of its user. To test this hypothesis the company has decided\\nto choose 160 of its engineers, and divide them into 4 groups of size 40. Each\\nmember in group i will then be given teaching package i,i = 1,2,3,4, to learn\\nthe new language. When all the engineers complete their study, a comprehen-\\nsive exam will be given. The company then wants to use the results of this\\nexamination to determine whether the computer teaching packages are really\\ninterchangeable or not. How can they do this?\\nBefore answering this question, let us note that we clearly desire to be able to\\nconclude that the teaching packages are indeed interchangeable when the av-\\nerage test scores in all the groups are similar and to conclude that the packages\\nare essentially different when there is a large variation among these average test\\nscores. However, to be able to reach such a conclusion, we should note that the\\nmethod of division of the 160 engineers into 4 groups is of vital importance.\\nFor example, suppose that the members of the ﬁrst group score signiﬁcantly\\nhigher than those of the other groups. What can we conclude from this? Speciﬁ-\\ncally, is this result due to teaching package 1 being a superior teaching package,\\nor is it due to the fact that the engineers in group 1 are just better learners?\\nTo be able to conclude the former, it is essential that we divide the 160 engi-\\nneers into the 4 groups in such a way to make it extremely unlikely that one\\nof these groups is inherently superior. The time-tested method for doing this is\\nto divide the engineers into 4 groups in a completely random fashion. That is,\\nwe should do it in such a way so that all possible divisions are equally likely;\\nfor in this case, it would be very unlikely that any one group would be sig-\\nniﬁcantly superior to any other group. So let us suppose that the division of\\nthe engineers was indeed done “at random.” (Whereas it is not at all obvious\\nhow this can be accomplished, one efﬁcient procedure is to start by arbitrarily\\nnumbering the 160 engineers. Then generate a random permutation of the in-\\ntegers 1,2,...,160 and put the engineers whose numbers are among the ﬁrst\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00019-3\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n453'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 463}, page_content='454 CHAPTER 10: Analysis of variance\\n40 of the permutation into group 1, those whose numbers are among the 41st\\nthrough the 80th of the permutation into group 2, and so on.)\\nIt is now probably reasonable to suppose that the test score of a given indi-\\nvidual should be approximately a normal random variable having parameters\\nthat depend on the package from which he was taught. Also, it is probably\\nreasonable to suppose that whereas the average test score of an engineer will\\ndepend on the teaching package she was exposed to, the variability in the test\\nscore will result from the inherent variation of 160 different people and not\\nfrom the particular package used. Thus, if we let Xij,i = 1, ...,4,j = 1, ...,40,\\ndenote the test score of the jth engineer in group i, a reasonable model might\\nbe to suppose that the Xij are independent random variables with Xij having\\na normal distribution with unknown mean μi and unknown variance σ 2. The\\nhypothesis that the teaching packages are interchangeable is then equivalent to\\nthe hypothesis that μ1 = μ2 = μ3 = μ4.\\nIn this chapter, we present a technique that can be used to test such a hypothe-\\nsis. This technique, which is rather general and can be used to make inferences\\nabout a multitude of parameters relating to population means, is known as the\\nanalysis of variance.\\n10.2\\nAn overview\\nWhereas hypothesis tests concerning two population means were studied in\\nChapter 8, tests concerning multiple population means will be considered in\\nthe present chapter. In Section 10.3, we suppose that we have been provided\\nsamples of size n from m distinct populations and that we want to use these\\ndata to test the hypothesis that the m population means are equal. Since the\\nmean of a random variable depends only on a single factor, namely, the sam-\\nple the variable is from, this scenario is said to constitute a one-way analysis\\nof variance. A procedure for testing the hypothesis is presented. In addition, in\\nSection 10.3.2 we show how to obtain multiple comparisons of the\\n\\x02m\\n2\\n\\x03\\ndiffer-\\nences between the pairs of population means, and in Section 10.3.3 we show\\nhow the equal means hypothesis can be tested when the m sample sizes are not\\nall equal.\\nIn Sections 10.4 and 10.5, we consider models that assume that there are two\\nfactors that determine the mean value of a variable. In these models, the vari-\\nables can be thought of as being arranged in a rectangular array, with the mean\\nvalue of a speciﬁed variable depending both on the row and on the column\\nin which it is located. Such a model is called a two-way analysis of variance. In\\nthese sections we suppose that the mean value of a variable depends on its row\\nand column in an additive fashion; speciﬁcally, that the mean of the variable\\nin row i, column j can be written as μ + αi + βj. In Section 10.4, we show how\\nto estimate these parameters, and in Section 10.5 how to test hypotheses to the\\neffect that a given factor — either the row or the column in which a variable is'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 464}, page_content='10.2 An overview\\n455\\nlocated — does not affect the mean. In Section 10.6, we consider the situation\\nwhere the mean of a variable is allowed to depend on its row and column in\\na nonlinear fashion, thus allowing for a possible interaction between the two\\nfactors. We show how to test the hypothesis that there is no interaction, as well\\nas ones concerning the lack of a row effect and the lack of a column effect on\\nthe mean value of a variable.\\nIn all of the models considered in this chapter, we assume that the data are\\nnormally distributed with the same (although unknown) variance σ 2. The\\nanalysis of variance approach for testing a null hypothesis H0 concerning mul-\\ntiple parameters relating to the population means is based on deriving two\\nestimators of the common variance σ 2. The ﬁrst estimator is a valid estimator\\nof σ 2 whether the null hypothesis is true or not, while the second one is a valid\\nestimator only when H0 is true. In addition, when H0 is not true this latter es-\\ntimator will tend to exceed σ 2. The test will be to compare the values of these\\ntwo estimators, and to reject H0 when the ratio of the second estimator to the\\nﬁrst one is sufﬁciently large. In other words, since the two estimators should be\\nclose to each other when H0 is true (because they both estimate σ 2 in this case)\\nwhereas the second estimator should tend to be larger than the ﬁrst when H0\\nis not true, it is natural to reject H0 when the second estimator is signiﬁcantly\\nlarger than the ﬁrst.\\nWe will obtain estimators of the variance σ 2 by making use of certain facts\\nconcerning chi-square random variables, which we now present. Suppose that\\nX1,...,XN are independent normal random variables having possibly differ-\\nent means but a common variance σ 2, and let μi = E[Xi],i = 1,...,N. Since\\nthe variables\\nZi = (Xi −μi)/σ,\\ni = 1,...,N\\nhave standard normal distributions, it follows from the deﬁnition of a chi-\\nsquare random variable that\\nN\\n\\x04\\ni=1\\nZ2\\ni =\\nN\\n\\x04\\ni=1\\n(Xi −μi)2/σ 2\\n(10.2.1)\\nis a chi-square random variable with N degrees of freedom. Now, suppose that\\neach of the values μi,i = 1,...,N, can be expressed as a linear function of a\\nﬁxed set of k unknown parameters. Suppose, further, that we can determine\\nestimators of these k parameters, which thus gives us estimators of the mean\\nvalues μi. If we let ˆμi denote the resulting estimator of μi,i = 1,...,N, then it\\ncan be shown that the quantity\\nN\\n\\x04\\ni=1\\n(Xi −ˆμi)2/σ 2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 465}, page_content='456 CHAPTER 10: Analysis of variance\\nwill have a chi-square distribution with N −k degrees of freedom.\\nIn other words, we start with\\nN\\n\\x04\\ni=1\\n(Xi −E[Xi])2/σ 2\\nwhich is a chi-square random variable with N degrees of freedom. If we now\\nwrite each E[Xi] as a linear function of k parameters and then replace each\\nof these parameters by its estimator, then the resulting expression remains chi-\\nsquare but with a degree of freedom that is reduced by 1 for each parameter\\nthat is replaced by its estimator.\\nFor an illustration of the preceding, consider the case where all the means are\\nknown to be equal; that is,\\nE[Xi] = μ,\\ni = 1, ...,N\\nThus k = 1, because there is only one parameter that needs to be estimated.\\nSubstituting X, the estimator of the common mean μ, for μi in Equa-\\ntion (10.2.1), results in the quantity\\nN\\n\\x04\\ni=1\\n(Xi −X)2/σ 2\\n(10.2.2)\\nand the conclusion is that this quantity is a chi-square random variable with\\nN −1 degrees of freedom. But in this case where all the means are equal, it fol-\\nlows that the data X1,...,XN constitute a sample from a normal population,\\nand thus Equation (10.2.2) is equal to (N −1)S2/σ 2, where S2 is the sample\\nvariance. In other words, the conclusion in this case is just the well-known re-\\nsult (see Section 6.5.2) that (N −1)S2/σ 2 is a chi-square random variable with\\nN −1 degrees of freedom.\\n10.3\\nOne-way analysis of variance\\nConsider m independent samples, each of size n, where the members of the\\nith sample — Xi1,Xi2,...,Xin — are normal random variables with unknown\\nmean μi and unknown variance σ 2. That is,\\nXij ∼N(μi,σ 2),\\ni = 1,...,m,\\nj = 1,...,n\\nWe will be interested in testing\\nH0 : μ1 = μ2 = ··· = μm'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 466}, page_content='10.3 One-way analysis of variance 457\\nversus\\nH1 : not all the means are equal\\nThat is, we will be testing the null hypothesis that all the population means\\nare equal against the alternative that at least two of them differ. One way of\\nthinking about this is to imagine that we have m different treatments, where\\nthe result of applying treatment i on an item is a normal random variable with\\nmean μi and variance σ2. We are then interested in testing the hypothesis that\\nall treatments have the same effect, by applying each treatment to a (different)\\nsample of n items and then analyzing the result.\\nSince there are a total of nm independent normal random variables Xij, it\\nfollows that the sum of the squares of their standardized versions will be a\\nchi-square random variable with nm degrees of freedom. That is,\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −E[Xij])2/σ 2 =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −μi)2/σ 2 ∼χ2\\nnm\\n(10.3.1)\\nTo obtain estimators for the m unknown parameters μ1,...,μm, let Xi. denote\\nthe average of all the elements in sample i; that is,\\nXi. =\\nn\\n\\x04\\nj=1\\nXij/n\\nThe variable Xi. is the sample mean of the ith population, and as such is\\nthe estimator of the population mean μi, for i = 1,...,m. Hence, if in Equa-\\ntion (10.3.1) we substitute the estimators Xi. for the means μi, for i = 1,...,m,\\nthen the resulting variable\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −Xi.)2/σ 2\\n(10.3.2)\\nwill have a chi-square distribution with nm−m degrees of freedom. (Recall that\\n1 degree of freedom is lost for each parameter that is estimated.) Let\\nSSW =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −Xi.)2\\nand so the variable in Equation (10.3.2) is SSW/σ 2. Because the expected value\\nof a chi-square random variable is equal to its number of degrees of freedom,\\nit follows upon taking the expectation of the variable in (10.3.2) that\\nE[SSW]/σ 2 = nm −m'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 467}, page_content='458 CHAPTER 10: Analysis of variance\\nor, equivalently,\\nE[SSW/(nm −m)] = σ 2\\nWe thus have our ﬁrst estimator of σ 2, namely, SSW/(nm −m). Also, note that\\nthis estimator was obtained without assuming anything about the truth or fal-\\nsity of the null hypothesis.\\nDeﬁnition. The statistic\\nSSW =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −Xi.)2\\nis called the within samples sum of squares because it is obtained by substituting\\nthe sample population means for the population means in expression (10.3.1).\\nThe statistic\\nSSW/(nm −m)\\nis an estimator of σ 2.\\nOur second estimator of σ 2 will only be a valid estimator when the null hy-\\npothesis is true. So let us assume that H0 is true and so all the population\\nmeans μi are equal, say, μi = μ for all i. Under this condition it follows that\\nthe m sample means X1.,X2.,...,Xm. will all be normally distributed with the\\nsame mean μ and the same variance σ 2/n. Hence, the sum of squares of the m\\nstandardized variables\\nXi. −μ\\n\\x05\\nσ 2/n\\n= √n(Xi. −μ)/σ\\nwill be a chi-square random variable with m degrees of freedom. That is, when\\nH0 is true,\\nn\\nm\\n\\x04\\ni=1\\n(Xi. −μ)2/σ 2 ∼χ2\\nm\\n(10.3.3)\\nNow, when all the population means are equal to μ, then the estimator of μ is\\nthe average of all the nm data values. That is, the estimator of μ is X.., given by\\nX.. =\\nm\\n\\x06\\ni=1\\nn\\x06\\nj=1\\nXij\\nnm\\n=\\nm\\n\\x06\\ni=1\\nXi.\\nm\\nIf we now substitute X.. for the unknown parameter μ in expression 10.5.1, it\\nfollows, when H0 is true, that the resulting quantity\\nn\\nm\\n\\x04\\ni=1\\n(Xi. −X..)2/σ 2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 468}, page_content='10.3 One-way analysis of variance 459\\nwill be a chi-square random variable with m −1 degrees of freedom. That is, if\\nwe deﬁne SSb by\\nSSb = n\\nm\\n\\x04\\ni=1\\n(Xi. −X..)2\\nthen it follows that, when H0 is true, SSb/σ 2 is chi-square with m −1 degrees\\nof freedom.\\nFrom the above we obtain that when H0 is true,\\nE[SSb]/σ 2 = m −1\\nor, equivalently,\\nE[SSb/(m −1)] = σ 2\\n(10.3.4)\\nSo, when H0 is true, SSb/(m −1) is also an estimator of σ 2.\\nDeﬁnition. The statistic\\nSSb = n\\nm\\n\\x04\\ni=1\\n(Xi. −X..)2\\nis called the between samples sum of squares. When H0 is true, SSb/(m −1) is an\\nestimator of σ 2.\\nThus we have shown that\\nSSW/(nm −m)\\nalways estimatesσ 2\\nSSb/(m −1)\\nestimatesσ 2 whenH0 is true\\nBecause it can be shown that SSb/(m −1) will tend to exceed σ 2 when H0 is\\nnot true,1 it is reasonable to let the test statistic be given by\\nT S =\\nSSb/(m −1)\\nSSW/(nm −m)\\nand to reject H0 when T S is sufﬁciently large.\\nTo determine how large T S needs to be to justify rejecting H0, we use the fact\\nthat it can be shown that if H0 is true then SSb and SSW are independent. It\\nfollows from this that, when H0 is true, T S has an F-distribution with m −\\n1 numerator and nm −m denominator degrees of freedom. Let Fα,m−1,nm−m\\ndenote the 100(1 −α) percentile of this distribution — that is,\\nP{Fm−1,nm−m > Fα,m−1,nm−m} = α\\n1A proof is given at the end of this subsection.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 469}, page_content='460 CHAPTER 10: Analysis of variance\\nwhere we are using the notation Fr,s to represent an F-random variable with r\\nnumerator and s denominator degrees of freedom.\\nThe signiﬁcance level α test of H0 is as follows:\\nreject\\nH0\\nif\\nSSb/(m −1)\\nSSW/(nm −m) > Fα,m−1,nm−m\\ndo not reject\\nH0\\notherwise\\nAnother way of doing the computations for the hypothesis test that all the\\npopulation means are equal is by computing the p-value. If the value of the\\ntest statistic is T S = v, then the p-value will be given by\\np-value = P{Fm−1,nm−m ≥v}\\nThe R command > 1 −pf (v,m −1,nm −m) will return the p-value.\\nThe following algebraic identity, called the sum of squares identity, is useful when\\ndoing the computations by hand.\\nThe sum of squares identity\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\nX2\\nij = nmX2\\n.. + SSb + SSW\\nWhen computing by hand, the quantity SSb deﬁned by\\nSSb = n\\nm\\n\\x04\\ni=1\\n(Xi. −X..)2\\nshould be computed ﬁrst. Once SSb has been computed, SSW can be deter-\\nmined from the sum of squares identity. That is, \\x06m\\ni=1\\n\\x06n\\nj=1 X2\\nij and X2\\n.. should\\nalso be computed and then SSW determined from\\nSSW =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\nX2\\nij −nmX2\\n.. −SSb\\nExample 10.3.a. An auto rental ﬁrm is using 15 identical motors that are ad-\\njusted to run at a ﬁxed speed to test 3 different brands of gasoline. Each brand\\nof gasoline is assigned to exactly 5 of the motors. Each motor runs on 10 gallons\\nof gasoline until it is out of fuel. The following represents the total mileages ob-\\ntained by the different motors:\\nGas 1:\\n220\\n251\\n226\\n246\\n260\\nGas 2:\\n244\\n235\\n232\\n242\\n225\\nGas 3:\\n252\\n272\\n250\\n238\\n256'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 470}, page_content='10.3 One-way analysis of variance 461\\nTest the hypothesis that the average mileage obtained is not affected by the type\\nof gas used. Use the 5 percent level of signiﬁcance.\\nSolution. Let us do the computations of Example 10.3.a by hand. The ﬁrst\\nthing to note is that subtracting a constant from each data value will not affect\\nthe value of the test statistic. So we subtract 220 from each data value to get the\\nfollowing information.\\nGas\\nMileage\\n\\x02\\njXij\\n\\x02\\njX 2\\nij\\n1\\n0\\n31\\n6\\n26\\n40\\n103\\n3273\\n2\\n24\\n15\\n12\\n22\\n5\\n78\\n1454\\n3\\n32\\n52\\n30\\n18\\n36\\n168\\n6248\\nNow m = 3 and n = 5 and\\nX1. = 103/5 = 20.6\\nX2. = 78/5 = 15.6\\nX3. = 168/5 = 33.6\\nX.. = (103 + 78 + 168)/15 = 23.2667,\\nX2\\n.. = 541.3393\\nThus,\\nSSb = 5[(20.6 −23.2667)2 + (15.6 −23.2667)2 + (33.6 −23.2667)2] = 863.3335\\nAlso,\\n\\x04\\x04\\nX2\\nij = 3273 + 1454 + 6248 = 10,975\\nand, from the sum of squares identity,\\nSSW = 10,975 −15(541.3393) −863.3335 = 1991.5785\\nThe value of the test statistic is thus\\nT S =\\n863.3335/2\\n1991.5785/12 = 2.60\\nUsing R now gives the p-value\\n>\\n1 −pf (2.60,2,12)\\n[1]\\n0.1153232\\nBecause the p-value is greater than .05, we cannot, at the 5 percent level of\\nsigniﬁcance, reject the null hypothesis that the gasolines give equal mileage.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 471}, page_content='462 CHAPTER 10: Analysis of variance\\nLet us now show that\\nE[SSb/(m −1)] ≥σ 2\\nwith equality only when H0 is true. So, we must show that\\nE\\n\\x07 m\\n\\x04\\ni=1\\n(Xi. −X..)2/(m −1)\\n\\x08\\n≥σ 2/n\\nwith equality only when H0 is true. To verify this, let μ. = \\x06m\\ni=1 μi/m be the\\naverage of the means. Also, for i = 1,...,m, let\\nYi = Xi. −μi + μ.\\nBecause Xi. is normal with mean μi and variance σ 2/n, it follows that Yi is\\nnormal with mean μ. and variance σ 2/n. Consequently, Y1,...,Ym constitutes\\na sample from a normal population having variance σ 2/n. Let\\nY. =\\nm\\n\\x04\\ni=1\\nYi/m = X.. −μ. + μ. = X..\\nbe the average of these variables. Now,\\nXi. −X.. = Yi + μi −μ. −Y.\\nConsequently,\\nE\\n\\x07 m\\n\\x04\\ni=1\\n(Xi. −X..)2\\n\\x08\\n= E\\n\\x07 m\\n\\x04\\ni=1\\n(Yi −Y. + μi −μ.)2\\n\\x08\\n= E\\n\\x07 m\\n\\x04\\ni=1\\n[(Yi −Y.)2 + (μi −μ.)2 + 2(μi −μ.)(Yi −Y.)]\\n\\x08\\n= E\\n\\x07 m\\n\\x04\\ni=1\\n(Yi −Y.)2\\n\\x08\\n+\\nm\\n\\x04\\ni=1\\n(μi −μ.)2\\n+ 2\\nm\\n\\x04\\ni=1\\n(μi −μ.)E[Yi −Y.]\\n= (m −1)σ 2/n +\\nm\\n\\x04\\ni=1\\n(μi −μ.)2 + 2\\nm\\n\\x04\\ni=1\\n(μi −μ.)E[Yi −Y.]\\n= (m −1)σ 2/n +\\nm\\n\\x04\\ni=1\\n(μi −μ.)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 472}, page_content='10.3 One-way analysis of variance 463\\nwhere the next to last equality follows because the sample variance \\x06m\\ni=1(Yi −\\nY.)2/(m −1) is an unbiased estimator of its population variance σ 2/n and the\\nﬁnal equality because E[Yi] −E[Y.] = μ. −μ. = 0. Dividing by m −1 gives that\\nE\\n\\x07 m\\n\\x04\\ni=1\\n(Xi. −X..)2/(m −1)\\n\\x08\\n= σ 2/n +\\nm\\n\\x04\\ni=1\\n(μi −μ.)2/(m −1)\\nand the result follows because \\x06m\\ni=1(μi −μ.)2 ≥0, with equality only when all\\nthe μi are equal.\\nTable 10.1 sums up the results of this section.\\nTable 10.1 One-Way ANOVA Table.\\nSource of\\nVariation\\nSum of Squares\\nDegrees of\\nFreedom\\nValue of Test\\nStatistic\\nBetween samples\\nSSb = n\\x06m\\ni=1(Xi. −X..)2\\nm −1\\nWithin samples\\nSSW = \\x06m\\ni=1\\n\\x06n\\nj=1(Xij −Xi.)2\\nnm −m\\nT S =\\nSSb/(m−1)\\nSSW /(nm−m)\\nSigniﬁcance level α test:\\nreject H0 if T S ≥Fα,m−1,nm−m\\ndo not reject otherwise\\nIf T S = v, then p-value = P {Fm−1,nm−m ≥v}\\n10.3.1\\nUsing R to do the computations\\nTo begin, note that to deﬁne a matrix in R, the matrix content is ﬁlled along the\\ncolumn orientation. For instance, suppose we wanted to input the following\\nmatrix:\\n⎛\\n⎝\\n2\\n1\\n3\\n4\\n1\\n5\\n2\\n6\\n5\\n3\\n7\\n9\\n⎞\\n⎠\\nCalling the matrix A, this is done as follows:\\n>\\nA = matrix(c(2,1,5,1,5,3,3,2,7,4,6,9),nrow = 3,ncol = 4)\\nThe R convention is that after giving the elements of the matrix, we then give\\nthe number of rows and then the number of columns, so we could have left\\nout nrow = and ncol= and just deﬁne A by\\n>\\nA = matrix(c(2,1,5,1,5,3,3,2,7,4,6,9),3,4)\\nTo display the matrix, we would input'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 473}, page_content='464 CHAPTER 10: Analysis of variance\\n> A\\nto obtain the output\\n(,1)\\n(,2)\\n(,3)\\n(,4)\\n(1,)\\n2\\n1\\n3\\n4\\n(2,)\\n1\\n5\\n2\\n6\\n(3,)\\n5\\n3\\n7\\n9\\nAnother way to deﬁne a matrix is to ﬁrst deﬁne the column vectors. For in-\\nstance, to input the matrix A, we can do the following:\\n>\\nv1 = c(2,1,5)\\n>\\nv2 = c(1,5,3)\\n>\\nv3 = c(3,2,7)\\n>\\nv4 = c(4,6,9)\\n>\\nA = matrix(c(v1,v2,v3,v4),3,4)\\n>\\nA\\n(,1)\\n(,2)\\n(,3)\\n(,4)\\n(1,)\\n2\\n1\\n3\\n4\\n(2,)\\n1\\n5\\n2\\n6\\n(3,)\\n5\\n3\\n7\\n9\\nNote that the last 2 numbers in the R term matrix(c(v1,v2,v3,v4),3,4) signify\\nthat the matrix, whose column vectors are v1,v2,v3,v4, consists of 3 rows and\\n4 columns.\\nIf A is a matrix, and x a scalar, then A + x is the matrix that results when\\nx is added to each element of A. Raising a matrix to a power results in each\\nelement of the matrix being raised to that particular power. For instance, if A is\\nthe matrix [ai,j], then A2 is the matrix [a2\\ni,j]. (This convention is not consistent\\nwith the usual notion of “matrix multiplication”.) To sum all the elements of\\nthe matrix A, just input the R command sum(A); to obtain the average of all\\nthese elements use mean(A). So, for instance, if we wanted the sum of the\\nsquares of all elements of the matrix A and wanted to call this sum ss, we\\nwould input\\n> ss = sum(A2)\\n> ss\\nto obtain\\n[1] 260\\nIf we wanted each of the column sums of the matrix A, we would use the R com-\\nmand colSums(A); for row sums we use rowSums(A). For instance, inputting\\n> colSums(A)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 474}, page_content='10.3 One-way analysis of variance 465\\nyields the output\\n[1] 8 9 12 19\\nwhereas\\n> rowSums(A)\\ngives\\n[1] 10 14 24\\nTo obtain averages of the columns or the rows of A, we use the R commands\\ncolMeans(A) and rowMeans(A).\\nSuppose now that A is a matrix with m rows and n columns, and that, for num-\\nbers b1,...,bm, we wanted the matrix obtained when b1 is subtracted from each\\nelement in the ﬁrst row of A, b2 is subtracted from each element in the second\\nrow, and so on. We can obtain the desired matrix by using the R command\\n“sweep”. The desired matrix, which we designate as B, is obtained as follows:\\n>\\nb = c(b1,...,bm)\\n>\\nB = sweep(A,1,b)\\nFor instance, suppose\\n> A = matrix(c(2,3,3,5,1,4),2,3)\\n> A\\n(,1)\\n(,2)\\n(,3)\\n(1,)\\n2\\n3\\n1\\n(2,)\\n3\\n5\\n4\\nTo subtract, from each value of the matrix, the average of the values in its row,\\ndo the following:\\n> sweep(A,1,rowMeans(A))\\n(,1)\\n(,2)\\n(,3)\\n(1,)\\n0\\n1\\n−1\\n(2,)\\n−1\\n1\\n0\\nThe 1 in “sweep(A,1,c(b1,...,bm))” is to indicate that we want bi subtracted\\nfrom each element in row i. If, for a vector c1,...,cn, we wanted the matrix, call\\nit C, that results when cj is subtracted from each element in column j of the\\nmatrix A, for all j = 1,...,n, then we use the R command\\n>\\nd = c(c1,...,cn)\\n>\\nC = sweep(A,2,d)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 475}, page_content='466 CHAPTER 10: Analysis of variance\\nFor instance, suppose we wanted the matrix that adds the average of the ele-\\nments of column j of matrix A to each of the elements of that column. We can\\nget this by\\n> sweep(A,2,−colMeans(A))\\n(,1)\\n(,2)\\n(,3)\\n(1,)\\n4.5\\n7\\n3.5\\n(2,)\\n5.5\\n9\\n6.5\\nWe are now ready to use R to solve the one-way analysis of variance problems.\\nLet us consider Example 10.3.a. Start by inputting the data, with row i being\\nthe mileage from gas i,i = 1,2,3. Calling this matrix G, we have\\n>\\nG = matrix(c(220,244,252,251,235,272,226,232,250,246,242,238,\\n260,225,256),3,5)\\nLetting rm denote the vector of row means, and mm be the average of all the\\ndata we input\\n> rm = rowMeans(G)\\n> mm = mean(G)\\n> d = rm −mm\\n> SSb = 5 ∗sum(d2)\\n> SSw = sum(G2) −3 ∗5 ∗mm2 −SSb\\n> T S = (SSb/2)/(SSw/12)\\n> 1 −pf (T S,2,12)\\nthe output would be the p-value\\n[1] 0.1152489\\n10.3.2\\nMultiple comparisons of sample means\\nWhen the null hypothesis of equal means is rejected, we are often interested in\\na comparison of the different sample means μ1, ...,μm. One procedure that is\\noften used for this purpose is known as the T -method. For a speciﬁed value of\\nα, this procedure gives joint conﬁdence intervals for all the\\n\\x02m\\n2\\n\\x03\\ndifferences μi −\\nμj,i ̸= j,i,j = 1, ...,m, such that with probability 1 −α all of the conﬁdence\\nintervals will contain their respective quantities μi −μj. The T -method is based\\non the following result:\\nWith probability 1 −α, for every i ̸= j\\nXi. −Xj. −W < μi −μj < Xi. −Xj. + W'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 476}, page_content='10.3 One-way analysis of variance 467\\nwhere\\nW = 1\\n√nC(m,nm −m,α)\\n\\x05\\nSSW/(nm −m)\\nand where the values of C(m,nm −m,α) are given, for α = .05 and α = .01, in\\nTable A.2 of the Appendix.\\nExample 10.3.b. A college administrator claims that there is no difference in\\nﬁrst-year grade point averages for students entering the college from any of\\nthree different city high schools. The following data give the ﬁrst-year grade\\npoint averages of 12 randomly chosen students, 4 from each of the three high\\nschools. At the 5 percent level of signiﬁcance, do these data disprove the claim\\nof the administrator? If so, determine conﬁdence intervals for the difference\\nin means of students from the different high schools, such that we can be 95\\npercent conﬁdent that all of the interval statements are valid.\\nSchool 1\\nSchool 2\\nSchool 3\\n3.2\\n3.4\\n2.8\\n3.4\\n3.0\\n2.6\\n3.3\\n3.7\\n3.0\\n3.5\\n3.3\\n2.7\\nSolution. To begin, note that there are m = 3 samples, each of size n = 4. An R\\ncomputation yields the results:\\nSSW/9 = .0431\\np-value = .0046\\nso the hypothesis of equal mean scores for students from the three schools is\\nrejected.\\nTo determine the conﬁdence intervals for the differences in the population\\nmeans, note ﬁrst that the sample means are\\nX1. = 3.350,\\nX2. = 3.350,\\nX3. = 2.775\\nFrom Table A.2 of the Appendix, we see that C(3, 9, .05) = 3.95; thus, as W =\\n1\\n√\\n43.95\\n√\\n.0431 = .410, we obtain the following conﬁdence intervals.\\n−.410 < μ1 −μ2 < .410\\n.165 < μ1 −μ3 < .985\\n.165 < μ2 −μ3 < .985\\nHence, with 95 percent conﬁdence, we can conclude that the mean grade point\\naverage of ﬁrst-year students from high school 3 is less than the mean average'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 477}, page_content='468 CHAPTER 10: Analysis of variance\\nof students from high school 1 or from high school 2 by an amount that is be-\\ntween .165 and .985, and that the difference in grade point averages of students\\nfrom high schools 1 and 2 is less than .410.\\n■\\n10.3.3\\nOne-way analysis of variance with unequal\\nsample sizes\\nThe model in the previous section supposed that there were an equal number\\nof data points in each sample. Whereas this is certainly a desirable situation\\n(see the Remark at the end of this section), it is not always possible to at-\\ntain. So let us now suppose that we have m normal samples of respective sizes\\nn1,n2, ... ,nm. That is, the data consist of the \\x06m\\ni=1 ni independent random\\nvariables Xij, i = 1,...,m, j = 1,...,ni, where\\nXij ∼N(μi,σ 2)\\nAgain we are interested in testing the hypothesis H0 that all means are equal.\\nTo derive a test of H0, we start with the fact that\\nm\\n\\x04\\ni=1\\nni\\n\\x04\\nj=1\\n(Xij −E[Xij])2/σ 2 =\\nm\\n\\x04\\ni=1\\nni\\n\\x04\\nj=1\\n(Xij −μi)2/σ 2\\nis a chi-square random variable with \\x06m\\ni=1 ni degrees of freedom. Hence, upon\\nreplacing each mean μi by its estimator Xi., the average of the elements in\\nsample i, we obtain\\nm\\n\\x04\\ni=1\\nni\\n\\x04\\nj=1\\n(Xij −Xi.)2/σ 2\\nwhich is chi-square with \\x06m\\ni=1 ni −m degrees of freedom. Therefore, letting\\nSSW =\\nm\\n\\x04\\ni=1\\nni\\n\\x04\\nj=1\\n(Xij −Xi.)2\\nit follows that SSW/\\n\\x02\\x06m\\ni=1 ni −m\\n\\x03\\nis an unbiased estimator of σ 2.\\nFurthermore, if H0 is true and μ is the common mean, then the random vari-\\nables Xi.,i = 1, ... ,m will be independent normal random variables with\\nE[Xi.] = μ,\\nVar(Xi.) = σ 2/ni\\nAs a result, when H0 is true'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 478}, page_content='10.3 One-way analysis of variance 469\\nm\\n\\x04\\ni=1\\n(Xi. −μ)2\\nσ 2/ni\\n=\\nm\\n\\x04\\ni=1\\nni(Xi. −μ)2/σ 2\\nis chi-square with m degrees of freedom; therefore, replacing μ in the preceding\\nby its estimator X.., the average of all the Xij, results in the statistic\\nm\\n\\x04\\ni=1\\nni(Xi. −X..)2/σ 2\\nwhich is chi-square with m −1 degrees of freedom. Thus, letting\\nSSb =\\nm\\n\\x04\\ni=1\\nni(Xi. −X..)2\\nit follows, when H0 is true, that SSb/(m −1) is also an unbiased estimator of\\nσ 2. Because it can be shown that when H0 is true the quantities SSb and SSW\\nare independent, it follows under this condition that the statistic\\nSSb/(m −1)\\nSSW\\n\\r\\x0e m\\n\\x06\\ni=1\\nni −m\\n\\x0f\\nis an F-random variable with m −1 numerator and \\x06m\\ni=1 ni −m denominator\\ndegrees of freedom. From this we can conclude that a signiﬁcance level α test\\nof the null hypothesis\\nH0 : μ1 = ··· = μm\\nis to let N = \\x06\\ni ni −m, and then\\nreject\\nH0\\nif\\nSSb/(m −1)\\nSSW\\n\\r\\x0e m\\n\\x06\\ni=1\\nni −m\\n\\x0f > Fm−1,N,α\\nnot reject\\nH0\\notherwise\\nRemark\\nWhen the samples are of different sizes we say that we are in the unbalanced\\ncase. Whenever possible it is advantageous to choose a balanced design over\\nan unbalanced one. For one thing, the test statistic in a balanced design is rela-\\ntively insensitive to slight departures from the assumption of equal population\\nvariances. (That is, the balanced design is more robust than the unbalanced\\none.)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 479}, page_content='470 CHAPTER 10: Analysis of variance\\n10.4\\nTwo-factor analysis of variance: introduction and\\nparameter estimation\\nWhereas the model of Section 10.3 enabled us to study the effect of a single\\nfactor on a data set, we can also study the effects of several factors. In this\\nsection, we suppose that each data value is affected by two factors.\\nExample 10.4.a. Four different standardized reading achievement tests were\\nadministered to each of 5 students, with the scores shown in the table resulting.\\nEach value in this set of 20 data points is affected by two factors, namely, the\\nexam and the student whose score on that exam is being recorded. The exam\\nfactor has 4 possible values, or levels, and the student factor has 5 possible\\nlevels.\\nExam\\nStudent\\n1\\n2\\n3\\n4\\n5\\n1\\n75\\n73\\n60\\n70\\n86\\n2\\n78\\n71\\n64\\n72\\n90\\n3\\n80\\n69\\n62\\n70\\n85\\n4\\n73\\n67\\n63\\n80\\n92\\nIn general, let us suppose that there are m possible levels of the ﬁrst factor and\\nn possible levels of the second. Let Xij denote the value obtained when the ﬁrst\\nfactor is at level i and the second factor is at level j. We will often portray the\\ndata set in the following array of rows and columns.\\nX11\\nX12\\n...\\nX1j\\n...\\nX1n\\nX21\\nX22\\n...\\nX2j\\n...\\nX2n\\nXi1\\nXi2\\n...\\nXij\\n...\\nXin\\nXm1\\nXm2\\n...\\nXmj\\n...\\nXmn\\nBecause of this we will refer to the ﬁrst factor as the “row” factor, and the second\\nfactor as the “column” factor.\\nAs in Section 10.3, we will suppose that the data Xij,i = 1,...,m j = 1,...,n\\nare independent normal random variables with a common variance σ 2. How-\\never, whereas in Section 10.3 we supposed that only a single factor affected the\\nmean value of a data point — namely, the sample to which it belongs — we\\nwill suppose in the present section that the mean value of data depends in an\\nadditive manner on both its row and its column.\\nIf, in the model of Section 10.3, we let Xij represent the value of the jth mem-\\nber of sample i, then that model could be symbolically represented as\\nE[Xij] = μi'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 480}, page_content='10.4 Two-factor analysis of variance: introduction and parameter estimation\\n471\\nHowever, if we let μ denote the average value of the μi — that is,\\nμ =\\nm\\n\\x04\\ni=1\\nμi/m\\nthen we can rewrite the model as\\nE[Xij] = μ + αi\\nwhere αi = μi −μ. With this deﬁnition of αi as the deviation of μi from the\\naverage mean value, it is easy to see that\\nm\\n\\x04\\ni=1\\nαi = 0\\nA two-factor additive model can also be expressed in terms of row and column\\ndeviations. If we let μij = E[Xij], then the additive model supposes that for\\nsome constants ai,i = 1, ...,m and bj,j = 1, ...,n\\nμij = ai + bj\\nContinuing our use of the “dot” (or averaging) notation, we let\\nμi. =\\nn\\n\\x04\\nj=1\\nμij/n,\\nμ.j =\\nm\\n\\x04\\ni=1\\nμij/m,\\nμ.. =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\nμij/nm\\nAlso, we let\\na. =\\nm\\n\\x04\\ni=1\\nai/m,\\nb. =\\nn\\n\\x04\\nj=1\\nbj/n\\nNote that\\nμi. =\\nn\\n\\x04\\nj=1\\n(ai + bj)/n = ai + b.\\nSimilarly,\\nμ.j = a. + bj,\\nμ.. = a. + b.\\nIf we now set\\nμ = μ.. = a. + b.\\nαi = μi. −μ = ai −a.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 481}, page_content='472 CHAPTER 10: Analysis of variance\\nβj = μ.j −μ = bj −b.\\nthen the model can be written as\\nμij = E[Xij] = μ + αi + βj\\nwhere\\nm\\n\\x04\\ni=1\\nαi =\\nn\\n\\x04\\nj=1\\nβj = 0\\nThe value μ is called the grand mean, αi is the deviation from the grand mean due\\nto row i, and βj is the deviation from the grand mean due to column j.\\nLet us now determine estimators of the parameters μ,αi,βj,i = 1,...,m,j =\\n1,...,n. To do so, continuing our use of “dot” notation, we let\\nXi. =\\nn\\n\\x04\\nj=1\\nXij/n = average of the values in row i\\nX.j =\\nm\\n\\x04\\ni=1\\nXij/m = average of the values in column j\\nX.. =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\nXij/nm = average of all data values\\nNow,\\nE[Xi.] =\\nn\\n\\x04\\nj=1\\nE[Xij]/n\\n= μ +\\nn\\n\\x04\\nj=1\\nαi/n +\\nn\\n\\x04\\nj=1\\nβj/n\\n= μ + αi\\nsince\\nn\\n\\x04\\nj=1\\nβj = 0\\nSimilarly, it follows that\\nE[X.j] = μ + βj\\nE[X..] = μ\\nBecause the preceding is equivalent to'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 482}, page_content='10.4 Two-factor analysis of variance: introduction and parameter estimation\\n473\\nE[X..] = μ\\nE[Xi. −X..] = αi\\nE[X.j −X..] = βj\\nwe see that unbiased estimators of μ,αi,βj — call them ˆμ, ˆαi, ˆβj — are given\\nby\\nˆμ = X..\\nˆαi = Xi. −X..\\nˆβj = X.j −X..\\n■\\nExample 10.4.b. The following data from Example 10.4.a give the scores ob-\\ntained when four different reading tests were given to each of ﬁve students. Use\\nit to estimate the parameters of the model.\\nExamination\\nStudent\\nRow Totals\\nXi.\\n1\\n2\\n3\\n4\\n5\\n1\\n75\\n73\\n60\\n70\\n86\\n364\\n72.8\\n2\\n78\\n71\\n64\\n72\\n90\\n375\\n75\\n3\\n80\\n69\\n62\\n70\\n85\\n366\\n73.2\\n4\\n73\\n67\\n63\\n80\\n92\\n375\\n75\\nColumn totals\\n306\\n280\\n249\\n292\\n353\\n1480\\n←grand total\\nX.j\\n76.5\\n70\\n62.25\\n73\\n88.25\\nX.. = 1480\\n20\\n= 74\\nSolution. The estimators are\\nˆμ = 74\\nˆα1 = 72.8 −74 = −1.2\\nˆβ1 = 76.5 −74 = 2.5\\nˆα2 = 75 −74 = 1\\nˆβ2 = 70 −74 = −4\\nˆα3 = 73.2 −74 = −.8\\nˆβ3 = 62.25 −74 = −11.75\\nˆα4 = 75 −74 = 1\\nˆβ4 = 73 −74 = −1\\nˆβ5 = 88.25 −74 = 14.25\\nTherefore, for instance, if one of the students is randomly chosen and then\\ngiven a randomly chosen examination, then our estimate of the mean score\\nthat will be obtained is ˆμ = 74. If we were told that examination i was taken,\\nthen this would increase our estimate of the mean score by the amount ˆαi; and\\nif we were told that the student chosen was number j, then this would increase\\nour estimate of the mean score by the amount ˆβj. Thus, for instance, we would\\nestimate that the score obtained on examination 1 by student 2 is the value of\\na random variable whose mean is ˆμ + ˆα1 + ˆβ2 = 74 −1.2 −4 = 68.8.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 483}, page_content='474 CHAPTER 10: Analysis of variance\\n10.5\\nTwo-factor analysis of variance:\\ntesting hypotheses\\nConsider the two-factor model in which one has data Xij,i = 1,...,m and j =\\n1,...,n. These data are assumed to be independent normal random variables\\nwith a common variance σ 2 and with mean values satisfying\\nE[Xij] = μ + αi + βj\\nwhere\\nm\\n\\x04\\ni=1\\nαi =\\nn\\n\\x04\\nj=1\\nβj = 0\\nIn this section, we will be concerned with testing the hypothesis\\nH0 : all αi = 0\\nagainst\\nH1 : not all the αi are equal to 0\\nThis null hypothesis states that there is no row effect, in that the value of a\\ndatum is not affected by its row factor level.\\nWe will also be interested in testing the analogous hypothesis for columns, that\\nis\\nH0 : all βj are equal to 0\\nagainst\\nH1 : not all βj are equal to 0\\nTo obtain tests for the above null hypotheses, we will apply the analysis of\\nvariance approach in which two different estimators are derived for the variance\\nσ 2. The ﬁrst will always be a valid estimator, whereas the second will only\\nbe a valid estimator when the null hypothesis is true. In addition, the second\\nestimator will tend to overestimate σ 2 when the null hypothesis is not true.\\nTo obtain our ﬁrst estimator of σ 2, we start with the fact that\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −E[Xij])2/σ 2 =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −μ −αi −βj)2/σ 2\\nis chi-square with nm degrees of freedom. If in the above expression we now\\nreplace the unknown parameters μ,α1,α2,...,αm,β1,β2,...,βn by their es-\\ntimators ˆμ, ˆα1, ˆα2,..., ˆαm, ˆβ1, ˆβ2,..., ˆβn, then it turns out that the resulting'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 484}, page_content='10.5 Two-factor analysis of variance: testing hypotheses\\n475\\nexpression will remain chi-square but will lose 1 degree of freedom for each\\nparameter that is estimated. To determine how many parameters are to be esti-\\nmated, we must be careful to remember that \\x06m\\ni=1 αi = \\x06n\\nj=1 βj = 0. Since the\\nsum of all the αi is equal to 0, it follows that once we have estimated m −1 of\\nthe αi then we have also estimated the ﬁnal one. Hence, only m −1 parameters\\nare to be estimated in order to determine all of the estimators ˆαi. For the same\\nreason, only n −1 of the βj need be estimated to determine estimators for all n\\nof them. Because μ also must be estimated, we see that the number of param-\\neters that need to be estimated is 1 + m −1 + n −1 = n + m −1. As a result, it\\nfollows that\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −ˆμ −ˆαi −ˆβj)2/σ 2\\nis a chi-square random variable with nm −(n + m −1) = (n −1)(m −1) degrees\\nof freedom.\\nSince ˆμ = X.., ˆαi = Xi. −X.., ˆβj = X.j −X.., it follows that ˆμ + ˆαi + ˆβj = Xi. +\\nX.j −X..; thus,\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −Xi. −X.j + X..)2/σ 2\\n(10.5.1)\\nis a chi-square random variable with (n −1)(m −1) degrees of freedom.\\nDeﬁnition. The statistic SSe deﬁned by\\nSSe =\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\n(Xij −Xi. −X.j + X..)2\\nis called the error sum of squares.\\nIf we think of the difference between a value and its estimated mean as being\\nan “error,” then SSe is equal to the sum of the squares of the errors. Since\\nSSe/σ 2 is just the expression in 10.5.1, we see that SSe/σ 2 is chi-square with\\n(n −1)(m −1) degrees of freedom. Because the expected value of a chi-square\\nrandom variable is equal to its number of degrees of freedom, we have that\\nE[SSe/σ 2] = (n −1)(m −1)\\nor\\nE[SSe/(n −1)(m −1)] = σ 2\\nThat is,\\nSSe/(n −1)(m −1)\\nis an unbiased estimator of σ 2.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 485}, page_content='476 CHAPTER 10: Analysis of variance\\nSuppose now that we want to test the null hypothesis that there is no row effect\\n— that is, we want to test\\nH0 : all the αi are equal to 0\\nagainst\\nH1 : not all the αi are equal to 0\\nTo obtain a second estimator of σ 2, consider the row averages Xi.,i = 1,...,m.\\nNote that, when H0 is true, each αi is equal to 0, and so\\nE[Xi.] = μ + αi = μ\\nBecause each Xi. is the average of n random variables, each having variance σ 2,\\nit follows that\\nVar(Xi.) = σ 2/n\\nThus, we see that when H0 is true\\nm\\n\\x04\\ni=1\\n(Xi. −E[Xi.])2/Var(Xi.) = n\\nm\\n\\x04\\ni=1\\n(Xi. −μ)2/σ 2\\nwill be chi-square with m degrees of freedom. If we now substitute X.. (the\\nestimator of μ) for μ in the preceding, then the resulting expression will remain\\nchi-square but with 1 less degree of freedom. We thus have the following:\\nwhen H0 is true\\nn\\nm\\n\\x04\\ni=1\\n(Xi. −X..)2/σ 2\\nis chi-square with m −1 degrees of freedom.\\nDeﬁnition. The statistic SSr is deﬁned by\\nSSr = n\\nm\\n\\x04\\ni=1\\n(Xi. −X..)2\\nand is called the row sum of squares.\\nWe saw earlier that when H0 is true, SSr/σ 2 is chi-square with m −1 degrees of\\nfreedom. As a result, when H0 is true,\\nE[SSr/σ 2] = m −1\\nor, equivalently,\\nE[SSr/(m −1)] = σ 2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 486}, page_content='10.5 Two-factor analysis of variance: testing hypotheses\\n477\\nIn addition, it can be shown that SSr/(m −1) will tend to be larger than σ 2\\nwhen H0 is not true. Thus, once again we have obtained two estimators of σ 2.\\nThe ﬁrst estimator, SSe/(n −1)(m −1), is a valid estimator whether or not the\\nnull hypothesis is true, whereas the second estimator, SSr/(m −1), is only a\\nvalid estimator of σ 2 when H0 is true and tends to be larger than σ 2 when H0\\nis not true.\\nWe base our test of the null hypothesis H0 that there is no row effect, on the\\nratio of the two estimators of σ 2. Speciﬁcally, we use the test statistic\\nT S =\\nSSr/(m −1)\\nSSe/(n −1)(m −1)\\nBecause the estimators can be shown to be independent when H0 is true, it\\nfollows that the signiﬁcance level α test is to\\nreject\\nH0\\nif\\nT S ≥Fα,m−1,(n−1)(m−1)\\ndo not reject\\nH0\\notherwise\\nAlternatively, the test can be performed by calculating the p-value. If the value\\nof the test statistic is v, then the p-value is given by\\np-value = P{Fm−1,(n−1)(m−1) ≥v}\\nA similar test can be derived for testing the null hypothesis that there is no col-\\numn effect — that is, that all the βj are equal to 0. The results are summarized\\nin Table 10.2. R can be used for the computations and the resulting p-value.\\nTable 10.2 Two-Factor ANOVA.\\nSum of Squares\\nDegrees of Freedom\\nRow\\nSSr = n\\x06m\\ni=1(Xi. −X..)2\\nm −1\\nColumn\\nSSc = m\\x06n\\nj=1(X.j −X..)2\\nn −1\\nError\\nSSe = \\x06m\\ni=1\\n\\x06n\\nj=1(Xij −Xi. −X.j + X..)2\\n(n −1)(m −1)\\nLet N = (n −1)(m −1)\\nNull\\nHypothesis\\nTest\\nStatistic\\nSigniﬁcance\\nLevel α Test\\np-value if\\nT S = v\\nAll αi = 0\\nSSr/(m −1)\\nSSe/N\\nReject if T S ≥Fα,m−1,N\\nP{Fm−1,N ≥v}\\nAll βj = 0\\nSSc/(n −1)\\nSSe/N\\nReject if T S ≥Fα,n−1,N\\nP{Fn−1,N ≥v}\\nExample 10.5.a. The following data2 represent the number of different\\nmacroinvertebrate species collected at 6 stations, located in the vicinity of a\\n2Taken from Wartz and Skinner, “A 12-year macroinvertebrate study in the vicinity of 2 thermal discharges\\nto the Susquehanna River near York Haven, PA.” Jour. of Testing and Evaluation. Vol. 12. No. 3, May 1984,\\n157–163.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 487}, page_content='478 CHAPTER 10: Analysis of variance\\nthermal discharge, from 1970 to 1977.\\nYear\\nStation\\n1\\n2\\n3\\n4\\n5\\n6\\n1970\\n53\\n35\\n31\\n37\\n40\\n43\\n1971\\n36\\n34\\n17\\n21\\n30\\n18\\n1972\\n47\\n37\\n17\\n31\\n45\\n26\\n1973\\n55\\n31\\n17\\n23\\n43\\n37\\n1974\\n40\\n32\\n19\\n26\\n45\\n37\\n1975\\n52\\n42\\n20\\n27\\n26\\n32\\n1976\\n39\\n28\\n21\\n21\\n36\\n28\\n1977\\n40\\n32\\n21\\n21\\n36\\n35\\nTo test the hypotheses that the data are unchanging (a) from year to year, and\\n(b) from station to station, we use R.\\n>\\nv1 = c(53,36,47,55,40,52,39,40)\\n>\\nv2 = c(35,34,37,31,32,42,28,32)\\n>\\nv3 = c(31,17,17,17,19,20,21,21)\\n>\\nv4 = c(37,21,31,23,26,27,21,21)\\n>\\nv5 = c(40,32,45,43,45,26,36,36)\\n>\\nv6 = c(43,18,26,37,37,32,28,35)\\n>\\nX = matrix(c(v1,v2,v3,v4,v5,v6),8,6)\\n>\\nH = sweep(X,1,rowMeans(X)) + mean(X)\\n>\\nC = sweep(H,2,colMeans(X))\\n>\\nSSe = sum(C2)\\n>\\nD = rowMeans(X) −mean(X)\\n>\\nSSr = 6 ∗sum(D2)\\n>\\nE = colMeans(X) −mean(X)\\n>\\nSSc = 8 ∗sum(E2)\\n>\\nT Srow = (SSr/7)/(SSe/35)\\n>\\nT Srow\\n[1]\\n3.602264\\n>\\n1 −pf (T Srow,7,35)\\n[1]\\n0.005037051\\n>\\nT Scol = (SSc/5)/(SSe/35)\\n>\\nT Scol\\n[1]\\n22.67062\\n>\\n1 −pf (T Scol,5,35)\\n[1]\\n4.413647e −10'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 488}, page_content='10.6 Two-way analysis of variance with interaction\\n479\\nThus, the value of the F statistic for testing that the data distribution does not\\ndepend on the year is 3.602, with a resulting p-value of 0.005; and the value\\nof the F statistic for testing that the data distribution does not depend on the\\nstation is 22.67 with a resulting p-value of 4 × 10−10. Hence, both hypotheses\\nare rejected at almost all signiﬁcance levels.\\n■\\n10.6\\nTwo-way analysis of variance with interaction\\nIn Sections 10.4 and 10.5, we considered experiments in which the distribution\\nof the observed data depended on two factors — which we called the row and\\ncolumn factors. Speciﬁcally, we supposed that the mean value of Xij, the data\\nvalue in row i and column j, can be expressed as the sum of two terms —\\none depending on the row of the element and one on the column. That is, we\\nsupposed that\\nXij ∼N(μ + αi + βj,σ 2),\\ni = 1,...,m,\\nj = 1,...,n\\nHowever, one weakness of this model is that in supposing that the row and\\ncolumn effects are additive, it does not allow for the possibility of a row and\\ncolumn interaction.\\nFor instance, consider an experiment designed to compare the mean number\\nof defective items produced by four different workers when using three dif-\\nferent machines. In analyzing the resulting data, we might suppose that the\\nincremental number of defects that resulted from using a given machine was\\nthe same for each of the workers. However, it is certainly possible that a ma-\\nchine could interact in a different manner with different workers. That is, there\\ncould be a worker–machine interaction that the additive model does not al-\\nlow for.\\nTo allow for the possibility of a row and column interaction, let\\nμij = E[Xij]\\nand deﬁne the quantities μ,αi,βj,γij,i = 1,...,m,j = 1,...,n as follows:\\nμ = μ..\\nαi = μi. −μ..\\nβj = μ.j −μ..\\nγij = μij −μi. −μ.j + μ..\\nIt is immediately apparent that'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 489}, page_content='480 CHAPTER 10: Analysis of variance\\nμij = μ + αi + βj + γij\\nand it is easy to check that\\nm\\n\\x04\\ni=1\\nαi =\\nn\\n\\x04\\nj=1\\nβj =\\nm\\n\\x04\\ni=1\\nγij =\\nn\\n\\x04\\nj=1\\nγij = 0\\nThe parameter μ is the average of all nm mean values; it is called the grand\\nmean. The parameter αi is the amount by which the average of the mean values\\nof the variables in row i exceeds the grand mean; it is called the effect of row i.\\nThe parameter βj is the amount by which the average of the mean values of the\\nvariables in column j exceeds the grand mean; it is called the effect of column j.\\nThe parameter γij = μij −(μ+αi +βj) is the amount by which μij exceeds the\\nsum of the grand mean and the increments due to row i and to column j; it is\\nthus a measure of the departure from row and column additivity of the mean\\nvalue μij, and is called the interaction of row i and column j.\\nAs we shall see, in order to be able to test the hypothesis that there are no\\nrow and column interactions — that is, that all γij = 0 — it is necessary to\\nhave more than one observation for each pair of factors. So let us suppose\\nthat we have l observations for each row and column. That is, suppose that\\nthe data are {Xijk,i = 1,...,m,j = 1,...,n,k = 1,...,l}, where Xijk is the kth\\nobservation in row i and column j. Because all observations are assumed to be\\nindependent normal random variables with a common variance σ 2, the model\\nis\\nXijk ∼N(μ + αi + βj + γij,σ 2)\\nwhere\\nm\\n\\x04\\ni=1\\nαi =\\nn\\n\\x04\\nj=1\\nβj =\\nm\\n\\x04\\ni=1\\nγij =\\nn\\n\\x04\\nj=1\\nγij = 0\\n(10.6.1)\\nWe will be interested in estimating the preceding parameters and in testing the\\nfollowing null hypotheses:\\nH r\\n0 : αi = 0,\\nfor all i\\nH c\\n0 : βj = 0,\\nfor all j\\nH int\\n0\\n: γij = 0,\\nfor all i,j\\nThat is, H r\\n0 is the hypothesis of no row effect; H c\\n0 is the hypothesis of no\\ncolumn effect; and H int\\n0\\nis the hypothesis of no row and column interac-\\ntion.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 490}, page_content='10.6 Two-way analysis of variance with interaction\\n481\\nTo estimate the parameters, note that it is easily veriﬁed from Equation (10.6.1)\\nand the identity\\nE[Xijk] = μij = μ + αi + βj + γij\\nthat\\nE[Xij.] = μij = μ + αi + βj + γij\\nE[Xi..] = μ + αi\\nE[X.j.] = μ + βj\\nE[X...] = μ\\nTherefore, with a “hat” over a parameter denoting the estimator of that param-\\neter, we obtain from the preceding that unbiased estimators are given by\\nˆμ = X...\\nˆβj = X.j. −X...\\nˆαi = Xi.. −X...\\nˆγij = Xij. −ˆμ −ˆβj −ˆαi = Xij. −Xi.. −X.j. + X...\\nTo develop tests for the null hypotheses H int\\n0 ,H r\\n0 , and H c\\n0 , start with the fact\\nthat\\nl\\x04\\nk=1\\nn\\n\\x04\\nj=1\\nm\\n\\x04\\ni=1\\n(Xijk −μ −αi −βj −γij)2\\nσ 2\\nis a chi-square random variable with nml degrees of freedom. Therefore,\\nl\\x04\\nk=1\\nn\\n\\x04\\nj=1\\nm\\n\\x04\\ni=1\\n(Xijk −ˆμ −ˆαi −ˆβj −ˆγij)2\\nσ 2\\nwill also be chi-square, but with 1 degree of freedom lost for each parameter\\nthat is estimated. Now, since \\x06\\ni αi = 0, it follows that m −1 of the αi need\\nto be estimated; similarly, n −1 of the βj need to be estimated. Also, since\\n\\x06\\ni γij = \\x06\\nj γij = 0, it follows that if we arrange all the γij in a rectangular\\narray having m rows and n columns, then all the row and column sums will\\nequal 0, and so the values of the quantities in the last row and last column\\nwill be determined by the values of all the others; hence we need only estimate\\n(m−1)(n−1) of these quantities. Because we also need to estimate μ, it follows\\nthat a total of'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 491}, page_content='482 CHAPTER 10: Analysis of variance\\nn −1 + m −1 + (n −1)(m −1) + 1 = nm\\nparameters need to be estimated. Since\\nˆμ + ˆαi + ˆβj + ˆγij = Xij.\\nit thus follows from the preceding that if we let\\nSSe =\\nl\\x04\\nk=1\\nn\\n\\x04\\nj=1\\nm\\n\\x04\\ni=1\\n(Xijk −Xij.)2\\nthen\\nSSe\\nσ 2 is chi-square with nm(l −1) degrees of freedom\\nTherefore,\\nSSe\\nnm(l −1) is an unbiased estimator of σ 2\\nSuppose now that we want to test the hypothesis that there are no row and\\ncolumn interactions — that is, we want to test\\nH int\\n0\\n: γij = 0,\\ni = 1,...,m,\\nj = 1,...,n\\nNow, if H int\\n0\\nis true, then the random variables Xij. will be normal with mean\\nE[Xij.] = μ + αi + βj\\nAlso, since each of these terms is the average of l normal random variables\\nhaving variance σ 2, it follows that\\nVar(Xij.) = σ 2/l\\nHence, under the assumption of no interactions,\\nn\\n\\x04\\nj=1\\nm\\n\\x04\\ni=1\\nl(Xij. −μ −αi −βj)2\\nσ 2\\nis a chi-square random variable with nm degrees of freedom. Since a total of\\n1 + m −1 + n −1 = n + m −1 of the parameters μ,αi,i = 1,...,m,βj,j =\\n1,...,n, must be estimated, it follows that if we let'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 492}, page_content='10.6 Two-way analysis of variance with interaction 483\\nSSint =\\nn\\n\\x04\\nj=1\\nm\\n\\x04\\ni=1\\nl(Xij. −ˆμ −ˆαi −ˆβj)2 =\\nn\\n\\x04\\nj=1\\nm\\n\\x04\\ni=1\\nl(Xij. −Xi.. −X.j. + X...)2\\nthen, under H int\\n0 ,\\nSSint\\nσ 2\\nis chi-square with (n −1)(m −1) degrees of freedom.\\nTherefore, under the assumption of no interactions,\\nSSint\\n(n −1)(m −1) is an unbiased estimator of σ 2.\\nBecause it can be shown that, under the assumption of no interactions, SSe\\nand SSint are independent, it follows that when H int\\n0\\nis true\\nFint = SSint/(n −1)(m −1)\\nSSe/nm(l −1)\\nis an F-random variable with (n −1)(m −1) numerator and nm(l −1) denom-\\ninator degrees of freedom. This gives rise to the following signiﬁcance level α\\ntest of\\nH int\\n0\\n: allγij = 0\\nNamely,\\nreject\\nH int\\n0\\nif\\nSSint/(n −1)(m −1)\\nSSe/nm(l −1)\\n> Fα,(n−1)(m−1),nm(l−1)\\ndo not reject\\nH int\\n0\\notherwise\\nAlternatively, we can compute the p-value. If Fint = v, then the p-value of the\\ntest of the null hypothesis that all interactions equal 0 is\\np-value = P{F(n−1)(m−1),nm(l−1) > v}\\nIf we want to test the null hypothesis\\nH r\\n0 : αi = 0,i = 1,...,m\\nthen we use the fact that when H r\\n0 is true, Xi.. is the average of nl independent\\nnormal random variables, each with mean μ and variance σ 2. Hence, under\\nH r\\n0 ,\\nE[Xi..] = μ,\\nVar(Xi..) = σ 2/nl'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 493}, page_content='484 CHAPTER 10: Analysis of variance\\nand so\\nm\\n\\x04\\ni=1\\nnl (Xi.. −μ)2\\nσ 2\\nis chi-square with m degrees of freedom. Thus, if we let\\nSSr =\\nm\\n\\x04\\ni=1\\nnl(Xi.. −ˆμ)2 =\\nm\\n\\x04\\ni=1\\nnl(Xi.. −X..)2\\nthen, when H r\\n0 is true,\\nSSr\\nσ 2 is chi-square with m −1 degrees of freedom\\nand so\\nSSr\\nm −1 is an unbiased estimator of σ 2\\nBecause it can be shown that, under H r\\n0 ,SSe and SSr are independent, it fol-\\nlows that when H r\\n0 is true\\nSSr/(m −1)\\nSSe/nm(l −1) is an Fm−1,nm(l −1) random variable\\nThus we have the following signiﬁcance level α test of\\nH r\\n0 : allαi = 0\\nversus\\nH r\\n1 : at least one αi ̸= 0\\nNamely,\\nreject\\nH r\\n0\\nif\\nSSr/(m −1)\\nSSe/nm(l −1) > Fα,m−1,nm(l−1)\\ndo not reject\\nH r\\n0\\notherwise\\nAlternatively, if\\nSSr/(m −1)\\nSSe/nm(l −1) = v, then\\np-value = P{Fm−1,nm(l−1) > v}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 494}, page_content='10.6 Two-way analysis of variance with interaction\\n485\\nBecause an analogous result can be shown to hold when testing H0: all βj = 0,\\nwe obtain the ANOVA information shown in Table 10.3.\\nNote that all of the preceding tests call for rejection only when their related\\nF-statistic is large. The reason that only large (and not small) values call for\\nrejection of the null hypothesis is that the numerator of the F-statistic will\\ntend to be larger when H0 is not true than when it is, whereas the distribution\\nof the denominator will be the same whether or not H0 is true.\\nExample 10.6.a. The life of a particular type of generator is thought to be in-\\nﬂuenced by the material used in its construction and also by the temperature\\nat the location where it is utilized. The following table represents lifetime data\\non 24 generators made from three different types of materials and utilized at\\ntwo different temperatures. Do the data indicate that the material and the tem-\\nperature do indeed affect the lifetime of a generator? Is there evidence of an\\ninteraction effect?\\nMaterial\\nTemperature\\n10◦C\\n18◦C\\n1\\n135, 150\\n50, 55\\n176, 85\\n64, 38\\n2\\n150, 162\\n76, 88\\n171, 120\\n91, 57\\n3\\n138, 111\\n68, 60\\n140, 106\\n74, 51\\nSolution. A computation yields that the value of the F-statistic for testing that\\nthere is no interaction between material and temperature is 0.64525, resulting\\nin\\np-value\\n=\\nP(F2,18 > 0.64525)\\n>\\n1 −pf (0.64525,2,18)\\n[1]\\n0.5362424\\nThus the hypothesis of no interaction cannot be rejected.\\nA computation yields that the value of the F-statistic for testing that the lifetime\\ndoes not depend on the material used (that is, there is no row effect) is 2.47976,\\nwith resulting\\np-value\\n=\\nP(F2,18 > 2.47976)\\n>\\n1 −pf (2.47976,2,18)\\n[1]\\n0.111889\\nThus, the hypothesis of no row effect would not be rejected at the 10 percent\\nlevel of signiﬁcance.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 495}, page_content='486 CHAPTER 10: Analysis of variance\\nTable 10.3 Two-way ANOVA with l Observations per Cell: N = nm(l −1).\\nSource of\\nVariation\\nDegrees of\\nFreedom\\nSum of Squares\\nF-Statistic\\nLevel α Test\\np-Value if F = v\\nRow\\nm −1\\nSSr = ln\\n\\x04m\\ni=1(Xi.. −X...)2\\nFr = SSr/(m −1)\\nSSe/N\\nReject H r\\n0\\nif Fr > Fα,m−1,N\\nP {Fm−1,N > v}\\nColumn\\nn −1\\nSSe = lm\\n\\x04n\\nj=1(X.j. −X...)2\\nFc = SSc/(n −1)\\nSSe/N\\nReject H c\\n0\\nif Fc > Fα,n−1,N\\nP {Fn−1,N > v}\\nInteraction\\n(n −1)(m −1)\\nSSint = l\\n\\x04n\\nj=1\\n×\\n\\x04m\\ni=1(Xij. −Xi.. −X.j. + X...)2\\nFint = SSint/(n −1)(m −1)\\nSSe/N\\nReject H int\\n0\\nif Fint > Fα,(n−1)(m−1),N\\nP {F(n−1)(m−1),N > v}\\nError\\nN\\nSSe =\\n\\x04l\\nk=1\\n\\x04n\\nj=1\\n×\\n\\x04m\\ni=1(Xijk −Xij.)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 496}, page_content='Problems 487\\nA computation shows that the value of the F-statistic for testing that the life-\\ntime does not depend on the temperature (that is, there is no column effect) is\\n69.63223, resulting in\\np-value\\n=\\nP(F2,18 > 69.63223)\\n=\\n1 −pf (69.63223,1,18)\\n[1]\\n1.337273e −07\\nConsequently, the hypothesis that there is no column effect is rejected.\\n■\\nProblems\\n1. A puriﬁcation process for a chemical involves passing it, in solution,\\nthrough a resin on which impurities are adsorbed. A chemical engineer\\nwishing to test the efﬁciency of 3 different resins took a chemical solution\\nand broke it into 15 batches. She tested each resin 5 times and then mea-\\nsured the concentration of impurities after passing through the resins.\\nHer data were as follows:\\nConcentration of Impurities\\nResin I\\nResin II\\nResin III\\n.046\\n.038\\n.031\\n.025\\n.035\\n.042\\n.014\\n.031\\n.020\\n.017\\n.022\\n.018\\n.043\\n.012\\n.039\\nTest the hypothesis that there is no difference in the efﬁciency of the\\nresins.\\n2. We want to know what type of ﬁlter should be used over the screen of\\na cathode-ray oscilloscope in order to have a radar operator easily pick\\nout targets on the presentation. A test to accomplish this has been set\\nup. A noise is ﬁrst applied to the scope to make it difﬁcult to pick out\\na target. A second signal, representing the target, is put into the scope,\\nand its intensity is increased from zero until detected by the observer.\\nThe intensity setting at which the observer ﬁrst notices the target signal\\nis then recorded. This experiment is repeated 20 times with each ﬁlter.\\nThe numerical value of each reading listed in the table of data is pro-\\nportional to the target intensity at the time the operator ﬁrst detects the\\ntarget.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 497}, page_content='488 CHAPTER 10: Analysis of variance\\nFilter No. 1\\nFilter No. 2\\nFilter No. 3\\n90\\n88\\n95\\n87\\n90\\n95\\n93\\n97\\n89\\n96\\n87\\n98\\n94\\n90\\n96\\n88\\n96\\n81\\n90\\n90\\n92\\n84\\n90\\n79\\n101\\n100\\n105\\n96\\n93\\n98\\n90\\n95\\n92\\n82\\n86\\n85\\n93\\n89\\n97\\n90\\n92\\n90\\n96\\n98\\n87\\n87\\n95\\n90\\n99\\n102\\n101\\n101\\n105\\n100\\n79\\n85\\n84\\n98\\n97\\n102\\nTest, at the 5 percent level of signiﬁcance, the hypothesis that the ﬁlters\\nare the same.\\n3. Explain why we cannot efﬁciently test the hypothesis H0 : μ1 = μ2 = ··· =\\nμm by running t-tests on all of the\\n\\x02m\\n2\\n\\x03\\npairs of samples.\\n4. A machine shop contains 3 ovens that are used to heat metal specimens.\\nSubject to random ﬂuctuations, they are all supposed to heat to the same\\ntemperature. To test this hypothesis, temperatures were noted on 15 sep-\\narate heatings. The following data resulted.\\nOven\\nTemperature\\n1\\n492.4, 493.6, 498.5, 488.6, 494\\n2\\n488.5, 485.3, 482, 479.4, 478\\n3\\n502.1, 492, 497.5, 495.3, 486.7\\nDo the ovens appear to operate at the same temperature? Test at the 5\\npercent level of signiﬁcance. What is the p-value?\\n5. Four standard chemical procedures are used to determine the mag-\\nnesium content in a certain chemical compound. Each procedure is\\nused four times on a given compound with the following data result-\\ning.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 498}, page_content='Problems 489\\nMethod\\n1\\n2\\n3\\n4\\n76.42\\n80.41\\n74.20\\n86.20\\n78.62\\n82.26\\n72.68\\n86.04\\n80.40\\n81.15\\n78.84\\n84.36\\n78.20\\n79.20\\n80.32\\n80.68\\nDo the data indicate that the procedures yield equivalent results?\\n6. Twenty overweight individuals, each more than 40 pounds overweight,\\nwere randomly assigned to one of two diets. After 10 weeks, the total\\nweight losses (in pounds) of the individuals on each of the diets were as\\nfollows:\\nWeight Loss\\nDiet 1\\nDiet 2\\n22.2\\n24.2\\n23.4\\n16.8\\n24.2\\n14.6\\n16.1\\n13.7\\n9.4\\n19.5\\n12.5\\n17.6\\n18.6\\n11.2\\n32.2\\n9.5\\n8.8\\n30.1\\n7.6\\n21.5\\nTest, at the 5 percent level of signiﬁcance, the hypothesis that the two\\ndiets have equal effect.\\n7. In a test of the ability of a certain polymer to remove toxic wastes from\\nwater, experiments were conducted at three different temperatures. The\\ndata below give the percentages of the impurities that were removed by\\nthe polymer in 21 independent attempts.\\nLow Temperature\\nMedium Temperature\\nHigh Temperature\\n42\\n36\\n33\\n41\\n35\\n44\\n37\\n32\\n40\\n29\\n38\\n36\\n35\\n39\\n44\\n40\\n42\\n37\\n32\\n34\\n45\\nTest the hypothesis that the polymer performs equally well at all three\\ntemperatures. Use the (a) 5 percent level of signiﬁcance and (b) 1 per-\\ncent level of signiﬁcance.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 499}, page_content='490 CHAPTER 10: Analysis of variance\\n8. In the one-factor analysis of variance model with n observations per sam-\\nple, let S2\\ni ,i = 1,...,m denote the sample variances for the m samples.\\nShow that\\nSSW = (n −1)\\nm\\n\\x04\\ni=1\\nS2\\ni\\n9. The following data relate to the ages at death of a certain species of rats\\nthat were fed 1 of 3 types of diets. Thirty rats of a type having a short life\\nspan were randomly divided into 3 groups of size 10 each. The sample\\nmeans and sample variances of the ages at death (measured in months)\\nof the 3 groups are as follows:\\nVery Low Calorie\\nModerate Calorie\\nHigh Calorie\\nSample mean\\n22.4\\n16.8\\n13.7\\nSample variance\\n24.0\\n23.2\\n17.1\\nTest the hypothesis, at the 5 percent level of signiﬁcance, that the mean\\nlifetime of a rat is not affected by its diet. What about at the 1 percent\\nlevel?\\n10. Plasma bradykininogen levels are related to the body’s ability to resist\\ninﬂammation. In a 1968 study (Eilam, N., Johnson, P. K., Johnson, N. L.,\\nand Creger, W., “Bradykininogen levels in Hodgkin’s disease,” Cancer, 22,\\npp. 631–634), levels were measured in normal patients, in patients with\\nactive Hodgkin’s disease, and in patients with inactive Hodgkin’s disease.\\nThe following data (in micrograms of bradykininogen per milliliter of\\nplasma) resulted.\\nNormal\\nActive Hodgkin’s Disease\\nInactive Hodgkin’s Disease\\n5.37\\n3.96\\n5.37\\n5.80\\n3.04\\n10.60\\n4.70\\n5.28\\n5.02\\n5.70\\n3.40\\n14.30\\n3.40\\n4.10\\n9.90\\n8.60\\n3.61\\n4.27\\n7.48\\n6.16\\n5.75\\n5.77\\n3.22\\n5.03\\n7.15\\n7.48\\n5.74\\n6.49\\n3.87\\n7.85\\n4.09\\n4.27\\n6.82\\n5.94\\n4.05\\n7.90\\n6.38\\n2.40\\n8.36\\nTest, at the 5 percent level of signiﬁcance, the hypothesis that the mean\\nbradykininogen levels are the same for all three groups.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 500}, page_content='Problems 491\\n11. A study of the trunk ﬂexor muscle strength of 75 girls aged 3 to 7 was\\nreported by Baldauf, K., Swenson, D., Medeiros, J., and Radtka, S., “Clin-\\nical assessment of trunk ﬂexor muscle strength in healthy girls 3 to 7,”\\nPhysical Therapy, 64, pp. 1203–1208, 1984. With muscle strength graded\\non a scale of 0 to 5, and with 15 girls in each age group, the following\\nsample means and sample standard deviations resulted.\\nAge\\n3\\n4\\n5\\n6\\n7\\nSample mean\\n3.3\\n3.7\\n4.1\\n4.4\\n4.8\\nSample standard deviation\\n.9\\n1.1\\n1.1\\n.9\\n.5\\nTest, at the 5 percent level of signiﬁcance, the hypothesis that the mean\\ntrunk ﬂexor strength is the same for all ﬁve age groups.\\n12. An emergency room physician wanted to know whether there were any\\ndifferences in the amount of time it takes for three different inhaled\\nsteroids to clear a mild asthmatic attack. Over a period of weeks she\\nrandomly administered these steroids to asthma sufferers, and noted the\\ntime it took for the patients’ lungs to become clear. Afterward, she discov-\\nered that 12 patients had been treated with each type of steroid, with the\\nfollowing sample means (in minutes) and sample variances resulting.\\nSteroid\\nXi\\nS2\\ni\\nA\\n32\\n145\\nB\\n40\\n138\\nC\\n30\\n150\\na.\\nTest the hypothesis that the mean time to clear a mild asthmatic\\nattack is the same for all three steroids. Use the 5 percent level of\\nsigniﬁcance.\\nb.\\nFind conﬁdence intervals for all quantities μi −μj that, with 95\\npercent conﬁdence, are valid.\\n13. Five servings each of three different brands of processed meat were tested\\nfor fat content. The following data (in fat percentage per gram) resulted.\\nBrand\\n1\\n2\\n3\\nFat\\ncontent\\n32\\n41\\n36\\n34\\n32\\n37\\n31\\n33\\n30\\n35\\n29\\n28\\n33\\n35\\n33\\na.\\nDoes the fat content differ depending on the brand?\\nb.\\nFind conﬁdence intervals for all quantities μi −μj that, with 95\\npercent conﬁdence, are valid.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 501}, page_content='492 CHAPTER 10: Analysis of variance\\n14. A nutritionist randomly divided 15 bicyclists into 3 groups of 5 each.\\nThe ﬁrst group was given a vitamin supplement to take with each of their\\nmeals during the next 3 weeks. The second group was instructed to eat a\\nparticular type of high-ﬁber whole-grain cereal for the next 3 weeks. The\\nﬁnal group was instructed to eat as they normally do. After the 3-week\\nperiod elapsed, the nutritionist had each of the bicyclists ride 6 miles.\\nThe following times were recorded.\\nVitamin group:\\n15.6 16.4 17.2\\n15.5 16.3\\nFiber cereal group: 17.1 16.3 15.8\\n16.4 16.0\\nControl group:\\n15.9 17.2 16.4\\n15.4 16.8\\na.\\nAre the data consistent with the hypothesis that neither the vitamin\\nnor the ﬁber cereal affected the bicyclists’ speeds? Use the 5 percent\\nlevel of signiﬁcance.\\nb.\\nFind conﬁdence intervals for all quantities μi −μj that, with 95\\npercent conﬁdence, are valid.\\n15. Test the hypothesis that the following three independent samples all\\ncome from the same normal probability distribution.\\nSample 1\\nSample 2\\nSample 3\\n35\\n29\\n44\\n37\\n38\\n52\\n29\\n34\\n56\\n27\\n30\\n30\\n32\\n16. For data xij,i = 1,...,m,j = 1,...,n, show that\\nx.. =\\nm\\n\\x04\\ni=1\\nxi./m =\\nn\\n\\x04\\nj=1\\nx.j/n\\n17. If xij = i + j2, determine\\na.\\n\\x043\\nj=1\\n\\x042\\ni=1xij\\nb.\\n\\x042\\ni=1\\n\\x043\\nj=1xij\\n18. If xij = ai + bj, show that\\nm\\n\\x04\\ni=1\\nn\\n\\x04\\nj=1\\nxij = n\\nm\\n\\x04\\ni=1\\nai + m\\nn\\n\\x04\\nj=1\\nbj\\n19. A study has been made on pyrethrum ﬂowers to determine the content\\nof pyrethrin, a chemical used in insecticides. Four methods of extracting\\nthe chemical are used, and samples are obtained from ﬂowers stored un-\\nder three conditions: fresh ﬂowers, ﬂowers stored for 1 year, and ﬂowers'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 502}, page_content='Problems\\n493\\nstored for 1 year but treated. It is assumed that there is no interaction\\npresent. The data are in Table 10.4.\\nTable 10.4\\nPyrethrin Content, Per-\\ncent.\\nStorage\\nCondition\\nMethod\\nA\\nB\\nC\\nD\\n1\\n1.35\\n1.13\\n1.06\\n.98\\n2\\n1.40\\n1.23\\n1.26\\n1.22\\n3\\n1.49\\n1.46\\n1.40\\n1.35\\nSuggest a model for the preceding information, and use the data to esti-\\nmate its parameters.\\n20. The following data refer to the number of deaths per 10,000 adults in a\\nlarge Eastern city in the different seasons for the years 1982 to 1986.\\nYear\\nWinter\\nSpring\\nSummer\\nFall\\n1982\\n33.6\\n31.4\\n29.8\\n32.1\\n1983\\n32.5\\n30.1\\n28.5\\n29.9\\n1984\\n35.3\\n33.2\\n29.5\\n28.7\\n1985\\n34.4\\n28.6\\n33.9\\n30.1\\n1986\\n37.3\\n34.1\\n28.5\\n29.4\\na.\\nAssuming a two-factor model, estimate the parameters.\\nb.\\nTest the hypothesis that death rates do not depend on the season.\\nUse the 5 percent level of signiﬁcance.\\nc.\\nTest, at the 5 percent level of signiﬁcance, the hypothesis that there\\nis no effect due to the year.\\n21. For the model of Problem 19:\\na.\\nDo the methods of extraction appear to differ?\\nb.\\nDo the storage conditions affect the content? Test at the α = .05 level\\nof signiﬁcance.\\n22. Three different washing machines were employed to test four different\\ndetergents. The following data give a coded score of the effectiveness of\\neach washing.\\nMachine\\n1\\n2\\n3\\nDetergent 1\\n53\\n50\\n59\\nDetergent 2\\n54\\n54\\n60\\nDetergent 3\\n56\\n58\\n62\\nDetergent 4\\n50\\n45\\n57\\na.\\nEstimate the improvement in mean value when using detergent 1\\nover using detergents (i) 2; (ii) 3; (iii) 4.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 503}, page_content='494 CHAPTER 10: Analysis of variance\\nb.\\nEstimate the improvement in mean value when using machine 3 as\\nopposed to using machine (i) 1; (ii) 2.\\nc.\\nTest the hypothesis that the detergent used does not affect the score.\\nd.\\nTest the hypothesis that the machine used does not affect the score.\\nUse, in both (c) and (d), the 5 percent level of signiﬁcance.\\n23. An experiment was devised to test the effects of running 3 different types\\nof gasoline with 3 possible types of additives. The experiment called for 9\\nidentical motors to be run with 5 gallons for each of the pairs of gasoline\\nand additives. The following data resulted.\\nTable 10.5 Mileage Obtained.\\nGasoline\\nAdditive\\n1\\n2\\n3\\n1\\n124.1\\n131.5\\n127\\n2\\n126.4\\n130.6\\n128.4\\n3\\n127.2\\n132.7\\n125.6\\na.\\nTest the hypothesis that the gasoline used does not affect the\\nmileage.\\nb.\\nTest the hypothesis that the additives are equivalent.\\nc.\\nWhat assumptions are you making?\\n24. Suppose in Problem 6 that the 10 people placed on each diet consisted\\nof 5 men and 5 women, with the following data.\\nDiet 1\\nDiet 2\\nWomen\\n7.6\\n19.5\\n8.8\\n17.6\\n12.5\\n16.8\\n16.1\\n13.7\\n18.6\\n21.5\\nMen\\n22.2\\n30.1\\n23.4\\n24.2\\n24.2\\n9.5\\n32.2\\n14.6\\n9.4\\n11.2\\na.\\nTest the hypothesis that there is no interaction between gender and\\ndiet.\\nb.\\nTest the hypothesis that the diet has the same effect on men and\\nwomen.\\n25. A researcher is interested in comparing the breaking strength of different\\nlaminated beams made from 3 different types of glue and 3 varieties of\\nwood. To make the comparison, 5 beams of each of the 9 combinations'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 504}, page_content='Problems 495\\nwere manufactured and then put under a stress test. The following table\\nindicates the pressure readings at which each of the beams broke.\\nWood\\nGlue\\nG1\\nG2\\nG3\\nW1\\n196\\n208\\n214\\n216\\n258\\n250\\n247\\n216\\n235\\n240\\n264\\n248\\n221\\n252\\n272\\nW2\\n216\\n228\\n215\\n217\\n246\\n247\\n240\\n224\\n235\\n219\\n261\\n250\\n236\\n241\\n255\\nW3\\n230\\n242\\n212\\n218\\n255\\n251\\n232\\n244\\n216\\n224\\n261\\n258\\n228\\n222\\n247\\na.\\nTest the hypothesis that the wood and glue effect is additive.\\nb.\\nTest the hypothesis that the wood used does not affect the breaking\\nstrength.\\nc.\\nTest the hypothesis that the glue used does not affect the breaking\\nstrength.\\n26. A study was made as to how the concentration of a certain drug in the\\nblood, 24 hours after being injected, is inﬂuenced by age and gender. An\\nanalysis of the blood samples of 40 people given the drug yielded the\\nfollowing concentrations (in milligrams per cubic centimeter).\\nAge Group\\n11–25\\n26–40\\n41–65\\nOver 65\\nMale\\n52\\n52.5\\n53.2\\n82.4\\n56.6\\n49.6\\n53.6\\n86.2\\n68.2\\n48.7\\n49.8\\n101.3\\n82.5\\n44.6\\n50.0\\n92.4\\n85.6\\n43.4\\n51.2\\n78.6\\nFemale\\n68.6\\n60.2\\n58.7\\n82.2\\n80.4\\n58.4\\n55.9\\n79.6\\n86.2\\n56.2\\n56.0\\n81.4\\n81.3\\n54.2\\n57.2\\n80.6\\n77.2\\n61.1\\n60.0\\n82.2\\na.\\nTest the hypothesis of no age and gender interaction.\\nb.\\nTest the hypothesis that gender does not affect the blood concentra-\\ntion.\\nc.\\nTest the hypothesis that age does not affect blood concentration.\\n27. Suppose, in Problem 23, that there has been some controversy about\\nthe assumption of no interaction between gasoline and additive used.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 505}, page_content='496 CHAPTER 10: Analysis of variance\\nTo allow for the possibility of an interaction effect between gasoline and\\nadditive, it was decided to run 36 motors — 4 in each grouping. The\\nfollowing data resulted.\\nGasoline\\nAdditive\\n1\\n2\\n3\\n1\\n126.2\\n130.4\\n127\\n124.8\\n131.6\\n126.6\\n125.3\\n132.5\\n129.4\\n127.0\\n128.6\\n130.1\\n2\\n127.2\\n142.1\\n129.5\\n126.6\\n132.6\\n142.6\\n125.8\\n128.5\\n140.5\\n128.4\\n131.2\\n138.7\\n3\\n127.1\\n132.3\\n125.2\\n128.3\\n134.1\\n123.3\\n125.1\\n130.6\\n122.6\\n124.9\\n133.0\\n120.9\\na.\\nDo the data indicate an interaction effect?\\nb.\\nDo the gasolines appear to give equal results?\\nc.\\nTest whether or not there is an additive effect or whether all additives\\nwork equally well.\\nd.\\nWhat conclusions can you draw?\\n28. An experiment has been devised to test the hypothesis that an elderly\\nperson’s memory retention can be improved by a set of “oxygen treat-\\nments.” A group of scientists administered these treatments to men and\\nwomen. The men and women were each randomly divided into 4 groups\\nof 5 each, and the people in the ith group were given treatments over\\nan (i −1) week interval, i = 1,2,3,4. (The 2 groups not given any treat-\\nments served as “controls.”) The treatments were set up in such a manner\\nthat all individuals thought they were receiving the oxygen treatments\\nfor the total 3 weeks. After treatment ended, a memory retention test was\\nadministered. The results (with higher scores indicating higher memory\\nretentions) are shown in the table.\\na.\\nTest whether or not there is an interaction effect.\\nb.\\nTest the hypothesis that the length of treatment does not affect mem-\\nory retention.\\nc.\\nIs there a gender difference?\\nd.\\nA randomly chosen group of 5 elderly men, without receiving any\\noxygen treatment, were given the memory retention test. Their scores\\nwere 37, 35, 33, 39, 29. What conclusions can you draw?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 506}, page_content='Problems 497\\nTable 10.6 Scores.\\nNumber of Weeks of Oxygen Treatment\\n0\\n1\\n2\\n3\\nMen\\n42\\n39\\n38\\n42\\n54\\n52\\n50\\n55\\n46\\n51\\n47\\n39\\n38\\n50\\n45\\n38\\n51\\n47\\n43\\n51\\nWomen\\n49\\n48\\n27\\n61\\n44\\n51\\n42\\n55\\n50\\n52\\n47\\n45\\n45\\n54\\n53\\n40\\n43\\n40\\n58\\n42\\n29. In a study of platelet production, 16 rats were put at an altitude of 15,000\\nfeet, while another 16 were kept at sea level (Rand, K., Anderson, T.,\\nLukis, G., and Creger, W., “Effect of hypoxia on platelet level in the rat,”\\nClinical Research, 18, p. 178, 1970). Half of the rats in both groups had\\ntheir spleens removed. The ﬁbrinogen levels on day 21 are reported be-\\nlow.\\nSpleen Removed\\nNormal Spleen\\nAltitude\\n528\\n434\\n444\\n331\\n338\\n312\\n342\\n575\\n338\\n472\\n331\\n444\\n288\\n575\\n319\\n384\\nSea Level\\n294\\n272\\n254\\n275\\n352\\n350\\n241\\n350\\n291\\n466\\n175\\n388\\n241\\n425\\n238\\n344\\na.\\nTest the hypothesis that there are no interactions.\\nb.\\nTest the hypothesis that there is no effect due to altitude.\\nc.\\nTest the hypothesis that there is no effect due to spleen removal. In\\nall cases, use the 5 percent level of signiﬁcance.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 507}, page_content='498 CHAPTER 10: Analysis of variance\\n30. Suppose that μ,α1,...,αm,β1,...,βn and μ′,α′\\n1,...,α′\\nm,β′\\n1,...,β′\\nn are\\nsuch that\\nμ + αi + βj = μ′ + α′\\ni + β′\\nj\\nfor all i,j\\n\\x04\\ni\\nαi =\\n\\x04\\ni\\nα′\\ni =\\n\\x04\\nj\\nβj =\\n\\x04\\nj\\nβ′\\nj = 0\\nShow that\\nμ = μ′, αi = α′\\ni, βj = β′\\nj\\nfor all i and j. This shows that the parameters μ,α1,...,αm,β1,...,βn in\\nour representation of two-factor ANOVA are uniquely determined.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 508}, page_content='CHAPTER 11\\nGoodness of ﬁt tests and categorical data\\nanalysis\\n11.1\\nIntroduction\\nWe are often interested in determining whether or not a particular probabilistic\\nmodel is appropriate for a given random phenomenon. This determination\\noften reduces to testing whether a given random sample comes from some\\nspeciﬁed, or partially speciﬁed, probability distribution. For example, we may a\\npriori feel that the number of industrial accidents occurring daily at a particular\\nplant should constitute a random sample from a Poisson distribution. This\\nhypothesis can then be tested by observing the number of accidents over a\\nsequence of days and then testing whether it is reasonable to suppose that the\\nunderlying distribution is Poisson. Statistical tests that determine whether a\\ngiven probabilistic mechanism is appropriate are called goodness of ﬁt tests.\\nThe classical approach to obtaining a goodness of ﬁt test of a null hypothesis\\nthat a sample has a speciﬁed probability distribution is to partition the possible\\nvalues of the random variables into a ﬁnite number of regions. The numbers of\\nthe sample values that fall within each region are then determined and com-\\npared with the theoretical expected numbers under the speciﬁed probability\\ndistribution, and when they are signiﬁcantly different the null hypothesis is\\nrejected. The details of such a test are presented in Section 11.2, where it is as-\\nsumed that the null hypothesis probability distribution is completely speciﬁed.\\nIn Section 11.3, we show how to do the analysis when some of the parameters\\nof the null hypothesis distribution are left unspeciﬁed; that is, for instance, the\\nnull hypothesis might be that the sample distribution is a normal distribution,\\nwithout specifying the mean and variance of this distribution. In Sections 11.4\\nand 11.5, we consider situations where each member of a population is clas-\\nsiﬁed according to two distinct characteristics, and we show how to use our\\nprevious analysis to test the hypothesis that the characteristics of a randomly\\nchosen member of the population are independent. As an application, we show\\nhow to test the hypothesis that m population all have the same discrete prob-\\nability distribution. Finally, in the optional section, Section 11.6, we return to\\nthe problem of testing that sample data come from a speciﬁed probability dis-\\ntribution, which we now assume is continuous. Rather than discretizing the\\ndata so as to be able to use the test of Section 11.2, we treat the data as given\\nand make use of the Kolmogorov–Smirnov test.\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00020-X\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n499'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 509}, page_content='500 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\n11.2\\nGoodness of ﬁt tests when all parameters are\\nspeciﬁed\\nSuppose that n independent random variables — Y1,...,Yn, each taking on one\\nof the values 1,2,...,k — are to be observed and we are interested in testing\\nthe null hypothesis that {pi,i = 1,...,k} is the probability mass function of\\nthe Yj. That is, if Y represents any of the Yj, then the null hypothesis is\\nH0 : P{Y = i} = pi,\\ni = 1,...,k\\nwhereas the alternative hypothesis is\\nH1 : P{Y = i} ̸= pi,\\nfor some i = 1,...,k\\nTo test the foregoing hypothesis, let Xi,i = 1,...,k, denote the number of the\\nYj’s that equal i. Then as each Yj will independently equal i with probability\\nP{Y = i}, it follows that, under H0,Xi is binomial with parameters n and pi.\\nHence, when H0 is true,\\nE[Xi] = npi\\nand so (Xi −npi)2 will be an indication as to how likely it appears that pi\\nindeed equals the probability that Y = i. When this is large, say, in relationship\\nto npi, then it is an indication that H0 is not correct. Indeed such reasoning\\nleads us to consider the following test statistic:\\nT =\\nk\\n\\x02\\ni=1\\n(Xi −npi)2\\nnpi\\n(11.2.1)\\nand to reject the null hypothesis when T is large.\\nTo determine the critical region, we need ﬁrst specify a signiﬁcance level α and\\nthen we must determine that critical value c such that\\nPH0{T ≥c} = α\\nThat is, we need to determine c so that the probability that the test statistic\\nT is at least as large as c, when H0 is true, is α. The test is then to reject the\\nhypothesis, at the α level of signiﬁcance, when T ≥c and to accept when T < c.\\nIt remains to determine c. The classical approach to doing so is to use the re-\\nsult that when n is large T will have, when H0 is true, approximately (with the\\napproximation becoming exact as n approaches inﬁnity) a chi-square distribu-\\ntion with k −1 degrees of freedom. Hence, for n large, c can be taken to equal\\nχ2\\nα,k−1; and so the approximate α level test is\\nreject\\nH0\\nif\\nT ≥χ2\\nα,k−1\\naccept\\nH0\\notherwise'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 510}, page_content='11.2 Goodness of ﬁt tests when all parameters are speciﬁed\\n501\\nIf the observed value of T is T = t, then the preceding test is equivalent to\\nrejecting H0 if the signiﬁcance level α is at least as large as the p-value given by\\np-value = PH0{T ≥t}\\n≈P{χ2\\nk−1 ≥t}\\nwhere χ2\\nk−1 is a chi-square random variable with k −1 degrees of freedom.\\nAn accepted rule of thumb as to how large n need be for the foregoing to be a\\ngood approximation is that it should be large enough so that npi ≥1 for each\\ni,i = 1,...,k, and also at least 80 percent of the values npi should exceed 5.\\nRemarks\\n(a) A computationally simpler formula for T can be obtained by expanding the\\nsquare in Equation (11.2.1) and using the results that \\x03\\ni pi = 1 and \\x03\\ni Xi = n\\n(why is this true?):\\nT =\\nk\\n\\x02\\ni=1\\nX2\\ni −2npiXi + n2p2\\ni\\nnpi\\n(11.2.2)\\n=\\n\\x02\\ni\\nX2\\ni /npi −2\\n\\x02\\ni\\nXi + n\\n\\x02\\ni\\npi\\n=\\n\\x02\\ni\\nX2\\ni /npi −n\\n(b) The intuitive reason why T , which depends on the k values X1,...,Xk, has\\nonly k −1 degrees of freedom is that 1 degree of freedom is lost because of the\\nlinear relationship \\x03\\ni Xi = n.\\n(c) Whereas the proof that, asymptotically, T has a chi-square distribution is\\nadvanced, it can be easily shown when k = 2. In this case, since X1 + X2 = n,\\nand p1 + p2 = 1, we see that\\nT = (X1 −np1)2\\nnp1\\n+ (X2 −np2)2\\nnp2\\n= (X1 −np1)2\\nnp1\\n+ (n −X1 −n[1 −p1])2\\nn(1 −p1)\\n= (X1 −np1)2\\nnp1\\n+ (X1 −np1)2\\nn(1 −p1)\\n= (X1 −np1)2\\nnp1(1 −p1)\\nsince\\n1\\np +\\n1\\n1 −p =\\n1\\np(1 −p)\\nHowever, X1 is a binomial random variable with mean np1 and variance\\nnp1(1 −p1) and thus, by the normal approximation to the binomial, it follows'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 511}, page_content='502 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nthat (X1 −np1)/ √np1(1 −p1) has, for large n, approximately a standard nor-\\nmal distribution, and so its square has approximately a chi-square distribution\\nwith 1 degree of freedom.\\nExample 11.2.a. In recent years, a correlation between mental and physi-\\ncal well-being has increasingly become accepted. An analysis of birthdays\\nand death days of famous people could be used as further evidence in\\nthe study of this correlation. To use these data, we are supposing that be-\\ning able to look forward to something betters a person’s mental state, and\\nthat a famous person would probably look forward to his or her birth-\\nday because of the resulting attention, affection, and so on. If a famous\\nperson is in poor health and dying, then perhaps anticipating his birthday\\nwould “cheer him up and therefore improve his health and possibly de-\\ncrease the chance that he will die shortly before his birthday.” The data might\\ntherefore reveal that a famous person is less likely to die in the months\\nbefore his or her birthday and more likely to die in the months after-\\nward.\\nSolution. To test this, a sample of 1251 (deceased) Americans was randomly\\nchosen from Who Was Who in America, and their birth and death days were\\nnoted. (The data are taken from D. Phillips, “Death Day and Birthday: An\\nUnexpected Connection,” in Statistics: A Guide to the Unknown, Holden-Day,\\n1972.) The data are summarized in Table 11.1.\\nIf the death day does not depend on the birthday, then it would seem that\\neach of the 1251 individuals would be equally likely to fall in any of the 12\\ncategories. Thus, let us test the null hypothesis\\nH0 = pi = 1\\n12,\\ni = 1,...,12\\nSince npi = 1251/12 = 104.25, the chi-square test statistic for this hypothesis\\nis\\nT = (90)2 + (100)2 + (87)2 + ··· + (106)2\\n104.25\\n−1251\\n= 17.192\\nThe p-value is\\np-value\\n≈\\nP(χ2\\n11 ≥17.192)\\n>\\n1 −pchisq(17.192,11)\\n[1]\\n0.1023232\\nThe results of this test leave us somewhat up in the air about the hypoth-\\nesis that an approaching birthday has no effect on an individual’s remain-\\ning lifetime. For whereas the data are not quite strong enough (at least, at'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 512}, page_content='11.2 Goodness of ﬁt tests when all parameters are speciﬁed 503\\nTable 11.1 Number of Deaths Before, During, and After the Birth Month.\\n6\\nMonths\\nBefore\\n5\\nMonths\\nBefore\\n4\\nMonths\\nBefore\\n3\\nMonths\\nBefore\\n2\\nMonths\\nBefore\\n1\\nMonth\\nBefore\\nThe\\nMonth\\n1\\nMonth\\nAfter\\n2\\nMonths\\nAfter\\n3\\nMonths\\nAfter\\n4\\nMonths\\nAfter\\n5\\nMonths\\nAfter\\nNumber of\\ndeaths\\n90\\n100\\n87\\n96\\n101\\n86\\n119\\n118\\n121\\n114\\n113\\n106\\nn = 1251.\\nn/12 = 104.25.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 513}, page_content='504 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nthe 10 percent level of signiﬁcance) to reject this hypothesis, they are cer-\\ntainly suggestive of its possible falsity. This raises the possibility that per-\\nhaps we should not have allowed as many as 12 data categories, and that\\nwe might have obtained a more powerful test by allowing for a fewer num-\\nber of possible outcomes. For instance, let us determine what the result\\nwould have been if we had coded the data into 4 possible outcomes as fol-\\nlows:\\noutcome 1 = −6,−5,−4\\noutcome 2 = −3,−2,−1\\noutcome 3 = 0,1,2\\noutcome 4 = 3,4,5\\nThat is, for instance, an individual whose death day occurred 3 months before\\nhis or her birthday would be placed in outcome 2. With this classiﬁcation, the\\ndata would be as follows:\\nOutcome\\nNumber of\\nTimes Occurring\\n1\\n277\\n2\\n283\\n3\\n358\\n4\\n333\\nn = 1251.\\nn/4 = 312.75.\\nThe test statistic for testing H0 = pi = 1/4,i = 1,2,3,4 is\\nT = (277)2 + (283)2 + (358)2 + (333)2\\n312.75\\n−1.251\\n= 14.775\\nHence, as χ2\\n.01,3 = 11.345, the null hypothesis would be rejected even at the 1\\npercent level of signiﬁcance. Indeed, R yields\\np-value = P{χ2\\n3 ≥14.775} = 1 −pchisq(14.775,3) = 0.00201938\\nThe foregoing analysis is, however, subject to the criticism that the null hypoth-\\nesis was chosen after the data were observed. Indeed, while there is nothing\\nincorrect about using a set of data to determine the “correct way” of phrasing\\na null hypothesis, the additional use of those data to test that very hypothesis\\nis certainly questionable. Therefore, to be quite certain of the conclusion to be\\ndrawn from this example, it seems prudent to choose a second random sam-\\nple — coding the values as before — and again test H0 : pi = 1/4,i = 1,2,3,4\\n(see Problem 3).\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 514}, page_content='11.2 Goodness of ﬁt tests when all parameters are speciﬁed 505\\nR can be used to both compute the value of the test statistic and determine\\nthe resulting p-value. Suppose x = c(x1,...,xk) is the vector giving the num-\\nber of times that each of the k possible values occurred, and suppose we\\nwant to test the null hypothesis that the probability vector for outcomes is\\np = c(p1,...,pk). To do so, just use the R commands\\n>\\nx = c(x1,...,xk)\\n>\\nchisq.test(x,p = c(p1,...,pk))\\nFor instance, for the example just considered\\n>\\nx = c(277,283,358,333)\\n>\\nchisq.test(x,p = c(1/4,1/4,1/4,1/4))\\nChi-squared test for given probabilities\\ndata :\\nx\\nX −squared = 14.775,\\ndf = 3, p-value = 0.00202\\nExample 11.2.b. A contractor who purchases a large number of ﬂuorescent\\nlightbulbs has been told by the manufacturer that these bulbs are not of uni-\\nform quality but rather have been produced in such a way that each bulb\\nproduced will, independently, either be of quality level A, B, C, D, or E, with\\nrespective probabilities .15, .25, .35, .20, .05. However, the contractor feels that\\nhe is receiving too many type E (the lowest quality) bulbs, and so he decides to\\ntest the producer’s claim by taking the time and expense to ascertain the qual-\\nity of 30 such bulbs. Suppose that he discovers that of the 30 bulbs, 3 are of\\nquality level A, 6 are of quality level B, 9 are of quality level C, 7 are of quality\\nlevel D, and 5 are of quality level E. Do these data, at the 5 percent level of\\nsigniﬁcance, enable the contractor to reject the producer’s claim?\\nSolution. We use R as follows:\\n>\\nx = c(3,6,9,7,5)\\n>\\nchisq.test(x,p = c(.15,.25,.35,.20,.05))\\nChi-squared test for given probabilities\\ndata : x\\nX −squared = 9.3476,\\ndf = 4, p-value = 0.05297\\nWarning message:\\nIn chi-sq.test(x, p = c(.15,.25,.35,.20,.05))\\nChi-squared approximation may be incorrect\\nNote that R gives the warning that, because of the relatively small numbers\\nfalling in each category, the chi-squared approximation for the distribution\\nof the test statistic may not be totally accurate. Assuming that its accuracy is'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 515}, page_content='506 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nnot an issue, the hypothesis would not be rejected at the 5 percent level of\\nsigniﬁcance (but since it would be rejected at all signiﬁcance levels above .053,\\nthe contractor should certainly remain skeptical).\\n■\\n11.2.1\\nDetermining the critical region by simulation1\\nFrom 1900 when Karl Pearson ﬁrst showed that T has approximately (becom-\\ning exact as n approaches inﬁnity) a chi-square distribution with k −1 degrees\\nof freedom, until relatively recently, this approximation was the only means\\navailable for determining the p-value of the goodness of ﬁt test. However, with\\nthe advent of inexpensive, fast, and easily available computational power a sec-\\nond, potentially more accurate, approach has become available: namely, the\\nuse of simulation to obtain to a high level of accuracy the p-value of the test\\nstatistic.\\nThe simulation approach is as follows. First, the value of T is determined —\\nsay, T = t. Now to determine whether or not to accept H0, at a given signiﬁ-\\ncance level α, we need to know the probability that T would be at least as large\\nas t when H0 is true. To determine this probability, we simulate n indepen-\\ndent random variables Y (1)\\n1 ,...,Y (1)\\nn\\neach having the probability mass function\\n{pi,i = 1,...,k} — that is,\\nP{Y (1)\\nj\\n= i} = pi,\\ni = 1,...,k,\\nj = 1,...,n\\nNow let\\nX(1)\\ni\\n= number j : Y (1)\\nj\\n= i\\nand set\\nT (1) =\\nk\\n\\x02\\ni=1\\n(X(1)\\ni\\n−npi)2\\nnpi\\nNow repeat this procedure by simulating a second set, independent of the ﬁrst\\nset, of n independent random variables Y (2)\\n1 ,...,Y (2)\\nn\\neach having the proba-\\nbility mass function {pi,i = 1,...,k} and then, as for the ﬁrst set, determining\\nT (2). Repeating this a large number, say, r, of times yields r independent ran-\\ndom variables T (1),T (2),...,T (r), each of which has the same distribution as\\ndoes the test statistic T when H0 is true. Hence, by the law of large numbers,\\nthe proportion of the Ti that are as large as t will be very nearly equal to the\\nprobability that T is as large as t when H0 is true—that is,\\nnumber l : T (l) ≥t\\nr\\n≈PH0{T ≥t}\\n1Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 516}, page_content='11.2 Goodness of ﬁt tests when all parameters are speciﬁed\\n507\\nIn fact, by letting r be large, the foregoing can be considered to be, with high\\nprobability, almost an equality. Hence, if that proportion is less than or equal\\nto α, then the p-value, equal to the probability of observing a T as large as t\\nwhen H0 is true, is less than α and so H0 should be rejected.\\nRemarks\\n(a) To utilize the foregoing simulation approach to determine whether or not\\nto accept H0 when T is observed, we need to specify how one can simulate, or\\ngenerate, a random variable Y such that P{Y = i} = pi,i = 1,...,k. One way is\\nas follows:\\nStep 1: Generate a random number U.\\nStep 2: If\\np1 + ··· + pi−1 ≤U < p1 + ··· + pi\\nset Y = i (where p1 + ··· + pi−1 ≡0 when i = 1). That is,\\nU < p1 ⇒Y = 1\\np1 ≤U < p1 + p2 ⇒Y = 2\\n...\\np1 + ··· + pi−1 ≤U < p1 + ··· + pi ⇒Y = i\\n...\\np1 + ··· + pn−1 < U ⇒Y = n\\nSince a random number is equivalent to a uniform (0, 1) random variable, we\\nhave that\\nP{a < U < b} = b −a,\\n0 < a < b < 1\\nand so\\nP{Y = i} = P{p1 + ··· + pi−1 < U < p1 + ··· + pi} = pi\\n(b) A signiﬁcant question that remains is how many simulation runs are nec-\\nessary. It has been shown that the value r = 100 is usually sufﬁcient at the\\nconventional 5 percent level of signiﬁcance.2\\nExample 11.2.c. Let us reconsider the problem presented in Example 11.2.b.\\nA simulation study yielded the result\\nPH0{T ≤9.52381} = .95\\n2See Hope, A., “A Simpliﬁed Monte Carlo Signiﬁcance Test Procedure,” J. of Royal Statist. Soc., B. 30,\\n582–598, 1968.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 517}, page_content='508 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nand so the critical value should be 9.52381, which is remarkably close to\\nχ2\\n.05,4 = 9.488 given as the critical value by the chi-square approximation. This\\nis most interesting since the rule of thumb for when the chi-square approxi-\\nmation can be applied — namely, that each npi ≥1 and at least 80 percent of\\nthe npi exceed 5 — does not apply, thus raising the possibility that it is rather\\nconservative.\\n■\\n11.3\\nGoodness of ﬁt tests when some parameters are\\nunspeciﬁed\\nWe can also perform goodness of ﬁt tests of a null hypothesis that does not\\ncompletely specify the probabilities {pi,i = 1,...,k}. For instance, consider the\\nsituation previously mentioned in which one is interested in testing whether\\nthe number of accidents occurring daily in a certain industrial plant is Poisson\\ndistributed with some unknown mean λ. To test this hypothesis, suppose that\\nthe daily number of accidents is recorded for n days — let Y1,...,Yn be these\\ndata. To analyze these data we must ﬁrst address the difﬁculty that the Yi can\\nassume an inﬁnite number of possible values. However, this is easily dealt with\\nby breaking up the possible values into a ﬁnite number k of regions and then\\nconsidering the region in which each Yi falls. For instance, we might say that\\nthe outcome of the number of accidents on a given day is in region 1 if there\\nare 0 accidents, region 2 if there is 1 accident, and region 3 if there are 2 or 3\\naccidents, region 4 if there are 4 or 5 accidents, and region 5 if there are more\\nthan 5 accidents. Hence, if the distribution is indeed Poisson with mean λ, then\\np1 = P{Y = 0} = e−λ\\n(11.3.1)\\np2 = P{Y = 1} = λe−λ\\np3 = P{Y = 2} + P{Y = 3} = e−λλ2\\n2\\n+ e−λλ3\\n6\\np4 = P{Y = 4} + P{Y = 5} = e−λλ4\\n24\\n+ e−λλ5\\n120\\np5 = P{Y > 5} = 1 −e−λ −λe−λ −e−λλ2\\n2\\n−e−λλ3\\n6\\n−e−λλ4\\n24\\n−e−λλ5\\n120\\nThe second difﬁculty we face in obtaining a goodness of ﬁt test results from\\nthe fact that the mean value λ is not speciﬁed. Clearly, the intuitive thing to do\\nis to assume that H0 is true and then estimate it from the data — say, ˆλ is the\\nestimate of λ — and then compute the test statistic\\nT =\\nk\\n\\x02\\ni=1\\n(Xi −n ˆpi)2\\nn ˆpi'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 518}, page_content='11.3 Goodness of ﬁt tests when some parameters are unspeciﬁed\\n509\\nwhere Xi is, as before, the number of Yj that fall in region i,i = 1,...,k, and\\nˆpi is the estimated probability of the event that Yj falls in region i, which is\\ndetermined by substituting ˆλ for λ in expression (11.3.1) for pi.\\nIn general, this approach can be utilized whenever there are unspeciﬁed pa-\\nrameters in the null hypothesis that are needed to compute the quantities\\npi,i = 1,...,k. Suppose now that there are m such unspeciﬁed parameters and\\nthat they are to be estimated by the method of maximum likelihood. It can\\nthen be proven that when n is large, the test statistic T will have, when H0 is\\ntrue, approximately a chi-square distribution with k −1 −m degrees of free-\\ndom. (In other words, one degree of freedom is lost for each parameter that\\nneeds to be estimated.) The test is, therefore, to\\nreject\\nH0\\nif\\nT ≥χ2\\nα,k−1−m\\naccept\\nH0\\notherwise\\nAn equivalent way of performing the foregoing is to ﬁrst determine the value\\nof the test statistic T , say T = t, and then compute\\np-value ≈P{χ2\\nk−1−m ≥t}\\nThe hypothesis would be rejected if α ≥p-value.\\nExample 11.3.a. Suppose the weekly number of accidents over a 30-week pe-\\nriod is as follows:\\n8\\n0\\n0\\n1\\n3\\n4\\n0\\n2\\n12\\n5\\n1\\n8\\n0\\n2\\n0\\n1\\n9\\n3\\n4\\n5\\n3\\n3\\n4\\n7\\n4\\n0\\n1\\n2\\n1\\n2\\nTest the hypothesis that the number of accidents in a week has a Poisson dis-\\ntribution.\\nSolution. Since the total number of accidents in the 30 weeks is 95, the maxi-\\nmum likelihood estimate of the mean of the Poisson distribution is\\nˆλ = 95\\n30 = 3.16667\\nSince the estimate of P{Y = i} is then\\nP{Y = i} est= e−ˆλˆλi\\ni!\\nwe obtain, after some computation, that with the ﬁve regions as given in the\\nbeginning of this section,\\nˆp1 = .04214\\nˆp2 = .13346\\nˆp3 = .43434'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 519}, page_content='510 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nˆp4 = .28841\\nˆp5 = .10164\\nUsing the data values X1 = 6, X2 = 5, X3 = 8, X4 = 6, X5 = 5, we can now use\\nR to obtain ﬁrst the value of the test statistic T = \\x035\\ni=1\\n(Xi−30 ˆpi)2\\n30 ˆpi\\n, and then the\\nresulting p-value.\\n>\\nx = c(6,5,8,6,5)\\n>\\np = c(.04214,.13346,.43434,.28841,.10164)\\n>\\ny = (x −30 ∗p)2\\n>\\ns = y/(30 ∗p)\\n>\\nT = sum(s)\\n>\\nT\\n[1]\\n21.99156\\nWe now obtain the p-value\\np-value = 1 −pchisq(21.99156,3) = 6.549542 × 10−5\\nand so the hypothesis of an underlying Poisson distribution is rejected.\\n(Clearly, there were too many weeks having 0 accidents for the hypothesis that\\nthe underlying distribution is Poisson with mean 3.167 to be tenable.)\\n■\\n11.4\\nTests of independence in contingency tables\\nIn this section, we consider problems in which each member of a population\\ncan be classiﬁed according to two distinct characteristics — which we shall\\ndenote as the X-characteristic and the Y-characteristic. We suppose that there\\nare r possible values for the X-characteristic and s for the Y-characteristic, and\\nlet\\nPij = P{X = i,Y = j}\\nfor i = 1,...,r,j = 1,...,s. That is, Pij represents the probability that a ran-\\ndomly chosen member of the population will have X-characteristic i and\\nY-characteristic j. The different members of the population will be assumed\\nto be independent. Also, let\\npi = P{X = i} =\\ns\\n\\x02\\nj=1\\nPij,\\ni = 1,...,r\\nand\\nqj = P{Y = j} =\\nr\\n\\x02\\ni=1\\nPij,\\nj = 1,...,s'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 520}, page_content='11.4 Tests of independence in contingency tables\\n511\\nThat is, pi is the probability that an arbitrary member of the population will\\nhave X-characteristic i, and qj is the probability it will have Y-characteristic j.\\nWe are interested in testing the hypothesis that a population member’s X- and\\nY-characteristics are independent. That is, we are interested in testing\\nH0 : Pij = piqj,\\nfor all\\ni = 1,...,r\\nj = 1,...,s\\nagainst the alternative\\nH1 : Pij ̸= piqj,\\nfor some\\ni,j\\ni = 1,...,r\\nj = 1,...,s\\nTo test this hypothesis, suppose that n members of the population have\\nbeen sampled, with the result that Nij of them have simultaneously had\\nX-characteristic i and Y-characteristic j,i = 1,...,r,j = 1,...,s.\\nSince the quantities pi,i = 1,...,r, and qj,j = 1,...,s are not speciﬁed by the\\nnull hypothesis, they must ﬁrst be estimated. Now since\\nNi =\\ns\\n\\x02\\nj=1\\nNij,\\ni = 1,...,r\\nrepresents the number of the sampled population members that have X-charac-\\nteristic i, a natural (in fact, the maximum likelihood) estimator of pi is\\nˆpi = Ni\\nn ,\\ni = 1,...,r\\nSimilarly, letting\\nMj =\\nr\\n\\x02\\ni=1\\nNij,\\nj = 1,...,s\\ndenote the number of sampled members having Y-characteristic j, the estima-\\ntor for qj is\\nˆqj = Mj\\nn ,\\nj = 1,...,s\\nAt ﬁrst glance, it may seem that we have had to use the data to estimate r +\\ns parameters. However, since the pi’s and qj’s have to sum to 1 — that is,\\n\\x03r\\ni=1 pi = \\x03s\\nj=1 qj = 1 — we need estimate only r −1 of the p’s and s −1\\nof the q’s. (For instance, if r were equal to 2, then an estimate of p1 would\\nautomatically provide an estimate of p2 since p2 = 1 −p1.) Hence, we actually'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 521}, page_content='512 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nneed estimate r −1 + s −1 = r + s −2 parameters, and since each population\\nmember has k = rs different possible values, it follows that the resulting test\\nstatistic will, for large n, have approximately a chi-square distribution with rs −\\n1 −(r + s −2) = (r −1)(s −1) degrees of freedom.\\nFinally, since\\nE[Nij] = nPij\\n= npiqj\\nwhen H0 is true\\nit follows that the test statistic is given by\\nT =\\ns\\n\\x02\\nj=1\\nr\\n\\x02\\ni=1\\n(Nij −n ˆpi ˆqj)2\\nn ˆpi ˆqj\\n=\\ns\\n\\x02\\nj=1\\nr\\n\\x02\\ni=1\\nN2\\nij\\nn ˆpi ˆqj\\n−n\\nIf the value of the test statistic is T = t, then the resultant p-value is\\np-value\\n=\\nPH0(T ≥t)\\n≈\\nP(χ2\\n(r−1)(s−1) > t)\\n=\\n1 −pchisq(t,(r −1)(s −1))\\nExample 11.4.a. A sample of 300 people was randomly chosen, and the sam-\\npled individuals were classiﬁed as to their gender and political afﬁliation,\\nDemocrat, Republican, or Independent. The following table, called a contin-\\ngency table, displays the resulting data.\\ni\\nj\\nTotal\\nDemocrat\\nRepublican\\nIndependent\\nWomen\\n68\\n56\\n32\\n156\\nMen\\n52\\n72\\n20\\n144\\nTotal\\n120\\n128\\n52\\n300\\nThus, for instance, the contingency table indicates that the sample of size 300\\ncontained 68 women who classiﬁed themselves as Democrats, 56 women who\\nclassiﬁed themselves as Republicans, and 32 women who classiﬁed themselves\\nas Independents; that is, N11 = 68,N12 = 56, and N13 = 32. Similarly, N21 =\\n52,N22 = 72, and N23 = 20.\\nUse these data to test the hypothesis that a randomly chosen individual’s gen-\\nder and political afﬁliation are independent.\\nSolution. From the above data, we obtain that the 6 values of n ˆpi ˆqj = NiMj/n\\nare as follows:\\nN1M1\\nn\\n= 156 × 120\\n300\\n= 62.40'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 522}, page_content='11.4 Tests of independence in contingency tables\\n513\\nN1M2\\nn\\n= 156 × 128\\n300\\n= 66.56\\nN1M3\\nn\\n= 156 × 52\\n300\\n= 27.04\\nN2M1\\nn\\n= 144 × 120\\n300\\n= 57.60\\nN2M2\\nn\\n= 144 × 128\\n300\\n= 61.44\\nN2M3\\nn\\n= 144 × 52\\n300\\n= 24.96\\nThe value of the test statistic is thus\\nT S = (68 −62.40)2\\n62.40\\n+ (56 −66.56)2\\n66.56\\n+ (32 −27.04)2\\n27.04\\n+ (52 −57.60)2\\n57.60\\n+ (72 −61.44)2\\n61.44\\n+ (20 −24.96)2\\n24.96\\n= 6.433\\nNoting that (r −1)(s −1) = 2, we use R to obtain the p-value.\\np-value\\n≈\\nP(χ2\\n6 > 6.433)\\n=\\n1 −pchisq(6.433,2)\\n[1]\\n0.04009515\\nThus, the null hypothesis that gender and political afﬁliation are independent\\nis rejected at the 5 percent level of signiﬁcance.\\n■\\nExample 11.4.b. A company operates four machines on three separate shirts\\ndaily. The following contingency table presents the data during a 6-month time\\nperiod, concerning the machine breakdowns that resulted.\\nTable 11.2 Number of Breakdowns.\\nMachine\\nTotal per Shift\\nA\\nB\\nC\\nD\\nShift 1\\n10\\n12\\n6\\n7\\n35\\nShift 2\\n10\\n24\\n9\\n10\\n53\\nShift 3\\n13\\n20\\n7\\n10\\n50\\nTotal per Machine\\n33\\n56\\n22\\n27\\n138\\nSuppose we are interested in determining whether a machine’s breakdown\\nprobability during a particular shift is inﬂuenced by that shift. In other words,\\nwe are interested in testing, for an arbitrary breakdown, whether the machine\\ncausing the breakdown and the shift on which the breakdown occurred are\\nindependent.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 523}, page_content='514 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nSolution. A direct computation gives that the value of test statistic is 1.8148.\\nConsequently, the p-value is\\np-value\\n≈\\nP(χ2\\n6 > 1.8148)\\n=\\n1 −pchisq(1.8148,6)\\n[1]\\n0.9359191\\nThus, the hypothesis that the machine that causes a breakdown and the shift\\non which the breakdown occurs are independent is accepted.\\n■\\n11.5\\nTests of independence in contingency tables having\\nﬁxed marginal totals\\nIn Example 11.4.a, we were interested in determining whether gender and po-\\nlitical afﬁliation were dependent in a particular population. To test this hypoth-\\nesis, we ﬁrst chose a random sample of people from this population and then\\nnoted their characteristics. However, another way in which we could gather\\ndata is to ﬁx in advance the numbers of men and women in the sample and\\nthen choose random samples of those sizes from the subpopulations of men\\nand women. That is, rather than let the numbers of women and men in the\\nsample be determined by chance, we might decide these numbers in advance.\\nBecause doing so would result in ﬁxed speciﬁed values for the total numbers of\\nmen and women in the sample, the resulting contingency table is often said to\\nhave ﬁxed margins (since the totals are given in the margins of the table).\\nIt turns out that even when the data are collected in the manner prescribed\\nabove, the same hypothesis test as given in Section 11.4 can still be used to test\\nfor the independence of the two characteristics. The test statistic remains\\nT S =\\n\\x02\\ni\\n\\x02\\nj\\n(Nij −ˆeij)2\\nˆeij\\nwhere\\nNij = number of members of sample who have both X-characteristic i\\nand Y-characteristic j\\nNi = number of members of sample who have X-characteristic i\\nMj = number of members of sample who have Y-characteristic j\\nand\\nˆeij = n ˆpi ˆqj = NiMj\\nn\\nwhere n is the total size of the sample.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 524}, page_content='11.5 Tests of independence in contingency tables having ﬁxed marginal totals\\n515\\nIn addition, it is still true that when H0 is true, T S will approximately have\\na chi-square distribution with (r −1)(s −1) degrees of freedom. (The quan-\\ntities r and s refer, of course, to the numbers of possible values of the X-\\nand Y-characteristic, respectively.) In other words, the test of the independence\\nhypothesis is unaffected by whether the marginal totals of one characteristic\\nare ﬁxed in advance or result from a random sample of the entire popula-\\ntion.\\nExample 11.5.a. A randomly chosen group of 20,000 nonsmokers and one of\\n10,000 smokers were followed over a 10-year period. The following data relate\\nthe numbers of them that developed lung cancer during that period.\\nSmokers\\nNonsmokers\\nTotal\\nLung cancer\\n62\\n14\\n76\\nNo lung cancer\\n9938\\n19,986\\n29,924\\nTotal\\n10,000\\n20,000\\n30,000\\nTest the hypothesis that smoking and lung cancer are independent. Use the 1\\npercent level of signiﬁcance.\\nSolution. The estimates of the expected number to fall in each ij cell when\\nsmoking and lung cancer are independent are\\nˆe11 = (76)(10,000)\\n30,000\\n= 25.33\\nˆe12 = (76)(20,000)\\n30,000\\n= 50.67\\nˆe21 = (29,924)(10,000)\\n30,000\\n= 9974.67\\nˆe22 = (29,924)(20,000)\\n30,000\\n= 19,949.33\\nTherefore, the value of the test statistic is\\nT S = (62 −25.33)2\\n25.33\\n+ (14 −50.67)2\\n50.67\\n+ (9938 −9974.67)2\\n9974.67\\n+ (19,986 −19,949.33)2\\n19,949.33\\n= 53.09 + 26.54 + .13 + .07 = 79.83\\nThus,\\np-value ≈P(χ2\\n1 > 79.83) = 1 −pchisq(79.83,1) ≈0'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 525}, page_content='516 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nand so the null hypothesis that whether a randomly chosen person develops\\nlung cancer is independent of whether or not that person is a smoker would be\\nrejected at all levels of signiﬁcance.\\n■\\nWe now show how to use the framework of this section to test the hypothesis\\nthat m discrete population distributions are equal. Consider m separate popu-\\nlations, each of whose members takes on one of the values 1,...,n. Suppose\\nthat a randomly chosen member of population i will have value j with proba-\\nbility\\npi, j,\\ni = 1,...,m,\\nj = 1,...,n\\nand consider a test of the null hypothesis\\nH0 : p1, j = p2, j = p3, j = ··· = pm, j,\\nfor each j = 1,...,n\\nTo obtain a test of this null hypothesis, consider ﬁrst the superpopulation con-\\nsisting of all members of each of the m populations. Any member of this\\nsuperpopulation can be classiﬁed according to two characteristics. The ﬁrst\\ncharacteristic speciﬁes which of the m populations the member is from, and\\nthe second characteristic speciﬁes its value. The hypothesis that the population\\ndistributions are equal becomes the hypothesis that, for each value, the propor-\\ntion of members of each population having that value are the same. But this\\nis exactly the same as saying that the two characteristics of a randomly chosen\\nmember of the superpopulation are independent. (That is, the value of a ran-\\ndomly chosen superpopulation member is independent of the population to\\nwhich this member belongs.)\\nTherefore, we can test H0 by randomly choosing sample members from each\\npopulation. If we let Mi denote the sample size from population i and let Ni,j\\ndenote the number of values from that sample that are equal to j, i = 1,...,m,\\nj = 1,...,n, then we can test H0 by testing for independence in the following\\ncontingency table.\\nValue\\nPopulation\\nTotals\\n1\\n2\\ni\\nm\\n1\\nN1,1\\nN2,1 ...\\nNi,1 ...\\nNm,1\\nN1\\n2\\n...\\nj\\nN1,j\\nN2,j ...\\nNi,j ...\\nNm,j\\nNj\\n...\\nn\\nN1,n\\nN2,n ...\\nNi,n\\nNm,n\\nNn\\nTotals\\nM1\\nM2 ...\\nMi ...\\nMm\\nNote that Nj denotes the number of sampled members that have value j.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 526}, page_content='11.6 The Kolmogorov–Smirnov goodness of ﬁt test for continuous data 517\\nExample 11.5.b. A recent study reported that 500 female ofﬁce workers were\\nrandomly chosen and questioned in each of four different countries. One of\\nthe questions related to whether these women often received verbal or sexual\\nabuse on the job. The following data resulted.\\nCountry\\nNumber Reporting Abuse\\nAustralia\\n28\\nGermany\\n30\\nJapan\\n51\\nUnited States\\n55\\nBased on these data, is it plausible that the proportions of female ofﬁce workers\\nwho often feel abused at work are the same for these countries?\\nSolution. Putting the above data in the form of a contingency table gives the\\nfollowing.\\nCountry\\nTotals\\n1\\n2\\n3\\n4\\nReceive abuse\\n28\\n30\\n58\\n55\\n171\\nDo not receive abuse\\n472\\n470\\n442\\n445\\n1829\\nTotals\\n500\\n500\\n500\\n500\\n2000\\nWe can now test the null hypothesis by testing for independence in the preced-\\ning contingency table. R gives that the value of the test statistic and the resulting\\np-value are\\nT S = 19.51,\\np-value ≈.0002\\nTherefore, the hypothesis that the percentages of women who feel they are\\nbeing abused on the job are the same for these countries is rejected at the 1\\npercent level of signiﬁcance (and, indeed, at any signiﬁcance level above .02\\npercent).\\n■\\n11.6\\nThe Kolmogorov–Smirnov goodness of ﬁt test for\\ncontinuous data3\\nSuppose now that Y1,...,Yn represents sample data from a continuous distri-\\nbution, and that we wish to test the null hypothesis H0 that F is the population\\ndistribution, where F is a speciﬁed continuous distribution function. One ap-\\nproach to testing H0 is to break up the set of possible values of the Yj into k\\ndistinct intervals, say,\\n(y0,y1),(y1,y2),...,(yk−1,yk),\\nwhere\\ny0 = −∞,yk = +∞\\n3Optional section.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 527}, page_content='518 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nand then consider the discretized random variables Y d\\nj ,j = 1,...,n, deﬁned by\\nY d\\nj = i\\nif Yj lies in the interval (yi−1,yi)\\nThe null hypothesis then implies that\\nP{Y d\\nj = i} = F(yi) −F(yi−1),\\ni = 1,...,k\\nand this can be tested by the chi-square goodness of ﬁt test already presented.\\nThere is, however, another way of testing that the Yj come from the continu-\\nous distribution function F that is generally more efﬁcient than discretizing; it\\nworks as follows. After observing Y1,...,Yn, let Fe be the empirical distribution\\nfunction deﬁned by\\nFe(x) = #i : Yi ≤x\\nn\\nThat is, Fe(x) is the proportion of the observed values that are less than or equal\\nto x. Because Fe(x) is a natural estimator of the probability that an observation\\nis less than or equal to x, it follows that, if the null hypothesis that F is the\\nunderlying distribution is correct, it should be close to F(x). Since this is so for\\nall x, a natural quantity on which to base a test of H0 is the test quantity\\nD ≡Maximum\\nx\\n|Fe(x) −F(x)|\\nwhere the maximum is over all values of x from −∞to +∞. The quantity D is\\ncalled the Kolmogorov–Smirnov test statistic.\\nTo compute the value of D for a given data set Yj = yj,j = 1,...,n, let y(1),\\ny(2),...,y(n) denote the values of the yj in increasing order. That is,\\ny( j) = jth smallest of y1,...,yn\\nFor example, if n = 3 and y1 = 3,y2 = 5,y3 = 1, then y(1) = 1,y(2) = 3,y(3) = 5.\\nSince Fe(x) can be written\\nFe(x) =\\n⎧\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\\n0\\nif x < y(1)\\n1\\nn\\nif y(1) ≤x < y(2)\\n...\\nj\\nn\\nif y( j) ≤x < y( j+1)\\n...\\n1\\nif y(n) ≤x'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 528}, page_content='11.6 The Kolmogorov–Smirnov goodness of ﬁt test for continuous data 519\\nwe see that Fe(x) is constant within the intervals (y( j−1),y( j)) and then jumps\\nby 1/n at the points y(1),...,y(n). Since F(x) is an increasing function of x\\nthat is bounded by 1, it follows that the maximum value of Fe(x) −F(x) is\\nnonnegative and occurs at one of the points y( j),j = 1,...,n (see Fig. 11.1).\\nThat is,\\nMaximum\\nx\\n{Fe(x) −F(x)} = Maximum\\nj=1,...,n\\n\\x08j\\nn −F(y( j))\\n\\t\\n(11.6.1)\\nSimilarly, the maximum value of F(x) −Fe(x) is also nonnegative and occurs\\nimmediately before one of the jump points y( j); and so\\nMaximum\\nx\\n{F(x) −Fe(x)} = Maximum\\nj=1,...,n\\n\\x08\\nF(y( j)) −j −1\\nn\\n\\t\\n(11.6.2)\\nFrom Equations (11.6.1) and (11.6.2), we see that\\nD = Maximum\\nx\\n|Fe(x) −F(x)|\\n= Maximum{Maximum{Fe(x) −F(x)},Maximum{F(x) −Fe(x)}}\\n= Maximum\\n\\x08j\\nn −F(y( j)),F(y( j)) −j −1\\nn\\n,j = 1,...,n\\n\\t\\n(11.6.3)\\nEquation (11.6.3) can be used to compute the value of D.\\nSuppose now that the Yj are observed and their values are such that D = d.\\nSince a large value of D would appear to be inconsistent with the null hypoth-\\nesis that F is the underlying distribution, it follows that the p-value for this\\ndata set is given by\\np-value = PF {D ≥d}\\nFIGURE 11.1\\nn = 5.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 529}, page_content='520 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nwhere we have written PF to make explicit that this probability is to be com-\\nputed under the assumption that H0 is correct (and so F is the underlying\\ndistribution).\\nThe above p-value can be approximated by a simulation that is made easier by\\nthe following proposition, which shows that PF {D ≥d} does not depend on\\nthe underlying distribution F. This result enables us to estimate the p-value\\nby doing the simulation with any continuous distribution F we choose [thus\\nallowing us to use the uniform (0, 1) distribution].\\nProposition 11.6.1. PF {D ≥d} is the same for any continuous distribution F.\\nProof.\\nPF {D ≥d} = PF\\n\\x08\\nMaximum\\nx\\n\\n\\n\\n\\n#i : Yi ≤x\\nn\\n−F(x)\\n\\n\\n\\n\\n ≥d\\n\\t\\n= PF\\n\\x08\\nMaximum\\nx\\n\\n\\n\\n\\n#i : F(Yi) ≤F(x)\\nn\\n−F(x)\\n\\n\\n\\n\\n ≥d\\n\\t\\n= P\\n\\x08\\nMaximum\\nx\\n\\n\\n\\n\\n#i : Ui ≤F(x)\\nn\\n−F(x)\\n\\n\\n\\n\\n ≥d\\n\\t\\nwhere U1,...,Un are independent uniform (0,1) random variables. The ﬁrst\\nequality following because F is an increasing function and so Y ≤x is equiva-\\nlent to F(Y) ≤F(x); and the second because of the result (whose proof is left\\nas an exercise) that if Y has the continuous distribution F then the random\\nvariable F(Y) is uniform on (0, 1).\\nContinuing the above, we see by letting y = F(x) and noting that as x ranges\\nfrom −∞to +∞, F(x) ranges from 0 to 1, that\\nPF {D ≥d} = P\\n\\x08\\nMaximum\\n0≤y≤1\\n\\n\\n\\n\\n#i : Ui ≤y\\nn\\n−y\\n\\n\\n\\n\\n ≥d\\n\\t\\nwhich shows that the distribution of D, when H0 is true, does not depend on\\nthe actual distribution F.\\n■\\nIt follows from the above proposition that after the value of D is determined\\nfrom the data, say, D =d, the p-value can be obtained by doing a simulation\\nwith the uniform (0, 1) distribution. That is, we generate a set of n random\\nnumbers U1,...,Un and then check whether or not the inequality\\nMaximum\\n0≤y≤1\\n\\n\\n\\n\\n#i : Ui ≤y\\nn\\n−y\\n\\n\\n\\n\\n ≥d\\nis valid. This is then repeated many times and the proportion of times that it is\\nvalid is our estimate of the p-value of the data set. As noted earlier, the left side'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 530}, page_content='11.6 The Kolmogorov–Smirnov goodness of ﬁt test for continuous data 521\\nof the inequality can be computed by ordering the random numbers and then\\nusing the identity\\nMax\\n\\n\\n\\n\\n#i : Ui ≤y\\nn\\n−y\\n\\n\\n\\n\\n = Max\\n\\x08j\\nn −U( j),U( j) −j −1\\nn\\n,j = 1,...,n\\n\\t\\nwhere U( j) is the jth smallest value of U1,...,Un. For example, if n = 3 and\\nU1 = .7, U2 = .6,U3 = .4, then U(1) = .4,U(2) = .6,U(3) = .7 and the value of D\\nfor this data set is\\nD = Max\\n\\x081\\n3 −.4, 2\\n3 −.6, 1 −.7, .4, .6 −1\\n3, .7 −2\\n3\\n\\t\\n= .4\\nA signiﬁcance level α test can be obtained by considering the quantity D∗de-\\nﬁned by\\nD∗= (√n + .12 + .11/√n)D\\nLetting d∗\\nα be such that\\nPF {D∗≥d∗\\nα} = α\\nthen the following are accurate approximations for d∗\\nα for a variety of values:\\nd∗\\n.1 = 1.224,\\nd∗\\n.05 = 1.358,\\nd∗\\n.025 = 1.480,\\nd∗\\n.01 = 1.626\\nThe level α test would reject the null hypothesis that F is the distribution if the\\nobserved value of D∗is at least as large as d∗\\nα.\\nExample 11.6.a. Suppose we want to test the hypothesis that a given popula-\\ntion distribution is exponential with mean 100; that is, F(x) = 1 −e−x/100. If\\nthe (ordered) values from a sample of size 10 from this distribution are\\n66,72,81,94,112,116,124,140,145,155\\nwhat conclusion can be drawn?\\nSolution. To answer the above, we ﬁrst employ Equation (11.6.3) to compute\\nthe value of the Kolmogorov–Smirnov test quantity D. After some computation\\nthis gives the result D = .4831487, which results in\\nD∗= .48315(\\n√\\n10 + 0.12 + 0.11/\\n√\\n10) = 1.603\\nBecause this exceeds d∗\\n.025 = 1.480, it follows that the null hypothesis that the\\ndata come from an exponential distribution with mean 100 would be rejected\\nat the 2.5 percent level of signiﬁcance. (On the other hand, it would not be\\nrejected at the 1 percent level of signiﬁcance.)\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 531}, page_content='522 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nProblems\\n1. According to the Mendelian theory of genetics, a certain garden pea plant\\nshould produce either white, pink, or red ﬂowers, with respective proba-\\nbilities 1\\n4, 1\\n2, 1\\n4. To test this theory, a sample of 564 peas was studied with\\nthe result that 141 produced white, 291 produced pink, and 132 pro-\\nduced red ﬂowers. Using the chi-square approximation, what conclusion\\nwould be drawn at the 5 percent level of signiﬁcance?\\n2. To ascertain whether a certain die was fair, 1000 rolls of the die were\\nrecorded, with the following results.\\nOutcome\\nNumber of Occurrences\\n1\\n158\\n2\\n172\\n3\\n164\\n4\\n181\\n5\\n160\\n6\\n165\\nTest the hypothesis that the die is fair (that is, that pi = 1\\n6,i = 1,...,6) at\\nthe 5 percent level of signiﬁcance. Use the chi-square approximation.\\n3. Determine the birth and death dates of 100 famous individuals and, us-\\ning the four-category approach of Example 11.2.a, test the hypothesis that\\nthe death month is not affected by the birth month. Use the chi-square\\napproximation.\\n4. It is believed that the daily number of electrical power failures in a certain\\nMidwestern city is a Poisson random variable with mean 4.2. Test this\\nhypothesis if over 150 days the number of days having i power failures is\\nas follows:\\nFailures\\nNumber of Days\\n0\\n0\\n1\\n5\\n2\\n22\\n3\\n23\\n4\\n32\\n5\\n22\\n6\\n19\\n7\\n13\\n8\\n6\\n9\\n4\\n10\\n4\\n11\\n0\\n5. Among 100 vacuum tubes tested, 41 had lifetimes of less than 30 hours,\\n31 had lifetimes between 30 and 60 hours, 13 had lifetimes between 60'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 532}, page_content='Problems\\n523\\nand 90 hours, and 15 had lifetimes of greater than 90 hours. Are these\\ndata consistent with the hypothesis that a vacuum tube’s lifetime is expo-\\nnentially distributed with a mean of 50 hours?\\n6. The past output of a machine indicates that each unit it produces will be\\ntop grade\\nwith probability\\n.40\\nhigh grade\\nwith probability\\n.30\\nmedium grade\\nwith probability\\n.20\\nlow grade\\nwith probability\\n.10\\nA new machine, designed to perform the same job, has produced 500\\nunits with the following results.\\ntop grade\\n234\\nhigh grade\\n117\\nmedium grade\\n81\\nlow grade\\n68\\nCan the difference in output be ascribed solely to chance?\\n7. The neutrino radiation from outer space was observed during several\\ndays. The frequencies of signals were recorded for each sidereal hour and\\nare as given below in Table 11.3:\\nTable 11.3 Frequency of Neutrino Radiation from Outer\\nSpace.\\nHour\\nStarting at\\nFrequency\\nof Signals\\nHour\\nStarting at\\nFrequency\\nof Signals\\n0\\n24\\n12\\n29\\n1\\n24\\n13\\n26\\n2\\n36\\n14\\n38\\n3\\n32\\n15\\n26\\n4\\n33\\n16\\n37\\n5\\n36\\n17\\n28\\n6\\n41\\n18\\n43\\n7\\n24\\n19\\n30\\n8\\n37\\n20\\n40\\n9\\n37\\n21\\n22\\n10\\n49\\n22\\n30\\n11\\n51\\n23\\n42\\nTest whether the signals are uniformly distributed over the 24-hour pe-\\nriod.\\n8. Neutrino radiation was observed over a certain period and the number\\nof hours in which 0, 1, 2,. . . signals were received was recorded.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 533}, page_content='524 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\nNumber of\\nSignals per Hour\\nNumber of Hours with\\nThis Frequency of Signals\\n0\\n1924\\n1\\n541\\n2\\n103\\n3\\n17\\n4\\n1\\n5\\n1\\n6 or more\\n0\\nTest the hypothesis that the observations come from a population having\\na Poisson distribution with mean .3.\\n9. In a certain region, insurance data indicate that 82 percent of drivers have\\nno accidents in a year, 15 percent have exactly 1 accident, and 3 percent\\nhave 2 or more accidents. In a random sample of 440 engineers, 366 had\\nno accidents, 68 had exactly 1 accident, and 6 had 2 or more. Can you\\nconclude that engineers follow an accident proﬁle that is different from\\nthe rest of the drivers in the region?\\n10. A study was instigated to see if southern California earthquakes of at least\\nmoderate size (having values of at least 4.4 on the Richter scale) are more\\nlikely to occur on certain days of the week than on others. The catalogs\\nyielded the following data on 1100 earthquakes.\\nDay\\nSun\\nMon\\nTues\\nWed\\nThurs\\nFri\\nSat\\nNumber of Earthquakes\\n156\\n144\\n170\\n158\\n172\\n148\\n152\\nTest, at the 5 percent level, the hypothesis that an earthquake is equally\\nlikely to occur on any of the 7 days of the week.\\n11. Sometimes reported data ﬁt a model so well that it makes one suspicious\\nthat the data are not being accurately reported. For instance, a friend of\\nmine has reported that he tossed a fair coin 40,000 times and obtained\\n20,004 heads and 19,996 tails. Is such a result believable? Explain your\\nreasoning.\\n12. ] Use simulation to determine the p-value and compare it with the result\\nyou obtained using the chi-square approximation in Problem 1. Let the\\nnumber of simulation runs be\\na.\\n1000;\\nb.\\n5000;\\nc.\\n10,000.\\n13. A sample of size 120 had a sample mean of 100 and a sample standard\\ndeviation of 15. Of these 120 data values, 3 were less than 70; 18 were\\nbetween 70 and 85; 30 were between 85 and 100; 35 were between 100\\nand 115; 32 were between 115 and 130; and 2 were greater than 130. Test\\nthe hypothesis that the sample distribution was normal.\\n14. In Problem 4, test the hypothesis that the daily number of failures has a\\nPoisson distribution.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 534}, page_content='Problems 525\\n15. A random sample of 500 migrant families was classiﬁed by region and\\nincome (in units of $1000). The following data resulted.\\nIncome\\nSouth\\nNorth\\n0–10\\n42\\n53\\n10–20\\n55\\n90\\n20–30\\n47\\n88\\n>30\\n36\\n89\\nDetermine the p-value of the test that a family’s income and region are\\nindependent.\\n16. The following data relate the mother’s age and the birthweight (in grams)\\nof her child.\\nMaternal Age\\nBirthweight\\nLess Than 2500 Grams\\nMore Than 2500 Grams\\n20 years or less\\n10\\n40\\nGreater than 20\\n15\\n135\\nTest the hypothesis that the baby’s birthweight is independent of the\\nmother’s age.\\n17. Repeat Problem 16 with all of the data values doubled — that is, with\\nthese data:\\n20\\n80\\n30\\n270\\n18. The number of infant mortalities as a function of the baby’s birthweight\\n(in grams) for 72,730 live white births in New York in 1974 is as follows:\\nBirthweight\\nOutcome at the End of 1 Year\\nAlive\\nDead\\nLess than 2500\\n4597\\n618\\nGreater than 2500\\n67,093\\n422\\nTest the hypothesis that the birthweight is independent of whether or not\\nthe baby survives its ﬁrst year.\\n19. An experiment designed to study the relationship between hypertension\\nand cigarette smoking yielded the following data.\\nNonsmoker\\nModerate Smoker\\nHeavy Smoker\\nHypertension\\n20\\n38\\n28\\nNo hypertension\\n50\\n27\\n18\\nTest the hypothesis that whether or not an individual has hypertension\\nis independent of how much that person smokes.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 535}, page_content='526 CHAPTER 11: Goodness of ﬁt tests and categorical data analysis\\n20. The following table shows the number of defective, acceptable, and su-\\nperior items in samples taken both before and after the introduction of a\\nmodiﬁcation in the manufacturing process.\\nDefective\\nAcceptable\\nSuperior\\nBefore 25\\n218\\n22\\nAfter 9\\n103\\n14\\nIs this change signiﬁcant at the .05 level?\\n21. A sample of 300 cars having cellular phones and one of 400 cars without\\nphones were tracked for 1 year. The following table gives the number of\\nthese cars involved in accidents over that year.\\nAccident\\nNo Accident\\nCellular phone\\n22\\n278\\nNo phone\\n26\\n374\\nUse the above to test the hypothesis that having a cellular phone in your\\ncar and being involved in an accident are independent. Use the 5 percent\\nlevel of signiﬁcance.\\n22. To study the effect of ﬂuoridated water supplies on tooth decay, two com-\\nmunities of roughly the same socioeconomic status were chosen. One of\\nthese communities had ﬂuoridated water while the other did not. Ran-\\ndom samples of 200 teenagers from both communities were chosen, and\\nthe numbers of cavities they had were determined. The following data\\nresulted.\\nCavities\\nFluoridated Town\\nNonﬂuoridated Town\\n0\\n154\\n133\\n1\\n20\\n18\\n2\\n14\\n21\\n3 or more\\n12\\n28\\nDo these data establish, at the 5 percent level of signiﬁcance, that the\\nnumber of dental cavities a person has is not independent of whether\\nthat person’s water supply is ﬂuoridated? What about at the 1 percent\\nlevel?\\n23. To determine if a malpractice lawsuit is more likely to follow certain types\\nof surgeries, random samples of three different types of surgeries were\\nstudied, and the following data resulted.\\nType of Operation\\nNumber Sampled\\nNumber Leading to a Lawsuit\\nHeart surgery\\n400\\n16\\nBrain surgery\\n300\\n19\\nAppendectomy\\n300\\n7'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 536}, page_content='Problems 527\\nTest the hypothesis that the percentages of the surgical operations that\\nlead to lawsuits are the same for each of the three types.\\na.\\nUse the 5 percent level of signiﬁcance.\\nb.\\nUse the 1 percent level of signiﬁcance.\\n24. In a famous article (S. Russell, “A red sky at night. . . ,” Metropolitan Mag-\\nazine London, 61, p. 15, 1926) the following data set of frequencies of\\nsunset colors and whether each was followed by rain was presented.\\nSky Color\\nNumber of Observations\\nNumber Followed by Rain\\nRed\\n61\\n26\\nMainly red\\n194\\n52\\nYellow\\n159\\n81\\nMainly yellow\\n188\\n86\\nRed and yellow\\n194\\n52\\nGray\\n302\\n167\\nTest the hypothesis that whether it rains tomorrow is independent of the\\ncolor of today’s sunset.\\n25. Data are said to be from a lognormal distribution with parameters μ and σ\\nif the natural logarithms of the data are normally distributed with mean\\nμ and standard deviation σ. Use the Kolmogorov–Smirnov test with sig-\\nniﬁcance level .05 to decide whether the following lifetimes (in days) of a\\nsample of cancer-bearing mice that have been treated with a certain can-\\ncer therapy might come from a lognormal distribution with parameters\\nμ = 3 and σ = 4.\\n24,12,36,40,16,10,12,30,38,14,22,18'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 537}, page_content='CHAPTER 12\\nNonparametric hypothesis tests\\n12.1\\nIntroduction\\nIn this chapter, we shall develop some hypothesis tests in situations where the\\ndata come from a probability distribution whose underlying form is not speci-\\nﬁed. That is, it will not be assumed that the underlying distribution is normal,\\nor exponential, or any other given type. Because no particular parametric form\\nfor the underlying distribution is assumed, such tests are called nonparametric.\\nThe strength of a nonparametric test resides in the fact that it can be applied\\nwithout any assumption on the form of the underlying distribution. Of course,\\nif there is justiﬁcation for assuming a particular parametric form, such as the\\nnormal, then the relevant parametric test should be employed.\\nIn Section 12.2, we consider hypotheses concerning the median of a contin-\\nuous distribution and show how the sign test can be used in their study. In\\nSection 12.3, we consider the signed rank test, which is used to test the hypoth-\\nesis that a continuous population distribution is symmetric about a speciﬁed\\nvalue. In Section 12.4, we consider the two-sample problem, where one wants\\nto use data from two separate continuous distributions to test the hypothesis\\nthat the distributions are equal, and present the rank sum test. Finally, in Sec-\\ntion 12.5 we study the runs test, which can be used to test the hypothesis that\\na sequence of 0’s and 1’s constitutes a random sequence that does not follow\\nany speciﬁed pattern.\\n12.2\\nThe sign test\\nLet X1,...,Xn denote a sample from a continuous distribution F and suppose\\nthat we are interested in testing the hypothesis that the median of F, call it m,\\nis equal to a speciﬁed value m0. That is, consider a test of\\nH0 : m = m0\\nversus\\nH1 : m ̸= m0\\nwhere m is such that F(m) = .5.\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00021-1\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n529'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 538}, page_content='530 CHAPTER 12: Nonparametric hypothesis tests\\nThis hypothesis can easily be tested by noting that each of the observations\\nwill, independently, be less than m0 with probability F(m0). Hence, if we let\\nIi =\\n\\x02 1\\nif Xi < m0\\n0\\nif Xi ≥m0\\nthen I1,...,In are independent Bernoulli random variables with parameter p =\\nF(m0); and so the null hypothesis is equivalent to stating that this Bernoulli\\nparameter is equal to 1\\n2. Now, if v is the observed value of \\x03n\\ni=1 Ii — that is, if\\nv is the number of data values less than m0 — then it follows from the results\\nof Section 8.6 that the p-value of the test that this Bernoulli parameter is equal\\nto 1\\n2 is\\np-value = 2min(P{Bin(n,1/2) ≤v},P {Bin(n,1/2) ≥v})\\n(12.2.1)\\nwhere Bin(n,p) is a binomial random variable with parameters n and p.\\nHowever,\\nP{Bin(n,p) ≥v} = P{n −Bin(n,p) ≤n −v}\\n= P{Bin(n,1 −p) ≤n −v}\\n(why?)\\nand so we see from Equation (12.2.1) that the p-value is given by\\np-value = 2min(P{Bin(n,1/2) ≤v},P {Bin(n,1/2) ≤n −v})\\n(12.2.2)\\n=\\n⎧\\n⎪⎨\\n⎪⎩\\n2P{Bin(n,1/2) ≤v}\\nif v ≤n\\n2\\n2P{Bin(n,1/2) ≤n −v}\\nif v ≥n\\n2\\nSince the value of v = \\x03n\\ni=1 Ii depends on the signs of the terms Xi −m0, the\\nforegoing is called the sign test.\\nExample 12.2.a. If a sample of size 200 contains 120 values that are less than\\nm0 and 80 values that are greater, what is the p-value of the test of the hypoth-\\nesis that the median is equal to m0?\\nSolution. From Equation (12.2.2), the p-value is equal to twice the probability\\nthat binomial random variable with parameters 200, 1\\n2 is less than or equal\\nto 80.\\nUsing R gives\\n>\\n2 ∗pbinom(80,200,.5)\\n[1]\\n0.005685156\\nTherefore, the p-value is .00568, and so the null hypothesis would be rejected\\nat even the 1 percent level of signiﬁcance.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 539}, page_content='12.2 The sign test\\n531\\nThe sign test can also be used in situations analogous to ones in which the\\npaired t-test was previously applied. For instance, let us reconsider Exam-\\nple 8.4.c, which is interested in testing whether or not a recently instituted\\nindustrial safety program has had an effect on the number of man-hours lost\\nto accidents. For each of 10 plants, the data consisted of the pair Xi, Yi, which\\nrepresented, respectively, the average weekly loss at plant i before and after the\\nprogram. Letting Zi = Xi −Yi, i = 1,...,10, it follows that if the program had\\nnot had any effect, then Zi,i = 1,...,10, would be a sample from a distribu-\\ntion whose median value is 0. Since the resulting values of Zi, — namely, 7.5,\\n−2.3, 2.6, 3.7, 1.5, −.5, −1, 4.9, 4.8, 1.6 — contain three whose sign is negative\\nand seven whose sign is positive, it follows that the hypothesis that the median\\nof Z is 0 should be rejected at signiﬁcance level α if\\n3\\n\\x08\\ni=0\\n\\t10\\ni\\n\\n\\t1\\n2\\n\\n10\\n≤α\\n2\\nSince\\n3\\n\\x08\\ni=0\\n\\t10\\ni\\n\\n\\t1\\n2\\n\\n10\\n= 176\\n1024 = .172\\nit follows that the hypothesis would be accepted at the 5 percent signiﬁcance\\nlevel (indeed, it would be accepted at all signiﬁcance levels less than the p-value\\nequal to .344).\\nThus, the sign test does not enable us to conclude that the safety program\\nhas had any statistically signiﬁcant effect, which is in contradiction to the re-\\nsult obtained in Example 8.4.c when it was assumed that the differences were\\nnormally distributed. The reason for this disparity is that the assumption of\\nnormality allows us to take into account not only the number of values greater\\nthan 0 (which is all the sign test considers) but also the magnitude of these val-\\nues. (The next test to be considered, while still being nonparametric, improves\\non the sign test by taking into account whether those values that most differ\\nfrom the hypothesized median value m0 tend to lie on one side of m0 — that\\nis, whether they tend to be primarily bigger or smaller than m0.)\\nWe can also use the sign test to test one-sided hypotheses about a population\\nmedian. For instance, suppose that we want to test\\nH0 : m ≤m0\\nversus\\nH1 : m > m0\\nwhere m is the population median and m0 is some speciﬁed value. Let p denote\\nthe probability that a population value is less than m0, and note that if the null\\nhypothesis is true then p ≥1/2, and if the alternative is true then p < 1/2 (see\\nFigure 12.1).'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 540}, page_content='532 CHAPTER 12: Nonparametric hypothesis tests\\nFIGURE 12.1\\nTo use the sign test to test the preceding hypothesis, choose a random sample\\nof n members of the population. If v of them have values that are less than m0,\\nthen the resulting p-value is the probability that a value of v or smaller would\\nhave occurred by chance if each element had probability 1/2 of being less than\\nm0. That is,\\np-value = P{Bin(n,1/2) ≤v}\\nExample 12.2.b. A ﬁnancial institution has decided to open an ofﬁce in a\\ncertain community if it can be established that the median annual income\\nof families in the community is greater than $90,000. To obtain information,\\na random sample of 80 families was chosen, and the family incomes deter-\\nmined. If 28 of these families had annual incomes below and 52 had annual\\nincomes above $90,000, is this signiﬁcant enough to establish, say, at the 5 per-\\ncent level of signiﬁcance, that the median annual income in the community is\\ngreater than $90,000?\\nSolution. We need to see if the data are sufﬁcient to enable us to reject the null\\nhypothesis when testing\\nH0 : m ≤90\\nversus\\nH1 : m > 90'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 541}, page_content='12.3 The signed rank test\\n533\\nThe preceding is equivalent to testing\\nH0 : p ≥1/2\\nversus\\nH1 : p < 1/2\\nwhere p is the probability that a randomly chosen member of the population\\nhas an annual income of less than $90,000. Therefore, the p-value is\\np-value = P(Bin(80,1/2) ≤28) = pbinom(28,80,1/2) = 0.004841425\\nand so the null hypothesis that the median income is less than or equal to\\n$90,000 is rejected.\\n■\\nA test of the one-sided null hypothesis that the median is at least m0 is obtained\\nsimilarly. If a random sample of size n is chosen, and v of the resulting values\\nare less than m0, then the resulting p-value is\\np-value = P{Bin(n,1/2) ≥v}\\n12.3\\nThe signed rank test\\nThe sign test can be employed to test the hypothesis that the median of a con-\\ntinuous distribution F is equal to a speciﬁed value m0. However, in many\\napplications one is really interested in testing not only that the median is\\nequal to m0 but that the distribution is symmetric about m0. That is, if X has\\ndistribution function F, then one is often interested in testing the hypothesis\\nH0 : P{X < m0 −a} = P{X > m0 + a} for all a > 0 (see Figure 12.2). Whereas\\nthe sign test could still be employed to test the foregoing hypothesis, it suffers\\nin that it compares only the number of data values that are less than m0 with\\nthe number that are greater than m0 and does not take into account whether\\nor not one of these sets tends to be farther away from m0 than the other. A\\nnonparametric test that does take this into account is the so-called signed rank\\ntest. It is described as follows.\\nLet Yi = Xi −m0, i = 1,...,n and rank (that is, order) the absolute values\\n|Y1|,|Y2|,...,|Yn|. Set, for j = 1,...,n,\\nIj =\\n⎧\\n⎨\\n⎩\\n1\\nif the jth smallest value comes from a data value that is smaller\\nthan m0\\n0\\notherwise\\nNow, whereas \\x03n\\nj=1 Ij represents the test statistic for the sign test, the signed\\nrank test uses the statistic T = \\x03n\\nj=1 jIj. That is, like the sign test it considers\\nthose data values that are less than m0, but rather than giving equal weight to\\neach such value it gives larger weights to those data values that are farthest away\\nfrom m0.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 542}, page_content='534 CHAPTER 12: Nonparametric hypothesis tests\\nFIGURE 12.2\\nA symmetric density: m = 3,\\nf (x) =\\n⎧\\n⎨\\n⎩\\nmax{0,.4(x −3) +\\n√\\n.4}\\nx ≤3\\nmax{0,−.4(x −3) +\\n√\\n.4}\\nx > 3.\\nExample 12.3.a. If n = 4, m0 = 2, and the data values are X1 = 4.2, X2 = 1.8,\\nX3 = 5.3, X4 = 1.7, then the rankings of |Xi −2| are .2, .3, 2.2, 3.3. Since the\\nﬁrst of these values — namely, .2 — comes from the data point X2, which is less\\nthan 2, it follows that I1 = 1. Similarly, I2 = 1, and I3 and I4 equal 0. Hence,\\nthe value of the test statistic is T = 1 + 2 = 3.\\n■\\nWhen H0 is true, the mean and variance of the test statistic T are easily com-\\nputed. This is accomplished by noting that, since the distribution of Yj =\\nXj −m0 is symmetric about 0, for any given value of |Yj| — say, |Yj| = y —\\nit is equally likely that either Yj = y or Yj = −y. From this fact it can be seen\\nthat under H0, I1,...,In will be independent random variables such that\\nP{Ij = 1} = 1\\n2 = P{Ij = 0},\\nj = 1,...,n\\nHence, we can conclude that under H0,\\nE[T ] = E\\n⎡\\n⎣\\nn\\n\\x08\\nj=1\\njIj\\n⎤\\n⎦\\n=\\nn\\n\\x08\\nj=1\\nj\\n2 = n(n + 1)\\n4\\n(12.3.1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 543}, page_content='12.3 The signed rank test\\n535\\nVar(T ) = Var\\n⎛\\n⎝\\nn\\n\\x08\\nj=1\\njIj\\n⎞\\n⎠\\n=\\nn\\n\\x08\\nj=1\\nj2 Var(Ij)\\n=\\nn\\n\\x08\\nj=1\\nj2\\n4 = n(n + 1)(2n + 1)\\n24\\n(12.3.2)\\nwhere the fact that the variance of the Bernoulli random variable Ij is 1\\n2(1 −\\n1\\n2) = 1\\n4 is used.\\nIt can be shown that for moderately large values of n (n > 25 is often quoted\\nas being sufﬁcient) T will, when H0 is true, have approximately a normal dis-\\ntribution with mean and variance as given by Equations (12.3.1) and (12.3.2).\\nAlthough this approximation can be used to derive an approximate level α test\\nof H0 (which has been the usual approach until the recent advent of fast and\\ncheap computational power), we shall not pursue this approach but rather will\\ndetermine the p-value for given test data by an explicit computation of the\\nrelevant probabilities. This is accomplished as follows.\\nSuppose we desire a signiﬁcance level α test of H0. Since the alternative hypoth-\\nesis is that the median is not equal to m0, a two-sided test is called for. That is,\\nif the observed value of T is equal to t, then H0 should be rejected if either\\nPH0{T ≤t} ≤α\\n2\\nor\\nPH0{T ≥t} ≤α\\n2\\n(12.3.3)\\nThe p-value of the test data when T = t is given by\\np-value = 2min(PH0{T ≤t},PH0{T ≥t})\\n(12.3.4)\\nThat is, if T = t, the signed rank test calls for rejection of the null hypothesis\\nif the signiﬁcance level α is at least as large as this p-value. The amount of\\ncomputation necessary to compute the p-value can be reduced by utilizing the\\nfollowing equality (whose proof will be given at the end of the section).\\nPH0{T ≥t} = PH0\\n\\x02\\nT ≤n(n + 1)\\n2\\n−t\\n\\x13\\nUsing Equation (12.3.4), the p-value is given by\\np-value = 2min\\n\\t\\nPH0{T ≤t},PH0\\n\\x02\\nT ≤n(n + 1)\\n2\\n−t\\n\\x13\\n= 2PH0{T ≤t∗}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 544}, page_content='536 CHAPTER 12: Nonparametric hypothesis tests\\nwhere\\nt∗= min\\n\\t\\nt, n(n + 1)\\n2\\n−t\\n\\nIt remains to compute PH0{T ≤t∗}. To do so, let Pk(i) denote the probability,\\nunder H0, that the signed rank statistic T will be less than or equal to i when\\nthe sample size is k. We will determine a recursive formula for Pk(i) starting\\nwith k = 1. When k = 1, since there is only a single data value, which, when H0\\nis true, is equally likely to be either less than or greater than m0, it follows that\\nT is equally likely to be either 0 or 1. Thus\\nP1(i) =\\n⎧\\n⎨\\n⎩\\n0\\ni < 0\\n1\\n2\\ni = 0\\n1\\ni ≥1\\n(12.3.5)\\nNow suppose the sample size is k. To compute Pk(i), we condition on the value\\nof Ik as follows:\\nPk(i) = PH0\\n⎧\\n⎨\\n⎩\\nk\\n\\x08\\nj=1\\njIj ≤i\\n⎫\\n⎬\\n⎭\\n= PH0\\n⎧\\n⎨\\n⎩\\nk\\n\\x08\\nj=1\\njIj ≤i|Ik = 1\\n⎫\\n⎬\\n⎭PH0{Ik = 1}\\n+ PH0\\n⎧\\n⎨\\n⎩\\nk\\n\\x08\\nj=1\\njIj ≤i|Ik = 0\\n⎫\\n⎬\\n⎭PH0{Ik = 0}\\n= PH0\\n⎧\\n⎨\\n⎩\\nk−1\\n\\x08\\nj=1\\njIj ≤i −k|Ik = 1\\n⎫\\n⎬\\n⎭PH0{Ik = 1}\\n+ PH0\\n⎧\\n⎨\\n⎩\\nk−1\\n\\x08\\nj=1\\njIj ≤i|Ik = 0\\n⎫\\n⎬\\n⎭PH0{Ik = 0}\\n= PH0\\n⎧\\n⎨\\n⎩\\nk−1\\n\\x08\\nj=1\\njIj ≤i −k\\n⎫\\n⎬\\n⎭PH0{Ik = 1} + PH0\\n⎧\\n⎨\\n⎩\\nk−1\\n\\x08\\nj=1\\njIj ≤i\\n⎫\\n⎬\\n⎭PH0{Ik = 0}\\nwhere the last equality utilized the independence of I1,...,Ik−1 and Ik (when\\nH0 is true). Now \\x03k−1\\nj=1 jIj has the same distribution as the signed rank statistic\\nof a sample of size k −1, and since\\nPH0{Ik = 1} = PH0{Ik = 0} = 1\\n2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 545}, page_content='12.3 The signed rank test\\n537\\nwe see that\\nPk(i) = 1\\n2Pk−1(i −k) + 1\\n2Pk−1(i)\\n(12.3.6)\\nStarting with Equation (12.3.5), the recursion given by Equation (12.3.6) can be\\nsuccessfully employed to compute P2(·), then P3(·), and so on, stopping when\\nthe desired value Pn(t∗) has been obtained.\\nExample 12.3.b. For the data of Example 12.3.a,\\nt∗= min\\n\\t\\n3, 4 · 5\\n2\\n−3\\n\\n= 3\\nHence the p-value is 2P4(3), which is computed as follows:\\nP2(0) = 1\\n2[P1(−2) + P1(0)] = 1\\n4\\nP2(1) = 1\\n2[P1(−1) + P1(1)] = 1\\n2\\nP2(2) = 1\\n2[P1(0) + P1(2)] = 3\\n4\\nP2(3) = 1\\n2[P1(1) + P1(3)] = 1\\nP3(0) = 1\\n2[P2(−3) + P2(0)] = 1\\n8\\nsince P2(−3) = 0\\nP3(1) = 1\\n2[P2(−2) + P2(1)] = 1\\n4\\nP3(2) = 1\\n2[P2(−1) + P2(2)] = 3\\n8\\nP3(3) = 1\\n2[P2(0) + P2(3)] = 5\\n8\\nP4(0) = 1\\n2[P3(−4) + P3(0)] = 1\\n16\\nP4(1) = 1\\n2[P3(−3) + P3(1)] = 1\\n8\\nP4(2) = 1\\n2[P3(−2) + P3(2)] = 3\\n16\\nP4(3) = 1\\n2[P3(−1) + P3(3)] = 5\\n16\\n■\\nR can be used to obtain the value of the test statistic T , and the resulting\\np-value. (R does not directly give T , but instead the value of V = n(n + 1)/2 −\\nT .) To use R to run the signed rank test (sometimes called the Wilcoxon signed\\nrank test) to test the hypothesis that the data set x1,...,xn is symmetric about\\nthe value m0, do the following:\\n>x = c(x1,...,xn)\\n>wilcox.test(x,mu=m0)\\nThe result will be the p-value of the test. For instance, for the data of Exam-\\nple 12.3.a, we obtain the following:\\n>x = c(4.2,1.8,5.3,1.7)\\n>wilcox.test(x,mu = 2)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 546}, page_content='538 CHAPTER 12: Nonparametric hypothesis tests\\nWilcoxon signed rank test\\ndata: x\\nV = 7, p-value = 0.625\\nalternative hypothesis: true location is not equal to 2.\\nWe end this section with a proof of the equality\\nPH0{T ≥t} = PH0\\n\\x02\\nT ≤n(n + 1)\\n2\\n−t\\n\\x13\\nTo verify the foregoing, note ﬁrst that 1 −Ij will equal 1 if the jth smallest\\nvalue of |Y1|,...,|Yn| comes from a data value larger than m0, and it will equal\\n0 otherwise. Hence, if we let\\nT 1 =\\nn\\n\\x08\\nj=1\\nj(1 −Ij)\\nthen T 1 will represent the sum of the ranks of the |Yj| that correspond to data\\nvalues larger than m0. By symmetry, T 1 will have, under H0, the same distribu-\\ntion as T . Now\\nT 1 =\\nn\\n\\x08\\nj=1\\nj −\\nn\\n\\x08\\nj=1\\njIj = n(n + 1)\\n2\\n−T\\nand so\\nP{T ≥t} = P{T 1 ≥t}\\nsince T and T 1 have the same distribution\\n= P\\n\\x02n(n + 1)\\n2\\n−T ≥t\\n\\x13\\n= P\\n\\x02\\nT ≤n(n + 1)\\n2\\n−t\\n\\x13\\n12.4\\nThe two-sample problem\\nSuppose that one is considering two different methods for producing items\\nhaving measurable characteristics with an interest in determining whether the\\ntwo methods result in statistically identical items.\\nTo attack this problem let X1,...,Xn denote a sample of the measurable values\\nof n items produced by method 1, and, similarly, let Y1,...,Ym be the corre-\\nsponding value of m items produced by method 2. If we let F and G, both\\nassumed to be continuous, denote the distribution functions of the two sam-\\nples, respectively, then the hypothesis we wish to test is H0 : F = G.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 547}, page_content='12.4 The two-sample problem\\n539\\nOne procedure for testing H0 — which is known by such names as the rank sum\\ntest, the Mann-Whitney test, or the Wilcoxon test — calls initially for ranking,\\nor ordering, the n+m data values X1,...,Xn,Y1,...,Ym. Since we are assuming\\nthat F and G are continuous, this ranking will be unique — that is, there will\\nbe no ties. Give the smallest data value rank 1, the second smallest rank 2,...,\\nand the (n + m)th smallest rank n + m. Now, for i = 1,...,n, let\\nRi = rank of the data value Xi\\nThe rank sum test utilizes the test statistic T equal to the sum of the ranks from\\nthe ﬁrst sample — that is,\\nT =\\nn\\n\\x08\\ni=1\\nRi\\nExample 12.4.a. An experiment designed to compare two treatments against\\ncorrosion yielded the following data in pieces of wire subjected to the two\\ntreatments.\\nTreatment 1\\n65.2, 67.1, 69.4, 78.2, 74, 80.3\\nTreatment 2\\n59.4, 72.1, 68, 66.2, 58.5\\n(The data represent the maximum depth of pits in units of one thousandth of\\nan inch.) The ordered values are 58.5, 59.4, 65.2∗, 66.2, 67.1∗, 68, 69.4∗, 72.1,\\n74∗, 78.2∗, 80.3∗with an asterisk noting that the data value was from sample\\n1. Hence, the value of the test statistic is T = 3 + 5 + 7 + 9 + 10 + 11 = 45.\\n■\\nSuppose that we desire a signiﬁcance level α test of H0. If the observed value of\\nT is T = t, then H0 should be rejected if either\\nPH0{T ≤t} ≤α\\n2\\nor\\nPH0{T ≥t} ≤α\\n2\\n(12.4.1)\\nThat is, the hypothesis that the two samples are equivalent should be rejected\\nif the sum of the ranks from the ﬁrst sample is either too small or too large to\\nbe explained by chance.\\nSince for integral t,\\nP{T ≥t} = 1 −P{T < t}\\n= 1 −P{T ≤t −1}\\nit follows from Equation (12.4.1) that H0 should be rejected if either\\nPH0{T ≤t} ≤α\\n2\\nor\\nPH0{T ≤t −1} ≥1 −α\\n2\\n(12.4.2)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 548}, page_content='540 CHAPTER 12: Nonparametric hypothesis tests\\nTo compute the probabilities in Equation (12.4.2), let P(N,M,K) denote the\\nprobability that the sum of the ranks of the ﬁrst sample will be less than or\\nequal to K when the sample sizes are N and M and H0 is true. We will now\\ndetermine a recursive formula for P(N,M,K), which will then allow us to\\nobtain the desired quantities P(n,m,t) = PH0{T ≤t} and P(n,m,t −1).\\nTo compute the probability that the sum of the ranks of the ﬁrst sample is less\\nthan or equal to K when N and M are the sample sizes and H0 is true, let us\\ncondition on whether the largest of the N +M data values belongs to the ﬁrst or\\nsecond sample. If it belongs to the ﬁrst sample, then the sum of the ranks of this\\nsample is equal to N + M plus the sum of the ranks of the other N −1 values\\nfrom the ﬁrst sample. Hence this sum will be less than or equal to K if the sum\\nof the ranks of the other N −1 values is less than or equal to K −(N + M).\\nBut since the remaining N −1 + M — that is, all but the largest — values all\\ncome from the same distribution (when H0 is true), it follows that the sum of\\nthe ranks of N −1 of them will be less than or equal to K −(N + M) with\\nprobability P(N −1,M,K −N −M). By a similar argument we can show that,\\ngiven that the largest value is from the second sample, the sum of the ranks of\\nthe ﬁrst sample will be less than or equal to K with probability P(N,M −1,K).\\nAlso, since the largest value is equally likely to be any of the N + M values\\nX1,...,XN,Y1,...,YM, it follows that it will come from the ﬁrst sample with\\nprobability N/(N + M). Putting these together, we thus obtain that\\nP(N,M,K) =\\nN\\nN + M P(N −1,M,K −N −M)\\n+\\nM\\nN + M P(N,M −1,K)\\n(12.4.3)\\nStarting with the boundary condition\\nP(1,0,K) =\\n\\x02 0\\nK ≤0\\n1\\nK > 0 ,\\nP(0,1,K) =\\n\\x02 0\\nK < 0\\n1\\nK ≥0\\nEquation (12.4.3) can be solved recursively to obtain P(n,m,t −1) and\\nP(n,m,t).\\nExample 12.4.b. Suppose we wanted to determine P(2,1,3). We use Equation\\n(12.4.3) as follows:\\nP(2,1,3) = 2\\n3P(1,1,0) + 1\\n3P(2,0,3)\\nand\\nP(1,1,0) = 1\\n2P(0,1,−2) + 1\\n2P(1,0,0) = 0\\nP(2,0,3) = P(1,0,1)\\n= P(0,0,0) = 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 549}, page_content='12.4 The two-sample problem\\n541\\nHence,\\nP(2,1,3) = 1\\n3\\nwhich checks since in order for the sum of the ranks of the two X values to be\\nless than or equal to 3, the largest of the values X1, X2, Y1, must be Y1, which,\\nwhen H0 is true, has probability 1\\n3.\\n■\\nSince the rank sum test calls for rejection when either\\n2P(n,m,t) ≤α\\nor\\nα ≥2[1 −P(n,m,t −1)]\\nit follows that the p-value of the test statistic when T = t is\\np-value = 2min{P(n,m,t),1 −P(n,m,t −1)}\\nR can be used to obtain the value of the test statistic T , called W by R, and\\nthe resulting p-value. To use the rank sum test (called by R the Wilcoxon rank\\nsum test) to test the hypothesis that the data sets x1,...,xn and y1,...,ym are\\nsamples from a common distribution, do the following\\n>x = c(x1,...,xn)\\n>y = c(y1,...,ym)\\n>wilcox.test(x,y)\\nThe result will be the p-value of the test. For instance, for the data of Exam-\\nple 12.4.a, we obtain the following:\\n>x = c(65.2,67.1,69.4,78.2,74,80.3)\\n>y = c(59.4,72.1,68,66.2,58.5)\\n>wilcox.test(x,y)\\nWilcoxon rank sum test\\ndata: x and y\\nW = 24, p-value = 0.1255\\nalternative hypothesis: true location shift is not equal to 0.\\n12.4.1\\nTesting the equality of multiple probability\\ndistributions\\nWhereas the preceding sections showed how to test the hypothesis that two\\npopulation distributions are identical, we are sometimes faced with the sit-\\nuation where there are more than two populations. So suppose there are k\\npopulations, that Fi is the distribution function of some measurable value of'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 550}, page_content='542 CHAPTER 12: Nonparametric hypothesis tests\\nthe elements of population i, and that we are interested in testing the null\\nhypothesis\\nH0 :\\nF1 = F2 = ··· = Fk\\nagainst the alternative\\nH1 :\\nnot all of the Fi are equal\\nTo test the preceding null hypothesis, suppose that independent samples are\\ndrawn from each of the k populations. Let ni denote the size of the sample\\nchosen from population i,i = 1,...,k. and let N = \\x03k\\ni=1 ni denote the total\\nnumber of data values obtained. Now, rank these N data values from the small-\\nest to largest, and let Ri denote the sum of the ranks of the ni data values from\\npopulation i, i = 1,...,k.\\nNow, when H0 is true, the rank of any individual data value is equally likely\\nto be any of the ranks 1,...,N, and thus the expected value of its rank is\\n1+2+···+N\\nN\\n= N+1\\n2 . Consequently, with ¯r = N+1\\n2 , it follows when H0 is true that\\nthe expected sum of the ranks of the ni data values from population i is ni ¯r.\\nThat is, when H0 is true\\nE[Ri] = ni ¯r.\\nDrawing inspiration from the goodness of ﬁt test, let us consider the test statis-\\ntic\\nT =\\nk\\n\\x08\\ni=1\\n(Ri −ni ¯r)2\\nni ¯r\\nand use a test that rejects the null hypothesis when T is large. Now,\\nT =1\\n¯r\\nk\\n\\x08\\ni=1\\nR2\\ni −2Rini ¯r + n2\\ni ¯r2\\nni\\n=1\\n¯r\\nk\\n\\x08\\ni=1\\nR2\\ni\\nni\\n−2\\nk\\n\\x08\\ni=1\\nRi + ¯r\\nk\\n\\x08\\ni=1\\nni\\n=1\\n¯r\\nk\\n\\x08\\ni=1\\nR2\\ni\\nni\\n−N ¯r\\nwhere the ﬁnal equality used that \\x03k\\ni=1 Ri is the sum of the ranks of all N =\\n\\x03\\ni ni data values and so\\nk\\n\\x08\\ni=1\\nRi = 1 + 2 + ··· + N = N(N + 1)\\n2\\n= N ¯r'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 551}, page_content='12.4 The two-sample problem\\n543\\nHence, rejecting H0 when T is large is equivalent to rejecting H0 when\\n\\x03k\\ni=1 R2\\ni /ni is large. So we might as well let the test statistic be\\nT S =\\nk\\n\\x08\\ni=1\\nR2\\ni\\nni\\nTo determine the appropriate α level signiﬁcance test, we need the distribution\\nof T S when H0 is true. While its exact distribution is rather complicated, we\\ncan use the result that, when H0 is true and all ni are at least 5, the distribution\\nof\\n12\\nN(N + 1)T S −3(N + 1)\\nis approximately that of a chi-squared random variable with k −1 degrees of\\nfreedom. Using this, we see that an approximate signiﬁcance level α test of the\\nnull hypothesis that all distributions are identical is to\\nreject H0\\nif\\n12\\nN(N + 1)T S −3(N + 1) ≥χ2\\nk−1,α\\nThe preceding is known as the Kruskal–Wallis test.\\nExample 12.4.c. The following data give the number of visitors to a medium-\\nsize Los Angeles library on Tuesdays, Wednesdays, and Thursdays of 10 succes-\\nsive weeks.\\nTuesday visitors\\n721, 660, 622, 738, 820, 707, 672, 589, 902, 688\\nWednesday visitors\\n604, 626, 744, 802, 691, 665, 711, 715, 661, 729\\nThursday visitors\\n642, 480, 705, 584, 713, 654, 704, 522, 683, 708\\nAre these data consistent with the hypothesis that the distributions of the num-\\nber of visitors for the three days are identical?\\nSolution. Ordering the N = 30 data values, gives that the sum of the ranks of\\nthe three samples are\\nR1 = 176,\\nR2 = 175,\\nR3 = 114\\nTherefore,\\n12\\nN(N + 1)T S −3(N + 1) =\\n12\\n30 · 31\\n1762 + 1752 + 1142\\n10\\n−93 = 3.254\\nBecause χ2\\n2,.05 = qchisq(1 −.05,2) = 5.99, the null hypothesis that the distri-\\nbutions of the number of visitors for each of the three weekdays are identical\\ncannot be rejected at the 5 percent level of signiﬁcance. Indeed, the resulting\\np-value is\\np-value = P{χ2\\n2 ≥3.254} = 1 −pchisq(3.254,2) = 0.1965182\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 552}, page_content='544 CHAPTER 12: Nonparametric hypothesis tests\\n12.5\\nThe runs test for randomness\\nA basic assumption in much of statistics is that a set of data constitutes a ran-\\ndom sample from some population. However, it is sometimes the case that the\\ndata are not generated by a truly random process but by one that may follow\\na trend or a type of cyclical pattern. In this section, we will consider a test —\\ncalled the runs test — of the hypothesis H0 that a given data set constitutes a\\nrandom sample.\\nTo begin, let us suppose that each of the data values is either a 0 or a 1. That\\nis, we shall assume that each data value can be dichotomized as being either\\na success or a failure. Let X1,...,XN denote the set of data. Any consecutive\\nsequence of either 1’s or 0’s is called a run. For instance, the data set\\n1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1\\ncontains 11 runs — 6 runs of 1 and 5 runs of 0. Suppose that the data set\\nX1,...,XN contains n 1’s and m 0’s, where n + m = N, and let R denote the\\nnumber of runs. Now, if H0 were true, then X1,...,XN would be equally likely\\nto be any of the N!/(n!m!) permutations of n 1’s and m 0’s, and therefore,\\ngiven a total of n 1’s and m 0’s, it follows that, under H0, the probability mass\\nfunction of R, the number of runs is given by\\nPH0{R = k} = number of permutations of n 1’s and m 0’s resulting in k runs\\n\\t n + m\\nn\\n\\nThis number of permutations can be explicitly determined and it can be shown\\nthat\\nPH0{R = 2k} = 2\\n\\t m −1\\nk −1\\n\\n\\t n −1\\nk −1\\n\\n\\t m + n\\nn\\n\\n(12.5.1)\\nPH0{R = 2k + 1} =\\n\\t m −1\\nk −1\\n\\n\\t n −1\\nk\\n\\n+\\n\\t m −1\\nk\\n\\n\\t n −1\\nk −1\\n\\n\\t n + m\\nn\\n\\nIf the data contain n 1’s and m 0’s, then the runs test calls for rejection of the\\nhypothesis that the data constitutes a random sample if the observed number\\nof runs is either too large or too small to be explained by chance. Speciﬁcally,\\nif the observed number of runs is r, then the p-value of the runs test is\\np-value = 2min(PH0{R ≥r},PH0{R ≤r})'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 553}, page_content='12.5 The runs test for randomness\\n545\\nUsing Equation (12.5.1), the exact p-value can be computed when the sample\\nsize is not too large.\\nExample 12.5.a. The following is the result of the last 30 games played by an\\nathletic team, with W signifying a win and L a loss.\\nW W W LW W LW W LW LW W LW W W W LW LW W W LW LW L\\nAre these data consistent with pure randomness?\\nSolution. To test the hypothesis of randomness, note that the data, which con-\\nsist of 20W’s and 10 L’s, contain 20 runs. To see whether this justiﬁes rejection\\nat, say, the 5 percent level of signiﬁcance, we can use the preceding to compute\\nthe p-value, which turns out to equal 0.01845. Therefore, the hypothesis of ran-\\ndomness would be rejected at the 5 percent level of signiﬁcance. (The striking\\nthing about these data is that the team always came back to win after losing a\\ngame, which would be quite unlikely if all outcomes containing 20 wins and\\n10 losses were equally likely.)\\n■\\nThe above can also be used to test for randomness when the data values are\\nnot just 0’s and 1’s. To test whether the data X1,...,XN constitute a random\\nsample, let s-med denote the sample median. Also let n denote the number\\nof data values that are less than or equal to s-med and m the number that are\\ngreater. (Thus, if N is even and all data values are distinct, then n = m = N/2.)\\nDeﬁne I1,...,IN by\\nIj =\\n\\x02 1\\nif Xj ≤s-med\\n0\\notherwise\\nNow, if the original data constituted a random sample, then the number of\\nruns in I1,...,IN would have a probability mass function given by Equation\\n(12.5.1). Thus, it follows that we can use the preceding runs test on the data\\nvalues I1,...,IN to test that the original data are random.\\nExample 12.5.b. The lifetime of 19 successively produced storage batteries is\\nas follows:\\n145152148155176134184132145162165\\n185174198179194201169182\\nThe sample median is the 10th smallest value — namely, 169. The data indi-\\ncating whether the successive values are less than or equal to or greater than\\n169 are as follows:\\n1111010111100000010'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 554}, page_content='546 CHAPTER 12: Nonparametric hypothesis tests\\nHence, the number of runs is 8. Using Equation (12.5.1), it can be shown that\\np-value = .357\\nThus the hypothesis of randomness is accepted.\\n■\\nIt can be shown that, when n and m are both large and H0 is true, R will have\\napproximately a normal distribution with mean and standard deviation given\\nby\\nμ = 2nm\\nn + m + 1\\nand\\nσ =\\n\\x17\\n2nm(2nm −n −m)\\n(n + m)2(n + m −1)\\n(12.5.2)\\nTherefore, when n and m are both large\\nPH0{R ≤r} = PH0\\n\\x02R −μ\\nσ\\n≤r −μ\\nσ\\n\\x13\\n≈P\\n\\x02\\nZ ≤r −μ\\nσ\\n\\x13\\n,\\nZ ∼N(0,1)\\n= \\x05\\n\\tr −μ\\nσ\\n\\nand, similarly,\\nPH0{R ≥r} ≈1 −\\x05\\n\\tr −μ\\nσ\\n\\nHence, for large n and m, the p-value of the runs test for randomness is approx-\\nimately given by\\np-value ≈2min\\n\\x02\\n\\x05\\n\\tr −μ\\nσ\\n\\n,1 −\\x05\\n\\tr −μ\\nσ\\n\\n\\x13\\nwhere μ and σ are given by Equation (12.5.2) and r is the observed number of\\nruns.\\nExample 12.5.c. Suppose that a sequence of sixty 1’s and sixty 0’s resulted in\\n75 runs. Since\\nμ = 61\\nand\\nσ =\\n\\x18\\n3,540\\n119 = 5.454\\nwe see that the approximate p-value is\\np-value ≈2min{\\x05(2.567),1 −\\x05(2.567)}\\n= 2 × (1 −.9949)\\n= .0102'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 555}, page_content='Problems 547\\nOn the other hand, it can be shown that the exact p-value is\\np-value = .0130\\nIf the number of runs were equal to 70 rather than 75, then the approximate\\np-value would be\\np-value ≈2[1 −\\x05(1.650)] = .0990\\nas opposed to the exact value of\\np-value = .1189\\n■\\nProblems\\n1. A new medicine against hypertension was tested on 18 patients. After 40\\ndays of treatment, the following changes of the diastolic blood pressure\\nwere observed.\\n−5,\\n−1,\\n+2,\\n+8,\\n−25,\\n+1,\\n+5,\\n−12,\\n−16\\n−9,\\n−8,\\n−18,\\n−5,\\n−22,\\n+4,\\n−21,\\n−15,\\n−11\\nUse the sign test to determine if the medicine has an effect on blood\\npressure. What is the p-value?\\n2. An engineering ﬁrm is involved in selecting a computer system, and the\\nchoice has been narrowed to two manufacturers. The ﬁrm submits eight\\nproblems to the two computer manufacturers and has each manufacturer\\nmeasure the number of seconds required to solve the design problem\\nwith the manufacturer’s software. The times for the eight design problems\\nare given below.\\nDesign problem\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nTime with computer A\\n15 32 17 26\\n42 29 12 38\\nTime with computer B\\n22 29\\n1 23 46 25 19 47\\nDetermine the p-value of the sign test when testing the hypothesis that\\nthere is no difference in the distribution of the time it takes the two types\\nof software to solve problems.\\n3. The published ﬁgure for the median systolic blood pressure of middle-\\naged men is 128. To determine if there has been any change in this value,\\na random sample of 100 men has been selected. Test the hypothesis that\\nthe median is equal to 128 if\\na.\\n60 men have readings above 128;\\nb.\\n70 men have readings above 128;\\nc.\\n80 men have readings above 128.\\nIn each case, determine the p-value.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 556}, page_content='548 CHAPTER 12: Nonparametric hypothesis tests\\n4. To test the hypothesis that the median weight of 16-year-old females\\nfrom Los Angeles is at least 110 pounds, a random sample of 200 such\\nfemales was chosen. If 120 females weighed less than 110 pounds, does\\nthis discredit the hypothesis? Use the 5 percent level of signiﬁcance. What\\nis the p-value?\\n5. In 2004, the national median salary of all U.S. ﬁnancial accountants was\\n$124,400. A recent random sample of 14 ﬁnancial accountants showed\\n2007 incomes of (in units of $1000)\\n125.5, 130.3, 133.0, 102.6, 198.0, 232.5, 106.8,\\n114.5, 122.0, 100.0, 118.8, 108.6, 312.7, 125.5\\nUse these data to test the hypothesis that the median salary of ﬁnancial\\naccountants in 2007 was not greater than in 2004. What is the p-value?\\n6. An experiment was initiated to study the effect of a newly developed gaso-\\nline detergent on automobile mileage. The following data, representing\\nmileage per gallon before and after the detergent was added for each of\\neight cars, resulted.\\nCar\\nMileage without\\nAdditive\\nMileage with\\nAdditive\\n1\\n24.2\\n23.5\\n2\\n30.4\\n29.6\\n3\\n32.7\\n32.3\\n4\\n19.8\\n17.6\\n5\\n25.0\\n25.3\\n6\\n24.9\\n25.4\\n7\\n22.2\\n20.6\\n8\\n21.5\\n20.7\\nFind the p-value of the test of the hypothesis that mileage is not affected\\nby the additive when\\na.\\nthe sign test is used;\\nb.\\nthe signed rank test is used.\\n7. Determine the p-value when using the signed rank statistic in Problems\\n1 and 2.\\n8. Twelve patients having high albumin content in their blood were treated\\nwith a medicine. Their blood content of albumin was measured before\\nand after treatment. The measured values are shown in the table.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 557}, page_content='Problems 549\\nBlood Content of Albumina\\nPatient\\nBefore Treatment\\nAfter Treatment\\n1\\n5.02\\n4.66\\n2\\n5.08\\n5.15\\n3\\n4.75\\n4.30\\n4\\n5.25\\n5.07\\n5\\n4.80\\n5.38\\n6\\n5.77\\n5.10\\n7\\n4.85\\n4.80\\n8\\n5.09\\n4.91\\n9\\n6.05\\n5.22\\n10\\n4.77\\n4.50\\n11\\n4.85\\n4.85\\n12\\n5.24\\n4.56\\na Values given in grams per 100 ml.\\nIs the effect of the medicine signiﬁcant at the 5 percent level?\\na.\\nUse the sign test.\\nb.\\nUse the signed rank test.\\n9. An engineer claims that painting the exterior of a particular aircraft affects\\nits cruising speed. To check this, the next 10 aircraft off the assembly line\\nwere ﬂown to determine cruising speed prior to painting, and were then\\npainted and reﬂown. The following data resulted.\\nAircraft\\nCruising Speed (knots)\\nNot Painted\\nPainted\\n1\\n426.1\\n416.7\\n2\\n418.4\\n403.2\\n3\\n424.4\\n420.1\\n4\\n438.5\\n431.0\\n5\\n440.6\\n432.6\\n6\\n421.8\\n404.2\\n7\\n412.2\\n398.3\\n8\\n409.8\\n405.4\\n9\\n427.5\\n422.8\\n10\\n441.2\\n444.8\\nDo the data uphold the engineer’s claim?\\n10. Ten pairs of duplicate spectrochemical determinations for nickel are pre-\\nsented below. The readings in column 2 were taken with one type of\\nmeasuring instrument and those in column 3 were taken with another\\ntype.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 558}, page_content='550 CHAPTER 12: Nonparametric hypothesis tests\\nSample\\nDuplicates\\n1\\n1.94\\n2.00\\n2\\n1.99\\n2.09\\n3\\n1.98\\n1.95\\n4\\n2.07\\n2.03\\n5\\n2.03\\n2.08\\n6\\n1.96\\n1.98\\n7\\n1.95\\n2.03\\n8\\n1.96\\n2.03\\n9\\n1.92\\n2.01\\n10\\n2.00\\n2.12\\nTest the hypothesis, at the 5 percent level of signiﬁcance, that the two\\nmeasuring instruments give equivalent results.\\n11. Let X1,...,Xn be a sample from the continuous distribution F hav-\\ning median m; and suppose we are interested in testing the hypothesis\\nH0 : m = m0 against the one-sided alternative H1 : m > m0. Present the\\none-sided analog of the signed rank test. Explain how the p-value would\\nbe computed.\\n12. In a study of bilingual coding, 12 bilingual (French and English) college\\nstudents are divided into two groups. Each group reads an article written\\nin French, and each answers a series of 25 multiple-choice questions cov-\\nering the content of the article. For one group the questions are written\\nin French; the other takes the examination in English. The score (total\\ncorrect) for the two groups is:\\nExamination in French\\n11\\n12\\n16 22 25 25\\nExamination in English\\n10 13\\n17 19\\n21\\n24\\nIs this evidence at the 5 percent signiﬁcance level that there is difﬁculty\\nin transferring information from one language to another?\\n13. Fifteen cities, of roughly equal size, are chosen for a trafﬁc safety study.\\nEight of them are randomly chosen, and in these cities a series of newspa-\\nper articles dealing with trafﬁc safety is run over a 1-month period. The\\nnumber of trafﬁc accidents reported in the month following this cam-\\npaign is as follows:\\nTreatment group\\n19\\n31\\n39 45\\n47 66 74 81\\nControl group\\n28 36 44 49\\n52 52 60\\nDetermine the exact p-value when testing the hypothesis that the articles\\nhave not had any effect.\\n14. Determine the p-value in Problem 13 by\\na.\\nusing the normal approximation;\\nb.\\nusing a simulation study.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 559}, page_content='Problems 551\\n15. The following are the weights of random samples of adult males from\\ndifferent political afﬁliations.\\nRepublicans:\\n204, 178, 195, 187, 240, 182, 152, 166\\nDemocrats:\\n175, 200, 168, 192, 156, 164, 180, 138\\nWe want to use these data to test the null hypothesis that the two distri-\\nbutions are identical.\\na.\\nFind the exact p-value.\\nb.\\nDetermine the p-value obtained when using the normal approxima-\\ntion.\\n16. In a 1943 experiment (Whitlock, H. V., and Bliss, D. H., “A bioassay tech-\\nnique for antihelminthics,” Journal of Parasitology, 29, pp. 48–58, 10),\\nalbino rats were used to study the effectiveness of carbon tetrachloride\\nas a treatment for worms. Each rat received an injection of worm larvae.\\nAfter 8 days, the rats were randomly divided into 2 groups of 5 each; each\\nrat in the ﬁrst group received a dose of .032 cc of carbon tetrachloride,\\nwhereas the dosage for each rat in the second group was .063 cc. Two\\ndays later the rats were killed, and the number of adult worms in each rat\\nwas determined. The numbers detected in the group receiving the .032\\ndosage were\\n421, 462, 400, 378, 413\\nwhereas they were\\n207, 17, 412, 74, 116\\nfor those receiving the .063 dosage. Do the data prove that the larger\\ndosage is more effective than the smaller?\\n17. In a 10-year study of the dispersal patterns of beavers (Sun, L. and Muller-\\nSchwarze, D., “Statistical resampling methods in biology: A case study\\nof beaver dispersal patterns,” American Journal of Mathematical and Man-\\nagement Sciences, 16, pp. 463–502, 1996) a total of 332 beavers were\\ntrapped in Allegheny State Park in southwestern New York. The beavers\\nwere tagged (so as to be identiﬁable when later caught) and released.\\nOver time a total of 32 of them , 9 female and 23 male, were discovered\\nto have resettled in other sites. The following data give the dispersal dis-\\ntances (in kilometers) between these beavers’ original and resettled sites\\nfor the females and for the males.\\nFemales:\\n.660,\\n.984,\\n.984,\\n1.992,\\n4.368,\\n6.960,\\n10.656,\\n21.600,\\n31.680\\nMales:\\n.288,\\n.312,\\n.456,\\n.528,\\n.576,\\n.720,\\n.792,\\n.984,\\n1.224,\\n1.584,\\n2.304,\\n2.328,\\n2.496,\\n2.688,\\n3.096,\\n3.408,\\n4.296,\\n4.884,\\n5.928,\\n6.192,\\n6.384,\\n13.224,\\n27.600\\nDo the data prove that the dispersal distances are gender related?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 560}, page_content='552 CHAPTER 12: Nonparametric hypothesis tests\\n18. The following data give the numbers of people who visit a local health\\nclinic in the day following\\na.\\na Saturday win by the local university football team;\\nb.\\na Saturday loss by the team;\\nc.\\na Saturday when the team does not play.\\nNumber following a win\\n71, 66, 62, 79, 80, 70, 66, 59, 89, 68\\nNumber following a loss\\n64, 62, 75, 81, 69, 67, 73, 71, 69, 74\\nNumber when no game\\n49, 48, 70, 58, 73, 65, 55, 52, 68, 74\\nDo these data prove that the resulting number of clinic visits depends\\non what happens with the football team? Test at the 5 percent level.\\n19. A production run of 50 items resulted in 11 defectives, with the defectives\\noccurring on the following items (where the items are numbered by their\\norder of production): 8, 12, 13, 14, 31, 32, 37, 38, 40, 41, 42. Can we\\nconclude that the successive items did not constitute a random sample?\\n20. The following data represent the successive quality levels of 25 articles:\\n100, 110, 122, 132, 99, 96, 88, 75, 45, 211, 154, 143, 161, 142, 99, 111,\\n105, 133, 142, 150, 153, 121, 126, 117, 155. Does it appear that these\\ndata are a random sample from some population?\\n21. Can we use the runs test if we consider whether each data value is less\\nthan or greater than some predetermined value rather than the value\\ns-med?\\nTable 12.1 Year and Magnitude (0 = moderate, 1 = strong)\\nof Major El Niño Events, 1800–1987.\\nYear\\nMagnitude\\nYear\\nMagnitude\\nYear\\nMagnitude\\n1803\\n1\\n1866\\n0\\n1918\\n0\\n1806\\n0\\n1867\\n0\\n1923\\n0\\n1812\\n0\\n1871\\n1\\n1925\\n1\\n1814\\n1\\n1874\\n0\\n1930\\n0\\n1817\\n0\\n1877\\n1\\n1932\\n1\\n1819\\n0\\n1880\\n0\\n1939\\n0\\n1821\\n0\\n1884\\n1\\n1940\\n1\\n1824\\n0\\n1887\\n0\\n1943\\n0\\n1828\\n1\\n1891\\n1\\n1951\\n0\\n1832\\n0\\n1896\\n0\\n1953\\n0\\n1837\\n0\\n1899\\n1\\n1957\\n1\\n1844\\n1\\n1902\\n0\\n1965\\n0\\n1850\\n0\\n1905\\n0\\n1972\\n1\\n1854\\n0\\n1907\\n0\\n1976\\n0\\n1857\\n0\\n1911\\n1\\n1982\\n1\\n1860\\n0\\n1914\\n0\\n1987\\n0\\n1864\\n1\\n1917\\n1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 561}, page_content='Problems 553\\n22. The following table (taken from Quinn, W.H., Neal, T.V., and Antuñez de\\nMayolo, S. E., 1987, “El Niño occurrences over the past four-and-a-half\\ncenturies,” Journal of Geophysical Research, 92 (C13), pp. 14,449–14,461)\\ngives the years and magnitude (either moderate or strong) of major El\\nNiño years between 1800 and 1987. Use it to test the hypothesis that the\\nsuccessive El Niño magnitudes constitute a random sample.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 562}, page_content='CHAPTER 13\\nQuality control\\n13.1\\nIntroduction\\nAlmost every manufacturing process results in some random variation in the\\nitems it produces. That is, no matter how stringently the process is being con-\\ntrolled, there is always going to be some variation between the items produced.\\nThis variation is called chance variation and is considered to be inherent to the\\nprocess. However, there is another type of variation that sometimes appears.\\nThis variation, far from being inherent to the process, is due to some assignable\\ncause and usually results in an adverse effect on the quality of the items pro-\\nduced. For instance, this latter variation may be caused by a faulty machine\\nsetting, or by poor quality of the raw materials presently being used, or by in-\\ncorrect software, or human error, or any other of a large number of possibilities.\\nWhen the only variation present is due to chance, and not to assignable cause,\\nwe say that the process is in control, and a key problem is to determine whether\\na process is in or is out of control.\\nThe determination of whether a process is in or out of control is greatly facili-\\ntated by the use of control charts, which are determined by two numbers — the\\nupper and lower control limits. To employ such a chart, the data generated by\\nthe manufacturing process are divided into subgroups and subgroup statistics\\n— such as the subgroup average and subgroup standard deviation — are com-\\nputed. When the subgroup statistic does not fall within the upper and lower\\ncontrol limit, we conclude that the process is out of control.\\nIn Sections 13.2 and 13.3, we suppose that the successive items produced\\nhave measurable characteristics, whose mean and variance are ﬁxed when the\\nprocess is in control. We show how to construct control charts based on sub-\\ngroup averages (in Section 13.2) and on subgroup standard deviations (in\\nSection 13.3). In Section 13.4, we suppose that rather than having a measur-\\nable characteristic, each item is judged by an attribute — that is, it is classiﬁed\\nas either acceptable or unacceptable. Then we show how to construct control\\ncharts that can be used to indicate a change in the quality of the items pro-\\nduced. In Section 13.5, we consider control charts in situations where each\\nitem produced has a random number of defects. Finally, in Section 13.6 we\\nconsider more sophisticated types of control charts — ones that don’t consider\\neach subgroup value in isolation but rather take into account the values of\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00022-3\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n555'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 563}, page_content='556 CHAPTER 13: Quality control\\nother subgroups. Three different control charts of this type — known as mov-\\ning average, exponential weighted moving average, and cumulative sum control\\ncharts — are presented in Section 13.6.\\n13.2\\nControl charts for average values: the x control\\nchart\\nSuppose that when the process is in control the successive items produced have\\nmeasurable characteristics that are independent, normal random variables with\\nmean μ and variance σ 2. However, due to special circumstances, suppose that\\nthe process may go out of control and start producing items having a different\\ndistribution. We would like to be able to recognize when this occurs so as to\\nstop the process, ﬁnd out what is wrong, and ﬁx it.\\nLet X1, X2,... denote the measurable characteristics of the successive items pro-\\nduced. To determine when the process goes out of control, we start by breaking\\nthe data up into subgroups of some ﬁxed size — call it n. The value of n is\\nchosen so as to yield uniformity within subgroups. That is, n may be chosen\\nso that all data items within a subgroup were produced on the same day, or on\\nthe same shift, or using the same settings, and so on. In other words, the value\\nof n is chosen so that it is reasonable that a shift in distribution would occur\\nbetween and not within subgroups. Typical values of n are 4, 5, or 6.\\nLet Xi, i = 1,2,... denote the average of the ith subgroup. That is,\\nX1 = X1 + ··· + Xn\\nn\\nX2 = Xn+1 + ··· + X2n\\nn\\nX3 = X2n+1 + ··· + X3n\\nn\\nand so on. Since, when in control, each of the Xi have mean μ and variance\\nσ 2, it follows that\\nE(Xi) = μ\\nVar(Xi) = σ 2\\nn\\nand so\\nXi −μ\\n\\x02\\nσ 2/n\\n∼N(0,1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 564}, page_content='13.2 Control charts for average values: the x control chart\\n557\\nThat is, if the process is in control throughout the production of subgroup i,\\nthen √n(Xi −μ)/σ has a standard normal distribution. Now it follows that a\\nstandard normal random variable Z will almost always be between −3 and +3.\\n(Indeed, P{−3 < Z < 3} = .9973.) Hence, if the process is in control through-\\nout the production of the items in subgroup i, then we would certainly expect\\nthat\\n−3 < √n Xi −μ\\nσ\\n< 3\\nor, equivalently, that\\nμ −3σ\\n√n < Xi < μ + 3σ\\n√n\\nThe values\\nUCL ≡μ + 3σ\\n√n\\nand\\nLCL ≡μ −3σ\\n√n\\nare called, respectively, the upper and lower control limits.\\nThe X control chart, which is designed to detect a change in the average value\\nof an item produced, is obtained by plotting the successive subgroup averages\\nXi and declaring that the process is out of control the ﬁrst time Xi does not\\nfall between LCL and UCL (see Figure 13.1).\\nExample 13.2.a. A manufacturer produces steel shafts having diameters that\\nshould be normally distributed with mean 3 mm and standard deviation .1\\nmm. Successive samples of four shafts have yielded the following sample aver-\\nages in millimeters.\\nSample\\nX\\nSample\\nX\\n1\\n3.01\\n6\\n3.02\\n2\\n2.97\\n7\\n3.10\\n3\\n3.12\\n8\\n3.14\\n4\\n2.99\\n9\\n3.09\\n5\\n3.03\\n10\\n3.20\\nWhat conclusion should be drawn?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 565}, page_content='558 CHAPTER 13: Quality control\\nFIGURE 13.1\\nControl chart for X, n = size of subgroup.\\nSolution. When in control the successive diameters have mean μ = 3 and stan-\\ndard deviation σ = .1, and so with n = 4 the control limits are\\nLCL = 3 −3(.1)\\n√\\n4\\n= 2.85,\\nUCL = 3 + 3(.1)\\n√\\n4\\n= 3.15\\nBecause sample number 10 falls above the upper control limit, it appears that\\nthere is reason to suspect that the mean diameter of shafts now differs from 3.\\n(Clearly, judging from the results of Samples 6 through 10 it appears to have\\nincreased beyond 3.)\\n■\\nRemarks\\n(a) The foregoing supposes that when the process is in control the underlying\\ndistribution is normal. However, even if this is not the case, by the central limit\\ntheorem it follows that the subgroup averages should have a distribution that\\nis roughly normal and so would be unlikely to differ from its mean by more\\nthan 3 standard deviations.\\n(b) It is frequently the case that we do not determine the measurable qualities\\nof all the items produced but only those of a randomly chosen subset of items.\\nIf this is so then it is natural to select, as a subgroup, items that are produced\\nat roughly the same time.\\nIt is important to note that even when the process is in control there is a chance\\n— namely, .0027 — that a subgroup average will fall outside the control limit'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 566}, page_content='13.2 Control charts for average values: the x control chart\\n559\\nand so one would incorrectly stop the process and hunt for the nonexistent\\nsource of trouble.\\nLet us now suppose that the process has just gone out of control by a change\\nin the mean value of an item from μ to μ + a where a > 0. How long will it\\ntake (assuming things do not change again) until the chart will indicate that\\nthe process is now out of control? To answer this, note that a subgroup average\\nwill be within the control limits if\\n−3 < √n X −μ\\nσ\\n< 3\\nor, equivalently, if\\n−3 −a√n\\nσ\\n< √n X −μ\\nσ\\n−a√n\\nσ\\n< 3 −a√n\\nσ\\nor\\n−3 −a√n\\nσ\\n< √n X −μ −a\\nσ\\n< 3 −a√n\\nσ\\nHence, since X is normal with mean μ+a and variance σ 2/n — and so √n(X−\\nμ −a)/σ has a standard normal distribution — the probability that it will fall\\nwithin the control limits is\\nP\\n\\x03\\n−3 −a√n\\nσ\\n< Z < 3 −a√n\\nσ\\n\\x04\\n= \\x03\\n\\x05\\n3 −a√n\\nσ\\n\\x06\\n−\\x03\\n\\x05\\n−3 −a√n\\nσ\\n\\x06\\n≈\\x03\\n\\x05\\n3 −a√n\\nσ\\n\\x06\\nand so the probability that it falls outside is approximately 1 −\\x03(3 −a√n/σ).\\nFor instance, if the subgroup size is n = 4, then an increase in the mean value\\nof 1 standard deviation — that is, a = σ — will result in the subgroup average\\nfalling outside of the control limits with probability 1 −\\x03(1) ≈.1587. Because\\neach subgroup average will independently fall outside the control limits with\\nprobability 1 −\\x03(3 −a√n/σ), it follows that the number of subgroups that\\nwill be needed to detect this shift has a geometric distribution with mean {1 −\\n\\x03(3 −a√n/σ)}−1. (In the case mentioned before with n = 4, the number of\\nsubgroups one would have to chart to detect a change in the mean of 1 standard\\ndeviation has a geometric distribution with mean 1/.1587 ≈6.301.)\\n13.2.1\\nCase of unknown μ and σ\\nIf one is just starting up a control chart and does not have reliable historical\\ndata, then μ and σ would not be known and would have to be estimated. To\\ndo so, we employ k of the subgroups where k should be chosen so that k ≥20'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 567}, page_content='560 CHAPTER 13: Quality control\\nand nk ≥100. If Xi,i = 1,...,k is the average of the ith subgroup, then it is\\nnatural to estimate μ by X the average of these subgroup averages. That is,\\nX = X1 + ··· + Xk\\nk\\nTo estimate σ, let Si denote the sample standard deviation of the ith subgroup,\\ni = 1,...,k. That is,\\nS1 =\\n\\x07\\n\\x08\\n\\x08\\n\\t\\nn\\n\\ni=1\\n(Xi −X1)2\\nn −1\\nS2 =\\n\\x07\\n\\x08\\n\\x08\\n\\t\\nn\\n\\ni=1\\n(Xn+i −X2)2\\nn −1\\n...\\nSk =\\n\\x07\\n\\x08\\n\\x08\\n\\t\\nn\\n\\ni=1\\n(X(k−1)n+i −Xk)2\\nn −1\\nLet\\nS = (S1 + ··· + Sk)/k\\nThe statistic S will not be an unbiased estimator of σ — that is, E[S] ̸= σ. To\\ntransform it into an unbiased estimator, we must ﬁrst compute E[S], which is\\naccomplished as follows:\\nE[S] = E[S1] + ··· + E[Sk]\\nk\\n(13.2.1)\\n= E[S1]\\nwhere the last equality follows since S1,...,Sk are independent and identically\\ndistributed (and thus have the same mean). To compute E[S1], we make use of\\nthe following fundamental result about normal samples — namely, that\\n(n −1)S2\\n1\\nσ 2\\n=\\nn\\n\\ni=1\\n(Xi −X1)2\\nσ 2\\n∼χ2\\nn−1\\n(13.2.2)\\nNow it is not difﬁcult to show (see Problem 3) that\\nE[\\n√\\nY] =\\n√\\n2\\x05(n/2)\\n\\x05( n−1\\n2 )\\nwhenY ∼χ2\\nn−1\\n(13.2.3)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 568}, page_content='13.2 Control charts for average values: the x control chart\\n561\\nSince\\nE[\\n\\x0b\\n(n −1)S2/σ 2] =\\n√\\nn −1E[S1]/σ\\nwe see from Equations (13.2.2) and (13.2.3) that\\nE[S1] =\\n√\\n2\\x05(n/2)σ\\n√\\nn −1\\x05( n−1\\n2 )\\nHence, if we set\\nc(n) =\\n√\\n2\\x05(n/2)\\n√\\nn −1\\x05( n−1\\n2 )\\nthen it follows from Equation (13.2.1) that S/c(n) is an unbiased estimator of σ.\\nTable 13.1 presents the values of c(n) for n=2 through n=10.\\nTable 13.1 Values of c(n).\\nc (2)\\n=\\n.7978849\\nc (3)\\n=\\n.8862266\\nc (4)\\n=\\n.9213181\\nc (5)\\n=\\n.9399851\\nc (6)\\n=\\n.9515332\\nc (7)\\n=\\n.9593684\\nc (8)\\n=\\n.9650309\\nc (9)\\n=\\n.9693103\\nc (10)\\n=\\n.9726596\\nTechnical remark\\nIn determining the values in Table 13.1, the computation of \\x05(n/2) and \\x05(n −\\n1\\n2) was based on the recursive formula\\n\\x05(a) = (a −1)\\x05(a −1)\\nwhich was established in Section 5.7. This recursion yields that, for integer n,\\n\\x05(n) = (n −1)(n −2)···3 · 2 · 1 · \\x05(1)\\n= (n −1)!\\nsince \\x05(1) =\\n\\x0c ∞\\n0\\ne−x dx = 1\\nThe recursion also yields that\\n\\x05\\n\\x05n + 1\\n2\\n\\x06\\n=\\n\\x05n −1\\n2\\n\\x06\\x05\\nn −3\\n2\\n\\x06\\n··· 3\\n2 · 1\\n2 · \\x05\\n\\x051\\n2\\n\\x06'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 569}, page_content='562 CHAPTER 13: Quality control\\nwith\\n\\x05\\n\\x051\\n2\\n\\x06\\n=\\n\\x0c ∞\\n0\\ne−xx−1/2 dx\\n=\\n\\x0c ∞\\n0\\ne−y2/2\\n√\\n2\\ny y dy\\nby x = y2\\n2\\ndx = y dy\\n=\\n√\\n2\\n\\x0c ∞\\n0\\ne−y2/2 dy\\n= 2√π\\n1\\n√\\n2π\\n\\x0c ∞\\n0\\ne−y2/2 dy\\n= 2√πP[N(0,1) > 0]\\n= √π\\nThe preceding estimates for μ and σ make use of all k subgroups and thus\\nare reasonable only if the process has remained in control throughout. To\\ncheck this, we compute the control limits based on these estimates of μ and\\nσ, namely,\\nLCL = X −\\n3S\\nc(n)√n\\n(13.2.4)\\nUCL = X +\\n3S\\nc(n)√n\\nWe now check that each of the subgroup averages Xi falls within these lower\\nand upper limits. Any subgroup whose average value does not fall within the\\nlimits is removed (we suppose that the process was temporarily out of control)\\nand the estimates are recomputed. We then again check that all the remaining\\nsubgroup averages fall within the control limits. If not, then they are removed,\\nand so on. Of course, if too many of the subgroup averages fall outside the\\ncontrol limits, then it is clear that no control has yet been established.\\nExample 13.2.b. Let us reconsider Example 13.2.a under the new supposition\\nthat the process is just beginning and so μ and σ are unknown. Also suppose\\nthat the sample standard deviations were as follows:\\nX\\nS\\nX\\nS\\n1\\n3.01\\n.12\\n6\\n3.02\\n.08\\n2\\n2.97\\n.14\\n7\\n3.10\\n.15\\n3\\n3.12\\n.08\\n8\\n3.14\\n.16\\n4\\n2.99\\n.11\\n9\\n3.09\\n.13\\n5\\n3.03\\n.09\\n10\\n3.20\\n.16'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 570}, page_content='13.2 Control charts for average values: the x control chart\\n563\\nSince X = 3.067, S = .122, c(4) = .9213, the control limits are\\nLCL = 3.067 −3(.122)\\n2 × .9213 = 2.868\\nUCL = 3.067 + 3(.122)\\n2 × .9213 = 3.266\\nSince all the Xi fall within these limits, we suppose that the process is in control\\nwith μ = 3.067 and σ = S/c(4) = .1324.\\nSuppose now that the values of the items produced are supposed to fall within\\nthe speciﬁcations 3 ± .1. Assuming that the process remains in control and that\\nthe foregoing are accurate estimates of the true mean and standard deviation,\\nwhat proportion of the items will meet the desired speciﬁcations?\\nSolution. To answer the foregoing, we note that when μ = 3.067 and σ =\\n.1324,\\nP{2.9 ≤X ≤3.1} = P\\n\\x032.9 −3.067\\n.1324\\n≤X −3.067\\n.1324\\n≤3.1 −3.067\\n.1324\\n\\x04\\n= \\x03(.2492) −\\x03(−1.2613)\\n= .5984 −(1 −.8964)\\n= .4948\\nHence, 49 percent of the items produced will meet the speciﬁcations.\\n■\\nRemarks\\n(a) The estimator X is equal to the average of all nk measurements and is thus\\nthe obvious estimator of μ. However, it may not immediately be clear why the\\nsample standard deviation of all the nk measurements, namely,\\nS ≡\\n\\x07\\n\\x08\\n\\x08\\n\\t\\nnk\\n\\ni=1\\n(Xi −X)2\\nnk −1\\nis not used as the initial estimator of σ. The reason it is not is that the process\\nmay not have been in control throughout the ﬁrst k subgroups, and thus this\\nlatter estimator could be far away from the true value. Also, it often happens\\nthat a process goes out of control by an occurrence that results in a change\\nof its mean value μ while leaving its standard deviation unchanged. In such a\\ncase, the subgroup sample deviations would still be estimators of σ, whereas\\nthe entire sample standard deviation would not. Indeed, even in the case where\\nthe process appears to be in control throughout, the estimator of σ presented\\nis preferred over the sample standard deviation S. The reason for this is that we'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 571}, page_content='564 CHAPTER 13: Quality control\\ncannot be certain that the mean has not changed throughout this time. That is,\\neven though all the subgroup averages fall within the control limits, and so we\\nhave concluded that the process is in control, there is no assurance that there\\nare no assignable causes of variation present (which might have resulted in a\\nchange in the mean that has not yet been picked up by the chart). It merely\\nmeans that for practical purposes it pays to act as if the process was in con-\\ntrol and let it continue to produce items. However, since we realize that some\\nassignable cause of variation might be present, it has been argued that S/c(n)\\nis a “safer” estimator than the sample standard deviation. That is, although it\\nis not quite as good when the process has really been in control throughout, it\\ncould be a lot better if there had been some small shifts in the mean.\\n(b) In the past, an estimator of σ based on subgroup ranges — deﬁned as\\nthe difference between the largest and smallest value in the subgroup — has\\nbeen employed. This was done to keep the necessary computations simple (it\\nis clearly much easier to compute the range than it is to compute the subgroup’s\\nsample standard deviation). However, with modern-day computational power\\nthis should no longer be a consideration, and since the standard deviation es-\\ntimator both has smaller variance than the range estimator and is more robust\\n(in the sense that it would still yield a reasonable estimate of the population\\nstandard deviation even when the underlying distribution is not normal), we\\nwill not consider the latter estimator in this text.\\n13.3\\nS-control charts\\nThe X control charts presented in the previous section are designed to pick up\\nchanges in the population mean. In cases where one is also concerned about\\npossible changes in the population variance, we can utilize an S-control chart.\\nAs before, suppose that, when in control, the items produced have a measur-\\nable characteristic that is normally distributed with mean μ and variance σ 2. If\\nSi is the sample standard deviation for the ith subgroup, that is,\\nSi =\\n\\x07\\n\\x08\\n\\x08\\n\\t\\nn\\n\\nj=1\\n(X(i−1)n+j −Xi)2\\n(n −1)\\nthen, as was shown in Section 13.2.1,\\nE[Si] = c(n)σ\\n(13.3.1)\\nIn addition,\\nVar(Si) = E[S2\\ni ] −(E[Si])2\\n(13.3.2)\\n= σ 2 −c2(n)σ 2\\n= σ 2[1 −c2(n)]'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 572}, page_content='13.3 S-control charts 565\\nwhere the next to last equality follows from Equation (13.2.2) and the fact that\\nthe expected value of a chi-square random variable is equal to its degrees of\\nfreedom parameter.\\nOn using the fact that, when in control, Si has the distribution of a constant\\n(equal to σ/\\n√\\nn −1) times the square root of a chi-square random variable with\\nn −1 degrees of freedom, it can be shown that Si will, with probability near to\\n1, be within 3 standard deviations of its mean. That is,\\nP{E[Si] −3\\n\\x02\\nVar(Si) < Si < E[Si] + 3\\n\\x02\\nVar(Si)} ≈.99\\nThus, using the formulas 13.3.1 and 13.3.2 for E[Si] and Var(Si), it is natural\\nto set the upper and lower control limits for the S chart by\\nUCL = σ[c(n) + 3\\n\\x02\\n1 −c2(n)]\\n(13.3.3)\\nLCL = σ[c(n) −3\\n\\x02\\n1 −c2(n)]\\nThe successive values of Si should be plotted to make certain they fall within\\nthe upper and lower control limits. When a value falls outside, the process\\nshould be stopped and declared to be out of control.\\nWhen one is just starting up a control chart and σ is unknown, it can be es-\\ntimated from S/c(n). Using the foregoing, the estimated control limits would\\nthen be\\nUCL = S[1 + 3\\n\\x0b\\n1/c2(n) −1]\\n(13.3.4)\\nLCL = S[1 −3\\n\\x0b\\n1/c2(n) −1]\\nAs in the case of starting up an X control chart, it should then be checked that\\nthe k subgroup standard deviations S1,S2,...,Sk all fall within these control\\nlimits. If any of them falls outside, then those subgroups should be discarded\\nand S recomputed.\\nExample 13.3.a. The following are the X and S values for 20 subgroups of size\\n5 for a recently started process.\\nSub-\\ngroup\\nX\\nS\\nSub-\\ngroup\\nX\\nS\\nSub-\\ngroup\\nX\\nS\\nSub-\\ngroup\\nX\\nS\\n1\\n35.1\\n4.2\\n6\\n36.4\\n4.5\\n11\\n38.1\\n4.2\\n16\\n41.3\\n8.2\\n2\\n33.2\\n4.4\\n7\\n35.9\\n3.4\\n12\\n37.6\\n3.9\\n17\\n35.7\\n8.1\\n3\\n31.7\\n2.5\\n8\\n38.4\\n5.1\\n13\\n38.8\\n3.2\\n18\\n36.3\\n4.2\\n4\\n35.4\\n3.2\\n9\\n35.7\\n3.8\\n14\\n34.3\\n4.0\\n19\\n35.4\\n4.1\\n5\\n34.5\\n2.6\\n10\\n27.2\\n6.2\\n15\\n43.2\\n3.5\\n20\\n34.6\\n3.7'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 573}, page_content='566 CHAPTER 13: Quality control\\nFIGURE 13.2\\nSince X = 35.94, S = 4.35, c(5) = .9400, we see from Equations (13.2.4) and\\n(13.3.4) that the preliminary upper and lower control limits for X and S are\\nUCL(X) = 42.149\\nLCL(X) = 29.731\\nUCL(S) = 9.087'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 574}, page_content='13.4 Control charts for the fraction defective\\n567\\nLCL(S) = −.386\\nThe control charts for X and S with the preceding control limits are shown in\\nFigures 13.2A and 13.2B. Since X10 and X15 fall outside the X control limits,\\nthese subgroups must be eliminated and the control limits recomputed. We\\nleave the necessary computations as an exercise.\\n■\\n13.4\\nControl charts for the fraction defective\\nThe X and S-control charts can be used when the data are measurements whose\\nvalues can vary continuously over a region. There are also situations in which\\nthe items produced have quality characteristics that are classiﬁed as either being\\ndefective or nondefective. Control charts can also be constructed in this latter\\nsituation.\\nLet us suppose that when the process is in control each item produced will\\nindependently be defective with probability p. If we let X denote the number\\nof defective items in a subgroup of n items, then assuming control, X will be\\na binomial random variable with parameters (n, p). If F = X/n is the fraction\\nof the subgroup that is defective, then assuming the process is in control, its\\nmean and standard deviation are given by\\nE[F] = E[X]\\nn\\n= np\\nn = p\\n\\x02\\nVar(F) =\\n\\r\\nVar(X)\\nn2\\n=\\n\\r\\nnp(1 −p)\\nn2\\n=\\n\\r\\np(1 −p)\\nn\\nHence, when the process is in control the fraction defective in a subgroup of\\nsize n should, with high probability, be between the limits\\nLCL = p −3\\n\\r\\np(1 −p)\\nn\\n,\\nUCL = p + 3\\n\\r\\np(1 −p)\\nn\\nThe subgroup size n is usually much larger than the typical values of between 4\\nand 10 used in X and S charts. The main reason for this is that if p is small and\\nn is not of reasonable size, then most of the subgroups will have zero defects\\neven when the process goes out of control. Thus, it would take longer than it\\nwould if n were chosen so that np were not close to zero to detect a shift in\\nquality.\\nTo start such a control chart it is, of course, necessary ﬁrst to estimate p. To do\\nso, choose k of the subgroups, where again one should try to take k ≥20, and\\nlet Fi denote the fraction of the ith subgroup that are defective. The estimate of\\np is given by F deﬁned by\\nF = F1 + ··· + Fk\\nk'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 575}, page_content='568 CHAPTER 13: Quality control\\nSince nFi is equal to the number of defectives in subgroup i, we see that Fk can\\nalso be expressed as\\nF = nF1 + ··· + nFk\\nnk\\n= total number of defectives in all the subgroups\\nnumber of items in the subgroups\\nIn other words, the estimate of p is just the proportion of items inspected that\\nare defective. The upper and lower control limits are now given by\\nLCL = F −3\\n\\x0e\\nF(1 −F)\\nn\\n,\\nUCL = F + 3\\n\\x0e\\nF(1 −F)\\nn\\nWe should now check whether the subgroup fractions F1,F2,...,Fk fall within\\nthese control limits. If some of them fall outside, then the corresponding sub-\\ngroups should be eliminated and F recomputed.\\nExample 13.4.a. Successive samples of 50 screws are drawn from the hourly\\nproduction of an automatic screw machine, with each screw being rated as ei-\\nther acceptable or defective. This is done for 20 such samples with the following\\ndata resulting.\\nSubgroup\\nDefectives\\nF\\nSubgroup\\nDefectives\\nF\\n1\\n6\\n.12\\n11\\n1\\n.02\\n2\\n5\\n.10\\n12\\n3\\n.06\\n3\\n3\\n.06\\n13\\n2\\n.04\\n4\\n0\\n.00\\n14\\n0\\n.00\\n5\\n1\\n.02\\n15\\n1\\n.02\\n6\\n2\\n.04\\n16\\n1\\n.02\\n7\\n1\\n.02\\n17\\n0\\n.00\\n8\\n0\\n.00\\n18\\n2\\n.04\\n9\\n2\\n.04\\n19\\n1\\n.02\\n10\\n1\\n.02\\n20\\n2\\n.04\\nWe can compute the trial control limits as follows:\\nF = total number defectives\\ntotal number items\\n=\\n34\\n1000 = .034\\nand so\\nUCL = .034 + 3\\n\\r\\n(.034)(.968)\\n50\\n= .1109\\nLCL = .034 −3\\n\\r\\n(.034)(.966)\\n50\\n= −.0429'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 576}, page_content='13.5 Control charts for number of defects\\n569\\nSince the proportion of defectives in the ﬁrst subgroup falls outside the upper\\ncontrol limit, we eliminate that subgroup and recompute F as\\nF = 34 −6\\n950\\n= .0295\\nThe new upper and lower control limits are .0295 ± √(.0295)(1 −.0295)/50, or\\nLCL = −.0423,\\nUCL = .1013\\nSince the remaining subgroups all have fraction defectives that fall within the\\ncontrol limits, we can accept that, when in control, the fraction of defective\\nitems in a subgroup should be below .1013.\\n■\\nRemark\\nNote that we are attempting to detect any change in quality even when this\\nchange results in improved quality. That is, we regard the process as being “out\\nof control” even when the probability of a defective item decreases. The reason\\nfor this is that it is important to notice any change in quality, for either better\\nor worse, to be able to evaluate the reason for the change. In other words,\\nif an improvement in product quality occurs, then it is important to analyze\\nthe production process to determine the reason for the improvement. (That is,\\nwhat are we doing right?)\\n13.5\\nControl charts for number of defects\\nIn this section, we consider situations in which the data are the numbers of de-\\nfects in units that consist of an item or group of items. For instance, it could be\\nthe number of defective rivets in an airplane wing, or the number of defective\\ncomputer chips that are produced daily by a given company. Because it is often\\nthe case that there are a large number of possible things that can be defective,\\nwith each of these having a small probability of actually being defective, it is\\nprobably reasonable to assume that the resulting number of defects has a Pois-\\nson distribution.1 So let us suppose that, when the process is in control, the\\nnumber of defects per unit has a Poisson distribution with mean λ.\\nIf we let Xi denote the number of defects in the ith unit, then, since the vari-\\nance of a Poisson random variable is equal to its mean, when the process is in\\ncontrol\\nE[Xi] = λ,\\nVar(Xi) = λ\\n1See Section 5.2 for a theoretical explanation.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 577}, page_content='570 CHAPTER 13: Quality control\\nHence, when in control each Xi should with high probability be within λ ±\\n3\\n√\\nλ, and so the upper and lower control limits are given by\\nUCL = λ + 3\\n√\\nλ,\\nLCL = λ −3\\n√\\nλ\\nAs before, when the control chart is started and λ is unknown, a sample of k\\nunits should be used to estimate λ by\\nX = (X1 + ··· + Xk)/k\\nThis results in trial control limits\\nX + 3\\n\\x02\\nX\\nand\\nX −3\\n\\x02\\nX\\nIf all the Xi,i = 1,...,k fall within these limits, then we suppose that the\\nprocess is in control with λ = X. If some fall outside, then these points are\\neliminated and we recompute X, and so on.\\nIn situations where the mean number of defects per item (or per day) is small,\\none should combine items (days) and use as data the number of defects in a\\ngiven number — say, n — of items (or days). Since the sum of independent\\nPoisson random variables remains a Poisson random variable, the data values\\nwill be Poisson distributed with a larger mean value λ. Such combining of\\nitems is useful when the mean number of defects per item is less than 25.\\nTo obtain a feel for the advantage in combining items, suppose that the mean\\nnumber of defects per item is 4 when the process is under control, and suppose\\nthat something occurs that results in this value changing from 4 to 6, that is,\\nan increase of 1 standard deviation occurs. Let us see how many items will be\\nproduced, on average, until the process is declared out of control when the\\nsuccessive data consist of the number of defects in n items.\\nSince the number of defects in a sample of n items is, when under control,\\nPoisson distributed with mean and variance equal to 4n, the control limits are\\n4n ± 3\\n√\\n4n or 4n ± 6√n. Now if the mean number of defects per item changes\\nto 6, then a data value will be Poisson with mean 6n and so the probability\\nthat it will fall outside the control limits — call it p(n) — is given by\\np(n) = P{Y > 4n + 6√n} + P{Y < 4n −6√n}\\nwhen Y is Poisson with mean 6n. Now\\np(n) ≈P{Y > 4n + 6√n}'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 578}, page_content='13.5 Control charts for number of defects\\n571\\n= P\\n\\x03Y −6n\\n√\\n6n\\n> 6√n −2n\\n√\\n6n\\n\\x04\\n≈P\\n\\x03\\nZ > 6√n −2n\\n√\\n6n\\n\\x04\\nwhere Z ∼N(0,1)\\n= 1 −\\x03\\n\\x05√\\n6 −2\\n\\rn\\n6\\n\\x06\\nBecause each data value will be outside the control limits with probability p(n),\\nit follows that the number of data values needed to obtain one outside the\\nlimits is a geometric random variable with parameter p(n), and thus has mean\\n1/p(n). Finally, since there are n items for each data value, it follows that the\\nnumber of items produced before the process is seen to be out of control has\\nmean value n/p(n):\\nAverage number of items produced while out of control\\n= n/(1 −\\x03(\\n√\\n6 −\\n\\x0b\\n2n\\n3 ))\\nWe plot this for various n in Table 13.2. Since larger values of n are better when\\nthe process is in control (because the average number of items produced before\\nthe process is incorrectly said to be out of control is approximately n/.0027),\\nit is clear from Table 13.2 that one should combine at least 9 of the items.\\nThis would mean that each data value (equal to the number of defects in the\\ncombined set) would have mean at least 9 × 4 = 36.\\nTable 13.2\\nn\\nAverage Number of Items\\n1\\n19.6\\n2\\n20.66\\n3\\n19.80\\n4\\n19.32\\n5\\n18.80\\n6\\n18.18\\n7\\n18.13\\n8\\n18.02\\n9\\n18\\n10\\n18.18\\n11\\n18.33\\n12\\n18.51\\nExample 13.5.a. The following data represent the number of defects discov-\\nered at a factory on successive units of 10 cars each.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 579}, page_content='572 CHAPTER 13: Quality control\\nCars\\nDefects\\nCars\\nDefects\\nCars\\nDefects\\nCars\\nDefects\\n1\\n141\\n6\\n74\\n11\\n63\\n16\\n68\\n2\\n162\\n7\\n85\\n12\\n74\\n17\\n95\\n3\\n150\\n8\\n95\\n13\\n103\\n18\\n81\\n4\\n111\\n9\\n76\\n14\\n81\\n19\\n102\\n5\\n92\\n10\\n68\\n15\\n94\\n20\\n73\\nDoes it appear that the production process was in control throughout?\\nSolution. Since X = 94.4, it follows that the trial control limits are\\nLCL = 94.4 −3\\n√\\n94.4 = 65.25\\nUCL = 94.4 + 3\\n√\\n94.4 = 123.55\\nSince the ﬁrst three data values are larger than UCL, they are removed and the\\nsample mean recomputed. This yields\\nX = (94.4)20 −(141 + 162 + 150)\\n17\\n= 84.41\\nand so the new trial control limits are\\nLCL = 84.41 −3\\n√\\n84.41 = 56.85\\nUCL = 84.41 + 3\\n√\\n84.41 = 111.97\\nAt this point since all remaining 17 data values fall within the limits, we could\\ndeclare that the process is now in control with a mean value of 84.41. However,\\nbecause it seems that the mean number of defects was initially high before set-\\ntling into control, it seems quite plausible that the data value X4 also originated\\nbefore the process was in control. Thus, it would seem prudent in this situation\\nto also eliminate X4 and recompute. Based on the remaining 16 data values,\\nwe obtain that\\nX = 82.56\\nLCL = 82.56 −3\\n√\\n82.56 = 55.30\\nUCL = 82.56 + 3\\n√\\n82.56 = 109.82\\nand so it appears that the process is now in control with a mean value of 82.56.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 580}, page_content='13.6 Other control charts for detecting changes in the population mean\\n573\\n13.6\\nOther control charts for detecting changes in the\\npopulation mean\\nThe major weakness of the X control chart presented in Section 13.2 is that\\nit is relatively insensitive to small changes in the population mean. That is,\\nwhen such a change occurs, since each plotted value is based on only a single\\nsubgroup and so tends to have a relatively large variance, it takes, on average,\\na large number of plotted values to detect the change. One way to remedy this\\nweakness is to allow each plotted value to depend not only on the most recent\\nsubgroup average but on some of the other subgroup averages as well. Three\\napproaches for doing this that have been found to be quite effective are based\\non (1) moving averages, (2) exponentially weighted moving averages, and (3)\\ncumulative sum control charts.\\n13.6.1\\nMoving-average control charts\\nThe moving-average control chart of span size k is obtained by continually\\nplotting the average of the k most recent subgroups. That is, the moving average\\nat time t, call it Mt, is deﬁned by\\nMt = Xt + Xt−1 + ··· + Xt−k+1\\nk\\nwhere Xi is the average of the values of subgroup i. The successive computa-\\ntions can be easily performed by noting that\\nkMt = Xt + Xt−1 + ··· + Xt−k+1\\nand, substituting t + 1 for t,\\nkMt+1 = Xt+1 + Xt + ··· + Xt−k+2\\nSubtraction now yields that\\nkMt+1 −kMt = Xt+1 −Xt−k+1\\nor\\nMt+1 = Mt + Xt+1 −Xt−k+1\\nk\\nIn words, the moving average at time t + 1 is equal to the moving average at\\ntime t plus 1/k times the difference between the newly added and the deleted\\nvalue in the moving average. For values of t less than k, Mt is deﬁned as the\\naverage of the ﬁrst t subgroups. That is,\\nMt = X1 + ··· + Xt\\nt\\nif t < k'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 581}, page_content='574 CHAPTER 13: Quality control\\nSuppose now that when the process is in control the successive values come\\nfrom a normal population with mean μ and variance σ 2. Therefore, if n is the\\nsubgroup size, it follows that Xi is normal with mean μ and variance σ 2/n.\\nFrom this we see that the average of m of the Xi will be normal with mean μ\\nand variance given by Var(Xi)/m = σ 2/nm and, therefore, when the process is\\nin control\\nE[Mt] = μ\\nVar(Mt) =\\n\\x0f\\nσ 2/nt\\nif t < k\\nσ 2/nk\\notherwise\\nBecause a normal random variable is almost always within 3 standard devi-\\nations of its mean, we have the following upper and lower control limits for\\nMt:\\nUCL =\\n\\x0f\\nμ + 3σ/√nt\\nif t < k\\nμ + 3σ/\\n√\\nnk\\notherwise\\nLCL =\\n\\x0f\\nμ −3σ/√nt\\nif t < k\\nμ −3σ/\\n√\\nnk\\notherwise\\nIn other words, aside from the ﬁrst k −1 moving averages, the process will be\\ndeclared out of control whenever a moving average differs from μ by more than\\n3σ/\\n√\\nnk.\\nExample 13.6.a. When a certain manufacturing process is in control, it pro-\\nduces items whose values are normally distributed with mean 10 and standard\\ndeviation 2. The following simulated data represent the values of 25 subgroup\\naverages of size 5 from a normal population with mean 11 and standard devia-\\ntion 2. That is, these data represent the subgroup averages after the process has\\ngone out of control with its mean value increasing from 10 to 11. Table 13.3\\npresents these 25 values along with the moving averages based on span size\\nk = 8 as well as the upper and lower control limits. The lower and upper con-\\ntrol limits for t > 8 are 9.051318 and 10.94868.\\nAs the reader can see, the ﬁrst moving average to fall outside its control limits\\noccurred at time 11, with other such occurrences at times 12, 13, 14, 16, and\\n25. (It is interesting to note that the usual control chart — that is, the moving\\naverage with k = 1 — would have declared the process out of control at time 7\\nsince X7 was so large. However, this is the only point where this chart would\\nhave indicated a lack of control (see Figure 13.3).\\nThere is an inverse relationship between the size of the change in the mean\\nvalue that one wants to guard against and the appropriate moving-average span\\nsize k. That is, the smaller this change is, the larger k ought to be.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 582}, page_content='13.6 Other control charts for detecting changes in the population mean 575\\nTable 13.3\\nt\\nXt\\nMt\\nLCL\\nUCL\\n1\\n9.617728\\n9.617728\\n7.316719\\n12.68328\\n2\\n10.25437\\n9.936049\\n8.102634\\n11.89737\\n3\\n9.876195\\n9.913098\\n8.450807\\n11.54919\\n4\\n10.79338\\n10.13317\\n8.658359\\n11.34164\\n5\\n10.60699\\n10.22793\\n8.8\\n11.2\\n6\\n10.48396\\n10.2706\\n8.904554\\n11.09545\\n7\\n13.33961\\n10.70903\\n8.95815\\n11.01419\\n8\\n9.462969\\n10.55328\\n9.051318\\n10.94868\\n...\\n...\\n9\\n10.14556\\n10.61926\\n10\\n11.66342\\n10.79539\\n∗11\\n11.55484\\n11.00634\\n∗12\\n11.26203\\n11.06492\\n∗13\\n12.31473\\n11.27839\\n∗14\\n9.220009\\n11.1204\\n15\\n11.25206\\n10.85945\\n16\\n10.48662\\n10.98741\\n17\\n9.025091\\n10.84735\\n18\\n9.693386\\n10.6011\\n19\\n11.45989\\n10.58923\\n20\\n12.44213\\n10.73674\\n21\\n11.18981\\n10.59613\\n22\\n11.56674\\n10.88947\\n23\\n9.869849\\n10.71669\\n24\\n12.11311\\n10.92\\n∗25\\n11.48656\\n11.22768\\n∗= Out of control.\\nFIGURE 13.3'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 583}, page_content='576 CHAPTER 13: Quality control\\n13.6.2\\nExponentially weighted moving-average control\\ncharts\\nThe moving-average control chart of Section 13.6.1 considered at each time t\\na weighted average of all subgroup averages up to that time, with the k most\\nrecent values being given weight 1/k and the others given weight 0. Since this\\nappears to be a most effective procedure for detecting small changes in the\\npopulation mean, it raises the possibility that other sets of weights might also\\nbe successfully employed. One set of weights that is often utilized is obtained\\nby decreasing the weight of each earlier subgroup average by a constant fac-\\ntor.\\nLet\\nWt = αXt + (1 −α)Wt−1\\n(13.6.1)\\nwhere α is a constant between 0 and 1, and where\\nW0 = μ\\nThe sequence of values Wt,t = 0,1,2,... is called an exponentially weighted mov-\\ning average. To understand why it has been given that name, note that if we\\ncontinually substitute for the W term on the right side of Equation (13.6.1), we\\nobtain that\\nWt = αXt + (1 −α)[αXt−1 + (1 −α)Wt−2]\\n(13.6.2)\\n= αXt + α(1 −α)Xt−1 + (1 −α)2Wt−2\\n= αXt + α(1 −α)Xt−1 + (1 −α)2[αXt−2 + (1 −α)Wt−3]\\n= αXt + α(1 −α)Xt−1 + α(1 −α)2Xt−2 + (1 −α)3Wt−3\\n...\\n= αXt + α(1 −α)Xt−1 + α(1 −α)2Xt−2 + ···\\n+ α(1 −α)t−1X1 + (1 −α)tμ\\nwhere the foregoing used the fact that W0 = μ. Thus we see from Equation\\n(13.6.2) that Wt is a weighted average of all the subgroup averages up to time t,\\ngiving weight α to the most recent subgroup and then successively decreasing\\nthe weight of earlier subgroup averages by the constant factor 1 −α, and then\\ngiving weight (1 −α)t to the in-control population mean.\\nThe smaller the value of α, the more even the successive weights. For instance,\\nif α = .1 then the initial weight is .1 and the successive weights decrease by the\\nfactor .9; that is, the weights are .1, .09, .081, .073, .066, .059, and so on. On'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 584}, page_content='13.6 Other control charts for detecting changes in the population mean\\n577\\nFIGURE 13.4\\nPlot of α(1 −α)i−1 when α = .4.\\nthe other hand, if one chooses, say, α = .4, then the successive weights are .4,\\n.24, .144, .087, .052, . . . . Since the successive weights α(1 −α)i−1,i = 1,2,...,\\ncan be written as\\nα(1 −α)i−1 = αe−βi\\nwhere\\nα =\\nα\\n1 −α ,\\nβ = −log(1 −α)\\nwe say that the successively older data values are “exponentially weighted” (see\\nFigure 13.4).\\nTo compute the mean and variance of the Wt, recall that, when in control, the\\nsubgroup averages Xi are independent normal random variables each having\\nmean μ and variance σ 2/n. Therefore, using Equation (13.6.2), we see that\\nE[Wt] = μ[α + α(1 −α) + α(1 −α)2 + ··· + α(1 −α)t−1 + (1 −α)t]\\n= μα[1 −(1 −α)t]\\n1 −(1 −α)\\n+ μ(1 −α)t\\n= μ\\nTo determine the variance, we again use Equation (13.6.2):\\nVar(Wt) = σ 2\\nn\\n\\x10\\nα2 + [α(1 −α)]2 + [α(1 −α)2]2 + ··· + [α(1 −α)t−1]2\\x11'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 585}, page_content='578 CHAPTER 13: Quality control\\n= σ 2\\nn α2[1 + β + β2 + ··· + βt−1]\\nwhere β = (1 −α)2\\n= σ 2α2[1 −(1 −α)2t]\\nn[1 −(1 −α)2]\\n= σ 2α[1 −(1 −α)2t]\\nn(2 −α)\\nHence, when t is large we see that, provided that the process has remained in\\ncontrol throughout,\\nE[Wt] = μ\\nVar(Wt) ≈\\nσ 2α\\nn(2 −α)\\nsince (1 −α)2t ≈0\\nThus, the upper and lower control limits for Wt are given by\\nUCL = μ + 3σ\\n\\r\\nα\\nn(2 −α)\\nLCL = μ −3σ\\n\\r\\nα\\nn(2 −α)\\nNote that the preceding control limits are the same as those in a moving-\\naverage control chart with span k (after the initial k values) when\\n3σ\\n√\\nnk\\n= 3σ\\n\\r\\nα\\nn(2 −α)\\nor, equivalently, when\\nk = 2 −α\\nα\\nor\\nα =\\n2\\nk + 1\\nExample 13.6.b. A repair shop will send a worker to a caller’s home to repair\\nelectronic equipment. Upon receiving a request, it dispatches a worker who is\\ninstructed to call in when the job is completed. Historical data indicate that\\nthe time from when the server is dispatched until he or she calls is a normal\\nrandom variable with mean 62 minutes and standard deviation 24 minutes. To\\nkeep aware of any changes in this distribution, the repair shop plots a standard\\nexponentially weighted moving-average (EWMA) control chart with each data\\nvalue being the average of 4 successive times, and with a weighting factor of\\nα = .25. If the present value of the chart is 60 and the following are the next 16\\nsubgroup averages, what can we conclude?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 586}, page_content='13.6 Other control charts for detecting changes in the population mean\\n579\\n48, 52, 70, 62, 57, 81, 56, 59, 77, 82, 78, 80, 74, 82, 68, 84\\nSolution. Starting with W0 = 60, the successive values of W1,...,W16 can be\\nobtained from the formula\\nWt = .25Xt + .75Wt−1\\nThis gives\\nW1 = (.25)(48) + (.75)(60) = 57\\nW2 = (.25)(52) + (.75)(57) = 55.75\\nW3 = (.25)(70) + (.75)(55.75) = 59.31\\nW4 = (.25)(62) + (.75)(59.31) = 59.98\\nW5 = (.25)(57) + (.75)(59.98) = 59.24\\nW6 = (.25)(81) + (.75)(59.24) = 64.68\\nand so on, with the following being the values of W7 through W16:\\n62.50, 61.61, 65.48, 69.60, 71.70, 73.78, 73.83, 75.87, 73.90, 76.43\\nSince\\n3\\n\\r\\n.25\\n1.75\\n24\\n√\\n4\\n= 13.61\\nthe control limits of the standard EWMA control chart with weighting factor\\nα = .25 are\\nLCL = 62 −13.61 = 48.39\\nUCL = 62 + 13.61 = 75.61\\nThus, the EWMA control chart would have declared the system out of con-\\ntrol after determining W14 (and also after W16). On the other hand, since a\\nsubgroup standard deviation is σ/√n = 12, it is interesting that no data value\\ndiffered from μ = 62 by even as much as 2 subgroup standard deviations, and\\nso the standard X control chart would not have declared the system out of\\ncontrol.\\n■\\nExample 13.6.c. Consider the data of Example 13.6.a but now use an expo-\\nnentially weighted moving-average control chart with α = 2/9. This gives rise\\nto the following data set.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 587}, page_content='580 CHAPTER 13: Quality control\\nt\\nXt\\nWt\\nt\\nXt\\nWt\\n1\\n9.617728\\n9.915051\\n14\\n9.220009\\n10.84522\\n2\\n10.25437\\n9.990456\\n15\\n11.25206\\n10.93563\\n3\\n9.867195\\n9.963064\\n16\\n10.48662\\n10.83585\\n4\\n10.79338\\n10.14758\\n17\\n9.025091\\n10.43346\\n5\\n10.60699\\n10.24967\\n18\\n9.693386\\n10.269\\n6\\n10.48396\\n10.30174\\n19\\n11.45989\\n10.53364\\n∗7\\n13.33961\\n10.97682\\n∗20\\n12.44213\\n10.95775\\n8\\n9.462969\\n10.64041\\n∗21\\n11.18981\\n11.00932\\n9\\n10.14556\\n10.53044\\n∗22\\n11.56674\\n11.13319\\n10\\n11.66342\\n10.78221\\n23\\n9.869849\\n10.85245\\n∗11\\n11.55484\\n10.95391\\n∗24\\n12.11311\\n11.13259\\n∗12\\n11.26203\\n11.02238\\n∗25\\n11.48656\\n11.21125\\n∗13\\n12.31473\\n11.30957\\n∗= Out of control.\\nFIGURE 13.5\\nSince\\nUCL = 10.94868\\nLCL = 9.051318\\nwe see that the process could be declared out of control as early as t=7 (see\\nFigure 13.5).\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 588}, page_content='13.6 Other control charts for detecting changes in the population mean 581\\n13.6.3\\nCumulative sum control charts\\nThe major competitor to the moving-average type of control chart for detecting\\na small- to moderate-sized change in the mean is the cumulative sum (often\\nreduced to cu-sum) control chart.\\nSuppose, as before, that X1,X2,... represent successive averages of subgroups\\nof size n and that when the process is in control these random variables have\\nmean μ and standard deviation σ/√n. Initially, suppose that we are only inter-\\nested in determining when an increase in the mean value occurs. The (one-\\nsided) cumulative sum control chart for detecting an increase in the mean\\noperates as follows: Choose positive constants d and B, and let\\nYj = Xj −μ −dσ/√n,\\nj ≥1\\nNote that when the process is in control, and so E[Xj] = μ,\\nE[Yj] = −dσ/√n < 0\\nNow, let\\nS0 = 0\\nSj+1 = max{Sj + Yj+1,0},\\nj ≥0\\nThe cumulative sum control chart having parameters d and B continually plots\\nSj, and declares that the mean value has increased at the ﬁrst j such that\\nSj > Bσ/√n\\nTo understand the rationale behind this control chart, suppose that we had\\ndecided to continually plot the sum of all the random variables Yi that have\\nbeen observed so far. That is, suppose we had decided to plot the successive\\nvalues of Pj, where\\nPj =\\nj\\n\\ni=1\\nYi\\nwhich can also be written as\\nP0 = 0\\nPj+1 = Pj + Yj+1,\\nj ≥0\\nNow, when the system has always been in control, all of the Yi have a negative\\nexpected value, and thus we would expect their sum to be negative. Hence, if the\\nvalue of Pj ever became large — say, greater than Bσ/√n — then this would be\\nstrong evidence that the process has gone out of control (by having an increase'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 589}, page_content='582 CHAPTER 13: Quality control\\nin the mean value of a produced item). The difﬁculty, however, is that if the\\nsystem goes out of control only after some large time, then the value of Pj at\\nthat time will most likely be strongly negative (since up to then we would have\\nbeen summing random variables having a negative mean), and thus it would\\ntake a long time for its value to exceed Bσ/√n. Therefore, to keep the sum\\nfrom becoming very negative while the process is in control, the cumulative\\nsum control chart employs the simple trick of resetting its value to 0 whenever\\nit becomes negative. That is, the quantity Sj is the cumulative sum of all of the\\nYi up to time j, with the exception that any time this sum becomes negative its\\nvalue is reset to 0.\\nExample 13.6.d. Suppose that the mean and standard deviation of a subgroup\\naverage are μ = 30 and σ/√n = 8, respectively, and consider the cumulative\\nsum control chart with d = .5, B = 5. If the ﬁrst eight subgroup averages are\\n29, 33, 35, 42, 36, 44, 43, 45\\nthen the successive values of Yj = Xj −30 −4 = Xj −34 are\\nY1 = −5,Y2 = −1,Y3 = 1,Y4 = 8,Y5 = 2,Y6 = 10,Y7 = 9,Y8 = 11\\nTherefore,\\nS1 = max{−5,0} = 0\\nS2 = max{−1,0} = 0\\nS3 = max{1,0} = 1\\nS4 = max{9,0} = 9\\nS5 = max{11,0} = 11\\nS6 = max{21,0} = 21\\nS7 = max{30,0} = 30\\nS8 = max{41,0} = 41\\nSince the control limit is\\nBσ/√n = 5(8) = 40\\nthe cumulative sum chart would declare that the mean has increased after ob-\\nserving the eighth subgroup average.\\n■\\nTo detect either a positive or a negative change in the mean, we employ two\\none-sided cumulative sum charts simultaneously. We begin by noting that a\\ndecrease in E[Xi] is equivalent to an increase in E[−Xi]. Hence, we can detect\\na decrease in the mean value of an item by running a one-sided cumulative sum\\nchart on the negatives of the subgroup averages. That is, for speciﬁed values d'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 590}, page_content='Problems 583\\nand B, not only do we plot the quantities Sj as before, but, in addition, we\\nlet\\nWj = −Xj −(−μ) −dσ/√n = μ −Xj −dσ/√n\\nand then also plot the values Tj, where\\nT0 = 0\\nTj+1 = max{Tj + Wj+1,0},\\nj ≥0\\nThe ﬁrst time that either Sj or Tj exceeds Bσ/√n, the process is said to be out\\nof control.\\nSumming up, the following steps result in a cumulative sum control chart for\\ndetecting a change in the mean value of a produced item: Choose positive con-\\nstants d and B; use the successive subgroup averages to determine the values\\nof Sj and Tj; declare the process out of control the ﬁrst time that either ex-\\nceeds Bσ/√n. Three common choices of the pair of values d and B are d = .25,\\nB = 8.00, or d = .50, B = 4.77, or d = 1, B = 2.49. Any of these choices results\\nin a control rule that has approximately the same false alarm rate as does the\\nX control chart that declares the process out of control the ﬁrst time a sub-\\ngroup average differs from μ by more than 3σ/√n. As a general rule of thumb,\\nthe smaller the change in mean that one wants to guard against, the smaller\\nshould be the chosen value of d.\\nProblems\\n1. Assume that items produced are supposed to be normally distributed\\nwith mean 35 and standard deviation 3. To monitor this process, sub-\\ngroups of size 5 are sampled. If the following represents the averages of\\nthe ﬁrst 20 subgroups, does it appear that the process was in control?\\nSubgroup No.\\nX\\nSubgroup No.\\nX\\n1\\n34.0\\n11\\n35.8\\n2\\n31.6\\n12\\n35.8\\n3\\n30.8\\n13\\n34.0\\n4\\n33.0\\n14\\n35.0\\n5\\n35.0\\n15\\n33.8\\n6\\n32.2\\n16\\n31.6\\n7\\n33.0\\n17\\n33.0\\n8\\n32.6\\n18\\n33.2\\n9\\n33.8\\n19\\n31.8\\n10\\n35.8\\n20\\n35.6\\n2. Suppose that a process is in control with μ = 14 and σ = 2. An X con-\\ntrol chart based on subgroups of size 5 is employed. If a shift in the mean'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 591}, page_content='584 CHAPTER 13: Quality control\\nof 2.2 units occurs, what is the probability that the next subgroup average\\nwill fall outside the control limits? On average, how many subgroups will\\nhave to be looked at in order to detect this shift?\\n3. If Y has a chi-square distribution with n −1 degrees of freedom, show\\nthat\\nE[\\n√\\nY] =\\n√\\n2\\n\\x05(n/2)\\n\\x05[(n −1)/2]\\n(Hint: Write\\nE[\\n√\\nY] =\\n\\x0c ∞\\n0\\n√yfχ2\\nn−1(y) dy\\n=\\n\\x0c ∞\\n0\\n√y e−y/2y(n−1)/2−1 dy\\n2(n−1)/2\\x05\\n\\x12(n −1)\\n2\\n\\x13\\n=\\n\\x0c ∞\\n0\\ne−y/2yn/2−1 dy\\n2(n−1)/2\\x05\\n\\x12(n −1)\\n2\\n\\x13\\nNow make the transformation x = y/2.)\\n4. Samples of size 5 are taken at regular intervals from a production process,\\nand the values of the sample averages and sample standard deviations\\nare calculated. Suppose that the sum of the X and S values for the ﬁrst\\n25 samples are given by\\n\\nXi = 357.2,\\n\\nSi = 4.88\\na.\\nAssuming control, determine the control limits for an X control\\nchart.\\nb.\\nSuppose that the measurable values of the items produced are sup-\\nposed to be within the limits 14.3 ± .45. Assuming that the pro-\\ncess remains in control with a mean and variance that is approxi-\\nmately equal to the estimates derived, approximately what percent-\\nage of the items produced will fall within the speciﬁcation lim-\\nits?\\n5. Determine the revised X and S-control limits for the data in Exam-\\nple 13.3.a.\\n6. In Problem 4, determine the control limits for an S-control chart.\\n7. The following are X and S values for 20 subgroups of size 5.\\na.\\nDetermine trial control limits for an X control chart.\\nb.\\nDetermine trial control limits for an S-control chart.\\nc.\\nDoes it appear that the process was in control throughout?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 592}, page_content='Problems 585\\nSubgroup\\nX\\nS\\nSubgroup\\nX\\nS\\nSubgroup\\nX\\nS\\n1\\n33.8\\n5.1\\n8\\n36.1\\n4.1\\n15\\n35.6\\n4.8\\n2\\n37.2\\n5.4\\n9\\n38.2\\n7.3\\n16\\n36.4\\n4.6\\n3\\n40.4\\n6.1\\n10\\n32.4\\n6.6\\n17\\n37.2\\n6.1\\n4\\n39.3\\n5.5\\n11\\n29.7\\n5.1\\n18\\n31.3\\n5.7\\n5\\n41.1\\n5.2\\n12\\n31.6\\n5.3\\n19\\n33.6\\n5.5\\n6\\n40.4\\n4.8\\n13\\n38.4\\n5.8\\n20\\n36.7\\n4.2\\n7\\n35.0\\n5.0\\n14\\n40.2\\n6.4\\nd.\\nIf your answer in part (c) is no, suggest values for upper and lower\\ncontrol limits to be used with succeeding subgroups.\\ne.\\nIf each item is supposed to have a value within 35 ± 10, what is\\nyour estimate of the percentage of items that will fall within this\\nspeciﬁcation?\\n8. Control charts for X and S are maintained on the shear strength of spot\\nwelds. After 30 subgroups of size 4, \\x14Xi = 12,660 and \\x14Si = 500. As-\\nsume that the process is in control.\\na.\\nWhat are the X control limits?\\nb.\\nWhat are the S-control limits?\\nc.\\nEstimate the standard deviation for the process.\\nd.\\nIf the minimum speciﬁcation for this weld is 400 pounds, what\\npercentage of the welds will not meet the minimum speciﬁca-\\ntion?\\n9. Control charts for X and S are maintained on resistors (in ohms). The\\nsubgroup size is 4. The values of X and S are computed for each sub-\\ngroup. After 20 subgroups, \\x14Xi = 8,620 and \\x14Si = 450.\\na.\\nCompute the values of the limits for the X and S charts.\\nb.\\nEstimate the value of σ on the assumption that the process is in\\nstatistical control.\\nc.\\nIf the speciﬁcation limits are 430 ± 30, what conclusions can you\\ndraw regarding the ability of the process to produce items within\\nthese speciﬁcations?\\nd.\\nIf μ is increased by 60, what is the probability of a subgroup average\\nfalling outside the control limits?\\n10. The following data refer to the amounts by which the diameters of 1\\n4 inch\\nball bearings differ from 1\\n4 inch in units of .001 inches. The subgroup size\\nis n = 5.\\na.\\nSet up trial control limits for X and S-control charts.\\nb.\\nDoes the process appear to have been in control throughout the\\nsampling?\\nc.\\nIf the answer to part (b) is no, construct revised control limits.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 593}, page_content='586 CHAPTER 13: Quality control\\nSubgroup\\nData Values\\n1\\n2.5\\n.5\\n2.0\\n−1.2\\n1.4\\n2\\n.2\\n.3\\n.5\\n1.1\\n1.5\\n3\\n1.5\\n1.3\\n1.2\\n−1.0\\n.7\\n4\\n.2\\n.5\\n−2.0\\n.0\\n−1.3\\n5\\n−.2\\n.1\\n.3\\n−.6\\n.5\\n6\\n1.1\\n−.5\\n.6\\n.5\\n.2\\n7\\n1.1\\n−1.0\\n−1.2\\n1.3\\n.1\\n8\\n.2\\n−1.5\\n−.5\\n1.5\\n.3\\n9\\n−2.0\\n−1.5\\n1.6\\n1.4\\n.1\\n10\\n−.5\\n3.2\\n−.1\\n−1.0\\n−1.5\\n11\\n.1\\n1.5\\n−.2\\n.3\\n2.1\\n12\\n.0\\n−2.0\\n−.5\\n.6\\n−.5\\n13\\n−1.0\\n−.5\\n−.5\\n−1.0\\n.2\\n14\\n.5\\n1.3\\n−1.2\\n−.5\\n−2.7\\n15\\n1.1\\n.8\\n1.5\\n−1.5\\n1.2\\n11. Samples of n = 6 items are taken from a manufacturing process at regular\\nintervals. A normally distributed quality characteristic is measured, and\\nX and S values are calculated for each sample. After 50 subgroups have\\nbeen analyzed, we have\\n50\\n\\ni=1\\nXi = 970\\nand\\n50\\n\\ni=1\\nSi = 85\\na.\\nCompute the control limit for the X and S-control charts. Assume\\nthat all points on both charts plot within the control limits.\\nb.\\nIf the speciﬁcation limits are 19 ± 4.0, what are your conclusions\\nregarding the ability of the process to produce items conforming to\\nspeciﬁcations?\\n12. The following data present the number of defective bearing and seal as-\\nsemblies in samples of size 100.\\nSample\\nNumber\\nNumber of\\nDefectives\\nSample\\nNumber\\nNumber of\\nDefectives\\n1\\n5\\n11\\n4\\n2\\n2\\n12\\n10\\n3\\n1\\n13\\n0\\n4\\n5\\n14\\n8\\n5\\n9\\n15\\n3\\n6\\n4\\n16\\n6\\n7\\n3\\n17\\n2\\n8\\n3\\n18\\n1\\n9\\n2\\n19\\n6\\n10\\n5\\n20\\n10'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 594}, page_content='Problems 587\\nDoes it appear that the process was in control throughout? If not, deter-\\nmine revised control limits if possible.\\n13. The following data represent the results of inspecting all personal com-\\nputers produced at a given plant during the past 12 days.\\nDay\\nNumber of Units\\nNumber Defective\\n1\\n80\\n5\\n2\\n110\\n7\\n3\\n90\\n4\\n4\\n80\\n9\\n5\\n100\\n12\\n6\\n90\\n10\\n7\\n80\\n4\\n8\\n70\\n3\\n9\\n80\\n5\\n10\\n90\\n6\\n11\\n90\\n5\\n12\\n110\\n7\\nDoes the process appear to have been in control? Determine control lim-\\nits for future production.\\n14. Suppose that when a process is in control each item will be defective\\nwith probability .04. Suppose that your control chart calls for taking daily\\nsamples of size 500. What is the probability that, if the probability of a\\ndefective item should suddenly shift to .08, your control chart would\\ndetect this shift on the next sample?\\n15. The following data represent the number of defective chips produced on\\nthe last 15 days: 121, 133, 98, 85, 101, 78, 66, 82, 90, 78, 85, 81, 100, 75,\\n89. Would you conclude that the process has been in control throughout\\nthese 15 days? What control limits would you advise using for future\\nproduction?\\n16. Surface defects have been counted on 25 rectangular steel plates, and the\\ndata are shown below. Set up a control chart. Does the process producing\\nthe plates appear to be in statistical control?\\nPlate Number\\nNumber of\\nDefects\\nPlate Number\\nNumber of\\nDefects\\n1\\n2\\n14\\n10\\n2\\n3\\n15\\n2\\n3\\n4\\n16\\n2\\n4\\n3\\n17\\n6\\ncontinued on next page'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 595}, page_content='588 CHAPTER 13: Quality control\\nTable 13.3 (continued)\\nPlate Number\\nNumber of\\nDefects\\nPlate Number\\nNumber of\\nDefects\\n5\\n1\\n18\\n5\\n6\\n2\\n19\\n4\\n7\\n5\\n20\\n6\\n8\\n0\\n21\\n3\\n9\\n2\\n22\\n7\\n10\\n5\\n23\\n0\\n11\\n1\\n24\\n2\\n12\\n7\\n25\\n4\\n13\\n8\\n17. The following data represent 25 successive subgroup averages and mov-\\ning averages of span size 5 of these subgroup averages. The data are\\ngenerated by a process that, when in control, produces normally dis-\\ntributed items having mean 30 and variance 40. The subgroups are of\\nsize 4. Would you judge that the process has been in control through-\\nout?\\nXt\\nMt\\nXt\\nMt\\n35.62938\\n35.62938\\n35.80945\\n32.34106\\n39.13018\\n37.37978\\n30.9136\\n33.1748\\n29.45974\\n34.73976\\n30.54829\\n32.47771\\n32.5872\\n34.20162\\n36.39414\\n33.17019\\n30.06041\\n33.37338\\n27.62703\\n32.2585\\n26.54353\\n31.55621\\n34.02624\\n31.90186\\n37.75199\\n31.28057\\n27.81629\\n31.2824\\n26.88128\\n30.76488\\n26.99926\\n30.57259\\n32.4807\\n30.74358\\n32.44703\\n29.78317\\n26.7449\\n30.08048\\n38.53433\\n31.96463\\n34.03377\\n31.57853\\n28.53698\\n30.86678\\n32.93174\\n30.61448\\n28.65725\\n31.03497\\n32.18547\\n31.67531\\n18. The data shown below give subgroup averages and moving averages of\\nthe values from Problem 17. The span of the moving averages is k = 8.\\nWhen in control the subgroup averages are normally distributed with\\nmean 50 and variance 5. What can you conclude?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 596}, page_content='Problems 589\\nXt\\nMt\\n50.79806\\n50.79806\\n46.21413\\n48.50609\\n51.85793\\n49.62337\\n50.27771\\n49.78696\\n53.81512\\n50.59259\\n50.67635\\n50.60655\\n51.39083\\n50.71859\\n51.65246\\n50.83533\\n52.15607\\n51.00508\\n54.57523\\n52.05022\\n53.08497\\n52.2036\\n55.02968\\n52.79759\\n54.25338\\n52.85237\\n50.48405\\n52.82834\\n50.34928\\n52.69814\\n50.86896\\n52.6002\\n52.03695\\n52.58531\\n53.23255\\n52.41748\\n48.12588\\n51.79759\\n52.23154\\n51.44783\\n19. Redo Problem 17 by employing an exponential weighted moving\\naverage-control chart with α = 1\\n3.\\n20. Analyze the data of Problem 18 with an exponential weighted moving-\\naverage control chart having α = 2\\n9.\\n21. Explain why a moving-average control chart with span size k must use\\ndifferent control limits for the ﬁrst k −1 moving averages, whereas an ex-\\nponentially weighted moving-average control chart can use the same con-\\ntrol limits throughout. [Hint: Argue that Var(Mt) decreases in t, whereas\\nVar(Wt) increases, and explain why this is relevant.]\\n22. Repeat Problem 17, this time using a cumulative sum control chart\\nwith\\na.\\nd = .25, B = 8;\\nb.\\nd = .5, B = 4.77.\\n23. Repeat Problem 18, this time using a cumulative sum control chart with\\nd = 1 and B = 2.49.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 597}, page_content='CHAPTER 14\\nLife testing∗\\n14.1\\nIntroduction\\nIn this chapter, we consider a population of items having lifetimes that are\\nassumed to be independent random variables with a common distribution that\\nis speciﬁed up to an unknown parameter. The problem of interest will be to use\\nwhatever data are available to estimate this parameter.\\nIn Section 14.2, we introduce the concept of the hazard (or failure) rate\\nfunction — a useful engineering concept that can be utilized to specify lifetime\\ndistributions. In Section 14.3, we suppose that the underlying life distribu-\\ntion is exponential and show how to obtain estimates (point, interval, and\\nBayesian) of its mean under a variety of sampling plans. In Section 14.4, we\\ndevelop a test of the hypothesis that two exponentially distributed populations\\nhave a common mean. In Section 14.5, we consider two approaches to estimat-\\ning the parameters of a Weibull distribution.\\n14.2\\nHazard rate functions\\nConsider a positive continuous random variable X, that we interpret as being\\nthe lifetime of some item, having distribution function F and density f. The\\nhazard rate (sometimes called the failure rate) function λ(t) of F is deﬁned by\\nλ(t) =\\nf (t)\\n1 −F(t)\\nTo interpret λ(t), suppose that the item has survived for t hours and we desire\\nthe probability that it will not survive for an additional time dt. That is, consider\\nP{X ∈(t,t + dt) | X > t}. Now\\nP{X ∈(t,t + dt)|X > t} = P{X ∈(t,t + dt),X > t}\\nP{X > t}\\n= P{X ∈(t,t + dt)}\\nP{X > t}\\n∗Optional chapter.\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00023-5\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n591'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 598}, page_content='592 CHAPTER 14: Life testing\\n≈\\nf (t)\\n1 −F(t)dt\\nThat is, λ(t) represents the conditional probability intensity that an item of age\\nt will fail in the next moment.\\nSuppose now that the lifetime distribution is exponential. Then, by the mem-\\noryless property of the exponential distribution it follows that the distribution\\nof remaining life for a t-year-old item is the same as for a new item. Hence λ(t)\\nshould be constant, which is veriﬁed as follows:\\nλ(t) =\\nf (t)\\n1 −F(t)\\n= λe−λt\\ne−λt\\n= λ\\nThus, the failure rate function for the exponential distribution is constant. The\\nparameter λ is often referred to as the rate of the distribution.\\nWe now show that the failure rate function λ(t),t ≥0, uniquely determines the\\ndistribution F. To show this, note that by deﬁnition\\nλ(s) =\\nf (s)\\n1 −F(s)\\n=\\nd\\nds F(s)\\n1 −F(s)\\n= d\\nds {−log[1 −F(s)]}\\nIntegrating both sides of this equation from 0 to t yields\\n\\x02 t\\n0\\nλ(s)ds = −log[1 −F(t)] + log[1 −F(0)]\\n= −log[1 −F(t)]\\nsince F(0) = 0\\nwhich implies that\\n1 −F(t) = exp\\n\\x03\\n−\\n\\x02 t\\n0\\nλ(s)ds\\n\\x04\\n(14.2.1)\\nHence a distribution function of a positive continuous random variable can be\\nspeciﬁed by giving its hazard rate function. For instance, if a random variable\\nhas a linear hazard rate function — that is, if\\nλ(t) = a + bt'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 599}, page_content='14.2 Hazard rate functions 593\\nthen its distribution function is given by\\nF(t) = 1 −e−at−bt2/2\\nand differentiation yields that its density is\\nf (t) = (a + bt)e−(at+bt2/2),\\nt ≥0\\nWhen a = 0, the foregoing is known as the Rayleigh density function.\\nExample 14.2.a. One often hears that the death rate of a person who smokes\\nis, at each age, twice that of a nonsmoker. What does this mean? Does it mean\\nthat a nonsmoker has twice the probability of surviving a given number of\\nyears as does a smoker of the same age?\\nSolution. If λs(t) denotes the hazard rate of a smoker of age t and λn(t) that\\nof a nonsmoker of age t, then the foregoing is equivalent to the statement that\\nλs(t) = 2λn(t)\\nThe probability that an A-year-old nonsmoker will survive until age B,A < B,\\nis\\nP{A-year-old nonsmoker reaches age B}\\n= P{nonsmoker’s lifetime > B|nonsmoker’s lifetime > A}\\n= 1 −Fnon(B)\\n1 −Fnon(A)\\n=\\nexp\\n\\x05\\n−\\n\\x06 B\\n0 λn(t)dt\\n\\x07\\nexp\\n\\x05\\n−\\n\\x06 A\\n0 λn(t)dt\\n\\x07\\nfrom Equation (14.2.1)\\n= exp\\n\\x03\\n−\\n\\x02 B\\nA\\nλn(t)dt\\n\\x04\\nwhereas the corresponding probability for a smoker is, by the same reasoning,\\nP{A-year-old smoker reaches age B} = exp\\n\\x03\\n−\\n\\x02 B\\nA\\nλs(t)dt\\n\\x04\\n= exp\\n\\x03\\n−2\\n\\x02 B\\nA\\nλn(t)dt\\n\\x04\\n=\\n\\x08\\nexp\\n\\x03\\n−\\n\\x02 B\\nA\\nλn(t)dt\\n\\x04\\t2\\nIn other words, of two individuals of the same age, one of whom is a smoker\\nand the other a nonsmoker, the probability that the smoker survives to any'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 600}, page_content='594 CHAPTER 14: Life testing\\ngiven age is the square (not one-half) of the corresponding probability for a\\nnonsmoker. For instance, if λn(t) = 1/20, 50 ≤t ≤60, then the probability that\\na 50-year-old nonsmoker reaches age 60 is e−1/2 = .607, whereas the corre-\\nsponding probability for a smoker is e−1 = .368.\\n■\\nRemark on terminology\\nWe will say that X has failure rate function λ(t) when more precisely we mean\\nthat the distribution function of X has failure rate function λ(t).\\n14.3\\nThe exponential distribution in life testing\\n14.3.1\\nSimultaneous testing — stopping at the rth failure\\nSuppose that we are testing items whose life distribution is exponential with\\nunknown mean θ. We put n independent items simultaneously on test and stop\\nthe experiment when there have been a total of r, r ≤n, failures. The problem\\nis to then use the observed data to estimate the mean θ.\\nThe observed data will be the following:\\nData:\\nx1 ≤x2 ≤··· ≤xr,\\ni1,i2,...,ir\\n(14.3.1)\\nwith the interpretation that the jth item to fail was item ij and it failed at time\\nxj. Thus, if we let Xi,i = 1,...,n denote the lifetime of component i, then the\\ndata will be as given in Equation (14.3.1) if\\nXi1 = x1,Xi2 = x2,...,Xir = xr\\nother n −r of the Xj are all greater than xr.\\nNow the probability density of Xij is\\nfXij (xj) = 1\\nθ e−xj /θ,\\nj = 1,...,r\\nand so, by independence, the joint probability density of Xij ,j = 1,...,r is\\nfXi1,..., Xir (x1,...,xr) =\\nr\\nj=1\\n1\\nθ e−xj /θ\\nAlso, the probability that the other n −r of the X’s are all greater than xr is,\\nagain using independence,\\nP{Xj > xr for j ̸= i1 or i2 ...or ir} = (e−xr/θ)n−r'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 601}, page_content='14.3 The exponential distribution in life testing\\n595\\nHence, we see that the likelihood of the observed data — call it L(x1,...,xr,\\ni1,...,ir) — is, for x1 ≤x2 ≤··· ≤xr,\\nL(x1,...,xr,i1,...,ir)\\n(14.3.2)\\n= fXi1, Xi2,..., Xir (x1,...,xr)P{Xj > xr,j ̸= i1,...,ir}\\n= 1\\nθ e−x1/θ ··· 1\\nθ e−xr/θ(e−xr/θ)n−r\\n= 1\\nθr exp\\n⎧\\n⎪⎪⎨\\n⎪⎪⎩\\n−\\nr\\x0f\\ni=1\\nxi\\nθ\\n−(n −r)xr\\nθ\\n⎫\\n⎪⎪⎬\\n⎪⎪⎭\\nRemark\\nThe likelihood in Equation (14.3.2) not only speciﬁes that the ﬁrst r failures\\noccur at times x1 ≤x2 ≤··· ≤xr but also that the r items to fail were, in order,\\ni1,i2,...,ir. If we only desired the density function of the ﬁrst r failure times,\\nthen since there are n(n −1)···(n −(r −1)) = n!/(n −r)! possible (ordered)\\nchoices of the ﬁrst r items to fail, it follows that the joint density is, for x1 ≤\\nx2 ≤··· ≤xr,\\nf (x1,x2,...,xr) =\\nn!\\n(n −r)!\\n1\\nθr exp\\n⎧\\n⎪⎪⎨\\n⎪⎪⎩\\n−\\nr\\x0f\\ni=1\\nxi\\nθ\\n−(n −r)\\nθ\\nxr\\n⎫\\n⎪⎪⎬\\n⎪⎪⎭\\nTo obtain the maximum likelihood estimator of θ, we take the logarithm of\\nboth sides of Equation (14.3.2). This yields\\nlogL(x1,...,xr,i1,...,ir) = −r logθ −\\nr\\x0f\\ni=1\\nxi\\nθ\\n−(n −r)xr\\nθ\\nand so\\n∂\\n∂θ logL(x1,...,xr,i1,...,ir) = −r\\nθ +\\nr\\x0f\\ni=1\\nxi\\nθ2\\n+ (n −r)xr\\nθ2\\nEquating to 0 and solving yields that ˆθ, the maximum likelihood estimate, is\\ngiven by\\nˆθ =\\nr\\x0f\\ni=1\\nxi + (n −r)xr\\nr'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 602}, page_content='596 CHAPTER 14: Life testing\\nHence, if we let X(i) denote the time at which the ith failure occurs (X(i) is\\ncalled the ith order statistic), then the maximum likelihood estimator of θ is\\nˆθ =\\nr\\x0f\\ni=1\\nX(i) + (n −r)X(r)\\nr\\n(14.3.3)\\n= τ\\nr\\nwhere τ, deﬁned to equal the numerator in Equation (14.3.3), is called the\\ntotal-time-on-test statistic. We call it this since the ith item to fail functions for a\\ntime X(i) (and then fails), i = 1,...,r, whereas the other n −r items function\\nthroughout the test (which lasts for a time X(r)). Hence the sum of the times\\nthat all the items are on test is equal to τ.\\nTo obtain a conﬁdence interval for θ, we will determine the distribution of\\nτ, the total time on test. Recalling that X(i) is the time of the ith failure, i =\\n1,...,r, we will start by rewriting the expression for τ. To write an expression\\nfor τ, rather than summing the total time on test of each of the items, let us\\nask how much additional time on test was generated between each successive\\nfailure. That is, let us denote by Yi,i = 1,...,r, the additional time on test\\ngenerated between the (i −1)st and ith failure. Now up to the ﬁrst X(1) time\\nunits (as all n items are functioning throughout this interval), the total time on\\ntest is\\nY1 = nX(1)\\nBetween the ﬁrst and second failures, there are a total of n −1 functioning\\nitems, and so\\nY2 = (n −1)(X(2) −X(1))\\nIn general, we have\\nY1 = nX(1)\\nY2 = (n −1)(X(2) −X(1))\\n...\\nYj = (n −j + 1)(X( j) −X( j−1))\\n...\\nYr = (n −r + 1)(X(r) −X(r−1))\\nand\\nτ =\\nr\\n\\x13\\nj=1\\nYj'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 603}, page_content='14.3 The exponential distribution in life testing\\n597\\nThe importance of the foregoing representation for τ follows from the fact that\\nthe distributions of the Yj’s are easily obtained as follows. Since X(1), the time\\nof the ﬁrst failure, is the minimum of n independent exponential lifetimes, each\\nhaving rate 1/θ, it follows from Proposition 5.6.1 that it is itself exponentially\\ndistributed with rate n/θ. That is, X(1) is exponential with mean θ/n, and so\\nnX(1) is exponential with mean θ. Also, at the moment when the ﬁrst failure\\noccurs, the remaining n −1 functioning items are, by the memoryless property\\nof the exponential, as good as new and so each will have an additional life\\nthat is exponential with mean θ; hence, the additional time until one of them\\nfails is exponential with rate (n −1)/θ. That is, independent of X(1), X(2) −\\nX(1) is exponential with mean θ/(n −1) and so Y2 = (n −1)(X(2) −X(1)) is\\nexponential with mean θ. Indeed, continuing this argument leads us to the\\nfollowing conclusion:\\nY1,...,Yr are independent exponential\\nrandom variables each having mean θ\\n(14.3.4)\\nHence, since the sum of independent and identically distributed exponential\\nrandom variables has a gamma distribution Corollary 5.7.2, we see that\\nτ ∼gamma(r,1/θ)\\nThat is, τ has a gamma distribution with parameters r and 1/θ. Equivalently, by\\nrecalling that a gamma random variable with parameters (r, 1/θ) is equivalent\\nto θ/2 times a chi-square random variable with 2r degrees of freedom (see\\nSection 5.8.1), we obtain that\\n2τ\\nθ ∼χ2\\n2r\\n(14.3.5)\\nThat is, 2τ/θ has a chi-square distribution with 2r degrees of freedom. Hence,\\nP\\n\\x14\\nχ2\\n1−α/2,2r < 2τ/θ < χ2\\nα/2,2r\\n\\x15\\n= 1 −α\\nand so a 100(1 −α) percent conﬁdence interval for θ is\\nθ ∈\\n\\x16\\n2τ\\nχ2\\nα/2,2r\\n,\\n2τ\\nχ2\\n1−α/2,2r\\n\\x17\\n(14.3.6)\\nOne-sided conﬁdence intervals can be similarly obtained.\\nExample 14.3.a. A sample of 50 transistors is simultaneously put on a test\\nthat is to be ended when the 15th failure occurs. If the total time on test of all\\ntransistors is equal to 525 hours, determine a 95 percent conﬁdence interval\\nfor the mean lifetime of a transistor. Assume that the underlying distribution\\nis exponential.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 604}, page_content='598 CHAPTER 14: Life testing\\nSolution. R gives\\nχ2\\n.025,30 = qchisq(.975,30) = 46.97924,\\nχ2\\n.975,30 = qchisq(.025,30) = 16.79077\\nand so, using Equation (14.3.6), we can assert with 95 percent conﬁdence that\\nθ ∈(22.35,62.17)\\n■\\nIn testing a hypothesis about θ, we can use Equation (14.3.6) to determine the\\np-value of the test data. For instance, suppose we are interested in the one-sided\\ntest of\\nH0 : θ ≥θ0\\nversus the alternative\\nH1 : θ < θ0\\nThis can be tested by ﬁrst computing the value of the test statistic 2τ/θ0 — call\\nthis value v — and then computing the probability that a chi-square random\\nvariable with 2r degrees of freedom would be as small as v. This probability is\\nthe p-value in the sense that it represents the (maximal) probability that such a\\nsmall value of 2τ/θ0 would have been observed if H0 were true. The hypothesis\\nshould then be rejected at all signiﬁcance levels at least as large as this p-value.\\nExample 14.3.b. A producer of batteries claims that the lifetimes of the items\\nit manufactures are exponentially distributed with a mean life of at least 150\\nhours. To test this claim, 100 batteries are simultaneously put on a test that is\\nslated to end when the 20th failure occurs. If, at the end of the experiment, the\\ntotal test time of all the 100 batteries is equal to 1800, should the manufac-\\nturer’s claim be accepted?\\nSolution. Since 2τ/θ0 = 3600/150 = 24, the p-value is\\np-value = P{χ2\\n40 ≤24}\\n= pchisq(24,40) = .02127977\\nHence, the manufacturer’s claim should be rejected at the 5 percent level of\\nsigniﬁcance (indeed at any signiﬁcance level at least as large as .021).\\n■\\nIt follows from Equation (14.3.5) that the accuracy of the estimator τ/r depends\\nonly on r and not on n, the number of items put on test. The importance of n\\nresides in the fact that by choosing it large enough we can ensure that the test\\nis, with high probability, of short duration. In fact, the moments of X(r), the\\ntime at which the test ends, are easily obtained. Since, with X(0) ≡0,\\nX( j) −X( j−1) =\\nYj\\nn −j + 1,\\nj = 1,...,r'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 605}, page_content='14.3 The exponential distribution in life testing\\n599\\nit follows upon summing that\\nX(r) =\\nr\\n\\x13\\nj=1\\nYj\\nn −j + 1\\nHence, from Equation (14.3.4), X(r) is the sum of r independent exponentials\\nhaving respective means θ/n, θ/(n −1), ..., θ/(n −r + 1). Using this, we see\\nthat\\nE[X(r)] =\\nr\\n\\x13\\nj=1\\nθ\\nn −j + 1 = θ\\nn\\n\\x13\\nj=n−r+1\\n1\\nj\\n(14.3.7)\\nVar(X(r)) =\\nr\\n\\x13\\nj=1\\n\\x18\\nθ\\nn −j + 1\\n\\x192\\n= θ2\\nn\\n\\x13\\nj=n−r+1\\n1\\nj2\\nwhere the second equality uses the fact that the variance of an exponential is\\nequal to the square of its mean. For large n, we can approximate the preceding\\nsums as follows:\\nn\\n\\x13\\nj=n−r+1\\n1\\nj ≈\\n\\x02 n\\nn−r+1\\ndx\\nx = log\\n\\x18\\nn\\nn −r + 1\\n\\x19\\nn\\n\\x13\\nj=n−r+1\\n1\\nj2 ≈\\n\\x02 n\\nn−r+1\\ndx\\nx2 =\\n1\\nn −r + 1 −1\\nn =\\nr −1\\nn(n −r + 1)\\nThus, for instance, if in Example 14.3.b the true mean life was 120 hours, then\\nthe expectation and variance of the length of the test are approximately given\\nby\\nE[X(20)] ≈120log\\n\\x18100\\n81\\n\\x19\\n= 25.29\\nVar(X(20)) ≈(120)2\\n19\\n100(81) = 33.78\\n14.3.2\\nSequential testing\\nSuppose now that we have an inﬁnite supply of items, each of whose lifetime\\nis exponential with an unknown mean θ, which are to be tested sequentially,\\nin that the ﬁrst item is put on test and on its failure the second is put on test,\\nand so on. That is, as soon as an item fails, it is immediately replaced on life\\ntest by the next item. We suppose that at some ﬁxed time T the text ends.\\nThe observed data will consist of the following:\\nData:\\nr,x1,x2,...,xr'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 606}, page_content='600 CHAPTER 14: Life testing\\nFIGURE 14.1\\nr failures by time T .\\nwith the interpretation that there has been a total of r failures with the ith item\\non test having functioned for a time xi. Now the foregoing will be the observed\\ndata if\\nXi = xi,\\ni = 1,...,r,\\nr\\n\\x13\\ni=1\\nxi < T\\n(14.3.8)\\nXr+1 > T −\\nr\\n\\x13\\ni=1\\nxi\\nwhere Xi is the functional lifetime of the ith item to be put in use. This follows\\nsince in order for there to be r failures, the rth failure must occur before time\\nT — and so \\x0fr\\ni=1 Xi < T — and the functional life of the (r + 1)st item must\\nexceed T −\\x0fr\\ni=1 Xi (see Figure 14.1).\\nFrom Equation (14.3.8), we obtain that the likelihood of the data r,x1,...,xr\\nis as follows:\\nf (r,x1,...,xr|θ)\\n= fX1,..., Xr(x1,...,xr)P\\n\\x1a\\nXr+1 > T −\\nr\\n\\x13\\ni=1\\nxi\\n\\x1b\\n,\\nr\\n\\x13\\ni=1\\nxi < T\\n= 1\\nθr e−\\x08r\\ni=1xi/θe−(T −\\x08r\\ni=1xi)/θ\\n= 1\\nθr e−T/θ\\nTherefore,\\nlogf (r,x1,...,xr|θ) = −r logθ −T\\nθ\\nand so\\n∂\\n∂θ logf (r,x1,...,xr|θ) = −r\\nθ + T\\nθ2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 607}, page_content='14.3 The exponential distribution in life testing\\n601\\nOn equating to 0 and solving, we obtain that the maximum likelihood estimate\\nfor θ is\\nˆθ = T\\nr\\nSince T is the total time on test of all items, it follows once again that the\\nmaximum likelihood estimate of the unknown exponential mean is equal to\\nthe total time on test divided by the number of observed failures in this time.\\nIf we let N(T) denote the number of failures by time T, then the maximum like-\\nlihood estimator of θ is T /N(T ). Suppose now that the observed value of N(T )\\nis N(T ) = r. To determine a 100(1 −α) percent conﬁdence interval estimate for\\nθ, we will ﬁrst determine the values θL and θU, which are such that\\nPθU {N(T ) ≥r} = α\\n2 ,\\nPθL{N(T ) ≤r} = α\\n2\\nwhere by Pθ(A) we mean that we are computing the probability of the event\\nA under the supposition that θ is the true mean. The 100(1 −α) percent conﬁ-\\ndence interval estimate for θ is\\nθ ∈(θL,θU)\\nTo understand why those values of θ for which either θ < θL or θ > θU are\\nnot included in the conﬁdence interval, note that Pθ{N(T )≥r} decreases and\\nPθ{N(T )≤r} increases in θ (why?). Hence,\\nif θ < θL,\\nthen Pθ{N(T ) ≤r} < PθL{N(T ) ≤r} = α\\n2\\nif θ > θU,\\nthen Pθ{N(T ) ≥r} < PθU {N(T ) ≥r} = α\\n2\\nIt remains to determine θL and θU. To do so, note ﬁrst that the event that\\nN(T ) ≥r is equivalent to the statement that the rth failure occurs before or at\\ntime T. That is,\\nN(T ) ≥r ⇔X1 + ··· + Xr ≤T\\nand so\\nPθ{N(T ) ≥r} = Pθ{X1 + ··· + Xr ≤T }\\n= P{\\t(r,1/θ) ≤T }\\n= P\\n\\x03θ\\n2χ2\\n2r ≤T\\n\\x04\\n= P\\n\\x14\\nχ2\\n2r ≤2T /θ\\n\\x15'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 608}, page_content='602 CHAPTER 14: Life testing\\nHence, upon evaluating the foregoing at θ = θU, and using the fact that P{χ2\\n2r ≤\\nχ2\\n1−α/2,2r} = α/2, we obtain that\\nα\\n2 = P\\n\\x03\\nχ2\\n2r ≤2T\\nθU\\n\\x04\\nand that\\n2T\\nθU\\n= χ2\\n1−α/2,2r\\nor\\nθU = 2T /χ2\\n1−α/2,2r\\nSimilarly, we can show that\\nθL = 2T /χ2\\nα/2,2r\\nand thus the 100(1 −α) percent conﬁdence interval estimate for θ is\\nθ ∈(2T /χ2\\nα/2,2r,\\n2T /χ2\\n1−α/2,2r)\\nExample 14.3.c. If a one-at-a-time sequential test yields 10 failures in the ﬁxed\\ntime of T = 500 hours, then the maximum likelihood estimate of θ is 500/10 =\\n50 hours. A 95 percent conﬁdence interval estimate of θ is\\n0 ∈(1000/χ2\\n.025,20, 1000/χ2\\n.975,20)\\nR yields that\\nχ2\\n.025,20 = 34.16961,\\nχ2\\n.975,20 = 9.590777\\nand so, with 95 percent conﬁdence,\\nθ ∈(29.27,103.52)\\n■\\nIf we wanted to test the hypothesis\\nH0 : θ = θ0\\nversus the alternative\\nH1 : θ ̸= θ0\\nthen we would ﬁrst determine the value of N(T ). If N(T ) = r, then the hypoth-\\nesis would be rejected provided either\\nPθ0{N(T ) ≤r} ≤α\\n2\\nor\\nPθ0{N(T ) ≥r} ≤α\\n2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 609}, page_content='14.3 The exponential distribution in life testing\\n603\\nIn other words, H0 would be rejected at all signiﬁcance levels greater than or\\nequal to the p-value given by\\np-value = 2 min(Pθ0{N(T ) ≥r},Pθ0{N(T ) ≤r})\\np-value = 2 min(Pθ0{N(T ) ≥r},1 −Pθ0{N(T ) ≥r + 1})\\n= 2 min\\n\\x18\\nP\\n\\x03\\nχ2\\n2r ≤2T\\nθ0\\n\\x04\\n,1 −P\\n\\x03\\nχ2\\n2(r+1) ≤2T\\nθ0\\n\\x04\\x19\\nThe p-value for a one-sided test is similarly obtained.\\nExample 14.3.d. A company claims that the mean lifetimes of the semiconduc-\\ntors it produces is at least 25 hours. To substantiate this claim, an independent\\ntesting service has decided to sequentially test, one at a time, the company’s\\nsemiconductors for 600 hours. If 30 semiconductors failed during this period,\\nwhat can we say about the validity of the company’s claim? Test at the 10 per-\\ncent level.\\nSolution. This is a one-sided test of\\nH0 : θ ≥25\\nversus\\nH1 : θ < 25\\nThe relevant probability for determining the p-value is the probability that\\nthere would have been as many as 30 failures if the mean life were 25. That\\nis,\\np-value = P25{N(600) ≥30}\\n= P{χ2\\n60 ≤1200/25}\\n= .1321236\\nfrom R\\nThus, H0 would be accepted when the signiﬁcance level is .10.\\n■\\n14.3.3\\nSimultaneous testing — stopping by a ﬁxed time\\nSuppose again that we are testing items whose life distributions are indepen-\\ndent exponential random variables with a common unknown mean θ. As in\\nSection 14.3.1, the n items are simultaneously put on test, but now we sup-\\npose that the test is to stop either at some ﬁxed time T or whenever all n items\\nhave failed — whichever occurs ﬁrst. The problem is to use the observed data\\nto estimate θ.\\nThe observed data will be as follows:\\nData :\\ni1,i2,...,ir,\\nx1,x2,...,xr\\nwith the interpretation that the preceding results when the r items numbered\\ni1,...,ir are observed to fail at respective times x1,...,xr and the other n −r\\nitems have not failed by time T.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 610}, page_content='604 CHAPTER 14: Life testing\\nSince an item will not have failed by time T if and only if its lifetime is greater\\nthan T, we see that the likelihood of the foregoing data is\\nf (i1,...,ir,x1,...,xr) = fXi1,...,Xir (x1,...,xr)P{Xj > T,j ̸= i1,..., ir}\\n= 1\\nθ e−x1/θ ··· 1\\nθ e−xr/θ(e−T/θ)n−r\\n= 1\\nθr exp\\n⎧\\n⎪⎪⎨\\n⎪⎪⎩\\n−\\nr\\x0f\\ni=1\\nxi\\nθ\\n−(n −r)T\\nθ\\n⎫\\n⎪⎪⎬\\n⎪⎪⎭\\nTo obtain the maximum likelihood estimates, take logs to obtain\\nlogf (i1,...,ir,x1,...,xr) = −r logθ −\\nr\\x0f\\n1\\nxi\\nθ\\n−(n −r)T\\nθ\\nHence,\\n∂\\n∂θ logf (i1,...,ir,x1,...,xr) = −r\\nθ +\\nr\\x0f\\n1\\nxi + (n −r)T\\nθ2\\nEquating to 0 and solving yields that ˆθ, the maximum likelihood estimate, is\\ngiven by\\nˆθ =\\nr\\x0f\\ni=1\\nxi + (n −r)T\\nr\\nHence, if we let R denote the number of items that fail by time T and let X(i) be\\nthe ith smallest of the failure times, i = 1,...,R, then the maximum likelihood\\nestimator of θ is\\nˆθ =\\nR\\x0f\\ni=1\\nX(i) + (n −R)T\\nR\\nLet τ denote the sum of the times that all items are on life test. Then, because\\nthe R items that fail are on test for times X(1),...,X(R) whereas the n −R non-\\nfailed items are all on test for time T, it follows that\\nτ =\\nR\\n\\x13\\ni=1\\nX(i) + (n −R)T'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 611}, page_content='14.3 The exponential distribution in life testing\\n605\\nand thus we can write the maximum likelihood estimator as\\nˆθ = τ\\nR\\nIn words, the maximum likelihood estimator of the mean life is (as in the life\\ntesting procedures of Sections 14.3.1 and 14.3.2) equal to the total time on test\\ndivided by the number of items observed to fail.\\nRemark\\nAs the reader may possibly have surmised, it turns out that for all possible\\nlife testing schemes for the exponential distribution, the maximum likelihood\\nestimator of the unknown mean θ will always be equal to the total time on test\\ndivided by the number of observed failures. To see why this is true, consider\\nany testing situation and suppose that the outcome of the data is that r items\\nare observed to fail after having been on test for times x1,...,xr, respectively,\\nand that s items have not yet failed when the test ends — at which time they\\nhad been on test for respective times y1,...,ys. The likelihood of this outcome\\nwill be\\nlikelihood = K 1\\nθ e−x1/θ ... 1\\nθ e−xr/θe−y1/θ ...e−ys/θ\\n= K\\nθr exp\\n⎧\\n⎪⎪⎪⎨\\n⎪⎪⎪⎩\\n−\\n\\x18 r\\x0f\\ni=1\\nxi +\\ns\\x0f\\ni=1\\nyi\\n\\x19\\nθ\\n⎫\\n⎪⎪⎪⎬\\n⎪⎪⎪⎭\\n(14.3.9)\\nwhere K, which is a function of the testing scheme and the data, does not de-\\npend on θ. (For instance, K may relate to a testing procedure in which the\\ndecision as to when to stop depends not only on the observed data but is\\nallowed to be random.) It follows from the foregoing that the maximum like-\\nlihood estimate of θ will be\\nˆθ =\\nr\\x0f\\ni=1\\nxi +\\ns\\x0f\\ni=1\\nyi\\nr\\n(14.3.10)\\nBut \\x0fr\\ni=1 xi + \\x0fs\\ni=1 yi is just the total-time-on-test statistic and so the maxi-\\nmum likelihood estimator of θ is indeed the total time on test divided by the\\nnumber of observed failures in that time.\\nThe distribution of τ/R is rather complicated for the life testing scheme de-\\nscribed in this section1 and thus we will not be able to easily derive a conﬁ-\\ndence interval estimator for θ. Indeed, we will not further pursue this problem\\nbut rather will consider the Bayesian approach to estimating θ.\\n1For instance, for the scheme considered, τ and R are not only both random but are also dependent.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 612}, page_content='606 CHAPTER 14: Life testing\\n14.3.4\\nThe Bayesian approach\\nSuppose that items having independent and identically distributed exponential\\nlifetimes with an unknown mean θ are put on life test. Then, as noted in the\\nremark given in Section 14.3.3, the likelihood of the data can be expressed as\\nf (data|θ) = K\\nθr e−t/θ\\nwhere t is the total time on test — that is, the sum of the time on test of all\\nitems used — and r is the number of observed failures for the given data.\\nLet λ = 1/θ denote the rate of the exponential distribution. In the Bayesian ap-\\nproach, it is more convenient to work with the rate λ rather than its reciprocal.\\nFrom the foregoing we see that\\nf (data|λ) = Kλre−λt\\nIf we suppose prior to testing, that λ is distributed according to the prior density\\ng(λ), then the posterior density of λ given the observed data is as follows:\\nf (λ|data) =\\nf (data|λ)g(λ)\\n\\x06\\nf (data|λ)g(λ)dλ\\n=\\nλre−λtg(λ)\\n\\x06\\nλre−λtg(λ)dλ\\n(14.3.11)\\nThe preceding posterior density becomes particularly convenient to work with\\nwhen g is a gamma density function with parameters, say, (b,a) — that is,\\nwhen\\ng(λ) = ae−aλ(aλ)b−1\\n\\t(b)\\n,\\nλ > 0\\nfor some nonnegative constants a and b. Indeed for this choice of g we have\\nfrom Equation (14.3.11) that\\nf (λ|data) = Ce−(a+t)λλr+b−1\\n= Ke−(a+t)λ[(a + t)λ]b+r−1\\nwhere C and K do not depend on λ. Because we recognize the preceding as the\\ngamma density with parameters (b + r,a + t), we can rewrite it as\\nf (λ|data) = (a + t)e−(a+t)λ[(a + t)λ]b+r−1\\n\\t(b + r)\\n,\\nλ > 0\\nIn other words, if the prior distribution of λ is gamma with parameters (b,a),\\nthen no matter what the testing scheme, the (posterior) conditional distribu-\\ntion of λ given the data is gamma with parameters (b + R,a + τ), where τ'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 613}, page_content='14.4 A two-sample problem\\n607\\nand R represent, respectively, the total-time-on-test statistic and the number of\\nobserved failures. Because the mean of a gamma random variable with param-\\neters (b,a) is equal to b/a (see Section 5.7), we can conclude that E[λ|data],\\nthe Bayes estimator of λ, is\\nE[λ|data] = b + R\\na + τ\\nExample 14.3.e. Suppose that 20 items having an exponential life distribution\\nwith an unknown rate λ are put on life test at various times. When the test is\\nended, there have been 10 observed failures — their lifetimes being (in hours)\\n5, 7, 6.2, 8.1, 7.9, 15, 18, 3.9, 4.6, 5.8. The 10 items that did not fail had, at\\nthe time the test was terminated, been on test for times (in hours) 3, 3.2, 4.1,\\n1.8, 1.6, 2.7, 1.2, 5.4, 10.3, 1.5. If prior to the testing it was felt that λ could be\\nviewed as being a gamma random variable with parameters (2, 20), what is the\\nBayes estimator of λ?\\nSolution. Since\\nτ = 116.1,\\nR = 10\\nit follows that the Bayes estimate of λ is\\nE[λ|data] =\\n12\\n136.1 = .088\\n■\\nRemark\\nAs we have seen, the choice of a gamma prior distribution for the rate of\\nan exponential distribution makes the resulting computations quite simple.\\nWhereas, from an applied viewpoint, this is not a sufﬁcient rationale, such a\\nchoice is often made with one justiﬁcation being that the ﬂexibility in ﬁxing\\nthe two parameters of the gamma prior usually enables one to reasonably ap-\\nproximate their true prior feelings.\\n14.4\\nA two-sample problem\\nA company has set up two separate plants to produce vacuum tubes. The com-\\npany supposes that tubes produced at Plant I function for an exponentially\\ndistributed time with an unknown mean θ1, whereas those produced at Plant\\nII function for an exponentially distributed time with unknown mean θ2. To\\ntest the hypothesis that there is no difference between the two plants (at least\\nin regard to the lifetimes of the tubes they produce), the company samples n\\ntubes from Plant I and m from Plant II and then utilizes these tubes to deter-\\nmine their lifetimes. How can they thus determine whether the two plants are\\nindeed identical?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 614}, page_content='608 CHAPTER 14: Life testing\\nIf we let X1,...,Xn denote the lifetimes of the n tubes produced at Plant I\\nand Y1,...,Ym denote the lifetimes of the m tubes produced at Plant II, then\\nthe problem is to test the hypothesis that θ1 = θ2 when the Xi,i = 1,...,n\\nare a random sample from an exponential distribution with mean θ1 and the\\nYi,i = 1,...,m are a random sample from an exponential distribution with\\nmean θ2. Moreover, the two samples are supposed to be independent.\\nTo develop a test of the hypothesis that θ1 = θ2, let us begin by noting that\\n\\x0fn\\ni=1Xi and \\x0fm\\ni=1 Yi (being the sum of independent and identically dis-\\ntributed exponentials) are independent gamma random variables with re-\\nspective parameters (n,1/θ1) and (m,1/θ2). Hence, by the equivalence of the\\ngamma and chi-square distribution it follows that\\n2\\nθ1\\nn\\n\\x13\\ni=1\\nXi ∼χ2\\n2n\\n2\\nθ2\\nm\\n\\x13\\ni=1\\nYi ∼χ2\\n2m\\nHence, it follows from the deﬁnition of the F-distribution that\\n2\\n2nθ1\\n\\x0fn\\ni=1 Xi\\n2\\n2mθ2\\n\\x0fm\\ni=1 Yi\\n∼Fn,m\\nThat is, if X and Y are the two sample means, respectively, then\\nθ2X\\nθ1Y\\nhas an F-distribution with n and m degrees of freedom.\\nHence, when the hypothesis θ1 = θ2 is true, we see that X/Y has an F-distribution\\nwith n and m degrees of freedom. This suggests the following test of the hypoth-\\nesis that θ1 = θ2.\\nTest:\\nH0 : θ1 = θ2 vs. alternative H1 : θ1 ̸= θ2\\nStep 1:\\nChoose a signiﬁcance level α.\\nStep 2:\\nDetermine the value of the test statistic X/Y — say its value is v.\\nStep 3:\\nCompute P {F ≤v} where F ∼Fn,m. If this probability is either less than α/2\\n(which occurs when X is signiﬁcantly less than Y) or greater than 1 −α/2 (which\\noccurs when X is signiﬁcantly greater than Y), then the hypothesis is rejected.\\nIn other words, the p-value of the test data is given by\\np-value = 2 min(P{F ≤v},1 −P{F ≤v})'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 615}, page_content='14.5 The Weibull distribution in life testing\\n609\\nExample 14.4.a. Test the hypothesis, at the 5 percent level of signiﬁcance, that\\nthe lifetimes of items produced at two given plants have the same exponential\\nlife distribution if a sample of size 10 from the ﬁrst plant has a total lifetime of\\n420 hours whereas a sample of 15 from the second plant has a total lifetime of\\n510 hours.\\nSolution. The value of the test statistic X/Y is 42/34 = 1.2353. To compute the\\nprobability that an F-random variable with parameters 10, 15 is less than this\\nvalue, we use R to obtain that\\npchisq(1.2353,10,15) = .6553309\\nBecause the p-value is equal to 2(1 −.6553) = .6894, we cannot reject H0.\\n■\\n14.5\\nThe Weibull distribution in life testing\\nWhereas the exponential distribution arises as the life distribution when the\\nhazard rate function λ(t) is assumed to be constant over time, there are many\\nsituations in which it is more realistic to suppose that λ(t) either increases or\\ndecreases over time. One example of such a hazard rate function is given by\\nλ(t) = αβtβ−1,\\nt > 0\\n(14.5.1)\\nwhere α and β are positive constants. The distribution whose hazard rate\\nfunction is given by Equation (14.5.1) is called the Weibull distribution with\\nparameters (α, β). Note that λ(t) increases when β > 1, decreases when β < 1,\\nand is constant (reducing to the exponential) when β = 1.\\nThe Weibull distribution function is obtained from Equation (14.5.1) as fol-\\nlows:\\nF(t) = 1 −exp\\n\\x03\\n−\\n\\x02 t\\n0\\nλ(s)ds\\n\\x04\\n,\\nt > 0\\n= 1 −exp{−αtβ}\\nDifferentiating yields its density function:\\nf (t) = αβtβ−1 exp{−αtβ},\\nt > 0\\n(14.5.2)\\nThis density is plotted for a variety of values of α and β in Figure 14.2.\\nSuppose now that X1,...,Xn are independent Weibull random variables each\\nhaving parameters (α,β), which are assumed unknown. To estimate α and β,\\nwe can employ the maximum likelihood approach. Equation (14.5.2) yields the\\nlikelihood, given by\\nf (x1,...,xn) = αnβnxβ−1\\n1\\n···xβ−1\\nn\\nexp\\n\\x1a\\n−α\\nn\\n\\x13\\ni=1\\nxβ\\ni\\n\\x1b'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 616}, page_content='610 CHAPTER 14: Life testing\\nFIGURE 14.2\\nWeibull density functions.\\nHence,\\nlogf (x1,...,xn) = nlogα + nlogβ + (β −1)\\nn\\n\\x13\\ni=1\\nlogxi −α\\nn\\n\\x13\\ni=1\\nxβ\\ni\\nand\\n∂\\n∂α logf (x1,...,xn) = n\\nα −\\nn\\n\\x13\\ni=1\\nxβ\\ni\\n∂\\n∂β logf (x1,...,xn) = n\\nβ +\\nn\\n\\x13\\ni=1\\nlogxi −α\\nn\\n\\x13\\ni=1\\nxβ\\ni logxi\\nEquating to zero shows that the maximum likelihood estimates ˆα and ˆβ are\\nthe solutions of\\nn\\nˆα =\\nn\\n\\x13\\ni=1\\nx\\nˆβ\\ni'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 617}, page_content='14.5 The Weibull distribution in life testing\\n611\\nn\\nˆβ\\n+\\nn\\n\\x13\\ni=1\\nlogxi = ˆα\\nn\\n\\x13\\ni=1\\nx\\nˆβ\\ni logxi\\nor, equivalently,\\nˆα =\\nn\\nn\\x0f\\ni=1\\nx\\nˆβ\\ni\\nn + ˆβ log\\n\\x16 n\\n\\ni=1\\nxi\\n\\x17\\n=\\nn ˆβ\\nn\\x0f\\ni=1\\nx\\nˆβ\\ni logxi\\nn\\x0f\\ni=1\\nx\\nˆβ\\ni\\nThis latter equation can then be solved numerically for ˆβ, which will then also\\ndetermine ˆα. However, rather than pursuing this approach any further, let us\\nconsider a second approach, which is not only computationally easier but ap-\\npears, as indicated by a simulation study, to yield more accurate estimates.\\n14.5.1\\nParameter estimation by least squares\\nLet X1,...,Xn be a sample from the distribution\\nF(x) = 1 −e−αxβ,\\nx ≥0\\nNote that\\nlog(1 −F(x)) = −αxβ\\nor\\nlog\\n\\x18\\n1\\n1 −F(x)\\n\\x19\\n= αxβ\\nand so\\nloglog\\n\\x18\\n1\\n1 −F(x)\\n\\x19\\n= β logx + logα\\n(14.5.3)\\nNow let X(1) < X(2) < ··· < X(n) denote the ordered sample values — that is,\\nfor i = 1,...,n,\\nX(i) = ith smallest of X1,...,Xn\\nand suppose that the data results in X(i) = x(i). If we were able to approximate\\nthe quantities log log(1/[1 −F(x(i))]) — say, by the values y1,...,yn — then\\nfrom Equation (14.5.3), we could conclude that\\nyi ≈β logx(i) + logα,\\ni = 1,...,n\\n(14.5.4)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 618}, page_content='612 CHAPTER 14: Life testing\\nWe could then choose α and β to minimize the sum of the squared errors —\\nthat is, α and β are chosen to\\nminimize\\nα,β\\nn\\n\\x13\\ni=1\\n(yi −β logx(i) −logα)2\\nIndeed, using Proposition 9.2.1 we obtain that the preceding minimum is at-\\ntained when α = ˆα, β = ˆβ where\\nˆβ =\\nn\\n\\x13\\ni=1\\nyi logx(i) −n logx ¯y\\nn\\n\\x13\\ni=1\\n(logx(i))2 −n(logx)2\\nlog ˆα = ¯y −β logx\\nwhere\\nlogx =\\nn\\n\\x13\\ni=1\\n(logx(i))\\n\\x1c\\nn,\\n¯y =\\nn\\n\\x13\\ni=1\\nyi\\n\\x1c\\nn\\nTo utilize the foregoing, we need to be able to determine values yi that approx-\\nimate log log(1/[1 −F(x(i))]) = log[−log(1 −F(x(i)))], i = 1, ...,n. We now\\npresent two different methods for doing this.\\nMethod 1: This method uses the fact that\\nE[F(X(i))] =\\ni\\n(n + 1)\\n(14.5.5)\\nand then approximates F(x(i)) by E[F(X(i))]. Thus, this method calls for using\\nyi = log{−log(1 −E[F(X(i))])}\\n(14.5.6)\\n= log\\n\\x03\\n−log\\n\\x18\\n1 −\\ni\\n(n + 1)\\n\\x19\\x04\\n= log\\n\\x03\\n−log\\n\\x18n + 1 −i\\nn + 1\\n\\x19\\x04\\nMethod 2: This method uses the fact that\\nE[−log(1 −F(X(i)))] = 1\\nn +\\n1\\nn −1 +\\n1\\nn −2 + ··· +\\n1\\nn −i + 1\\n(14.5.7)\\nand then approximates −log(1 −F(x(i))) by the foregoing. Thus, this second\\nmethod calls for setting\\nyi = log\\n\\x081\\nn +\\n1\\n(n −1) + ··· +\\n1\\n(n −i + 1)\\n\\t\\n(14.5.8)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 619}, page_content='Problems\\n613\\nRemarks\\n(a) It is not, at present, clear which method provides superior estimates\\nof the parameters of the Weibull distribution, and extensive simulation\\nstudies will be necessary to determine this.\\n(b) Proofs of equalities 14.5.5 and 14.5.7 [which hold whenever X(i) is the\\nith smallest of a sample of size n from any continuous distribution F]\\nare outlined in Problems 28–30.\\nProblems\\n1. A random variable whose distribution function is given by\\nF(t) = 1 −exp{−αtβ},\\nt ≥0\\nis said to have a Weibull distribution with parameters α, β. Compute its\\nfailure rate function.\\n2. If X and Y are independent random variables having failure rate func-\\ntions λx(t) and λy(t), show that the failure rate function of Z = min(X,\\nY) is\\nλz(t) = λx(t) + λy(t)\\n3. The lung cancer rate of a t-year-old male smoker, λ(t), is such that\\nλ(t) = .027 + .025\\n\\x18t −40\\n10\\n\\x194\\n,\\nt ≥40\\nAssuming that a 40-year-old male smoker survives all other hazards, what\\nis the probability that he survives to (a) age 50, (b) age 60, without con-\\ntracting lung cancer? In the foregoing we are assuming that he remains a\\nsmoker throughout his life.\\n4. Suppose the life distribution of an item has failure rate function λ(t) = t3,\\n0 < t < ∞.\\na.\\nWhat is the probability that the item survives to age 2?\\nb.\\nWhat is the probability that the item’s life is between .4 and 1.4?\\nc.\\nWhat is the mean life of the item?\\nd.\\nWhat is the probability a 1-year-old item will survive to age 2?\\n5. A continuous life distribution is said to be an IFR (increasing failure rate)\\ndistribution if its failure rate function λ(t) is nondecreasing in t.\\na.\\nShow that the gamma distribution with density\\nf (t) = λ2te−λt,\\nt > 0\\nis IFR.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 620}, page_content='614 CHAPTER 14: Life testing\\nb.\\nShow, more generally, that the gamma distribution with parameters\\nα, λ is IFR whenever α ≥1.\\nHint: Write\\nλ(t) =\\n\\x1d\\x06 ∞\\nt\\nλe−λs(λs)α−1 ds\\nλe−λt(λt)α−1\\n\\x1e−1\\n6. Show that the uniform distribution on (a, b) is an IFR distribution.\\n7. For the model of Section 14.3.1, explain how the following ﬁgure can be\\nused to show that\\nτ =\\nr\\n\\x13\\nj=1\\nYj\\nwhere\\nYj = (n −j + 1)(X( j) −X( j−1))\\n(Hint: Argue that both τ and \\x0fr\\nj=1 Yj equal the total area of the ﬁgure\\nshown.)\\n8. When 30 transistors were simultaneously put on a life test that was to\\nbe terminated when the 10th failure occurred, the observed failure times\\nwere (in hours) 4.1, 7.3, 13.2, 18.8, 24.5, 30.8, 38.1, 45.5, 53, 62.2. As-\\nsume an exponential life distribution.\\na.\\nWhat is the maximum likelihood estimate of the mean life of a tran-\\nsistor?\\nb.\\nCompute a 95 percent two-sided conﬁdence interval for the mean\\nlife of a transistor.\\nc.\\nDetermine a value c that we can assert, with 95 percent conﬁdence,\\nis less than the mean transistor life.\\nd.\\nTest at the α = .10 level of signiﬁcance the hypothesis that the mean\\nlifetime is 7.5 hours versus the alternative that it is not 7.5 hours.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 621}, page_content='Problems\\n615\\n9. Consider a test of H0 : θ = θ0 versus H1 : θ ̸= θ0 for the model of Sec-\\ntion 14.3.1. Suppose that the observed value of 2τ/θ0 is v. Show that the\\nhypothesis should be rejected at signiﬁcance level α whenever α is less\\nthan the p-value given by\\np-value = 2min(P{χ2\\n2r < v},1 −P{χ2\\n2r < v})\\nwhere χ2\\n2r is a chi-square random variable with 2r degrees of freedom.\\n10. Suppose 30 items are put on test that is scheduled to stop when the 8th\\nfailure occurs. If the failure times are, in hours, .35, .73, .99, 1.40, 1.45,\\n1.83, 2.20, 2.72, test, at the 5 percent level of signiﬁcance, the hypothe-\\nsis that the mean life is equal to 10 hours. Assume that the underlying\\ndistribution is exponential.\\n11. Suppose that 20 items are to be put on test that is to be terminated when\\nthe 10th failure occurs. If the lifetime distribution is exponential with\\nmean 10 hours, compute the following quantities.\\na.\\nThe mean length of the testing period.\\nb.\\nThe variance of the testing period.\\n12. Vacuum tubes produced at a certain plant are assumed to have an un-\\nderlying exponential life distribution having an unknown mean θ. To\\nestimate θ it has been decided to put a certain number n of tubes on test\\nand to stop the test at the 10th failure. If the plant ofﬁcials want the mean\\nlength of the testing period to be 3 hours when the value of θ is θ = 20,\\napproximately how large should n be?\\n13. A one-at-a-time sequential life testing scheme is scheduled to run for 300\\nhours. A total of 16 items fail within that time. Assuming an exponential\\nlife distribution with unknown mean θ (measured in hours):\\na.\\nDetermine the maximum likelihood estimate of θ.\\nb.\\nTest at the 5 percent level of signiﬁcance the hypothesis that θ = 20\\nversus the alternative that θ ̸= 20.\\nc.\\nDetermine a 95 percent conﬁdence interval for θ.\\n14. Using the fact that a Poisson process results when the times between suc-\\ncessive events are independent and identically distributed exponential\\nrandom variables, show that\\nP{X ≥n} = Fχ2\\n2n(x)\\nwhen X is a Poisson random variable with mean x/2 and Fχ2\\n2n is the\\nchi-square distribution function with 2n degrees of freedom. (Hint: Use\\nthe results of Section 14.3.2.)\\n15. From a sample of items having an exponential life distribution with un-\\nknown mean θ, items are tested in sequence. The testing continues until\\neither the rth failure occurs or after a time T elapses.\\na.\\nDetermine the likelihood function.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 622}, page_content='616 CHAPTER 14: Life testing\\nb.\\nVerify that the maximum likelihood estimator of θ is equal to the\\ntotal time on test of all items divided by the number of observed\\nfailures.\\n16. Verify that the maximum likelihood estimate corresponding to Equation\\n(14.3.9) is given by Equation (14.3.10).\\n17. A testing laboratory has facilities to simultaneously life test 5 compo-\\nnents. The lab tested a sample of 10 components from a common expo-\\nnential distribution by initially putting 5 on test and then replacing any\\nfailed component by one still waiting to be tested. The test was designed\\nto end either at 200 hours or when all 10 components had failed. If there\\nwere a total of 9 failures occurring at times 15, 28.2, 46, 62.2, 76, 86,\\n128, 153, 197, what is the maximum likelihood estimate of the mean\\nlife of a component?\\n18. Suppose that the remission time, in weeks, of leukemia patients that have\\nundergone a certain type of chemotherapy treatment is an exponential\\nrandom variable having an unknown mean θ. A group of 20 such pa-\\ntients is being monitored and, at present, their remission times are (in\\nweeks) 1.2, 1.8∗, 2.2, 4.1, 5.6, 8.4, 11.8∗, 13.4∗, 16.2, 21.7, 29∗, 41, 42∗,\\n42.4∗, 49.3, 60.5, 61∗, 94, 98, 99.2∗where an asterisk next to the data\\nmeans that the patient’s remission is continuing, whereas a data point\\nwithout an asterisk means that the remission ended at that time. What is\\nthe maximum likelihood estimate of θ?\\n19. In Problem 17, suppose that prior to the testing phase and based on past\\nexperience one felt that the value of λ = 1/θ could be thought of as the\\noutcome of a gamma random variable with parameters 1, 100. What is\\nthe Bayes estimate of λ?\\n20. What is the Bayes estimate of λ = 1/θ in Problem 18 if the prior distribu-\\ntion on λ is exponential with mean 1/30?\\n21. The following data represent failure times, in minutes, for two types of\\nelectrical insulation subject to a certain voltage stress.\\nType I\\n212, 88.5, 122.3, 116.4, 125, 132, 66\\nType II\\n34.6, 54, 162, 49, 78, 121, 128\\nTest the hypothesis that the two sets of data come from the same expo-\\nnential distribution.\\n22. Suppose that the life distributions of two types of transistors are both\\nexponential. To test the equality of means of these two distributions, n1\\ntype 1 transistors are simultaneously put on a life test that is scheduled\\nto end when there have been a total of r1 failures. Similarly, n2 type 2\\ntransistors are simultaneously put on a life test that is to end when there\\nhave been r2 failures.\\na.\\nUsing results from Section 14.3.1, show how the hypothesis that the\\nmeans are equal can be tested by using a test statistic that, when the\\nmeans are equal, has an F-distribution with 2r1 and 2r2 degrees of\\nfreedom.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 623}, page_content='Problems\\n617\\nb.\\nSuppose n1 = 20, r1 = 10 and n2 = 10, r2 = 7 with the following data\\nresulting.\\nType 1 failures at times:\\n10.4, 23.2, 31.4, 45, 61.1, 69.6, 81.3, 95.2, 112, 129.4\\nType 2 failures at times:\\n6.1, 13.8, 21.2, 31.6, 46.4, 66.7, 92.4\\nWhat is the smallest signiﬁcance level α for which the hypothesis of\\nequal means would be rejected? (That is, what is the p-value of the\\ntest data?)\\n23. If X is a Weibull random variable with parameters (α,β), show that\\nE[X] = α−1/β\\t(1 + 1/β)\\nwhere \\t(y) is the gamma function deﬁned by\\n\\t(y) =\\n\\x02 ∞\\n0\\ne−xxy−1 dx\\nHint: Write\\nE[X] =\\n\\x02 ∞\\n0\\ntαβtβ−1 exp{−αtβ} dt\\nand make the change of variables\\nx = αtβ,\\ndx = αβtβ−1 dt\\n24. Show that if X is a Weibull random variable with parameters (α,β), then\\nVar(X) = α−2/β\\n\\x1d\\n\\t\\n\\x18\\n1 + 2\\nβ\\n\\x19\\n−\\n\\x18\\n\\t\\n\\x18\\n1 + 1\\nβ\\n\\x19\\x192\\x1e\\n25. If the following are the sample data from a Weibull population having\\nunknown parameters α and β, determine the least square estimates of\\nthese quantities, using either of the methods presented.\\nData: 15.4,16.8,6.2,10.6,21.4,18.2,1.6,12.5,19.4,17\\n26. Show that if X is a Weibull random variable with parameters (α,β), then\\nαXβ is an exponential random variable with mean 1.\\n27. If U is uniformly distributed on (0, 1) — that is, U is a random num-\\nber — show that [−(1/α)logU]1/β is a Weibull random variable with\\nparameters (α,β).\\nThe next three problems are concerned with verifying Equations (14.5.5)\\nand (14.5.7).\\n28. If X is a continuous random variable having distribution function F,\\nshow that'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 624}, page_content='618 CHAPTER 14: Life testing\\na.\\nF(X) is uniformly distributed on (0, 1);\\nb.\\n1 −F(X) is uniformly distributed on (0, 1).\\n29. Let X(i) denote ith smallest of a sample of size n from a continuous dis-\\ntribution function F. Also, let U(i) denote the ith smallest from a sample\\nof size n from a uniform (0, 1) distribution.\\na.\\nArgue that the density function of U(i) is given by\\nfU(i)(t) =\\nn!\\n(n −i)!(i −1)!ti−1(1 −t)n−i,\\n0 < t < 1\\n[Hint: In order for the ith smallest of n uniform (0, 1) random vari-\\nables to equal t, how many must be less than t and how many must\\nbe greater? Also, in how many ways can a set of n elements be broken\\ninto three subsets of respective sizes i −1, 1, and n −i?]\\nb.\\nUse part (a) to show that E[U(i)]=i/(n+1). [Hint: To evaluate the\\nresulting integral, use the fact that the density in part (a) must inte-\\ngrate to 1.]\\nc.\\nUse part (b) and Problem 28a to conclude that E[F(X(i))] = i/(n +\\n1).\\n30. If U is uniformly distributed on (0, 1), show that −logU has an exponen-\\ntial distribution with mean 1. Now use Equation (14.3.7) and the results\\nof the previous problems to establish Equation (14.5.7).'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 625}, page_content='CHAPTER 15\\nSimulation, bootstrap statistical methods,\\nand permutation tests\\n15.1\\nIntroduction\\nIn this chapter we introduce two powerful modern statistical techniques: boot-\\nstrap statistical methods and permutation tests. Both are nonparametric pro-\\ncedures in the sense that they make no speciﬁc assumptions about the form\\nof any underlying probability distributions. Bootstrap methods enable us to\\nmeasure the efﬁcacy of an estimator of a parameter, while permutation tests\\nyield new ways to test certain statistical hypotheses. Both, however, require a\\nlarge amount of computation in their implementation. The most efﬁcient and\\neffective way of doing the needed computation uses simulation, the third topic\\nof this chapter.\\nIn Section 15.2 we introduce random numbers, which are the keys to a sim-\\nulation. We show how random numbers can be used to generate random\\npermutations and random subsets. In Section 15.2.1 we present the Monte\\nCarlo simulation method for approximating expectations. In Section 15.3 we\\nintroduce the method of bootstrap statistics and show how the needed analysis\\ncan be done by applying the Monte Carlos simulation method. In Section 15.4\\nwe discuss permutation tests, which are nonparametric tests for determining\\nwhether a sequence of data comes from a single population distribution. In\\nthe remaining sections we return to the study of simulation. In Sections 15.5\\nand 15.6 we show how random numbers can be used to generate the values of\\narbitrarily distributed discrete and continuous random variables, and in Sec-\\ntion 15.7 we consider the question of when to end a Monte Carlo simulation\\nstudy.\\n15.2\\nRandom numbers\\nThe value of a uniform (0,1) random variable is called a random number.\\nWhereas in the past, mechanical devices have often been used to generate ran-\\ndom numbers, today we commonly use random number generators to generate\\na sequence of pseudo random numbers. Such random number generators start\\nwith an initial value x0, called the seed, and then recursively determine values\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00024-7\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n619'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 626}, page_content='620 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nby ﬁrst specifying positive integers a, c, and m and then letting\\nxn+1 = (axn + c) modulo m,\\nn ≥0\\nwhere the preceding means that xn+1 is the remainder obtained when axn +\\nc is divided by m. Thus each xn is one of the values 0,1,...,m −1, and the\\nquantity xn/m is taken as the random number. It can be shown that for suitable\\nchoices of a, c, and m, the preceding gives rise to a sequence of numbers that\\nlooks as if it was generated by observing the values of independent uniform\\n(0,1) random variables. For this reason we call the numbers xn/m,n ≥1, pseudo\\nrandom numbers.\\nExample 15.2.a. If a = 3, c = 7, m = 23, then with x0 = 2\\nx1 = 3(2) + 7\\nmodulo 23 = 13\\nx2 = 3(13) + 7\\nmodulo 23 = 0\\nx3 = 3(0) + 7\\nmodulo 23 = 7\\nx4 = 3(7) + 7\\nmodulo 23 = 5\\nx5 = 3(5) + 7\\nmodulo 23 = 22\\nand so on. Consequently, using the seed x0 = 2, the pseudo random numbers\\nobtained are 13/23, 0,7/23, 5/23, 22/23,....\\n■\\nMost computers have built-in random number generators, and we shall take as\\nour starting point in simulation that we can generate the values of pseudo ran-\\ndom numbers; moreover, we will act as if these pseudo random numbers were\\nactually true random numbers. That is, we will act as if the sequence of random\\nnumbers were actually a sequence of values of a sample from the uniform (0,1)\\ndistribution.\\nRandom numbers are the key to any simulation study. This is illustrated in our\\nnext example, which is concerned with generating a random permutation.\\nExample 15.2.b. Suppose we want to generate a permutation of the numbers\\n1,2,...,n in such a manner that all n! possible permutations are equally likely.\\nTo accomplish this we can ﬁrst randomly choose one of the numbers 1,2,...,n\\nand put that number in position n. We can then randomly choose one of the\\nremaining n −1 numbers and put that number in position n −1, and then\\nrandomly choose one of the remaining n −2 numbers and put that number in\\nposition n −2, and so on (where “randomly choose” means that each of the\\npossible choices is equally likely to be made). However, so that we do not have\\nto directly consider exactly which elements remain to be placed, it is convenient\\nand effective to keep the numbers in an ordered list and then randomly choose\\nthe position of the number rather than the number itself. That is, starting with\\nany permutation r1,r2,...,rn of the numbers 1,2,...,n, randomly choose one\\nof the positions 1,...,n and then interchange the number in that position'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 627}, page_content='15.2 Random numbers\\n621\\nwith the number in position n. Then randomly choose one of the positions\\n1,...,n −1, and interchange the number in that position with the number in\\nposition n −1. Then randomly choose one of the positions 1,...,n −2, and\\ninterchange the number in that position with the number in position n −2,\\nand so on.\\nTo implement the preceding we need to be able to generate a random variable\\nthat is equally likely to take on any of the values 1,...,k. To accomplish this,\\nlet U denote a random number — that is, U is uniformly distributed over (0,1)\\n— and let Int(kU) be the integer part of kU — that is, it is the largest integer\\nless than or equal to kU. Then, for i = 1,...,k\\nP[Int(kU) + 1 = i]\\n=\\nP[Int(kU) = i −1]\\n=\\nP(i −1 ≤kU < i)\\n=\\nP( i−1\\nk ≤U < i\\nk)\\n=\\n1/k\\nThus, Int(kU) + 1 is equally likely to take on any of the values 1,...,k.\\nThe algorithm for generating a random permutation of the numbers 1,2,...,n\\ncan now be written as follows:\\n1. Let r1,r2,...,rn be any permutation of the numbers 1,2,...,n. (For in-\\nstance, we could have rj = j, j = 1,...,n.)\\n2. Set k = n. (The number to be put in position k is to be determined.)\\n3. Generate a random number U and let I = Int(kU) + 1.\\n4. Interchange the values of rI and rk.\\n5. Let k = k −1.\\n6. If k > 1, go to Step 3; if k = 1, go to step 7.\\n7. r1,...,rn is the desired permutation.\\nFor instance, suppose n = 4 and the initial permutation is 1, 2, 3, 4. If the ﬁrst\\nvalue of I — which is equally likely to be any of the numbers 1, 2, 3, 4 — is 2,\\nthen the number in position 2 is interchanged with the number in position 4\\nto give the new permutation 1, 4, 3, 2. If the next value of I — which is equally\\nlikely to be any of the numbers 1, 2, 3 — is 3, then the number in position 3 is\\ninterchanged with the one in position 3, so the permutation remains 1, 4, 3, 2.\\nIf the ﬁnal value of I — which is equally likely to be any of the numbers 1, 2\\n— is 1, then the number in position 1 is interchanged with the one in position\\n2 to give the ﬁnal permutation 4, 1, 3, 2.\\nAn important property of the preceding algorithm is that it can be used to gen-\\nerate a random subset of size r from the set {1,2,...,n}. For r ≤n/2, just follow\\nthe preceding algorithm until the elements in the ﬁnal r positions (that is, in\\npositions n,n −1,...,n −r + 1) are speciﬁed, and then take the numbers in'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 628}, page_content='622 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nthese positions as the random subset of size r. For r > n/2, rather than directly\\nchoosing the r numbers to be in the subset, it is quicker to choose the n −r\\nnumbers that are not in the subset. So in this case, follow the preceding algo-\\nrithm until the ﬁnal n −r positions are ﬁlled, and then take the numbers that\\nremain as the random subset of size r.\\n■\\n15.2.1\\nThe Monte Carlo simulation approach\\nSuppose we want to compute the expected value of a statistic h(X1,X2,...,Xn)\\nwhen X1,X2,...,Xn are independent and identically distributed random vari-\\nables having density function f (x). Using that the joint density function of\\nX1,X2,...,Xn is\\nf (x1,...,xn) = f (x1)f (x2)···f (xn)\\nwe can write that\\nE[h(X1,X2,...,Xn)]\\n=\\n\\x02 \\x02\\n···\\n\\x02\\nh(x1,...,xn)f (x1)f (x2)···f (xn)dx1 dx2 ···dxn\\nThe difﬁculty, however, with the preceding formula is that it is often impossi-\\nble to analytically compute the preceding multiple integral and also difﬁcult\\nto numerically evaluate it to within a speciﬁed accuracy. One approach that\\nremains is to approximate E[h(X1,X2,...,Xn)] by a simulation.\\nTo accomplish this approximation, start by generating the values of n indepen-\\ndent random variables X1\\n1,X1\\n2,...,X1\\nn, each having density function f , and\\nthen compute\\nY1 = h(X1\\n1,X1\\n2,...,X1\\nn)\\nNow generate the values of a second set of n independent random variables\\nhaving density function f that are also independent of the ﬁrst set. Calling this\\nsecond set of random variables X2\\n1,X2\\n2,...,X2\\nn, compute\\nY2 = h(X2\\n1,X2\\n2,...,X2\\nn)\\nContinue doing this until you have generated r sets of n independent random\\nvariables having density function f , and have computed the corresponding\\nvalues of Y. In this way, we would have generated values of r independent and\\nidentically random variables Yi = h(Xi\\n1,Xi\\n2,...,Xi\\nn), i = 1,...,r. Now, by the\\nstrong law of large numbers\\nlim\\nr→∞\\nY1 + ··· + Yr\\nr\\n= E[Yi] = E[h(X1,X2,...,Xn)]'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 629}, page_content='15.3 The bootstrap method\\n623\\nand so we can use the average of the generated values of the Yi’s as an estimate\\nof E[h(X1,X2,...,Xn)]. This approximation method is called the Monte Carlo\\nsimulation approach. Each time we generate a new value of Y we say that a\\nnew simulation run has been completed.\\nOf course in order to make use of the preceding approach we need to be able\\nto generate random variables having a speciﬁed density function. Although at\\npresent we only know how to do this for a uniform random variable — by\\nusing a random number generator — this will sufﬁce for the needed compu-\\ntations both in the bootstrap method and in running permutation tests. As a\\nresult, the next two sections will present these topics. We will then return to\\nthe simulation question of how to generate random variables having arbitrary\\ndistributions, as well as how to determine when to end a simulation study, in\\nthe ﬁnal sections of this chapter.\\n15.3\\nThe bootstrap method\\nLet X1,...,Xn be a sample from a population having distribution F, and sup-\\npose we want to use this sample to estimate a parameter θ of F. For instance,\\nθ could be the common mean or variance of the Xi. Suppose we have an es-\\ntimator d = d(X1,...,Xn) of θ and we would like to evaluate how good an\\nestimator of θ it is. One measure of the worth of d(X1,...,Xn) as an estimator\\nof θ is its mean square error, deﬁned as\\nMSEF (d) = EF [(d(X1,...,Xn) −θ)2]\\nThat is, MSEF (d) is the expected square of the distance between the estimator\\nd(X1,...,Xn) and the parameter θ, where we use the notation MSEF and EF\\nto indicate that the expected value is to be computed under the assumption that\\nX1,...,Xn are independent random variables having distribution function F.\\nHow can this quantity be estimated?\\nExample 15.3.a. If θ = E[Xi] is the mean of the distribution F, and\\nd(X1,...,Xn) = ¯Xn = \\x03n\\ni=1 Xi/n is the sample mean of the data values\\nX1,...,Xn, then because\\nEF [d(X1,...,Xn)] = EF [ ¯Xn] = EF [Xi] = θ\\nit follows that\\nMSEF ( ¯Xn) = EF [( ¯Xn −θ)2] = VarF ( ¯Xn) = σ 2/n\\nwhere σ 2 = VarF (Xi). Thus, in this case, MSEF ( ¯Xn) can be estimated by the\\nquantity S2\\nn/n, where\\nS2\\nn =\\n1\\nn −1\\nn\\n\\x04\\ni=1\\n(Xi −¯Xn)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 630}, page_content='624 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nis the sample variance of the data values X1,...,Xn, and can be used to esti-\\nmate the population variance σ 2.\\n■\\nWhereas in the preceding example it was easy to estimate the mean square error\\nof the sample mean as an estimator of a population mean, what if we initially\\nwanted to estimate the population variance? That is, what if θ = VarF (Xi). In\\nthis case we can use the sample variance as the estimator. However, while it\\nwas easy to come up with this estimator d(X1,...,Xn) = S2\\nn, it is not easy to\\nsee how to estimate its mean square error. One way is to use the approach of\\nbootstrap statistics, which we now present.\\nTo estimate the mean square error of the estimator d(X1,...,Xn) of the param-\\neter θ, suppose that the data values are Xi = xi, i = 1,...,n. For any x, let Fe(x)\\ndenote the proportion of the data values that are less than or equal to x. That\\nis,\\nFe(x) = number of i ≤n : xi ≤x\\nn\\nFor instance, if n = 5 and X1 = 5, X2 = 3, X3 = 9, X4 = 2, and X5 = 6, then\\nFe(x) =\\n⎧\\n⎪⎪⎪⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎪⎪⎪⎩\\n0\\nif\\nx < 2\\n1/5\\nif\\n2 ≤x < 3\\n2/5\\nif\\n3 ≤x < 5\\n3/5\\nif\\n5 ≤x < 6\\n4/5\\nif\\n6 ≤x < 9\\n1\\nif\\nx ≥9\\nThe function Fe(x) is called the empirical distribution function. When the values\\nx1,...,xn are all distinct, Fe is the distribution function of a random variable\\nXe that is equally likely to be any of the values x1,...,xn. That is, if the data val-\\nues are all distinct, then Fe is the distribution function of the random variable\\nXe such that\\nP(Xe = xi) = 1/n,\\ni = 1,...,n\\nWhen the data values are not all distinct, then Fe is the distribution function of\\nthe random variable Xe whose probability of being equal to any speciﬁed data\\nvalue is the number of times that value appears in the data set divided by n.\\nFor instance, if n = 3 and x1 = x2 = 1, x3 = 2 then Xe is a random variable that\\ntakes on the value 1 with probability 2/3 and 2 with probability 1/3. With this\\nunderstanding about the weight put on a distinct value, we will still say that Fe\\nis the distribution function of a random variable that is equally likely to be any\\nof the values x1,x2,...,xn.\\nNow, for any value of x, each of the data values Xi, i = 1,...,n, will be less than\\nor equal to x with probability F(x). Hence, by the strong law of large numbers'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 631}, page_content='15.3 The bootstrap method\\n625\\nit follows that the proportion of them that are less than or equal to x will, with\\nprobability 1, converge to F(x) as n goes to inﬁnity. Thus, for n large, Fe(x)\\nshould be close to F(x), indicating that the empirical distribution function Fe\\ncan be used as an estimator of the population distribution function F.\\nNow let θe have the same relationship to the distribution Fe as θ has to the\\ndistribution F. For instance, if θ is the variance of a random variable X having\\ndistribution F, then θe is the variance of a random variable Xe having distribu-\\ntion Fe. Now, if Fe is close to F, then it almost always follows that θe will be\\nclose to θ. (Technically speaking, this will be true provided that θ is a continu-\\nous function of the distribution F.) For these reasons we can approximate the\\nmean square error of the estimator d(X1,...,Xn) of θ as follows:\\nMSEF (d) = EF [(d(X1,...,Xn) −θ)2] ≈EFe[(d(X1,...,Xn) −θe)2]\\nwhere by EFe we mean that the expectation is to be taken under the assumption\\nthat X1,...,Xn are independent random variables, each having distribution\\nfunction Fe. That is, each of X1,...,Xn is equally likely to be any of the values\\nx1,...,xn.\\nThe quantity\\nMSEFe(d) = EFe[(d(X1,...,Xn) −θe)2]\\nis called the bootstrap estimate of the mean square error of d(X1,...,Xn) as an\\nestimator of θ.\\nLet us now see how well MSE(Fe) estimates MSE(F) in the one case where\\nits use as an estimator is not needed—namely, when estimating the mean of a\\ndistribution by the sample mean.\\nExample 15.3.b. Consider Example 15.3.a, where ¯Xn = \\x03n\\ni=1 Xi/n is used as\\nan estimator of the mean of the distribution F. Because Xe puts equal weight\\non each of the data values x1,...,xn, it follows, when θe is the mean of this\\ndistribution, that\\nθe = E[Xe] =\\nn\\n\\x04\\ni=1\\nxiP(Xe = xi) = 1\\nn\\nn\\n\\x04\\ni=1\\nxi = ¯xn\\nBecause\\nEFe[\\nn\\n\\x04\\ni=1\\nXi/n] = EFe[X] = θe = ¯xn'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 632}, page_content='626 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nit follows that\\nMSEFe( ¯Xn) = EFe[(\\nn\\n\\x04\\ni=1\\nXi/n −¯xn)2]\\n= VarFe(\\nn\\n\\x04\\ni=1\\nXi/n)\\n= 1\\nn VarFe(X)\\nNow,\\nVarFe(X) = EFe[(X −¯xn)2]\\n=\\nn\\n\\x04\\ni=1\\n(xi −¯xn)2PFe(X = xi)\\n= 1\\nn\\nn\\n\\x04\\ni=1\\n(xi −¯xn)2\\nThus, we have shown that\\nMSEFe( ¯Xn) = 1\\nn2\\nn\\n\\x04\\ni=1\\n(xi −¯xn)2\\nAs the usual estimator of MSEF ( ¯Xn) = 1\\nn VarF (X) is S2\\nn/n, whose observed\\nvalue is\\n1\\nn(n−1)\\n\\x03n\\ni=1(xi −¯xn)2, we see that the bootstrap estimate is almost\\nidentical to the usual estimate in this case.\\n■\\nAs previously noted, if the data values are Xi = xi, i = 1,...,n, then the em-\\npirical distribution function Fe puts equal weight 1/n on each of the points\\nxi; consequently, it is usually easy to compute the value of θe. To compute the\\nbootstrap estimate of the mean square error of the estimator d(X1,...,Xn) of\\nθ, we then have to compute\\nMSEFe(d) = EFe[(d(X1,...,Xn) −θe)2]\\nHowever, since the preceding expectation is to be computed under the as-\\nsumption that X1,...,Xn are all distributed according to Fe, it follows that\\nthe vector (X1,...,Xn) is equally likely to be any of the nn possible values\\n(xi1,xi2,...,xin), where each ij is one of the values 1,...,n. Consequently an\\nexact computation of MSEFe(d) is prohibitive unless n is small.\\nIt is, however, easy to approximate MSEFe(d) by a simulation. To do so, we\\ngenerate n independent random variables X1\\n1,...,X1\\nn having distribution Fe'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 633}, page_content='15.3 The bootstrap method\\n627\\nand use them to compute the value of\\nY1 = (d(X1\\n1,...,X1\\nn) −θe)2\\nWe then repeat this process and generate a second set of n independent random\\nvariables X2\\n1,...,X2\\nn having distribution Fe and use them to compute the value\\nof\\nY2 = (d(X2\\n1,...,X2\\nn) −θe)2\\nThis is then repeated a large number of times, say r, to obtain the values\\nY1,...,Yr. The average of these values, \\x03r\\ni=1 Yi/r, would be the approximation\\nof MSEFe(d), which would then be used as the estimate of MSEF (d).\\nRemark. It is easy to generate a random variable X having distribution Fe. Just\\ngenerate a random number U; let I = Int(nU) + 1, so that I is equally likely to\\nbe any of the values 1,...,n; and then set\\nX = xI\\nExample 15.3.c. Suppose we use the sample variance S2\\nn = \\x03n\\ni=1(Xi −\\n¯Xn)2/(n −1) of a sample of size n from the distribution F as an estimator\\nof σ 2, the variance of the distribution F. To estimate the mean square error of\\nthe sample variance, let the observed data be Xi = xi, i = 1,...,n.\\nBecause the distribution Fe puts equal weight on all of the values xi,i =\\n1,...,n, it follows that\\nEFe[X] =\\nn\\n\\x04\\ni=1\\nxiPFe(X = xi) =\\nn\\n\\x04\\ni=1\\nxi/n = ¯xn\\nshowing that θe, the variance of the distribution Fe, is given by\\nθe = VarFe(X) = EFe[(X −¯xn)2] =\\nn\\n\\x04\\ni=1\\n(xi −¯xn)2/n\\nConsequently,\\nMSEFe(S2\\nn) = EFe[(S2\\nn −θe)2] = EFe\\n\\t\\n\\x03n\\ni=1(Xi −¯Xn)2\\nn −1\\n−θe\\n\\x0b2\\x0c\\nTo approximate MSEFe(S2\\nn), we use simulation.\\nFor instance, suppose n = 8 and the data values were x1 = 5, x2 = 9, x3 = 12,\\nx4 = 8, x5 = 7, x6 = 15, x7 = 3, x8 = 6. Then\\n¯x8 = 8.125,\\nθe =\\n8\\n\\x04\\ni=1\\n(xi −¯x8)2/8 ≈13.11'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 634}, page_content='628 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nIn the following approach for the simulation-based approximation of\\nMSEFe(S2\\nn), the xi,i = 1,...,8 are as given in the preceding. There are to be\\na total of r simulation runs, with the variable N representing the number of\\nthe current simulation run. In each run we generate the values of 8 random\\nvariables XM, M = 1,...,8, distributed according to Fe. The quantities S and\\nSS represent running totals of, respectively, the sum of the XM and the sum of\\nthe squares of the XM so far generated in the run. When the run is completed,\\nthe sample variance SV is computed by using the identity\\n\\x038\\ni=1(Xi −¯X8)2\\n7\\n=\\n\\x038\\ni=1 X2\\ni −8 ¯X2\\n8\\n7\\n=\\n\\x038\\ni=1 X2\\ni −(\\x038\\ni=1 Xi)2/8\\n7\\n= SS −S2/8\\n7\\nThe squared difference between SV and θe = 13.11 is computed and then added\\nto T , the sum of the N −1 previous squared differences. When r runs have\\nbeen completed the simulation is ended; the average of the squared differences\\nbetween the sample variances and θe is the simulation-based approximation to\\nMSEFe(S2\\nn).\\n1. Let T = 0, N = 1\\n2. Let M = 1\\n3. S = 0, SS = 0\\n4. Generate a random number U\\n5. Set I = Int(8U) + 1\\n6. S = S + xI\\n7. SS = SS + x2\\nI\\n8. If M < 8, set M = M + 1 and go to 4\\n9. SV = (SS −S2/8)/7\\n10. Let T = T + (SV −13.11)2\\n11. If N < r, set N = N + 1 and go to 2\\n12. If N = r, return T /r as the approximation to MSEFe(S2\\nn)\\n■\\nNow suppose we wanted to estimate not the mean square error of the estimator\\nbut rather the probability that the estimator of θ will be within h of the actual\\nvalue of θ. That is, suppose we want to estimate\\nph ≡PF (|d(X1,...,Xn) −θ| ≤h)\\nTo obtain an estimator of the preceding, we use that\\nPF (|d(X1,...,Xn) −θ| ≤h) ≈PFe(|d(X1,...,Xn) −θe| ≤h)\\nand then employ simulation to estimate the right side of the preceding. That\\nis, after the data X1,...,Xn are observed to take on the values Xi = xi, i =\\n1,...,n, we let Fe be the empirical distribution. That is, Fe is the distribution\\nfunction of a random variable that is equally likely to take on any of the values\\nx1,...,xn. We next compute the value of θe. We then continually generate sets'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 635}, page_content='15.3 The bootstrap method\\n629\\nof n independent random variables from the distribution Fe. For each set of\\nvalues obtained, we compute d evaluated at these values, and check whether\\nthis quantity is within h of θe. The fraction of times that it is within h is our\\nsimulation-based estimate of\\nPFe(|d(X1,...,Xn) −θe| ≤h)\\nand is also what we use to estimate ph.\\nMore speciﬁcally, we use the original data to obtain Fe and the resulting value\\nof θe. We then decide on the number of simulation runs (typically between\\n104 and 105 will sufﬁce, but see Section 15.7 for speciﬁcs on how to deter-\\nmine the number of runs that should be performed). With r runs, we need to\\ngenerate r sets of n independent random variables from the distribution Fe.\\nWith xi,1,...,xi,n being the ith set of values generated, we compute the value\\nof di = d(xi,1,...,xi,n). The proportion of the values of i,i = 1,...,r, for which\\n|di −θe| ≤h is our estimate of ph ≡PF (|d(X1,...,Xn) −θ| ≤h).\\nExample 15.3.d. The following are the PSAT math scores of a random sample\\nof 16 students from a certain school district.\\n522,474,644,708,466,534,422,480,502,655,418,464,600,412,530,564\\nUse them to estimate\\n(a) the average score of all students in the district;\\n(b) the probability that the estimator of the district average will be within 5\\nof the actual district average;\\n(c) the probability that the estimator of the district average will be within 10\\nof the actual district average.\\nSolution. We suppose that the data constitute a random sample from a distri-\\nbution F with mean θ(F) = μ. The natural estimator of μ is the sample average\\n¯X, yielding the estimate\\nθe = ¯x = 524.7\\nThe probability ph, that the sample mean of a sample of size 16 will be within\\nh of the population mean, is estimated by\\nPFe(| ¯X16 −θe| ≤h) = PFe(| ¯X16 −524.7| ≤h)\\nwhere ¯X16 is the average of a sample of size 16 from the distribution that puts\\nprobability 1/16 on each of the original 16 data values. A simulation based on\\n105 simulation runs — with each run generating a sample of size 16 from Fe\\n— yielded the estimates .1801 and .3542 for h = 5 and h = 10, respectively.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 636}, page_content='630 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nBecause we are estimating the mean of the distribution by the sample average,\\nwe could also approximate the probability ph by making use of the central\\nlimit theorem. With μ and σ being the mean and standard deviation of F,\\nthe probability that the sample mean of a sample of size 16 is within h of μ\\ncan be approximated by using the fact that ¯X16 approximately has a normal\\ndistribution with mean μ and variance σ 2/16. Consequently, with Z being a\\nstandard normal random variable\\nP(−h ≤¯X16 −μ ≤h) = P( −h\\nσ/4 ≤\\n¯X16 −μ\\nσ/4\\n≤\\nd\\nσ/4)\\n≈P(−4h/σ ≤Z ≤4h/σ)\\n= 2\\x04(4h/σ) −1\\nAn easy calculation gives that the sample standard deviation of the 16 data\\nvalues is s = 89.14. Taking this value as an approximation of σ yields that\\n2\\x04(4h/σ) −1 ≈2\\x04(4h/89.1) −1\\nThus, the estimate of the probability that the sample mean is within 5 of\\nthe population mean is 2\\x04(.2245) −1 = .1776, whereas the estimate that it\\nis within 10 of the population mean is 2\\x04(.4490) −1 = .3466, which are quite\\nclose to the ones obtained by the nonparametric bootstrap approach. However,\\nit should be noted that the central limit theorem approximation would not be\\navailable to us if we were estimating some other parameter of the distribution\\naside from its mean.\\nFor instance, suppose we wanted to use the 16 data values to estimate σ, the\\nstandard deviation of the scores of all the students in the district. Using the\\nsample standard deviation as the estimator yields the estimate s = 89.1. Now\\nsuppose we wanted to estimate the probability that our estimator will be within\\n10 of σ. That is, suppose we wanted to estimate the probability that the sam-\\nple standard deviation of a sample of size 16 from the distribution F will be\\nwithin 10 of the actual standard deviation of F. To do so, we estimate this\\nby the probability that the sample standard deviation of a sample of size 16\\nfrom the empirical distribution Fe is within 10 of the standard deviation of Fe.\\nNow, because the distribution Fe puts equal weight on each of the 16 values\\nx1,...,x16, its mean is ¯x and its standard deviation is\\nσe =\\n\\r\\nEFe[(X −¯x)2] =\\n\\x0e\\n\\x0f\\n\\x0f\\n\\x10 1\\n16\\n16\\n\\x04\\ni=1\\n(xi −¯x)2 = 89.1\\n\\x11\\n15/16 = 86.27\\nConsequently the estimate of the probability that the sample standard devia-\\ntion differs from σ by at most 10 is\\nPFe(|S16 −σe| ≤10) = PFe(|S16 −86.27| ≤10)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 637}, page_content='15.4 Permutation tests\\n631\\nwhere S16 is the sample standard deviation of a sample of size 16 from the\\ndistribution Fe. This probability can be approximated by a simulation. Indeed,\\na simulation performed with 105 runs yielded the result\\nPFe(|S16 −86.27| ≤10) ≈.5424\\nso there is roughly a 54 percent chance that the actual standard deviation of all\\nstudent scores is within 79.1 and 99.1.\\n■\\n15.4\\nPermutation tests\\nSuppose we want to test the null hypothesis H0 that the data X1,...,XN is a\\nsample from some unspeciﬁed distribution. Permutation tests are tests of this hy-\\npothesis in which the p-value is computed conditional on knowing the set S of\\ndata values observed but without knowing which data value corresponds to X1,\\nwhich corresponds to X2, and so on. For instance, if N = 3 and X1 = 5, X2 = 7,\\nX3 = 2, then the p-value is computed conditional on the information that the\\nset of data values is S = {2,5,7}. The computation of the p-value makes use of\\nthe fact that, conditional on the set of data values S, each of the N! possible\\nways of assigning these N values to the original data is equally likely when the\\nnull hypothesis is true. That is, suppose that N = 3 and the set of data values\\nis, as in the preceding, S = {2,5,7}. Now the null hypothesis H0 states that X1,\\nX2, X3 are independent and identically distributed. Consequently, if H0 is true\\nthen, given the data set S, it follows that the vector (X1,X2,X3) is equally likely\\nto equal any of the 3! permutations of the values 2, 5, 7.\\nThe implementation of a permutation test is as follows. Depending on the\\nalternative hypothesis, a test statistic T (X1,...,XN) is chosen. Suppose, for the\\nmoment, that large values of the test statistic are evidence for the alternative\\nhypothesis. The data values are then observed, say that Xi =xi,i =1,...,N,\\nand the value of T (x1,...,xN) is calculated. Now let S = {x1,...,xN} be the\\nunordered set consisting of the N observed values. Then, if the value of the test\\nstatistic is T (x1,...,xN) = t, the resulting p-value of the null hypothesis that\\nresults from these data is\\np-value = PH0(T (X1,...,XN) ≥t|S = {x1,...,xN})\\nNow, under H0, X1,...,XN is equally likely to equal any of the N! permuta-\\ntions of x1,...,xN. Consequently, letting I1,...,IN be a random vector that is\\nequally likely to be any of the N! permutations of 1,...,N, we can write the\\npreceding p-value as\\np-value = P{T (xI1,xI2,...,xIN ) ≥t}\\n= number of permutations (i1,...,iN) : T (xi1,xi2,...,xiN ) ≥t\\nN!'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 638}, page_content='632 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nFor an illustration, suppose we are to observe data over N weeks, with Xi being\\nthe data value observed in week i,i = 1,...,N, and that we want to use these\\ndata to test the null hypothesis\\nH0 : X1,...,XN are independent and identically distributed\\nagainst\\nH1 : Xi tends to increase as i increases\\nNow if the null hypothesis is true and the data are independent and identically\\ndistributed, then, conditional on knowing the set of values X1,...,XN, but\\nnot knowing which value corresponds to X1 or which corresponds to X2 and\\nso on, the statistic \\x03N\\nj=1 jXj would be distributed as if we randomly paired up\\nthe two data sets {1,...,N} and {X1,...,XN} and then summed the products\\nof the N paired values. On the other hand, if the alternative hypothesis were\\ntrue, then \\x03N\\nj=1 jXj would tend to be larger than if we just randomly paired\\nthe values 1,...,N with the values X1,...,XN and then summed the products\\nof the N pairs. This is because the sum of the paired values of two sets of\\nequal size is largest when the largest values are paired with each other, the\\nsecond largest are paired with each other, and so on. (In statistical terms the\\ncorrelation coefﬁcient of data pairs (j,Xj),j = 1,...,N is large when the Xj\\ntend to increase as j increases.) Consequently, one possible permutation test\\nof H0 versus H1 is to\\n1. Observe the data values—say that Xj = xj, j = 1,...,N\\n2. Let t = \\x03N\\nj=1 jxj\\n3. Determine the p-value given by\\np-value = P(\\nN\\n\\x04\\nj=1\\nIjxj ≥t)\\nwhere I1,...,IN is equally likely to be any of the N! permutations of\\n1,...,N.\\nThe p-value in the preceding can be approximated by a simulation that uses\\nthe method of Example 15.2.b to generate random permutations.\\nExample 15.4.a. To determine if the weekly sales of DVD players is on a down-\\nward trend, the manager of a large electronics store has been tracking such sales\\nfor the past 12 weeks, with the following sales ﬁgures from week 1 to week 12\\n(the current week) resulting:\\n22,24,20,18,16,14,15,15,13,17,12,14\\nAre the data strong enough to reject the null hypothesis that the distribution of\\nsales is unchanging in time, and so enable the manager to conclude that there\\nis a downward trend in sales?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 639}, page_content='15.4 Permutation tests\\n633\\nSolution. Let the null hypothesis be that the distribution of sales is unchanged\\nover time, and let the alternative hypothesis be that there is a downward trend\\nin sales. Thus, if the alternative hypothesis is true then there would be a neg-\\native correlation between Xj, the sales during week j, and j. So a relatively\\nsmall value of \\x0312\\nj=1 jXj would be evidence in favor of the alternative hypoth-\\nesis. Now, with xj equal to the observed value of Xj, the sales data give that\\n12\\n\\x04\\nj=1\\njxj = 1178\\nHence, the p-value of the permutation test of the null hypothesis that the data\\ncome from the same distribution versus the alternative that the data tend to be\\ndecreasing in time is given by\\np-value = P\\n⎛\\n⎝\\n12\\n\\x04\\nj=1\\nIjxj ≤1178\\n⎞\\n⎠\\nwhere I1,...,I12 is equally likely to be any of the 12! permutations of 1,...,12.\\nA simulation, using 105 runs, yielded that\\np-value ≈.00039\\nleading us to reject the null hypothesis that the distribution is unchanging over\\ntime.\\n■\\nAlthough \\x03N\\nj=1 jXj is the test statistic most commonly used to test the null\\nhypothesis that X1,...,Xn are independent and identically distributed against\\nthe alternative that Xj tends to increase as j increases, it is not the only possi-\\nbility. Indeed, we could have chosen any test statistic of the form \\x03N\\nj=1 ajXj,\\nwhere a1 <a2 < ... <an. (For instance, we could have chosen aj =j2.) Analo-\\ngous to the preceding, the value of the statistic would ﬁrst be determined, say it\\nis t. Because the alternative hypothesis will tend to make \\x03N\\nj=1 ajXj larger than\\nit would be under the null hypothesis — since large values of the aj would tend\\nto be paired with large data values when the alternative hypothesis is true — we\\nwould again want to reject the null hypothesis when t is large. Consequently,\\nthe resulting p-value would be\\np-value = P\\n\\x16 N\\n\\x04\\nj=1\\naIj xj ≥t\\n\\x17\\nwhere I1,...,IN is equally likely to be any of the N! permutations of 1,...,N.\\nDepending on the alternative hypothesis, we could choose other constants\\naj,j = 1,...,N to test the null hypothesis that the data values are indepen-\\ndent and identically distributed. For instance, if the alternative was that the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 640}, page_content='634 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\ndata tended to be higher in the middle values and lower in the extremes, then\\nwe could let the test statistic be of the form T = \\x03N\\nj=1 ajXj, where a1,...,aN is\\nsuch that its middle values tend to be larger than its earlier or later values. For\\ninstance, we could use aj = j(N −j), j = 1,...,N. As this would again make\\nit more likely that larger data values are paired with larger constants when the\\nalternative hypothesis is true, we would again want to reject the null hypothesis\\nwhen T is large.\\n15.4.1\\nNormal approximations in permutation tests\\nAlthough not as accurate as doing a simulation, the p-value of a permutation\\ntest can be approximated by assuming that the test statistic is approximately\\nnormally distributed. Now, under the null hypothesis that the data values are\\nindependent and identically distributed, it follows that, given the data set S =\\n{x1,...,xN}, the random variable Xi is equally likely to be any of these N values\\nand the random vector (Xi,Xj),i ̸= j is equally likely to take on any of the\\nN(N −1) values xk, xr, r ̸= k. Consequently, given S = {x1,...,xN},\\nE[Xi] = 1\\nN\\nN\\n\\x04\\ni=1\\nxi = ¯x\\nE[X2\\ni ] = 1\\nN\\nN\\n\\x04\\ni=1\\nx2\\ni\\nE[XiXj] =\\n1\\nN(N −1)\\n\\x04\\nk\\n\\x04\\nr̸=k\\nxk xr\\n=\\n1\\nN(N −1)\\n\\n\\x04\\nk\\n\\x04\\nr\\nxk xr −\\n\\x04\\nk\\n\\x04\\nr=k\\nxk xr\\n\\x0b\\n=\\n1\\nN(N −1)\\n\\n\\x04\\nk\\nxk\\n\\x04\\nr\\nxr −\\n\\x04\\nk\\nx2\\nk\\n\\x0b\\n=\\n1\\nN(N −1)\\n\\nN2 ¯x2 −\\nN\\n\\x04\\nk=1\\nx2\\nk\\n\\x0b\\nSo, with v = Var(Xi) and c = Cov(Xi,Xj), i ̸= j, the preceding yields\\nE[Xi] = ¯x\\nv = Var(Xi) = 1\\nN\\nN\\n\\x04\\ni=1\\nx2\\ni −¯x2\\nc = Cov(Xi,Xj) =\\n1\\nN(N −1) (N2 ¯x2 −\\nN\\n\\x04\\nk=1\\nx2\\nk) −¯x2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 641}, page_content='15.4 Permutation tests\\n635\\n=\\n¯x2\\nN −1 −\\n1\\nN(N −1)\\nN\\n\\x04\\nk=1\\nx2\\nk\\n=\\n1\\nN −1 (¯x2 −\\nN\\n\\x04\\nk=1\\nx2\\nk/N)\\nwhich also shows that\\nv −c =\\n\\x03N\\ni=1 x2\\ni −N ¯x2\\nN −1\\nTherefore, when H0 is true, the test statistic T = \\x03N\\nj=1 jXj has mean\\nE[T ] = N(N + 1)\\n2\\n¯x\\nand variance\\nVar(T ) = Var\\n\\n N\\n\\x04\\nj=1\\njXj\\n\\x0b\\n=\\nN\\n\\x04\\nj=1\\nVar(jXj) +\\n\\x04\\ni\\n\\x04\\nj̸=i\\nCov(iXi,jXj)\\n= v\\nN\\n\\x04\\nj=1\\nj2 + c\\n\\x04\\ni\\n\\x04\\nj̸=i\\nij\\n= v\\nN\\n\\x04\\nj=1\\nj2 + c\\n⎛\\n⎝\\x04\\ni\\n\\x04\\nj\\nij −\\n\\x04\\ni\\n\\x04\\nj=i\\nij\\n⎞\\n⎠\\n= v\\nN\\n\\x04\\nj=1\\nj2 + c\\n⎛\\n⎝\\nN\\n\\x04\\ni=1\\ni\\nN\\n\\x04\\nj=1\\nj −\\nN\\n\\x04\\ni=1\\ni2\\n⎞\\n⎠\\n= (v −c)\\nN\\n\\x04\\nj=1\\nj2 + cN2(N + 1)2\\n4\\n= (v −c) N(N + 1)(2N + 1)\\n6\\n+ cN2(N + 1)2\\n4\\nUsing the preceding we can approximate the p-value of a permutation test by\\nassuming that the distribution of T , when H0 is true, is approximately normal.\\nExample 15.4.b. Again consider Example 15.4.a. A calculation yields that, un-\\nder H0,\\nE[T ] = 1300\\nVar(T ) = 1958.81'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 642}, page_content='636 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nThus, the normal approximation yields that\\np-value = PH0(T ≤1178)\\n= PH0( T −1300\\n√\\n1958.81\\n≤1178 −1300\\n√\\n1958.81\\n)\\n≈\\x04(−2.757)\\n= .0029\\nwhich is quite close to the value given by the simulation.\\nLet us now suppose that whereas the set of 12 data values was as before, they\\nnow appeared in the order\\n22,14,14,16,24,20,18,15,17,15,12,13\\nWith these data, the value of the test statistic is \\x0312\\nj=1 j Xj = 1233, and the\\nnormal approximation yields that\\np-value = PH0(T ≤1233)\\n= PH0( T −1300\\n√\\n1958.81\\n≤1233 −1300\\n√\\n1958.81\\n)\\n≈\\x04(−1.514)\\n= .065\\nFinally, suppose again that the set of 12 data values was as before, but suppose\\nthat they now appeared in the order\\n22,14,14,16,24,13,18,15,17,15,12,20\\nIn this case, the value of the test statistic is \\x0312\\nj=1 jXj = 1275. Thus, the normal\\napproximation yields that\\np-value = PH0(T ≤1275)\\n= PH0\\n\\n T −1300\\n√\\n1958.81\\n≤1275 −1300\\n√\\n1958.81\\n\\x0b\\n≈\\x04(−.565)\\n≈.286\\nA simulation of 105 runs yielded values quite similar to the preceding. The\\nsimulation gave\\nPH0(T ≤1233) ≈.068\\nand\\nPH0(T ≤1275) ≈.299'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 643}, page_content='15.4 Permutation tests\\n637\\nwhich are quite close to the values given by the normal approximation.\\n■\\nExample 15.4.c. For another indication as to the validity of a normal approx-\\nimation, suppose that N = 4, with the data appearing in the following order:\\n13,7,5,3\\nSuppose that we want to use these data to test the null hypothesis that the data\\nare a sample from some distribution against the alternative hypothesis that the\\ndata tend to be decreasing. The value of the test statistic is T = \\x03r\\nj=1 jXj = 54.\\nAn easy computation yields\\nc = −4.667,\\nv = 14\\nshowing that, under H0,\\nE[T ] = 70,\\nVar(T ) = 93.33\\nConsequently, with Z being a standard normal random, the normal approxi-\\nmation yields that\\np-value = PH0(T ≤54)\\n= PH0\\n\\n T −70\\n√\\n93.33\\n≤54 −70\\n√\\n93.33\\n\\x0b\\n≈P(Z ≤−1.656)\\n= .049\\nwhereas the exact value is\\np-value = PH0(T ≤54) = 1/4! ≈.042\\n■\\n15.4.2\\nTwo-sample permutation tests\\nPermutation tests are also useful in the two-sample problems where we test\\nwhether samples from two populations have the same underlying distribution.\\nSpeciﬁcally, let X1,...,Xn be a sample from an unknown population distribu-\\ntion F, and let Xn+1,...,Xn+m be an independent sample from an unknown\\npopulation distribution G, and suppose we want to use these data to test the\\nhypothesis that the two population distributions are identical against the al-\\nternative hypothesis that data from the second distribution tend to be larger\\nthan those from the ﬁrst. That is, we want to use these data to test the null\\nhypothesis\\nH0 : F = G'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 644}, page_content='638 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nagainst the alternative\\nH1 : data from G tend to be larger than data from F\\nIf the data values are Xi = xi, i = 1,...,n + m, then a permutation test of the\\npreceding null hypothesis is done conditional on knowing S = {x1,...,xn+m},\\nthe set of these n + m numbers in no particular order. Then if H0 is true, and\\nso all n + m random variables X1,...,Xn+m are independent and identically\\ndistributed, then given the set of values S, each subset of size n of this set is\\nequally likely to be the set of the data values of X1,...,Xn. Because the alter-\\nnative hypothesis is that data from the population distribution F tend to be\\nsmaller than data from the population distribution G, a reasonable test would\\nbe to reject the null hypothesis if the sum of the data values from the popula-\\ntion distribution F is smaller than might be expected by chance when n values\\nare randomly chosen from the data set S. More speciﬁcally, we can test H0 by\\ncomputing \\x03n\\ni=1 xi; say its value is t. Then the p-value of this permutation test\\nof H0 versus H1 would equal the probability that a random selection of n of\\nthe values x1,...,xn+m would be less than or equal to t. That is,\\np-value = P\\n\\x16\\x04\\ni∈R\\nxi ≤t\\n\\x17\\nwhere R is equally likely to be any of the\\n\\x18n+m\\nn\\n\\x19\\nsubsets of size n from the\\nset {1,2,...,n + m}. Whereas an exact computation of the preceding is possi-\\nble only when\\n\\x18n+m\\nn\\n\\x19\\nis small, a precise approximation is easily obtained by\\nsimulation. In each simulation run we use the method of Example 15.2.a to\\nrandomly generate a subset of n of the values 1,...,n + m. If R is the subset\\nobtained, then we check whether \\x03\\ni∈R xi is less than or equal to t. The fraction\\nof simulation runs for which this is the case is our estimate of the preceding\\np-value.\\nRemark. We can again use a normal approximation, rather than a simulation,\\nto estimate the p-value. Starting with the random subset R, equally likely to\\nbe any of the\\n\\x18n+m\\nn\\n\\x19\\nsubsets of size n from the set {1,2,...,n + m}, let, for\\ni = 1,...,n + m,\\nIi =\\n\\x1a\\n1,\\nif i ∈R\\n0,\\nif i /∈R\\nThen,\\n\\x04\\ni∈R\\nxi =\\nn+m\\n\\x04\\ni=1\\nxiIi'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 645}, page_content='15.5 Generating discrete random variables\\n639\\nBy a similar analysis as in Section 15.4.1, we can now show that\\nEH0\\n\\t\\x04\\ni∈R\\nxi\\n\\x0c\\n= EH0\\n\\t n+m\\n\\x04\\ni=1\\nxiIi\\n\\x0c\\n= n¯x\\nand\\nVarH0\\n\\n\\x04\\ni∈R\\nxi\\n\\x0b\\n= VarH0\\n\\n n+m\\n\\x04\\ni=1\\nxiIi\\n\\x0b\\n=\\nnm\\nn + m −1\\n\\n\\x03n+m\\ni=1 x2\\ni\\nn + m\\n−¯x2\\x0b\\nwhere ¯x = \\x03n+m\\ni=1 xi/(n + m).\\n15.5\\nGenerating discrete random variables\\nSuppose we want to generate the value of a random variable X having proba-\\nbility mass function\\nP(X = xi) = pi,i = 1,...,\\n\\x04\\ni\\npi = 1\\nTo generate the value of X, generate a random number U and set\\nX = xi\\nif\\np1 + ...pi−1 < U ≤p1 + ...pi−1 + pi\\nThat is,\\nX =\\n⎧\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\\nx1,\\nif U ≤p1\\nx2,\\nif p1 < U ≤p1 + p2\\nx3,\\nif p1 + p2 < U ≤p1 + p2 + p3\\n.\\n.\\n.\\nxi,\\nif p1 + ... + pi−1 < U ≤p1 + ... + pi−1 + pi\\n.\\n.\\n.\\nBecause U is uniformly distributed on (0,1), it follows that for 0 < a < b < 1\\nP(a < U ≤b) = b −a\\nConsequently,\\nP\\n⎛\\n⎝\\ni−1\\n\\x04\\nj=1\\npj < U ≤\\ni\\n\\x04\\nj=1\\npj\\n⎞\\n⎠= pi'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 646}, page_content='640 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nwhich shows that X has the desired probability mass function. This method of\\ngenerating X is called the discrete inverse transform method.\\nExample 15.5.a. To generate a Bernoulli random variable X such that\\nP(X = 1) = p = 1 −P(X = 0)\\ngenerate a random number U, and set\\nX =\\n\\x1b 1,\\nif U ≤p\\n0,\\nif U > p.\\n■\\nExample 15.5.b. Suppose now that we wanted to generate a binomial random\\nvariable X with parameters n and p. Recalling that X represents the number of\\nsuccesses in n independent trials when each trial is a success with probability\\np, we can generate X by generating the results of the n trials. That is, we can\\ngenerate n random numbers U1,...,Un, say that trial i is a success if Ui ≤p,\\nand then set\\nX = number of i : Ui ≤p\\nAnother possibility is to use the inverse transform method.\\nTo efﬁciently use the inverse transform method we need an efﬁcient method to\\nrecursively compute the values\\npi = P(X = i) =\\n\\x1cn\\ni\\n\\x1d\\npi(1 −p)n−i , i = 0,...,n\\nThis is accomplished by ﬁrst noting that\\n\\x18 n\\ni+1\\n\\x19\\n\\x18n\\ni\\n\\x19 =\\nn!\\n(n −i −1)!(i + 1)!\\n(n −i)!i!\\nn!\\n= n −i\\ni + 1\\nwhich yields that\\npi+1\\npi\\n= n −i\\ni + 1\\npi+1(1 −p)n−i−1\\npi(1 −p)n−i\\n= n −i\\ni + 1\\np\\n1 −p\\nThus,\\npi+1 = n −i\\ni + 1\\np\\n1 −p pi'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 647}, page_content='15.6 Generating continuous random variables\\n641\\nUsing the preceding, we are now ready to give the inverse transform method for\\ngenerating a binomial (n,p) random variable X. In the following, i represents\\nthe possible value of X, the variable P is the probability that X = i, and the\\nvariable F is the probability that X ≤i. (That is, for given i, P = pi and F =\\n\\x03i\\nj=0 pj.) Also, let α = p0 = (1 −p)n, and let b =\\np\\n1−p.\\n1. Set i = 0, P = α, F = α\\n2. Generate a random number U\\n3. If U ≤F set X = i and stop\\n4. P = n−i\\ni+1 b P\\n5. F = F + P\\n6. i = i + 1\\n7. Go to 3\\n(In the preceding, when we say that P = n−i\\ni+1 b P, we don’t mean this literally\\nas an algebraic identity; rather we mean that the value of P is to be changed.\\nIts new value is its old value multiplied by\\nn−i\\ni+1 b. Similarly, when we write\\nF = F + P we mean that the value of F is to be changed by adding P to its old\\nvalue.)\\nBecause the algorithm ﬁrst checks whether X = 0, then whether X = 1, and so\\non, it follows that the number of iterations needed (that is, the number of times\\nthat it goes to step 3) is one more than the ﬁnal value of X. So, on average, this\\nalgorithm requires E[X + 1] = np + 1 iterations to generate the value of X.\\n■\\n15.6\\nGenerating continuous random variables\\nLet F be the distribution function of a continuous random variable. For any\\nu between 0 and 1, the quantity F −1(u) is deﬁned to be that value x such\\nthat F(x) = u. That is, F(F −1(u)) = u. Because the distribution function of\\na continuous random variable is strictly increasing, it follows that there is a\\nunique value of F −1(u). We call F −1 the inverse function of F.\\nA general method for generating a continuous random variable having distri-\\nbution function F, known as the inverse transformation method, is based on the\\nfollowing proposition.\\nProposition 15.6.1. Let U be a uniform (0,1) random variable. For any con-\\ntinuous distribution function F, if we deﬁne\\nX = F −1(U)\\nthen X has distribution function F.\\nProof. Because a distribution function F is nondecreasing, it follows that for\\nany numbers a and b the inequality a ≤b is equivalent to the inequality F(a) ≤'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 648}, page_content='642 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nF(b). Consequently,\\nP(F −1(U) ≤x) = P(F(F −1(U)) ≤F(x))\\n= P(U ≤F(x))\\n= F(x)\\nthus showing that F −1(U) has distribution F.\\n■\\nExample 15.6.a (Generating an Exponential Random Variable). Let\\nF(x) = 1 −e−λx ,\\nx ≥0\\nbe the distribution function of an exponential random variable with parameter\\nλ. Then F −1(u) is that value x such that\\nu = F(x) = 1 −e−λx\\nor, equivalently,\\ne−λx = 1 −u\\nor\\n−λx = log(1 −u)\\nor\\nx = −1\\nλ log(1 −u)\\nSo, by Proposition 15.6.1, we can generate an exponential random variable X\\nwith parameter λ by generating a uniform (0,1) random variable U and setting\\nX = −1\\nλ log(1 −U)\\nBecause 1 −U is also a uniform (0,1) random variable, it follows that\\n−1\\nλ log(1 −U) and −1\\nλ log(U) have the same distribution, thus showing that\\nX = −1\\nλ log(U)\\nis also exponential with parameter λ.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 649}, page_content='15.6 Generating continuous random variables\\n643\\n15.6.1\\nGenerating a normal random variable\\nBecause inverting the distribution function of a normal random variable is\\ncomputationally involved, special methods are used for generating normal ran-\\ndom variables. The following one is known as the Box-Muller method.\\nTo begin, suppose that X and Y are independent standard normal random\\nvariables, so their joint density function is\\nf (x,y) =\\n1\\n√\\n2π\\ne−x2/2\\n1\\n√\\n2π\\ne−y2/2 = 1\\n2π e−(x2+y2)/2 ,\\n−∞< x,y < ∞\\nLet R, \\x08 be the polar coordinates of the point (X,Y). Now R2 = X2 + Y 2 is,\\nby deﬁnition, a chi-square random variable with 2 degrees of freedom, and as\\nshown in Section 5.8.1 this distribution is the same as an exponential distri-\\nbution with parameter 1/2 (that is, with mean 2). Consequently, the density\\nfunction of R2 is\\nfR2(r) = 1\\n2 e−r/2 ,\\n0 < r < ∞\\nConsider now the conditional joint density function of X, Y given that R2 = r.\\nBecause\\nf (x,y) = 1\\n2π e−r/2\\nwhen\\nx2 + y2 = r\\nis a constant when x2 + y2 = r, it is intuitive (and can be proven) that condi-\\ntional on R2 = r, the vector X, Y is uniformly distributed on the circumference\\nof the circle of radius √r. But this implies that, conditional on R2 = r, the\\npolar coordinate \\x08 of the point (X,Y) is uniformly distributed over (0,2π).\\nBecause this is true for all r, it follows that the polar coordinates R and \\x08 are\\nindependent, with R distributed as the square root of an exponential random\\nvariable with mean 2, and \\x08 being a uniform random variable on (0,2π).\\nUsing the preceding, we can generate independent standard normal random\\nvariables X and Y by ﬁrst generating their polar coordinates R and \\x08. Because\\n−log(U) is exponential with mean 1, we can generate the polar coordinates of\\n(X,Y) by generating independent uniform (0,1) random variables U1 and U2\\nand then setting\\nR2 = −2log(U1)\\nand\\n\\x08 = 2πU2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 650}, page_content='644 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nUsing the formula for going from the polar coordinates R, \\x08 back to the rect-\\nangular coordinates\\nX = R cos(\\x08),\\nY = R sin(\\x08)\\nshows that\\nX =\\n\\x11\\n−2log(U1)cos(2πU2)\\nY =\\n\\x11\\n−2log(U1)sin(2πU2)\\nare independent standard normal random variables.\\nTo generate normal random variables with mean μ and variance σ 2, just gen-\\nerate the independent standard normals X and Y and then take the variables\\nμ + σX and μ + σY.\\n15.7\\nDetermining the number of simulation runs\\nin a Monte Carlo study\\nSuppose we are going to generate r independent and identically distributed\\nrandom variables Y1,...,Yr having mean μ, so as to use\\n¯Yr =\\nr\\n\\x04\\ni=1\\nYi/r\\nas an estimator of μ. Now, with σ 2 being the variance of the Yi, it follows by\\nthe central limit theorem that ¯Yr will approximately have a normal distribution\\nwith mean μ and variance σ 2/r. Consequently, we can be 95 percent certain\\nthat μ will lie in the interval\\n( ¯Yr −1.96σ/√r,\\n¯Yr + 1.96σ/√r).\\n(More generally, we can be 100(1 −α) percent conﬁdent that μ will be between\\n¯Yr ± zα/2 σ/√r.)\\nThus, if σ 2 were known we could choose r to give ourselves the desired level of\\naccuracy. However, it is almost always the case that σ 2, like μ, will be unknown.\\nTo get around this difﬁculty, we can do a two-stage simulation experiment. In\\nthe ﬁrst stage, we generate k runs where k is typically much smaller than the\\nnumber we expect to use in the study. Doing these runs generates the values\\nof the random variables Y1,...,Yk. We then use the sample variance of these\\nvalues,\\nS2\\nk =\\n1\\nk −1\\nk\\n\\x04\\ni=1\\n(Yi −¯Yk)2'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 651}, page_content='Problems\\n645\\nto estimate σ 2. Then, acting as if that were the actual value of σ 2, we determine\\nan appropriate value for r. Then, in the second stage of the simulation, we\\ngenerate an additional r −k runs.\\nProblems\\n1. If x0 = 5, and\\nxn = 3xn−1\\nmod 5\\nﬁnd x1,x2,...,x10.\\n2. Another method of generating a random permutation, different from\\nthe one given in Example 15.2.b, is to successively generate a random\\npermutation of the numbers 1,2,...,n starting with n = 1, then n = 2,\\nand so on. (Of course, the random permutation when n = 1 is 1.) Once\\nwe have a random permutation of the numbers 1,...,n −1 — call it\\nP1,P2,...,Pn−1 — the random permutation of the numbers 1,...,n is\\nobtained by starting with the permutation P1,P2,...,Pn−1,n, then in-\\nterchanging the element in position n (namely, n) with the element in a\\nrandomly chosen position that is equally likely to be any of the positions\\n1,2,...,n.\\na.\\nWrite an algorithm that accomplishes the preceding.\\nb.\\nVerify when n = 2 and when n = 3 that all n! possible permutations\\nare equally likely.\\n3. Suppose that we are to observe the independent and identically dis-\\ntributed vectors (X1,Y1), (X2,Y2), ..., (Xn,Yn), and that we want to use\\nthese data to estimate θ ≡E[X1]/E[Y1].\\na.\\nGive an estimator of θ.\\nb.\\nExplain how you could estimate the mean square error of this esti-\\nmator.\\n4. Suppose that X1,...,Xn is a sample from a distribution whose variance\\nσ 2 is unknown. Suppose we are planning to estimate σ 2 by the sample\\nvariance S2 = \\x03n\\ni=1(Xi −¯X)2/(n −1), and we want to use the bootstrap\\ntechnique to estimate Var(S2).\\na.\\nIf n = 2 and X1 = 1 and X2 = 3, what is the bootstrap estimate of\\nVar(S2)?\\nb.\\nIf n = 15, and the data values are\\n5,4,9,6,21,17,11,20,7,10,21,15,13,8,6\\nuse simulation to obtain the bootstrap estimate of Var(S2).\\n5. Let X1,...,X8 be independent and identically distributed random vari-\\nables with mean μ. Let\\np = P\\n\\x16 8\\n\\x04\\ni=1\\nXi/8 < μ\\n\\x17'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 652}, page_content='646 CHAPTER 15: Simulation, bootstrap statistical methods, and permutation tests\\nEstimate p if the values of the Xi are 5, 2, 8, 6, 24, 6, 9, 4.\\n6. The following are a student’s weekly exam scores. Do they prove that the\\nstudent improved (as far as exam score) as the semester progressed?\\n68,64,72,80,72,84,76,86,94,92\\n7. A baseball player has the reputation of starting slowly at the beginning of\\na season but then continually improving as the season progresses. Do the\\nfollowing data, which indicate the number of hits he has in consecutive\\nﬁve-game strings of the season, strongly validate the player’s reputation?\\n8,3,7,12,4,7,13,6,0,9,12,4,4,6,10\\n8. A group of 16 mice were exposed to 300 rads of radiation at the age of 5\\nweeks. The group was then randomly divided into two subgroups. Mice\\nin the ﬁrst subgroup lived in a normal laboratory environment, whereas\\nthose from the second subgroup were raised in a special germ-free envi-\\nronment. The following data give the lifetimes, in days, of the mice in\\neach group:\\nGroup 1 lifetimes: 133, 145, 156, 159, 164, 202, 208, 222\\nGroup 2 lifetimes: 145, 148, 157, 171, 178, 191, 200, 204\\nUse a permutation test to test the hypothesis that the lifetime distribu-\\ntions are identical. Use the normal approximation to approximate the\\np-value.\\n9. Do Problem 13 in Chapter 12 by using a permutation test. Use the nor-\\nmal approximation to approximate the p-value.\\n10. Do Problem 16 in Chapter 12 by using a permutation test. Use the nor-\\nmal approximation to approximate the p-value.\\n11. Write an algorithm, similar to what was done in the text to generate a\\nbinomial random variable, that uses the discrete inverse transform algo-\\nrithm to generate a Poisson random variable with mean λ.\\n12. Show that the discrete inverse transform algorithm for generating a geo-\\nmetric random variable with parameter p reduces to the following:\\n1.\\nGenerate a random number U\\n2.\\nSet X = Int( log(1−U)\\nlog(1−p) ) + 1\\nGive a second algorithm for generating a geometric random variable with\\nparameter p that takes into account the probabilistic interpretation of\\nsuch a random variable.\\n13. Give a method for generating a random variable having density function\\nf (x) = ex/(e −1),\\n0 < x < 1\\n14. Give a method for generating a random variable having distribution func-\\ntion\\nF(x) = xn ,\\n0 < x < 1'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 653}, page_content='Problems 647\\n15. Give a method for generating a random variable having distribution func-\\ntion\\nF(x) = 1\\n2(x + x2),\\n0 < x < 1\\n16. Suppose that the following are the generated values of 20 random vari-\\nables from the distribution F, whose mean μ is unknown:\\n5,4,9,6,21,12,7,14,17,11,20,7,10,21,15,26,9,13,8,6\\nHow many additional random variables from F will we need to generate\\nif we want to be 99 percent certain that our estimate of μ is correct to\\nwithin ±0.1?'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 654}, page_content='CHAPTER 16\\nMachine learning and big data\\n16.1\\nIntroduction\\nThe proliferation of data along with the ability to quickly compute has in re-\\ncent years led to a variety of techniques for estimating probabilities that depend\\nless on assuming a particular model for the data and more on utilizing large\\namounts of data. Such techniques are often referred to as being “machine learn-\\ning”. This chapter will give an introduction to certain popular machine learning\\ntechniques. In Section 16.2 we introduce a prototypical model in which we\\nwant to estimate the probability that a particular cross country ﬂight will be\\nlate in arriving at its destination when there is a large amount of historical data.\\nWe suppose that any individual ﬂight can be described by a “characterizing vec-\\ntor” of values, where the components of the vector are qualitatively descriptive\\nin nature. The following sections then present approaches for estimating such\\nprobabilities. In Section 16.3 we introduce the “naive Bayes” approach, which\\nattempts to estimate the probability that the ﬂight of interest will be late by\\nwriting it as a conditional probability of lateness given the characteristics of\\nthe ﬂight. Using Bayes theorem then enables us to obtain an expression for this\\nprobability, whose parts can be estimated when a certain assumption related\\nto the independence of the components of a characterizing vector is satisﬁed.\\nIn Section 16.3.1 we give a modiﬁcation of the naive Bayes approach that can\\nimprove upon it when the independence assumption is clearly not justiﬁed.\\nIn Section 16.4 we introduce “nearest neighbor” rules. These rules use those\\ndata in the historical data set whose characterizing vector is close to that of the\\nﬂight of interest to estimate the probability that the ﬂight of interest will be late.\\nTo use such a rule, one ﬁrst deﬁnes, for each of the historical data, a distance\\nbetween the characteristic vector of the historical data and that of the ﬂight of\\ninterest. It then makes the most use of the historical data having the smallest\\ndistances. For instance, one such rule uses the k historical data values that are\\nthe k nearest neighbors of the ﬂight of interest and then takes the fraction of those\\nk ﬂights that were late as the estimate of the lateness probability. Section 16.4.1\\nintroduces variations of nearest neighbor rules that make use of all historical\\ndata values, giving more “weight” to those whose characterizing vectors are\\nclosest to the one of interest. One apparent weakness of nearest neighbor rules\\nis that, in determining distances between characterizing vectors, equal weight\\nIntroduction to Probability and Statistics for Engineers and Scientists. https://doi.org/10.1016/B978-0-12-824346-6.00025-9\\nCopyright © 2021 Elsevier Inc. All rights reserved.\\n649'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 655}, page_content='650 CHAPTER 16: Machine learning and big data\\nis given to all components of the vector, not allowing for the possibility that\\ncertain components might be far more important than others. Section 16.4.2\\ndeals with this by letting the weight given to a component depend on that\\ncomponent.\\nIn Section 16.5, we discuss how one can choose which of the preceding proce-\\ndures to use, and how effective it might be.\\nWhereas we have assumed to now that the elements of the characterizing vec-\\ntor are qualitative in nature, in Section 16.6 we suppose they are quantitative.\\nAssuming that an experiment results either in a success or failure, Subsections\\n16.6.1 and 16.6.2 introduce, respectively, nearest neighbor rules and logistic\\nregression approaches to estimating the probability that an experiment with\\ncharacterizing vector (x1,...,xn) will be a success.\\nIn Section 16.7 we consider a problem of a different type. A prototypical ex-\\nample of the general model is to suppose there are two different drugs that\\ncan be prescribed, and that each use of drug i,i = 1,2, will result in a cure with\\nprobability pi,i = 1,2, where p1 and p2 are unknown. Assuming that the result\\n(cure or not) of prescribing a particular drug is immediately known, the prob-\\nlem is to sequentially decide, based on the results or all earlier choices, which\\ndrug to prescribe to each subsequent patient. Such problems are referred to\\nas bandit problems. We present a decision procedure that calls for making ran-\\ndom choices; that is, at every stage it speciﬁes a probability α and calls for the\\ndecision maker to use drug 1 with probability α and drug 2 with probability\\n1 −α.\\n16.2\\nLate ﬂight probabilities\\nSuppose we are interested in estimating the probability that a nonstop ﬂight\\nfrom California to New York City will be late (where a ﬂight is said to be late\\nif it arrives at least thirty minutes past its scheduled arrival time). Without any\\nadditional information about the ﬂight we would probably estimate this prob-\\nability by looking at historical data concerning all such ﬂights and using the\\nproportion of those that arrived late as the estimate. However, typically we are\\ninterested in a particular ﬂight. So let us classify each ﬂight by the values of the\\nfollowing variables:\\nX1: the airline\\nX2: the city from which the ﬂight departs\\nX3: the arrival airport\\nX4: the day of the week\\nX5: the departure time\\nX6: the size of the plane\\nX7: the weather conditions'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 656}, page_content='16.3 The naive Bayes approach 651\\nSuppose that the possible values of these variables for\\nX1 are 1 = American, 2 = Delta, 3 = United, 4 = Jet Blue;\\nX2 are 1 = Los Angeles, 2 = San Francisco, 3 = San Diego, 4 = Oakland;\\nX3 are 1 = New York Kennedy, 2 = New York LaGuardia, 3 = Newark;\\nX4 are 1,...,7, with 1 = Monday, 2 = Tuesday, and so on;\\nX5 are 1 = early morning, 2 = late morning, 3 = early afternoon, 4 =\\nlate afternoon, 5 = early evening, 6 = late evening;\\nX6 are 1 = small, 2 = medium, 3 = large;\\nX7 are 1 = above average, 2 = average, 3 = stormy.\\nThus, for example a United Airlines ﬂight from Oakland to Newark, leaving\\non Wednesday in the early evening, on a large plane in above average weather\\nconditions would be coded as (X1,X2,X3,X4,X5,X6,X7) = (3,4,3,3,5,3,1).\\nWe use the vector (3,4,3,3,5,3,1) to identify such a ﬂight, and call it the char-\\nacterizing vector of the ﬂight. How can we estimate the probability that such a\\nﬂight will be late? One way, of course, would be to look at all previous ﬂights\\nin our historical database that have the same characterizing vector and estimate\\nthe lateness probability by the fraction of those ﬂights that were late. However,\\nbecause the number of possible ﬂights is, by the basic principle of counting,\\n4·4·3·7·6·3·3 = 18,144, even with a large historical data set there may not be\\nmany ﬂights of exactly the same type, and so such estimates might not be very\\naccurate. In the following sections we give different possible approaches to the\\npreceding problem.\\n16.3\\nThe naive Bayes approach\\nSuppose we want to estimate the probability that a ﬂight having characterizing\\nvector X1 = x1, X2 = x2, ...,X7 = x7 will be late. Letting L be the event that a\\nﬂight is late, let us write the preceding as a conditional probability P(L|X1 =\\nx1,...,X7 = x7). Now, from Bayes theorem we have\\nP(L|X1 = x1,...,X7 = x7) = P(L,X1 = x1,...,X7 = x7)\\nP(X1 = x1,...,X7 = x7)\\n= P(L)P(X1 = x1,...,X7 = x7|L)\\nP(X1 = x1,...,X7 = x7)\\nBecause P(L) is the probability that a randomly chosen ﬂight is late, its nat-\\nural estimate is the proportion of the ﬂights in our historical database that\\nwere late, and this is the estimate we will use. However, the natural estima-\\ntor of P(X1 = x1,...,X7 = x7), namely the fraction of ﬂights in the database\\nhaving characterizing vector (x1,...,x7), is often inaccurate because of the\\nlimited number of historical ﬂights having characterizing vector (x1,...,x7).\\nSimilarly, the corresponding natural estimator of P(X1 = x1,...,X7 = x7|L),'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 657}, page_content='652 CHAPTER 16: Machine learning and big data\\nnamely, the fraction of the late ﬂights in the database whose characterizing\\nvector is (x1,...,x7) would suffer from the same difﬁculty. To surmount this\\ndifﬁculty, we will act as if X1,X2,...,X7 were independent random variables,\\nand that they remain independent (but not necessarily with the same proba-\\nbilities) when we are given that the ﬂight is late. Now, under this assumption,\\nwe would have\\nP(X1 = x1,...,X7 = x7) = P(X1 = x1)P(X2 = x2)···P(X7 = x7)\\nand we can estimate P(Xi = xi) by the fraction of all ﬂights in our database\\nwhose characterizing vector has Xi = xi. For instance, we would estimate\\nP(X1 = 3) by the fraction of all ﬂights in our database that are ﬂown by United.\\nIn addition, because we are also assuming that X1,X2,...,X7 remain indepen-\\ndent even when we know the ﬂight is late, we have\\nP(X1 = x1,...,X7 = x7|L) = P(X1 = x1|L)P(X2 = x2|L)···P(X7 = x7|L)\\nThus, we can estimate P(Xi = xi|L) by the fraction of all late ﬂights in our\\ndatabase whose characterizing vector has Xi = xi. Consequently, if f (L) is the\\nfraction of all ﬂights in the database that arrived late; and fi(x) is the fraction of\\nall ﬂights in the database whose characterizing vector had Xi = x; and fi(x|L)\\nis the fraction of all the late ﬂights in the database whose characterizing vector\\nhad Xi = x, then our estimate of\\nP(L|X1 = x1,...,X7 = x7) = P(L)P(X1 = x1,...,X7 = x7|L)\\nP(X1 = x1,...,X7 = x7)\\ncalled the naive Bayes estimator, is\\nf (L)f1(x1|L)f2(x2|L) ··· f7(x7|L)\\nf1(x1)f2(x2) ··· f7(x7)\\nExample 16.3.a. Table 16.1 gives historical data of ﬂight latenesses of 79 ﬂights\\noriginating at a single airport when such ﬂights are characterized by a vector\\n(x1,x2,x3,x4) where x1 has possible values 1 and 2, x2 has possible values 1\\nand 2, x3 has possible values 1, 2 and 3, and x4 has possible values 1 and 2.\\nUsing the naive Bayes approach to estimate the probability that a ﬂight with\\ncharacterizing vector (2,1,2,2) will be late.\\nSolution. Because 25 of these 79 ﬂights were late, the estimate of P(L) is\\nf (L) = 25/79.\\nBecause 44 of the 79 ﬂights had x1 = 2, the estimate of P(X1 = 2) is f1(2) =\\n44/79.\\nBecause 34 of the 79 ﬂights had x2 = 1, the estimate of P(X2 = 1) is f2(1) =\\n34/79.\\nBecause 30 of the 79 ﬂights had x3 = 2, the estimate of P(X3 = 2) is'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 658}, page_content='16.3 The naive Bayes approach 653\\nTable 16.1 Historical Flight Data.\\nFlight vector\\nNumber of such\\nﬂights\\nNumber late\\n(1,1,1,1)\\n2\\n1\\n(1,1,1, 2)\\n2\\n0\\n(1,1,2,1)\\n2\\n0\\n(1,1,2,2)\\n1\\n0\\n(1,1,3,1)\\n3\\n1\\n(1,1,3,2)\\n3\\n0\\n(1,2,1,1)\\n4\\n1\\n(1,2,1,2)\\n3\\n1\\n(1,2,2,1)\\n5\\n2\\n(1,2,2,2)\\n3\\n1\\n(1,2,3,1)\\n3\\n1\\n(1,2,3,2)\\n4\\n2\\n(2,1,1,1)\\n4\\n2\\n(2,1,1,2)\\n3\\n1\\n(2,1,2,1)\\n5\\n2\\n(2,1,2,2)\\n4\\n1\\n(2,1,3,1)\\n3\\n1\\n(2,1,3,2)\\n2\\n0\\n(2,2,1,1)\\n3\\n1\\n(2,2,1,2)\\n4\\n1\\n(2,2,2,1)\\n5\\n2\\n(2,2,2,2)\\n5\\n3\\n(2,2,3,1)\\n3\\n1\\n(2,2,3,2)\\n3\\n0\\nf3(2) = 30/79.\\nBecause 37 of the 79 ﬂights had x4 = 2, the estimate of P(X4 = 2) is f4(2) =\\n37/79.\\nBecause 15 of the 25 late ﬂights had x1 = 2, the estimate of P(X1 = 2|L) is\\nf1(2|L) = 15/25.\\nBecause 9 of the 25 late ﬂights had x2 = 1, the estimate of P(X2 = 1|L) is\\nf2(1|L) = 9/25.\\nBecause 12 of the 25 late ﬂights had x3 = 2, the estimate of P(X3 = 2|L) is\\nf3(2|L) = 12/25.\\nBecause 10 of the 25 late ﬂights had x4 = 2, the estimate of P(X3 = 2|L) is\\nf4(2|L) = 10/25.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 659}, page_content='654 CHAPTER 16: Machine learning and big data\\nThe naive Bayes estimated probability that a ﬂight with characterizing vector\\n(2,1,2,2) will be late is thus\\nf (L)f1(2|L)f2(1|L)f3(2|L)f4(2|L)\\nf1(2)f2(1)f3(2)f4(2)\\n= 25 · 15 · 9 · 12 · 10 · 794\\n79 · 254 · 44 · 34 · 30 · 37 = .3078.\\nIt is interesting to compare this estimate with 1/4 = .25 which is the fraction\\nof the ﬂights with characterizing vector (2,1,2,2) that were late, or with the\\nestimate 25/79 = .3165 which is the fraction of all ﬂights that were late.\\n■\\n16.3.1\\nA variation of naive Bayes approach\\nSuppose, as in the preceding section, that we are interested in using histori-\\ncal data to estimate the probability that a situation characterized by the vector\\n(x1,...,xn) will result in a certain outcome. As indicated, the naive Bayes ap-\\nproach uses the historical data to estimate P(X1 = x1,...,Xn = xn) by assum-\\ning that X1,...,Xn are independent and so\\nP(X1 = x1,...,Xn = xn) = P(X1 = x1)···P(Xn = xn)\\nIt then used the proportion of the historical data whose component i value\\nwas xi as the estimate of P(Xi = xi), i = 1,...,n. However, there are many\\nexamples where the assumption that X1,...,Xn are independent is clearly not\\nviable. For instance, suppose that patients with physical symptoms similar to\\nthat produced by Crohn’s disease are given a battery of n medical tests, with\\nthe result of test i, call it Xi, being either 1 if the result is positive towards\\nthe disease or 0 if it is negative. Now, assuming the tests have some validity,\\nhaving Xi = 1 makes it more likely that the patient has the disease, which thus\\nmakes it more likely that Xj = 1. Consequently, X1,...,Xn, the results of the\\ntests, are clearly not independent. However, while not independent, it might\\nbe reasonable to suppose that they would be independent if we knew whether\\nor not the patient had the disease. That is, given whether or nor the patient\\nhas the disease, it might be reasonable to suppose that the results of the tests\\nare independent. Assuming so, if we let D be the event that a patient has the\\ndisease, we can use the identity\\nP(X1 = x1,...,Xn = xn) = P(X1 = x1,...,Xn = xn|D)P(D)\\n+ P(X1 = x1,...,Xn = xn|Dc)(1 −P(D))\\nThe assumption that X1,...,Xn are independent when we know whether the\\npatient has or does not have the disease, gives\\nP(X1 = x1,...,Xn = xn|D) = P(X1 = x1|D)···P(Xn = xn|D)\\nP(X1 = x1,...,Xn = xn|Dc) = P(X1 = x1|Dc)···P(Xn = xn|Dc)\\nWe can now estimate P(Xi = xi|D) by considering the patients in our historical\\ndatabase who had the disease and letting the estimate be the proportion of that'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 660}, page_content='16.3 The naive Bayes approach 655\\nTable 16.2 Historical Data.\\nCharacterizing\\nvector\\nNumber of\\npatients\\nNumber w.\\ncondition\\nNumber w.o.\\ncondition\\n(1,1,1)\\n4\\n1\\n3\\n(1,1,2)\\n3\\n0\\n3\\n(1,1,3)\\n5\\n1\\n4\\n(1,2,1)\\n6\\n1\\n5\\n(1,2,2)\\n7\\n2\\n5\\n(1,2,3)\\n5\\n3\\n2\\n(2,1,1)\\n6\\n3\\n3\\n(2,1,2)\\n7\\n2\\n5\\n(2,1,3)\\n5\\n1\\n4\\n(2,2,1)\\n6\\n2\\n4\\n(2,2,2)\\n9\\n4\\n5\\n(2,2,3)\\n5\\n1\\n4\\ngroup whose test i value was xi. Similarly, we can estimate P(Xi = xi|Dc) by\\nconsidering the patients in our historical database who did not have the disease\\nand letting the estimate be the proportion of those patients whose test i value\\nwas xi. Also, P(D) would be estimated by the proportion of the patients in our\\nhistorical database who had the disease, and P(Dc) by the proportion that did\\nnot have the disease.\\nExample 16.3.b. A series of three tests is given to each patient exhibiting\\nsymptoms of a certain medical condition, with the results of the three tests\\ncharacterized by a vector (x1,x2,x3) where x1 has possible values 1 and 2, x2\\nhas possible values 1 and 2, and x3 has possible values 1, 2 and 3. Historical\\ndata concerns 68 patients that had the test, giving for each their test result and\\nwhether or not they were eventually found to have the condition. It is summa-\\nrized by Table 16.2.\\nThus, for instance, 7 of the 68 patients detailed in the historical database had\\nthe characterizing vector (2,1,2), with 2 of these 7 having the condition. Use\\nthe preceding to estimate the probability that a new patient whose characteriz-\\ning vector is (2,1,2) has the condition.\\nSolution. Let X = (X1,X2,X3). Then, with C being the event that the patient\\nhas the medical condition, we start with the identity\\nP(C|X = (2,1,2)) = P(C,X = (2,1,2))\\nP(X = (2,1,2))\\n= P(C)P(X = (2,1,2)|C)\\nP(X = (2,1,2))\\n=\\nP(C)P(X = (2,1,2)|C)\\nP(X = (2,1,2)|C)P(C) + P(X = (2,1,2)|Cc)P(Cc)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 661}, page_content='656 CHAPTER 16: Machine learning and big data\\nNow, if we assume that X1, X2, X3 become independent if we know whether\\nor not the patient has the condition, then we have\\nP(X = (2,1,2)|C) = P(X1 = 2|C)P(X2 = 1|C)P(X3 = 2|C)\\nP(X = (2,1,2)|Cc) = P(X1 = 2|Cc)P(X2 = 1|Cc)P(X3 = 2|Cc)\\nWe are now ready to estimate the probability that a patient with test vector\\n(2,1,2) has the disease.\\nBecause 21 of the 68 patients had the condition, the estimate of P(C) is 21/68.\\nBecause of the 21 patients that had the condition\\n13 had x1 = 2, the estimate of P(X1 = 2|C) is 13/21.\\n8 had x2 = 1, the estimate of P(X2 = 1|C) is 8/21.\\n8 had x3 = 2, the estimate of P(X3 = 2|C) is 8/21.\\nBecause 47 of the 68 patients did not have the condition, the estimate of P(Cc)\\nis 47/68.\\nBecause of the 47 patients that did not have the condition\\n25 had x1 = 2, the estimate of P(X1 = 2|Cc) is 25/47.\\n22 had x2 = 1, the estimate of P(X2 = 1|Cc) is 22/47.\\n18 had x3 = 2, the estimate of P(X3 = 2|Cc) is 18/47.\\nThe estimate of P(X = (2,1,2)|C) is thus 13·8·8\\n213 = .08984.\\nThe estimate of P(X = (2,1,2)|Cc) is thus 25·22·18\\n473\\n= .09535.\\nThe estimate of P(C|X = (2,1,2)) is\\n(21/68)(.08984)\\n(21/68)(.08984) + (47/68)(.09535) = .2963.\\nThat is, our estimate of the probability that a patient with characterizing vector\\n(2,1,2) has the condition is .2963.\\n■\\nRemark. The preceding variation makes use of the idea of conditional inde-\\npendence, which results when random variables X1,...,Xn that need not be\\nindependent become independent when we are given some additional infor-\\nmation. For instance, suppose there are 2 coins, with coin one coming up heads\\non each ﬂip with probability .8, and coin two coming up heads with probabil-\\nity .3. Suppose one of these coins is chosen in a manner such that the chosen\\ncoin is equally likely to be either one. Now suppose the chosen coin is ﬂipped\\n8 times. Let Xi = 1 if coin toss i results in a head, and let Xi = 0 if coin toss i\\nresults in a tail. Now, if we knew that the ﬁrst 5 ﬂips all resulted in heads, then it\\nbecomes quite likely that we are using coin one, which makes it more likely (as\\nopposed to when we have no information about the ﬁrst 5 ﬂips) that the next\\nﬂip will also land heads. Consequently, X1,...,X8 are not independent. How-\\never, if we knew which coin had been chosen, then information about previous\\nﬂips would not be useful in determining the results of the forthcoming ﬂips.\\nThat is, if we let Y be the coin selected, then knowing whether Y = 1 or Y = 2,'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 662}, page_content='16.4 Distance-based estimators. The k-nearest neighbors rule\\n657\\nwould make X1,...,Xn independent. In such a case, we say that X1,...,Xn are\\nconditionally independent given the value of Y. This implies, for instance, that\\nP(X1 = 1,X2 = 1,X3 = 1|Y = 1)\\n= P(X1 = 1|Y = 1)P(X2 = 1|Y = 1)P(X3 = 1|Y = 1)\\n= (.8)(.8)(.8) = .512\\n■\\n16.4\\nDistance-based estimators. The k-nearest\\nneighbors rule\\nLet us deﬁne the distance, call it d, between two characterizing vectors x =\\n(x1,x2,...,x7) and y = (y1,y2,...,y7) by\\nd(x,y) =\\n7\\n\\x02\\ni=1\\nI{xi ̸= yi}\\nwhere\\nI{xi ̸= yi} =\\n\\x03\\n0,\\nif xi = yi\\n1\\nif xi ̸= yi\\nThat is, the distance between two characterizing vectors is the number of coor-\\ndinates in which their values differ. Distance-based estimators of the lateness\\nprobability of a ﬂight with characterizing vector (x1,x2,...,x7) are based on\\nthe supposition that two characterizing vectors whose distance is small should\\nhave similar lateness probabilities. The k-nearest neighbor rule is one popular\\nestimator. To utilize it, one determines the distances between (x1,x2,...,x7)\\nand all the characterizing vectors in the historical database. The rule chooses\\nthe k historical vectors whose distances from (x1,x2,...,x7) are smallest, and\\nthen uses the proportion of those k ﬂights that were late as the estimate of the\\nprobability that the (x1,x2,...,x7) ﬂight will be late. Another distance-based\\nestimator is one that selects all vectors in the historical database that are within\\na ﬁxed distance of (x1,x2,...,x7), and then uses the proportion of these ﬂights\\nthat were late as the estimate of the probability that an (x1,x2,...,x7) ﬂight\\nwill be late.\\nExample 16.4.a. Consider the data of Example 16.3.a Using the ﬁxed distance\\n1, the characterizing vectors whose distances from (2,1,2,2) are at most one\\nare (2,1,2,2), (1,1,2,2), (2,2,2,2)(2,1,1,2), (2,1,3,2), (2,1,2,1). Because 7\\nof the 20 ﬂights in the data set that had one of these characterizing vectors were\\nlate, the estimate that the (2,1,2,2) ﬂight will be late is 7/20 = .35.\\n■'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 663}, page_content='658 CHAPTER 16: Machine learning and big data\\n16.4.1\\nA distance-weighted method\\nAnother approach to only using some of the historical data values to obtain\\nour estimates is to use them all, but to give more weight to those data values\\nwhose characterizing vectors are closest to the one of interest. Suppose there are\\nN characterizing vectors in our historical database and we want to use them to\\nestimate the probability that a ﬂight with characterizing vector (x1,x2,...,x7)\\nwill be late. Let dj be the distance between the jth characterizing vector in our\\nhistorical data set and (x1,x2,...,x7), for j = 1,...,N. Also, let w1,...,wN be\\ndeﬁned by\\nwj =\\n1\\n1 + dj\\n,\\nj = 1,...,N.\\nThe quantity wj is the weight we will give to the jth historical data value when\\nestimating the probability that a ﬂight with characterizing vector (x1,x2,...,x7)\\nwill be late. Thus, a historical data value whose characterizing vector is the same\\nas the one of interest is given weight 1, one whose vector is a distance 1 from\\nthe vector of interest is given weight 1/2, and so on. The resulting estimate of\\nthe probability that a ﬂight with characterizing vector (x1,x2,...,x7) will be\\nlate is the ratio of the sum of the weights of all late ﬂights divided by the sum\\nof the weights of all ﬂights. Speciﬁcally, deﬁning Ij to equal 1 if the jth ﬂight\\nin our historical database was late and to equal 0 if it were not late, we estimate\\nthe desired probability by\\n\\x04N\\nj=1 Ijwj\\n\\x04N\\nj=1 wj\\n(16.4.1)\\nExample 16.4.b. Table 16.3 gives the historical data of Example 16.3.a, along\\nwith each vector’s distance, and resulting weight, from (2,1,2,2). Use it to\\nobtain the distance-weighted estimate of the probability that a ﬂight with char-\\nacterizing vector (2,1,2,2) will be late.\\nSolution. Because of the 79 ﬂights, 7 had weight 1/5, 23 had weight 1/4, 29\\nhad weight 1/3, 16 had weight 1/2, and 4 had weight 1, the sum of the weights\\nof all 79 ﬂights is 7/5 + 23/4 + 29/3 + 16/2 + 4 = 28.8167. On the other hand,\\nof the 25 late ﬂights 2 had weight 1/5, 9 had weight 1/4, 7 had weight 1/3, 6\\nhad weight 1/2, and 1 had weight 1. Thus, the sum of the weights of the late\\nﬂights is 2/5 + 9/4 + 7/3 + 6/2 + 1 = 8.9833, and so the estimate that a ﬂight\\nwith characterizing vector (2,1,2,2) will be late is\\n2/5+9/4+7/3+6/2+1\\n7/5+23/4+29/3+16/2+4 = .3117.\\n■\\nThere are other ways of weighing historical data results. For instance, one can\\nchoose a number β between 0 and 1 and then give weight βd to a historical data\\nwhose vector’s distance from (x1,...,x7) is d; or we could order the historical'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 664}, page_content='16.4 Distance-based estimators. The k-nearest neighbors rule\\n659\\nTable 16.3 Historical Flight Data.\\nFlight vector\\nNumber of\\nsuch ﬂights\\nNumber late\\n(2122)\\ndistance\\nWeight\\n(1,1,1,1)\\n2\\n1\\n3\\n1/4\\n(1,1,1, 2)\\n2\\n0\\n2\\n1/3\\n(1,1,2,1)\\n2\\n0\\n2\\n1/3\\n(1,1,2,2)\\n1\\n0\\n1\\n1/2\\n(1,1,3,1)\\n3\\n1\\n3\\n1/4\\n(1,1,3,2)\\n3\\n0\\n2\\n1/3\\n(1,2,1,1)\\n4\\n1\\n4\\n1/5\\n(1,2,1,2)\\n3\\n1\\n3\\n1/4\\n(1,2,2,1)\\n5\\n2\\n3\\n1/4\\n(1,2,2,2)\\n3\\n1\\n2\\n1/3\\n(1,2,3,1)\\n3\\n1\\n4\\n1/5\\n(1,2,3,2)\\n4\\n2\\n3\\n1/4\\n(2,1,1,1)\\n4\\n2\\n2\\n1/3\\n(2,1,1,2)\\n3\\n1\\n1\\n1/2\\n(2,1,2,1)\\n5\\n2\\n1\\n1/2\\n(2,1,2,2)\\n4\\n1\\n0\\n1\\n(2,1,3,1)\\n3\\n1\\n2\\n1/3\\n(2,1,3,2)\\n2\\n0\\n1\\n1/2\\n(2,2,1,1)\\n3\\n1\\n3\\n1/4\\n(2,2,1,2)\\n4\\n1\\n2\\n1/3\\n(2,2,2,1)\\n5\\n2\\n2\\n1/3\\n(2,2,2,2)\\n5\\n3\\n1\\n1/2\\n(2,2,3,1)\\n3\\n1\\n3\\n1/4\\n(2,2,3,2)\\n3\\n0\\n2\\n1/3\\nvalues in terms of their vector’s closeness to (x1,...,x7) and then give weight\\nβj to the jth nearest neighbor.\\n16.4.2\\nComponent-weighted distances\\nSuppose we want to estimate the probability that a ﬂight with characterizing\\nvector (x1,...,x7) will be late. Let Xi be the component i value of a randomly\\nchosen historical characterizing vector. Now, if the probability that a ﬂight\\nis late does not change much whether or not Xi = xi it would seem that we\\nneed not give much weight to component i when trying to assess the lateness\\nprobability of ﬂight with vector (x1,...,x7). With pL(xi) being the fraction of\\nhistorical ﬂights having Xi = xi that are late, and pL( ¯xi) being the fraction of\\nthose historical ﬂights having Xi ̸= xi that are late, let ai = |pL(xi) −pL( ¯xi)| be\\nthe absolute difference between these two fractions. Thus, ai is an estimate of\\nthe effect that Xi = xi has on the probability that the ﬂight is late. Taking the'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 665}, page_content='660 CHAPTER 16: Machine learning and big data\\npreceding into account, we can deﬁne a component weighted distance between\\nthe ﬂight vector x = (x1,...,x7) under consideration and the historical vector\\ny = (y1,...,y7) by\\nd(x,y) =\\n7\\n\\x02\\ni=1\\naiI{xi ̸= yi}\\n16.5\\nAssessing the approaches\\nWe now discuss how we can assess how good the procedures we have presented\\nare for determining the probability that a ﬂight is late. To evaluate the worth\\nof any of these procedures we start with our historical database, and randomly\\nselect from this set a subset of data values, called the testing set, which will\\nbe used to evaluate the procedure. For instance, suppose we have records of\\n3000 ﬂights. Of these, we should randomly choose a subset of some size, say\\n1000, to constitute the testing set and then see how well each procedure would\\nhave done when using the data concerning the remaining 2000 ﬂights as the\\nhistorical data to estimate the lateness probability of the ﬂights in the testing\\nset. The result of this will be 1000 estimated probabilities that the ﬂights in\\nthe testing set would be late, along with the information as to whether each\\nof them was late or not. That is, the result will be the 1000 pairs (pi,li),i =\\n1,...,1000 with pi being the estimate of the probability that the ith ﬂight of\\nthe testing set will be late, and li is equal to 1 if it were late and to 0 if it were\\nnot.\\nTo evaluate each procedure we give each of these pairs a “loss”, where the loss\\ngiven to the pair (pi,li) is\\nL =\\n\\x03\\n(1 −pi)2,\\nif li = 1\\np2\\ni ,\\nif li = 0\\nIn other words, the loss is the square of the probability (either 1−p or p) given\\nto the event (either not late or late) that did not occur. That is, if the method\\nyields an estimated probability of lateness equal to p, then the loss incurred is\\n(1−p)2 if the plane were late and p2 if it were not. For instance, if an estimated\\nlateness probability is 0, then a loss 1 is incurred if it is late and 0 if it is not;\\nif the estimated lateness probability is .2, then a loss .64 is incurred if it is late\\nand .04 if it is not; if the estimated lateness probability is .5, then a loss .25\\nis incurred whether it is late or not. Whichever procedure under consideration\\nhas the lowest sum of losses is considered the best procedure.\\nTo understand the rationale for the preceding loss rule, suppose the actual\\n(though unknown) probability that a particular plane is late is α, and let L\\ndenote the loss incurred when one estimates that the lateness probability is p.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 666}, page_content='16.5 Assessing the approaches\\n661\\nBecause the loss is (1 −p)2 if the plane is late and p2 if it is not, it follows that\\nthe expected loss is\\nE[L] = α(1 −p)2 + (1 −α)p2\\n= α −2pα + p2\\n= α + (p −α)2 −α2\\nConsequently, the estimate p that makes E[L] as small as possible is p = α.\\nThat is, the lowest expected loss results when the estimated probability is equal\\nto the actual probability α, with the expected loss increasing the further the es-\\ntimated probability p is from α. As a result, the procedure that yields the lowest\\ntotal sum of losses over the 1000 test cases is most likely the best procedure for\\nestimating the lateness probabilities.\\nAlthough the preceding can be used to compare the naive Bayes, the k-nearest\\nneighbor, and the within distance approaches so as to see which appears better\\nfor a given application, it still leaves open the question whether any of them is\\nany good. One way to check this is to compare the average loss resulting from\\nthe test data of these procedures with the average loss of a policy that ignores\\nthe characterizing vector and always estimates the lateness probability of the\\ntest cases by the fraction of all ﬂights in the historical data set that were late.\\nTo derive this quantity, suppose β was the fraction of the 3000 ﬂights that were\\nlate, and suppose that the predicted lateness probability for each ﬂight was β.\\nThen as the loss is (1 −β)2 when the ﬂight is late, and β2 when it is not, then\\nsince β is the proportion of late ﬂights, it follows that the average loss is\\nβ(1 −β)2 + (1 −β)β2 = β(1 −β)\\nExample 16.5.a. To obtain an idea how much of a decrease in average loss over\\nthat of the policy that ignores the characterizing vector can be achieved, sup-\\npose that the historical late probabilities, given the characterizing vector, can be\\nregarded as being the values of independent uniform (0,1) random variables.\\nAs 1/2 of the historical ﬂights will be late, the rule that always estimates the late\\nprobability as 1/2 will have average loss 1/4. On the other hand, suppose we\\nhave an optimal estimation procedure that always gives the correct probability.\\nBecause the average loss resulting when the estimate p is given for an event that\\noccurs with probability p is (1 −p)2p + p2(1 −p) = p −p2, it follows that the\\naverage loss with this optimal procedure is\\n\\x05 1\\n0 (p −p2)dp = 1/6.\\n■\\nRemark. Suppose there are N data values. When N is not too large, we can\\nassess each estimator by looking at how well it does for each of the individual\\ndata values when the other N −1 values are used as the historical data points.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 667}, page_content='662 CHAPTER 16: Machine learning and big data\\n16.6\\nWhen characterizing vectors are quantitative\\nIn this section, we again suppose that we are interested in estimating the prob-\\nability that an experiment having characterizing vector (x1,...,xn) is a success,\\nbut now we suppose that the elements of the vector (x1,...,xn) are quantita-\\ntive, rather than qualitative, in nature. That is, for instance, x1 could represent\\nthe weight of an individual. Is Section 16.6.1 we show how nearest neighbor\\nrules can be used, and in Section 16.6.2 we consider the logistic regression\\nmodel.\\n16.6.1\\nNearest neighbor rules\\nAs in the case of qualitative characterizing vectors, when estimating the prob-\\nability that an experiment with a quantitative characterizing vector (x1,...,xn)\\nwill result in a success, nearest neighbor rules give more weight to the his-\\ntorical data whose characterizing vectors are closest to (x1,...,xn). So to\\nbegin we need to deﬁne a distance between two characterizing vectors, say\\nx = (x1,...,xn) and y = (y1,...,yn). While it might seem natural to use the\\nEuclidean distance\\n\\x06\\x04n\\ni=1(xi −yi)2 or \\x04n\\ni=1 |xi −yi|, we must ﬁrst take into\\naccount that as the components of the vector refer to different entities they will\\nhave different units, which will affect their values. For instance, suppose we\\nare interested in estimating the probability that an item will be seriously dam-\\naged by an explosion, and that x1 refers to the distance of the item from the\\nexplosion. If the distance is measured in feet, then |x1 −y1| would be 3 times\\nwhat it would be if distance were measured in yards, thus changing which of\\nthe historical characterizing vectors (x1,...,xm) are closest to (y1,...,yn). To\\nﬁx this anomaly, we ﬁrst standardize the data. Letting mi and si be the sample\\nmean and variance of the component i value of all the historical characterizing\\nvectors, i = 1,...,n, redeﬁne all historical characterizing vectors as well as the\\none under consideration, so that that the characterizing vector (x1,...,xn) is\\nchanged to ( x1−m1\\ns1\\n,..., xn−mn\\nsn\\n). In this way we have standardized the histori-\\ncal vectors and the one under consideration so that the sample mean of each\\ncomponent value is 0 and the sample variance is 1. Now, with these standard-\\nized vectors, deﬁne the distance between x = (x1,...,xn) and y = (y1,...,yn)\\nas their Euclidean distance\\n\\x06\\x04n\\ni=1(xi −yi)2. (Although Euclidean distance is\\nmost commonly used, sometimes the distance function \\x04n\\ni=1 |xi −yi| is used.)\\nThe k nearest neighbor rule for estimating p(x1,...,xn), the probability that an\\nexperiment with characterizing vector (x1,...,xn) results in a success, ﬁnds the\\nk historical data whose characterizing vectors are closest to (x1,...,xn) and\\nthen takes the fraction of these that resulted in a success as the estimate.\\nWhereas the k nearest neighbor rule for estimating pi(x1,...,xn) only uses the\\nresults of the k nearest historical data values and then gives equal weight to all\\nof them, a modiﬁcation is to use the results of all the historical data, giving'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 668}, page_content='16.6 When characterizing vectors are quantitative 663\\nmore weight to those data whose vectors are closest to (x1,...,xn). Say there\\nare N historical data results. One possibility is to let Ij equal 1 if the jth closest\\nof these N resulted in a success and let it be 0 otherwise, and then estimate\\np(x1,...,xn) by\\n\\x04N\\nj=1 βj−1Ij\\n\\x04N\\nj=1 βj−1 =\\n1−β\\n1−βN\\n\\x04N\\nj=1 βj−1Ij , where 0 < β < 1 is a param-\\neter to be chosen. That is, this rule gives successive weights 1,β,β2....,βN−1 to\\nthe results of the experiments whose characterizing vectors are the closest, next\\nclosest, and so on, to (x1,...,xn).\\n16.6.2\\nLogistics regression\\nLogistics regression can also be used to estimate p(x1,...,xn), the probability\\nthat an experiment with characterizing vector x1,...,xn will result in a success.\\nThe model assumes that for constants b0,b1,...,bn, that will need to be esti-\\nmated,\\np(x1,...,xn) =\\neb0+\\x04n\\ni=1 bixi\\n1 + eb0+\\x04n\\ni=1 bixi\\nR can be used to estimate the parameters b0,b1,...,bn. For instance, suppose\\nthat n = 5 and that 6 experiments having characterizing vectors\\n(3,7,9,4,12)\\n(8,10,4,5,16)\\n(14,9,11,10,15)\\n(18,20,12,8,7)\\n(2,9,13,8,17)\\n(7,13,21,15,19)\\nhave yielded respective results (1,0,1,1,0,1), with 1 signifying a success and\\n0 a failure. (Thus, experiments with characterizing vectors (8,10,4,5,16) and\\n(2,9,13,8,17) resulted in failures, whereas the others were all successes.) To\\nuse R to estimate the parameters, for i = 1,...,5, let vi be the successive values\\nof the ith coordinate of the characterizing vectors of the 6 already run experi-\\nments, and then type the following:\\n>\\nv1 = c(3,8,14,18,2,7)\\n>\\nv2 = c(7,10,9,20,9,13)\\n>\\nv3 = c(9,4,11,12,8,15)\\n>\\nv4 = c(4,5,10,8,8,15)\\n>\\nv5 = c(12,16,15,7,17,19)\\n>\\ny = c(1,0,1,1,0,1)'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 669}, page_content='664 CHAPTER 16: Machine learning and big data\\n>\\nmodel = glm(y ∼v1 + v2 + v3 + v4 + v5)\\n>\\nmodel\\nPressing enter now yields the following output\\nCall: glm (formula = y ∼v1 + v2 + v3 + v4 + v5)\\nCoefﬁcients:\\n(Intercept)\\nv1\\nv2\\nv3\\nv4\\nv5\\n−5.25116\\n0.09965 0.04360 0.45959 −0.49604 0.29124\\nThat is, the estimated logistics regression equation is\\np(x1,...,x6)\\n=\\nexp{−5.25116 + 0.09965x1 + 0.04360x2 + 0.45959x3 −0.49604x4 + 0.29124x5}\\n1 + exp{−5.25116 + 0.09965x1 + 0.04360x2 + 0.45959x3 −0.49604x4 + 0.29124x5}\\n16.7\\nChoosing the best probability: a bandit problem\\nSuppose that there are two different drugs that can be prescribed for a certain\\nillness, and that each use of drug i will result in a success (that is, a cure) with\\nprobability pi,i = 1,2, where p1 and p2 are unknown. Assuming that the result\\n(cure or not) of prescribing a particular drug becomes immediately known, the\\nproblem is to sequentially decide, based on the results or all earlier choices,\\nwhich drug to prescribe to each subsequent patient. Because early papers on\\nthis topic often referred to a gambling casino slot machine having a left and\\na right arm (such machines were often called “two-armed bandits”) with the\\nplayer being able to elect at each gamble which arm to pull, these problems\\nare often referred to as bandit problems. Because p1 and p2 are unknown, each\\nchoice of which drug to use involves a balancing act of choosing the drug that\\nappears to be best versus choosing a drug so as to gather information about its\\nprobability. We now present a policy that tends to perform very well in that the\\ngreat majority of the time it chooses the better drug. The policy is randomized,\\nin the sense that at every stage it speciﬁes a probability that drug 1 should be\\nused. The policy is as follows:\\n■Use each drug once.\\n■Suppose a decision is to be made at a time when drug i has been used ni\\ntimes, with a total of si successes having resulting, i = 1,2. Let\\nf 1 = s1 + 1\\nn1 + 2,\\nf 2 = s2 + 1\\nn2 + 2\\nand let\\nr =\\nf 1 −f 2\\n√f 1(1 −f 1)/n1 + f 2(1 −f 2)/n2\\n.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 670}, page_content='16.7 Choosing the best probability: a bandit problem\\n665\\nWith \\x04 being the standard normal distribution function, let\\nx = \\x04(r)\\nNow use drug 1 with probability x and drug 2 with probability 1 −x.\\nFor instance, suppose that at a certain stage drug 1 had been used 9 times, with\\n5 cures resulting, and drug 2 had been used 5 times, with 2 cures resulting. In\\nthis situation, n1 = 9, s1 = 5, n2 = 5, s2 = 2, giving that f 1 = 6/11, f 2 = 3/7.\\nThus,\\nr =\\n6/11 −3/7\\n\\x06\\n30\\n121×9 +\\n12\\n49×5\\n= .4225,\\ngiving that\\nx = \\x04(.4225) = .6637.\\nHence, the procedure calls for using drug 1 with probability .6637 and drug\\n2 with probability 1 −.6637 = .3363. This can be accomplished by computer\\ngeneration of U, a uniform (0,1) random variable (referred to as a random\\nnumber). Because, for 0 < x < 1, a uniform (0,1) random variable is less than\\nx with probability x, the procedure would then use drug 1 if U < .6637 or drug\\n2 if U ≥.6637.\\nRemarks\\n1. R can be used to determine the action at each stage. Because the com-\\nmand runif(k) generates the value of k uniform (0,1) random variables,\\nwe would do the following to obtain the action when drug i has resulted\\nin si successes out of a total of ni uses, i = 1,2.\\n>\\nf 1 = (s1 + 1)/(n1 + 2)\\n>\\nf 2 = (s2 + 1)/(n2 + 2)\\n>\\nr = (f 1 −f 2)/sqrt(f 1 ∗(1 −f 1)/n1 + f 2 ∗(1 −f 2)/n2)\\n>\\nx = pnorm(r)\\n>\\nx\\n>\\nrunif(1)\\nThe next to last output gives x and the ﬁnal output gives the value of the\\nrandom number. If the random number is less than x then drug 1 should\\nbe used next; if not, then drug 2 should be used.\\nFor instance, suppose drug 1 has been used 9 times, with 5 successes re-\\nsulting; and drug 2 has been used 5 times with 2 successes. To determine\\nthe next action do\\n>\\nf 1 = 6/11'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 671}, page_content='666 CHAPTER 16: Machine learning and big data\\n>\\nf 2 = 3/7\\n>\\nr = (f 1 −f 2)/sqrt(f 1 ∗(1 −f 1)/9 + f 2 ∗(1 −f 2)/5)\\n>\\nx = pnorm(r)\\n>\\nx\\n>\\n[1] 0.6636754\\n>\\nrunif(1)\\n>\\n[1] 0.2600016\\nBecause .2600, the value of the random number, is less than x = .6637,\\ndrug 1 should be used next.\\n2. The values f 1 and f 2 are slight variations of the fraction of successes\\nthat have been obtained when using each drug, with the actual fractions\\nbeing slightly modiﬁed so as to keep their values from being either 0 or\\n1.\\n3. The type of bandit problem considered in the preceding has many ap-\\nplications beyond drug testing. For instance, an advertiser may want to\\nchoose between two different internet advertisements, with an advertise-\\nment registering a “success” if it results in a user clicking on it.\\n4. Bandit results generalize to cases having more than two possible choices.\\nProblems\\n1. An insurance company in setting its auto insurance rates keeps a char-\\nacterizing vector (x1,...,x7) on each of its policy holders. In this vector,\\nx1 refers to the sex of the policy holder, with 1 for male, and 2 for fe-\\nmale; x2 is age, with values 1 for less than 25, 2 for between 25 and 40,\\n3 for between 40 and 60, 4 for between 60 and 70, and 5 for over 70; x3\\nis for the type of music best liked, with 1 for classical, 2 for country or\\neasy listening, and 3 for rap or heavy metal; x4 refers to education, with 1\\nfor did not graduate high school, 2 for high school but did not graduate\\ncollege, 3 for college graduate but without higher degree, and 4 is for a\\npostgraduate degree; x5 relates to body mass index, with 1 being below\\n22, 2 being between 22 and 27, 3 being between 27 and 33, and 4 over 33;\\nx6 is height, with 1 for less than ﬁve feet two inches, 2 for between ﬁve-\\ntwo and ﬁve-eight, 3 for between ﬁve-eight and six feet, and 4 for over six\\nfeet; x7 refers to the number of miles driven annually, with 1 for low, 2\\nfor medium, and 3 for high.\\nThe following is a random sample of historical data relating to only the\\nﬁrst 2 coordinates of the preceding characterizing vector. For each vector\\n(x1,x2) it gives the number of policy holders having that vector and the\\nnumber of them that had at least one accident in the past year.\\na.\\nUse the naive Bayes approach to estimate the probability that a\\n36-year-old woman has an accident in the following year.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 672}, page_content='Problems\\n667\\nTable 16.4 Historical Data.\\nVector\\nNumber with\\nNumber having an\\naccident\\n(1,1)\\n240\\n9\\n(1,2)\\n1050\\n18\\n(1,3)\\n1400\\n20\\n(1,4)\\n457\\n15\\n(1,5)\\n145\\n12\\n(2,1)\\n226\\n5\\n(2,2)\\n940\\n12\\n(2,3)\\n1420\\n14\\n(2,4)\\n420\\n11\\n(2,5)\\n142\\n13\\nb.\\nUse the modiﬁed version of the naive Bayes approach to estimate\\nthe probability that a 36-year-old woman has an accident in the fol-\\nlowing year.\\nc.\\nUse the naive Bayes approach to estimate the probability that a\\n26-year-old man has an accident in the following year.\\nd.\\nUse the modiﬁed version of the naive Bayes approach to estimate the\\nprobability that a 24-year-old man has an accident in the following\\nyear.\\n2. In the preceding problem, use the weighted distance approach to esti-\\nmate the probabilities that\\na.\\na 36-year-old woman has an accident in the next year;\\nb.\\na 26-year-old man has an accident in the next year.\\n3. Using the loss function approach of Section 16.5, assess which technique,\\nnaive Bayes or weighted distance, did best in estimating the probability\\nthat a\\na.\\n36-year-old woman has an accident in the following year;\\nb.\\n26-year-old man has an accident in the next year.\\nIn all cases, use all but the individual data value under consideration\\nto estimate the probability that a person with that characterizing vector\\nwould have an accident.\\n4. For each of the 6440 data values given in Table 16.4, determine the loss\\nincurred when their accident probability is estimated, using data from\\nthe other 6439 values, by\\na.\\nthe naive Bayes method;\\nb.\\nthe weighted-distance method.\\nc.\\nWhich method appears to work better in this example?\\n5. Whereas the loss function presented in Section 16.5 supposes that a loss\\n(1 −p)2 is incurred when an event whose estimated probability is p\\noccurs, another popular loss function assumes a loss of −log(p) when\\nan event whose estimated probability is p occurs. Show that the latter'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 673}, page_content='668 CHAPTER 16: Machine learning and big data\\nloss function also has the property that if the actual (though unknown)\\nprobability that an event occurs is α, then the expected loss when one\\nestimates that this probability is p is minimized when p = α.\\n6. Gather real data that is quantitative and do a study on whether logistics\\nregression or nearest neighbor yields the better result.\\n7. Suppose drug 1 has been used 55 times, with 36 successes resulting; and\\ndrug 2 has been used 22 times with 12 successes. With what probability\\ndoes the procedure of Section 16.7 use drug 1 next? Use R to make the\\nrandom choice.'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 674}, page_content='Appendix of Tables\\nTable A.1 Standard Normal Distribution Function: \\x02(x) =\\n1\\n√\\n2π\\n\\x02 x\\n−∞\\ne−y2/2 dy.\\nx\\n.00\\n.01\\n.02\\n.03\\n.04\\n.05\\n.06\\n.07\\n.08\\n.09\\n.0\\n.5000\\n.5040\\n.5080\\n.5120\\n.5160\\n.5199\\n.5239\\n.5279\\n.5319\\n.5359\\n.1\\n.5398\\n.5438\\n.5478\\n.5517\\n.5557\\n.5596\\n.5636\\n.5675\\n.5714\\n.5753\\n.2\\n.5793\\n.5832\\n.5871\\n.5910\\n.5948\\n.5987\\n.6026\\n.6064\\n.6103\\n.6141\\n.3\\n.6179\\n.6217\\n.6255\\n.6293\\n.6331\\n.6368\\n.6406\\n.6443\\n.6480\\n.6517\\n.4\\n.6554\\n.6591\\n.6628\\n.6664\\n.6700\\n.6736\\n.6772\\n.6808\\n.6844\\n.6879\\n.5\\n.6915\\n.6950\\n.6985\\n.7019\\n.7054\\n.7088\\n.7123\\n.7157\\n.7190\\n.7224\\n.6\\n.7257\\n.7291\\n.7324\\n.7357\\n.7389\\n.7422\\n.7454\\n.7486\\n.7517\\n.7549\\n.7\\n.7580\\n.7611\\n.7642\\n.7673\\n.7704\\n.7734\\n.7764\\n.7794\\n.7823\\n.7852\\n.8\\n.7881\\n.7910\\n.7939\\n.7967\\n.7995\\n.8023\\n.8051\\n.8078\\n.8106\\n.8133\\n.9\\n.8159\\n.8186\\n.8212\\n.8238\\n.8264\\n.8289\\n.8315\\n.8340\\n.8365\\n.8389\\n1.0\\n.8413\\n.8438\\n.8461\\n.8485\\n.8508\\n.8531\\n.8554\\n.8577\\n.8599\\n.8621\\n1.1\\n.8643\\n.8665\\n.8686\\n.8708\\n.8729\\n.8749\\n.8770\\n.8790\\n.8810\\n.8830\\n1.2\\n.8849\\n.8869\\n.8888\\n.8907\\n.8925\\n.8944\\n.8962\\n.8980\\n.8997\\n.9015\\n1.3\\n.9032\\n.9049\\n.9066\\n.9082\\n.9099\\n.9115\\n.9131\\n.9147\\n.9162\\n.9177\\n1.4\\n.9192\\n.9207\\n.9222\\n.9236\\n.9251\\n.9265\\n.9279\\n.9292\\n.9306\\n.9319\\n1.5\\n.9332\\n.9345\\n.9357\\n.9370\\n.9382\\n.9394\\n.9406\\n.9418\\n.9429\\n.9441\\n1.6\\n.9452\\n.9463\\n.9474\\n.9484\\n.9495\\n.9505\\n.9515\\n.9525\\n.9535\\n.9545\\n1.7\\n.9554\\n.9564\\n.9573\\n.9582\\n.9591\\n.9599\\n.9608\\n.9616\\n.9625\\n.9633\\n1.8\\n.9641\\n.9649\\n.9656\\n.9664\\n.9671\\n.9678\\n.9686\\n.9693\\n.9699\\n.9706\\n1.9\\n.9713\\n.9719\\n.9726\\n.9732\\n.9738\\n.9744\\n.9750\\n.9756\\n.9761\\n.9767\\n2.0\\n.9772\\n.9778\\n.9783\\n.9788\\n.9793\\n.9798\\n.9803\\n.9808\\n.9812\\n.9817\\n2.1\\n.9821\\n.9826\\n.9830\\n.9834\\n.9838\\n.9842\\n.9846\\n.9850\\n.9854\\n.9857\\n2.2\\n.9861\\n.9864\\n.9868\\n.9871\\n.9875\\n.9878\\n.9881\\n.9884\\n.9887\\n.9890\\n2.3\\n.9893\\n.9896\\n.9898\\n.9901\\n.9904\\n.9906\\n.9909\\n.9911\\n.9913\\n.9916\\ncontinued on next page\\n669'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 675}, page_content='670 Appendix of Tables\\nTable A.1 (Standard Normal Distribution Function: \\x02(x) =\\n1\\n√\\n2π\\n\\x02 x\\n−∞\\ne−y2/2 dy.\\nContinued)\\nx\\n.00\\n.01\\n.02\\n.03\\n.04\\n.05\\n.06\\n.07\\n.08\\n.09\\n2.4\\n.9918\\n.9920\\n.9922\\n.9925\\n.9927\\n.9929\\n.9931\\n.9932\\n.9934\\n.9936\\n2.5\\n.9938\\n.9940\\n.9941\\n.9943\\n.9945\\n.9946\\n.9948\\n.9949\\n.9951\\n.9952\\n2.6\\n.9953\\n.9955\\n.9956\\n.9957\\n.9959\\n.9960\\n.9961\\n.9962\\n.9963\\n.9964\\n2.7\\n.9965\\n.9966\\n.9967\\n.9968\\n.9969\\n.9970\\n.9971\\n.9972\\n.9973\\n.9974\\n2.8\\n.9974\\n.9975\\n.9976\\n.9977\\n.9977\\n.9978\\n.9979\\n.9979\\n.9980\\n.9981\\n2.9\\n.9981\\n.9982\\n.9982\\n.9983\\n.9984\\n.9984\\n.9985\\n.9985\\n.9986\\n.9986\\n3.0\\n.9987\\n.9987\\n.9987\\n.9988\\n.9988\\n.9989\\n.9989\\n.9989\\n.9990\\n.9990\\n3.1\\n.9990\\n.9991\\n.9991\\n.9991\\n.9992\\n.9992\\n.9992\\n.9992\\n.9993\\n.9993\\n3.2\\n.9993\\n.9993\\n.9994\\n.9994\\n.9994\\n.9994\\n.9994\\n.9995\\n.9995\\n.9995\\n3.3\\n.9995\\n.9995\\n.9995\\n.9996\\n.9996\\n.9996\\n.9996\\n.9996\\n.9996\\n.9997\\n3.4\\n.9997\\n.9997\\n.9997\\n.9997\\n.9997\\n.9997\\n.9997\\n.9997\\n.9997\\n.9998'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 676}, page_content='Appendix of Tables 671\\nTable A.2 Values of C(m,d,α).\\nm\\nd\\nα\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n5\\n.05\\n3.64\\n4.60\\n5.22\\n5.67\\n6.03\\n6.33\\n6.58\\n6.80\\n6.99\\n7.17\\n.01\\n5.70\\n6.98\\n7.80\\n8.42\\n8.91\\n9.32\\n9.67\\n9.97\\n10.24\\n10.48\\n6\\n.05\\n3.46\\n4.34\\n4.90\\n5.30\\n5.63\\n5.90\\n6.12\\n6.32\\n6.49\\n6.65\\n.01\\n5.24\\n6.33\\n7.03\\n7.56\\n7.97\\n8.32\\n8.61\\n8.87\\n9.10\\n9.30\\n7\\n.05\\n3.34\\n4.16\\n4.68\\n5.06\\n5.36\\n5.61\\n5.82\\n6.00\\n6.16\\n6.30\\n.01\\n4.95\\n5.92\\n6.54\\n7.01\\n7.37\\n7.68\\n7.94\\n8.17\\n8.37\\n8.55\\n8\\n.05\\n3.26\\n4.04\\n4.53\\n4.89\\n5.17\\n5.40\\n5.60\\n5.77\\n5.92\\n6.05\\n.01\\n4.75\\n5.64\\n6.20\\n6.62\\n6.96\\n7.24\\n7.47\\n7.68\\n7.86\\n8.03\\n9\\n.05\\n3.20\\n3.95\\n4.41\\n4.76\\n5.02\\n5.24\\n5.43\\n5.59\\n5.74\\n5.87\\n.01\\n4.60\\n5.43\\n5.96\\n6.35\\n6.66\\n6.91\\n7.13\\n7.33\\n7.49\\n7.65\\n10\\n.05\\n3.15\\n3.88\\n4.33\\n4.65\\n4.91\\n5.12\\n5.30\\n5.46\\n5.60\\n5.72\\n.01\\n4.48\\n5.27\\n5.77\\n6.14\\n6.43\\n6.67\\n6.87\\n7.05\\n7.21\\n7.36\\n11\\n.05\\n3.11\\n3.82\\n4.26\\n4.57\\n4.82\\n5.03\\n5.20\\n5.35\\n5.49\\n5.61\\n.01\\n4.39\\n5.15\\n5.62\\n5.97\\n6.25\\n6.48\\n6.67\\n6.84\\n6.99\\n7.13\\n12\\n.05\\n3.08\\n3.77\\n4.20\\n4.51\\n4.75\\n4.95\\n5.12\\n5.27\\n5.39\\n5.51\\n.01\\n4.32\\n5.05\\n5.50\\n5.84\\n6.10\\n6.32\\n6.51\\n6.67\\n6.81\\n6.94\\n13\\n.05\\n3.06\\n3.73\\n4.15\\n4.45\\n4.69\\n4.88\\n5.05\\n5.19\\n5.32\\n5.43\\n.01\\n4.26\\n4.96\\n5.40\\n5.73\\n5.98\\n6.19\\n6.37\\n6.53\\n6.67\\n6.79\\n14\\n.05\\n3.03\\n3.70\\n4.11\\n4.41\\n4.64\\n4.83\\n4.99\\n5.13\\n5.25\\n5.36\\n.01\\n4.21\\n4.89\\n5.32\\n5.63\\n5.88\\n6.08\\n6.26\\n6.41\\n6.54\\n6.66\\n15\\n.05\\n3.01\\n3.67\\n4.08\\n4.37\\n4.59\\n4.78\\n4.94\\n5.08\\n5.20\\n5.31\\n.01\\n4.17\\n4.84\\n5.25\\n5.56\\n5.80\\n5.99\\n6.16\\n6.31\\n6.44\\n6.55\\n16\\n.05\\n3.00\\n3.65\\n4.05\\n4.33\\n4.56\\n4.74\\n4.90\\n5.03\\n5.15\\n5.26\\n.01\\n4.13\\n4.79\\n5.19\\n5.49\\n5.72\\n5.92\\n6.08\\n6.22\\n6.35\\n6.46\\n17\\n.05\\n2.98\\n3.63\\n4.02\\n4.30\\n4.52\\n4.70\\n4.86\\n4.99\\n5.11\\n5.21\\n.01\\n4.10\\n4.74\\n5.14\\n5.43\\n5.66\\n5.85\\n6.01\\n6.15\\n6.27\\n6.38\\n18\\n.05\\n2.97\\n3.61\\n4.00\\n4.28\\n4.49\\n4.67\\n4.82\\n4.96\\n5.07\\n5.17\\n.01\\n4.07\\n4.70\\n5.09\\n5.38\\n5.60\\n5.79\\n5.94\\n6.08\\n6.20\\n6.31\\n19\\n.05\\n2.96\\n3.59\\n3.98\\n4.25\\n4.47\\n4.65\\n4.79\\n4.92\\n5.04\\n5.14\\n.01\\n4.05\\n4.67\\n5.05\\n5.33\\n5.55\\n5.73\\n5.89\\n6.02\\n6.14\\n6.25\\n20\\n.05\\n2.95\\n3.58\\n3.96\\n4.23\\n4.45\\n4.62\\n4.77\\n4.90\\n5.01\\n5.11\\n.01\\n4.02\\n4.64\\n5.02\\n5.29\\n5.51\\n5.69\\n5.84\\n5.97\\n6.09\\n6.19\\n24\\n.05\\n2.92\\n3.53\\n3.90\\n4.17\\n4.37\\n4.54\\n4.68\\n4.81\\n4.92\\n5.01\\n.01\\n3.96\\n4.55\\n4.91\\n5.17\\n5.37\\n5.54\\n5.69\\n5.81\\n5.92\\n6.02\\n30\\n.05\\n2.89\\n3.49\\n3.85\\n4.10\\n4.30\\n4.46\\n4.60\\n4.72\\n4.82\\n4.92\\n.01\\n3.89\\n4.45\\n4.80\\n5.05\\n5.24\\n5.40\\n5.54\\n5.65\\n5.76\\n5.85\\n40\\n.05\\n2.86\\n3.44\\n3.79\\n4.04\\n4.23\\n4.39\\n4.52\\n4.63\\n4.73\\n4.82\\n.01\\n3.82\\n4.37\\n4.70\\n4.93\\n5.11\\n5.26\\n5.39\\n5.50\\n5.60\\n5.69\\n60\\n.05\\n2.83\\n3.40\\n3.74\\n3.98\\n4.16\\n4.31\\n4.44\\n4.55\\n4.65\\n4.73\\n.01\\n3.76\\n4.28\\n4.59\\n4.82\\n4.99\\n5.13\\n5.25\\n5.36\\n5.45\\n5.53\\n120\\n.05\\n2.80\\n3.36\\n3.68\\n3.92\\n4.10\\n4.24\\n4.36\\n4.47\\n4.56\\n4.64\\n.01\\n3.70\\n4.20\\n4.50\\n4.71\\n4.87\\n5.01\\n5.12\\n5.21\\n5.30\\n5.37\\n∞\\n.05\\n2.77\\n3.31\\n3.63\\n3.86\\n4.03\\n4.17\\n4.29\\n4.39\\n4.47\\n4.55\\n.01\\n3.64\\n4.12\\n4.40\\n4.60\\n4.76\\n4.88\\n4.99\\n5.08\\n5.16\\n5.23'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 677}, page_content='Index\\nA\\nAcceptance, of hypothesis, see\\nHypothesis testing\\nAdditive property, of chi-square\\nrandom variables, 273\\nAlgebra of events, 66, 66f, 67, 67f\\nAnalysis, of residuals, 395, 396f, 397\\nAnalysis of variance (ANOVA),\\n453–487\\nintroduction, 453, 454\\none-way, 454, 456–463, 463t,\\n464–469\\nmultiple comparisons of sample\\nmeans, 466–468\\nwith unequal sample sizes, 468,\\n469\\noverview, 454–456\\ntwo-way, 454\\nhypothesis testing for, 474–479\\ninteraction, 479–487\\nintroduction and parameter\\nestimation, 470–473\\nANOVA, see Analysis of variance\\nApproximately normal, 33, 34f\\nAssessment, of models, 395, 396f,\\n397\\nAssignable cause, 555\\nAssociation, causation v., 41\\nAssociative law, 66, 67\\nAveraging notation, in two-way\\nanalysis of variance, 471,\\n472\\nB\\nBalanced case, in one-way analysis of\\nvariance, 469\\nBandit problems, 650, 664, 666\\nBar graph, 12, 12t, 13f\\nBayes approach, naive, 649, 652\\nBayes estimator, 246, 287–292\\nBayes’ formula, 79, 79f, 80–86\\nBayes theorem, 649, 651\\nBayesian approach, 246, 606, 607\\nBehrens-Fisher problem, 333\\nBernoulli, Jacob, 6\\nBernoulli, James, 151\\nBernoulli density function, 427\\nBernoulli parameter, maximum\\nlikelihood estimator of,\\n247–250\\nBernoulli populations\\nhypothesis testing in, 339–345\\ntesting equality of parameters in\\ntwo of, 342–345\\nBernoulli random variables, 151–153,\\n153f, 154–158\\nconﬁdence interval for mean of,\\n275–279, 279t\\ngeneration of, 640\\nhypergeometric random variables\\nand, 167–169\\nsign test and, 529\\nsigned rank test and, 533, 534\\nBeta distribution, 289\\nBetween samples sum of squares,\\n459–461, 463t, 469\\nBias, of estimators, 281\\nBig data, 649–666\\nBimodal data set, 36, 36f\\nBinary output data, logistic regression\\nmodels for, 425, 426,\\n426f, 427, 428\\nBinomial distribution, hypothesis\\ntesting in, 339–345\\nBinomial random variables, 151–153,\\n153f, 154–158\\ncentral limit theorem and, 226,\\n227, 227f\\ngeneration of, 640, 641\\nhypergeometric random variables\\nand, 169–171, 236, 237\\nPoisson random variables and,\\n160–163\\nsign test and, 530\\nwith parameters, 157\\nBootstrap method, 623–631\\nmean square error and, 627, 628\\npopulation mean and, 623, 624,\\n628–631\\npopulation variance and, 623–625\\nprobability and, 629–631\\nrandom variable generation, 627\\nBox plot, 29, 29f\\nBox-Muller method, 643\\nC\\nCategorical data analysis, goodness of\\nﬁt tests and, 499–521\\ncritical region determination by\\nsimulation, 506–508\\ndummy variables for, 424, 425\\nintroduction, 499\\nKolmogorov–Smirnov goodness of\\nﬁt test for continuous data,\\n517–519, 519f, 520, 521\\ntests of independence in\\ncontingency tables,\\n510–514\\ntests of independence in\\ncontingency tables having\\nﬁxed marginal totals,\\n514–517\\n673'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 678}, page_content='674 Index\\ntests when all parameters are\\nspeciﬁed, 499–507\\ntests when some parameters are\\nunspeciﬁed, 508–510\\nCausation, association v., 41\\nCentral limit theorem, 224–227,\\n227f, 228–230\\napproximate distribution of\\nsample mean, 227–229\\nbinomial random variables and,\\n226, 227, 227f\\ndeﬁned, 224\\nindependent random variables,\\n237, 238\\nMonte Carlo study and, 644\\nprobability mass function and,\\n226, 227, 227f\\nsample size needed, 230, 230f\\nChance variation, 555\\nChannel noise disturbance, 184\\nCharacterizing vector, 649–663\\nhistorical, 662\\nquantitative, 662\\nChebyshev’s inequality, 29–31, 31t\\none-sided, 32, 33\\nrandom variables and, 139, 142\\nweak law of large numbers, 141\\nChi-square approximation, 508\\nChi-square density function,\\n201–203, 203f, 204, 205,\\n205f, 206\\nChi-square distribution, 201–203,\\n203f, 204, 205, 205f, 206,\\n336, 597\\ngamma random variables and,\\n204, 205, 205f\\nt-distribution and, 206, 206f, 207,\\n207f, 208, 208f\\nChi-square goodness of ﬁt tests,\\nhistory of, 6\\nChi-square probabilities, 203\\nChi-square random variables, 203,\\n203f\\nadditive property of, 273\\nestimators of variance from, 455,\\n456\\nF-distribution and, 208, 209, 209f\\nmean and variance of, 206\\nsample mean and variance\\ndistribution with, 233\\nChoosing, of normal prior, 290, 291\\nClass boundaries, 16, 17, 18f\\nClass intervals, 16, 17, 18f\\nCoefﬁcient of determination, sample\\ncorrelation coefﬁcient and,\\n392–395\\nCoefﬁcient of multiple\\ndetermination, 419\\nColumn factors\\nhypothesis testing for, 474–479\\nin two-way analysis of variance,\\n470\\ndeviation from grand mean due\\nto, 472\\nrow factor interaction with,\\n479–487\\nColumn sum of squares, 477\\nCombinations, permutations and, 74\\nCombining, of unbiased estimators,\\n282–284\\nCommon density function,\\nindependent random\\nvariables and, 112, 113\\nCommutative law, 66, 66f, 67\\nComparison, of sample means,\\n466–468\\nComplement\\nin sample space, 64\\nin Venn diagram, 66, 66f, 67\\nComponent weighted distances, 659,\\n660\\nComposite hypothesis, 306\\nComputational identity, of sum of\\nsquares of residuals, 375\\nConditional densities, 116\\nConditional distributions, 114–116,\\n342–345\\nrandom numbers and, 174, 175,\\n175t, 176, 177\\nConditional probability, 75–77, 77f,\\n78, 79, 115, 649, 651\\nindependent events and, 86–89\\nPoisson random variables and,\\n163–165\\nConditional probability density\\nfunction, 287–289\\nConditional probability mass\\nfunction, 114, 115\\nConﬁdence, 245\\nConﬁdence interval, see also speciﬁc\\nintervals, e.g. 95 Percent\\nconﬁdence interval\\nfor difference in means of two\\nnormal distributions, 270,\\n271, 271t, 272–275\\nfor difference in population\\nmeans, 466–468\\nfor estimating unknown mean,\\n257–262\\nfor exponential distribution in life\\ntesting, 596, 597\\nfor mean of Bernoulli random\\nvariable, 275–279, 279t\\nfor mean of exponential\\ndistribution, 280\\nfor normal mean with unknown\\nvariance, 265, 266, 268,\\n269\\nfor regression parameters\\nα, 386\\nβ, 381\\nfor unknown probability, 275, 276,\\n276t, 277–279\\nfor variance of normal distribution,\\n269, 270, 271t\\nin sequential testing for\\nexponential distribution\\nin life testing, 599–603\\none-sided lower, 259, 260, 279\\none-sided upper, 259–261, 265,\\n279\\nprediction interval v., 391\\ntwo-sided, 258, 260–268, 272,\\n275, 279, 295, 296\\nConﬁdence interval estimators\\nfor mean of exponential\\ndistribution, 280\\nof difference in means of two\\nnormal distributions,\\n270–275\\nof mean response, 386–389, 420,\\n421\\nContingency tables\\ntests of independence in, 510–514\\nwith ﬁxed marginal totals, tests of\\nindependence in, 514–517\\nContinuous data,\\nKolmogorov–Smirnov\\ngoodness of ﬁt test for,\\n517–519, 519f, 520, 521\\nContinuous random variables, 101,\\n103–105, 641–644\\nControl charts, 555\\nfor population mean, 573–583\\nmoving-average, 573–575\\nfraction defective, 567–569'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 679}, page_content='Index 675\\nmean, 556–558, 558f, 559–564,\\n566f\\ncase of unknown, 559–564\\nnumber of defect, 569–572\\nvariance, 564–566, 566f, 567\\nControl group, 174\\nControl limits\\nfor fraction defective, 568, 569\\nfor mean control charts, 556–558,\\n558f, 559\\nfor moving-average, 573–575, 575f\\nfor variance control charts, 565,\\n566, 566f, 567\\nControls, 345\\nCorrelation analysis, history of, 6\\nCounting\\nbasic principles of, 70–73\\nnotation and terminology, 73–75\\nCovariance, 132, 133\\nCritical region, 306, 308\\nfor goodness of ﬁt tests when all\\nparameters are speciﬁed,\\n500, 501\\none-sided, 315\\nsimulation for determination of,\\n506–508\\nCumulative distribution function,\\n101–103, 103f, 104, 106\\nexponential random variables and,\\n190\\nprobability density function and,\\n103, 104, 104f, 105\\nCumulative sum control charts,\\n581–583\\nD\\nData collection, descriptive statistics\\nand, 1, 2\\nData sets\\ndescribing, 12–19\\nfrequency tables and graphs, 12,\\n12t, 13f, 14\\ngrouped data, histograms,\\nogives, and stem and leaf\\nplots, 16–18, 18f, 19\\nrelative frequency tables and\\ngraphs, 12, 13, 13f, 14,\\n14f, 15\\nnormal and skewed, 33, 33f, 34,\\n34f, 35, 36\\nsummarizing, 19–29\\nsample mean, sample median,\\nand sample mode, 19–24\\nsample percentiles and box\\nplots, 26–28, 28t, 29, 29f\\nsample variance and sample\\nstandard deviation, 24–26\\nDefects, probability of, 339–346\\nDeMorgan’s laws, 67\\nDensity, mode of, 291\\nDensity function, see also Probability\\ndensity function\\nBernoulli, 427\\nchi-square, 201–203, 203f, 204,\\n205, 205f, 206\\ncommon, 112, 113\\nconditional probability, 287–289\\nF, 208, 209, 209f\\ngamma distribution and, 199–201\\njoint, 246, 247, 251, 254\\njoint probability, 109, 110\\nnormal, 179, 179f, 203, 203f\\nof logistics distribution, 209\\nof standard normal, 189\\nposterior, 287, 291\\nrandom variables and, 122\\nRayleigh, 593\\nt, 206, 206f, 263, 263f\\nWeibull, 609, 610f\\nDependent events, 86–88, 88f, 89\\nDependent variable, see Response\\nvariable\\nDES, see Diethylstilbestrol\\nDescriptive statistics\\nChebyshev’s inequality, 29–31, 31t\\ndata collection and, 1, 2\\ndescribing data sets, 12–19\\nfrequency tables and graphs, 12,\\n12t, 13f, 14f\\ngrouped data, histograms,\\nogives, and stem and leaf\\nplots, 16–18, 18f, 19\\nrelative frequency tables and\\ngraphs, 14–16\\nhistory of, 7\\nnormal data sets, 33, 33f, 34, 34f,\\n35, 36\\npaired data sets and the sample\\ncorrelation coefﬁcient, 36,\\n37, 37t, 38, 38f, 39, 40,\\n40f, 41–43\\nsummarizing data sets, 19–29\\nsample mean, sample median,\\nand sample mode, 19–24\\nsample percentiles and box\\nplots, 26–28, 28t, 29, 29f\\nsample variance and sample\\nstandard deviation, 24–26\\nDeviation from the grand mean due\\nto column j, 472\\nDeviation from the grand mean due\\nto row i, 472\\nDiethylstilbestrol (DES), 344\\nDifference, in means of two normal\\ndistributions, 270–275\\nDiscrete inverse transform method,\\n640, 641\\nDiscrete random variables, 101, 102\\nexpectation and, 117\\ngeneration of, 639–641\\nprobability mass function and,\\n102, 103, 639–641\\nDispersion parameter, 210\\nDistance weighted method, 658\\nDistribution\\nbinomial, hypothesis testing in,\\n339–345\\nchi-square, 201–203, 203f, 204,\\n205, 205f, 206, 336, 597\\nconditional, 346–348\\nexponential, conﬁdence interval\\nfor mean of, 280\\nF, 338\\ngamma, 597, 606, 607\\nhypothesis testing for determining\\nequality of m population\\ndistributions, 516, 517\\nlife, 255–257\\nmultivariate normal, 414\\nnormal\\nconﬁdence interval for variance\\nof, 269, 270, 271t\\nestimation of difference in\\nmeans of, 270–275\\nof least squares estimators,\\n371–377, 377f\\nof sample, goodness of ﬁt tests for,\\n499–508\\nPoisson\\ngoodness of ﬁt tests for, 508–510\\nhypothesis testing concerning\\nmean of, 345–348\\nvariance in, 405, 406\\nprior, 287–292, 606, 607\\nprobability, of estimator of mean\\nresponse, 387, 388\\nrate of, 592\\nuniform, estimating mean of, 254,\\n255'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 680}, page_content='676 Index\\nDistribution function, see also\\nCumulative distribution\\nfunction; Probability\\ndistribution function\\nempirical, 624–626\\nmoment generating function and,\\n139\\nof continuous random variable,\\n641\\nof normal random variables, 181,\\n182\\nof rank sum test, 539\\nPoisson, number of defects and,\\n569–572\\nprobability and, 101\\nrandom variables and, 101, 102,\\n624\\nsigned rank test for, 533\\ntwo-sample problem and, 538\\nDistribution results, summary of, 392\\nDistributions in R, 210, 211\\nDistributive law, 66, 67, 67f\\nDoll, R., 19\\nDot notation, in two-way analysis of\\nvariance, 471, 472\\nDouble-blind test, 174\\nDummy variables, for categorical\\ndata, 424, 425\\nE\\nEffect of column j, 480\\nEffect of row i, 480\\nEmpirical distribution function,\\n624–626\\nEmpirical rule, 35\\nEntropy, 120\\nEqual variance, testing equality of\\nmeans of two normal\\npopulations with, 333,\\n334t\\nEquality\\nof means of two normal\\npopulations, 326–330,\\n330f, 331–334, 334t, 335\\ncase of known variance,\\n326–328\\ncase of unknown and equal\\nvariance, 333, 334t\\ncase of unknown variance,\\n328–330, 330f, 331–333\\nhypothesis testing of, 326–330,\\n330f, 331–334, 334t, 335\\npaired t-test, 333–335\\nof m population distributions,\\nhypothesis testing for, 516,\\n517\\nof parameters in two Bernoulli\\npopulations, 342–345\\nof population means, hypothesis\\ntesting of, 454, 456–469\\nof variance, of two normal\\npopulations, 337, 338\\nError\\nmean square error of point\\nestimators, 281–287\\ntype I, 306, 307\\ntype II, 306, 310, 313\\nError sum of squares, 475, 477\\nEstimates\\ndeﬁned, 246\\ninterval, 245, 257–270\\nconﬁdence interval for normal\\nmean with unknown\\nvariance, 262, 263, 263f,\\n264–269\\nconﬁdence interval for variance\\nof normal distribution,\\n269, 270, 271t\\nfor unknown mean, 257–262\\nhypothesis testing v., 318\\nprediction, 268, 269\\nEstimation\\nof life distributions, 255–257\\nof mean of uniform distribution,\\n254, 255\\nof mean response, 420–423\\nof parameters, 245–292\\napproximate conﬁdence interval\\nfor mean of Bernoulli\\nrandom variable,\\n275–279, 279t\\nBayes estimator, 246, 287–292\\nconﬁdence interval for mean of\\nexponential distribution,\\n280\\nfor two-way analysis of variance,\\n470–473\\ninterval estimates, 245,\\n257–270, 318\\nintroduction, 245, 246\\nmaximum likelihood estimators,\\n245–257, 271, 291, 509\\nof difference in means of two\\nnormal distributions,\\n270–275\\nof life distributions, 255–257\\npoint estimator evaluation,\\n281–287\\nEstimators\\nBayes, 246, 287–292\\nbias of, 281\\nconﬁdence interval\\nfor mean of exponential\\ndistribution, 280\\nof difference in means of two\\nnormal distributions,\\n270–275\\nof mean response, 386–389,\\n420, 421\\ndeﬁned, 246\\nleast squares, 395\\ndistribution of, 371–377, 377f\\nfor Weibull distribution in life\\ntesting, 611–613\\nin multiple linear regressions,\\n410–419, 421\\nin polynomial regression,\\n406–409\\nof regression parameters,\\n367–370\\nmaximum likelihood, 245–257,\\n271, 291\\nevaluation of, 285–287\\nfor exponential distribution in\\nlife testing, 595, 596,\\n601–605\\nfor life distributions, 255–257\\nfor mean of exponential\\ndistribution, 280\\nfor Weibull distribution in life\\ntesting, 609–611\\nin logistic regression models,\\n425\\nin sequential testing for\\nexponential distribution\\nin life testing, 601, 602\\nleast squares estimators as, 374,\\n402\\nof Bernoulli parameter, 247–250\\nof difference in means of two\\nnormal distributions, 270\\nof normal population, 251–254\\nof Poisson parameter, 250, 251\\nweighted least squares\\nestimators as, 402\\nof deviance from grand mean, 472,\\n473\\nof grand mean, 472, 473'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 681}, page_content='Index 677\\nof variance, 455, 456\\nfor one-way analysis of variance,\\n454, 456–469\\nfor two-way analysis of variance,\\n474–479\\nfor two-way analysis of variance\\nwith interaction, 479–487\\nweighted least squares, 400–406\\npoint\\nevaluation of, 281–287\\nfor hypothesis testing, 307, 308\\nof mean response, 386, 420\\npooled, 275, 303, 329\\nunbiased, 281–287\\nEvaluation, of point estimator,\\n281–287\\nEvents\\nalgebra of, 66, 66f, 67, 67f\\nindependent, 86–88, 88f, 89\\nodds of, 69\\nExpectation, 117–121, 121f\\nof a random variable function,\\n123, 124\\nof sums of random variables,\\n124–128\\nproperties of, 121–128\\nExpected value, see Expectation\\nExponential distribution\\nconﬁdence interval for mean of,\\n280\\ngamma distribution and, 201\\nin life testing, 594–607\\nBayesian approach, 606, 607\\nsequential testing, 599–603\\nsequential testing with stopping\\nat rth failure, 594–599\\nsimulation testing with stopping\\nby ﬁxed time, 603–605\\nPoisson process and, 193–196\\nExponential random variables,\\n190–194, 194f, 195–199\\ngeneration of, 642\\nmemoryless, 191\\nmoment generating functions and,\\n190\\nPoisson process, 193, 194, 194f,\\n195, 196\\nprobability and, 190, 192\\nsample means for, 230\\nExponentially weighted\\nmoving-average control\\ncharts, 576, 577, 577f,\\n578–580, 580f\\nF\\nF-density function, 208, 209, 209f\\nF-distribution, 208, 209, 209f, 338\\nF-statistic, in two-way analysis of\\nvariance with interaction,\\n485–487\\nFailure rate, see Hazard rate\\nFinite populations, sampling\\ndistributions from,\\n234–238\\nFirst quartile, 27–29\\nFisher, Ronald A., 6\\nFisher-Irwin test, 343\\nFixed margins, contingency tables\\nwith, tests of\\nindependence in, 514–517\\nFlight of interest, 649\\nFraction defective control charts,\\n567–569\\nFrequency interpretation\\nof expectations, 117\\nprobability, 63\\nFrequency tables and graphs, 12, 12t,\\n13f, 14, 14f\\nfrequency histogram, 16\\nfrequency polygon, 12, 14, 14f\\nrelative, 14, 15, 15f, 16\\nsample mean and, 21, 23\\nsample median and, 22–24\\nsample mode and, 23, 24\\nFriendship network, 130, 130f\\nFriendship paradox, 130, 131\\nFuture response, prediction interval\\nof, 389–391\\nin multiple linear regression, 420,\\n422, 424\\nG\\nGalton, Francis, 6, 381, 382\\nGamma density, 201, 202f\\nGamma distribution, 199, 597, 606,\\n607\\nGamma function, 199, 200\\nGamma random variables, 201, 202f\\nchi-square distribution and, 204,\\n205, 205f\\nGauss, Karl Friedrich, 6\\nGeneration\\nof random numbers, 619–622\\nof random variables, 507, 627,\\n639–641\\nGini index, 43–48, 48f\\nGoodness of ﬁt tests, 499–521\\ncritical region determination by\\nsimulation, 506–508\\nintroduction, 499\\nKolmogorov–Smirnov goodness of\\nﬁt test for continuous data,\\n517–519, 519f, 520, 521\\ntests of independence in\\ncontingency tables,\\n510–514\\ntests of independence in\\ncontingency tables having\\nﬁxed marginal totals,\\n514–517\\ntests when all parameters are\\nspeciﬁed, 500–508\\ntests when some parameters are\\nunspeciﬁed, 508–510\\nGosset, W.S., 6\\nGrand mean, 472, 473, 480\\nGraunt, John, 4, 5\\nGrouped data, 16–18, 18f, 19, 20\\nH\\nHalley, Edmund, 5\\nHardy’s lemma, 38\\nHazard rate, 591\\nHazard rate functions, 591–594\\nHill, A.B., 19\\nHistograms, 16, 17, 18f\\nnormal, 33, 33f, 34–36, 36f\\nHistorical data, 649, 650, 654, 655,\\n658, 662\\nbase, 649, 651, 654, 655, 657, 658,\\n660, 661\\nﬂights, 651–653, 659\\nvalues, 649, 658, 662\\nHypergeometric random variables,\\n167–171\\nBernoulli random variables and,\\n168\\nbinomial random variables and,\\n170, 171, 236, 237\\nmean and variance of, 167–169\\nHypothesis testing, 305–348\\nfor two-way analysis of variance,\\n474–479\\nin Bernoulli populations, 339–345\\ninterval estimates v., 318\\nintroduction, 305\\nmultiple linear regression and, 418,\\n419'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 682}, page_content='678 Index\\nof equality of means of two normal\\npopulations, 326–335\\ncase of known variance,\\n326–328\\ncase of unknown and unequal\\nvariance, 333, 334t\\ncase of unknown variance,\\n328–330, 330f, 331–333\\nof equality of m population\\ndistributions, 516, 517\\nof equality of population means,\\n454, 456–469\\nof equality of variance of two\\nnormal populations, 337,\\n338\\nof independence in contingency\\ntables, 510–514\\nof independence in contingency\\ntables having ﬁxed\\nmarginal totals, 514–517\\nof independence of characteristics\\nof population member,\\n510–514\\nof mean of normal population,\\n307–326\\ncase of known variance, 307–319\\ncase of unknown variance,\\n319–326\\nof mean of Poisson distribution,\\n345–348\\nof multiple population means, 454\\nof probability distribution of\\nsample, 499–508\\nof regression parameters\\nα, 386\\nβ, 379, 380\\nof regression to mean, 382, 383\\nof row and column interaction,\\n479–487\\nof variance of normal population,\\n336–338\\npaired t-test, 333–335\\nrobustness of, 318\\nsigniﬁcance levels, 306, 307\\nI\\nIndependence, tests of\\nin contingency tables, 510–514\\nin contingency tables having ﬁxed\\nmarginal totals, 514–517\\nIndependent events, 86–88, 88f, 89\\nIndependent increment assumption,\\n194, 196\\nIndependent random variables,\\n111–114\\ncentral limit theorem, 224–230,\\n237\\nmoment generating functions of,\\n138, 139\\nsample mean and variance\\ndistribution with, 233\\nsigned rank test and, 534\\nIndependent variable, see Input\\nvariable\\nIndicator random variable, 100, 101\\ncovariance of, 133\\nexpectation for, 118\\nvariance of, 129, 130\\nIndividual moment generating\\nfunctions, 138, 139\\nIndividual probability mass function,\\njoint and, 106–109\\nInferential statistics\\nhistory of, 4–7\\nprobability models and, 2, 3\\nInheritance, regression to mean and,\\n381, 382, 382f, 383, 384,\\n384f\\nInput variable, 365\\nvariation in response to, 392–395,\\n400–404, 404f, 405, 406\\nInteraction, two-way analysis of\\nvariance with, 455,\\n479–487\\nIntersection\\nin Venn diagram, 66, 66f\\nof sample space, 65\\nInterval estimates, 245, 257–270\\nconﬁdence interval for normal\\nmean with unknown\\nvariance, 262–268\\nconﬁdence interval for variance of\\nnormal distribution,\\n269–271\\nfor unknown mean, 257–262\\nhypothesis testing v., 318\\nprediction, 268, 269\\nInverse transformation method, 640,\\n641\\nith order statistic, 596\\nJ\\nJoint cumulative probability\\ndistribution function, 105,\\n113, 114\\nJoint density\\nconditional densities and, 116\\nrandom numbers and, 178, 179\\nJoint probability density function,\\n109, 110, 246, 247, 251,\\n254\\nJoint probability mass function\\nconditional probability mass\\nfunction and, 115\\nindividual and, 106–109\\nJointly continuous, 109, 110, 112, 113\\nJointly distributed random variables,\\n105–116\\nconditional distributions, 114–116\\nindependent, 111–114\\nK\\nk-nearest neighbor rule, 657, 662\\nKolmogorov–Smirnov goodness of ﬁt\\ntest, for continuous data,\\n517–519, 519f, 520, 521\\nKolmogorov–Smirnov test statistic,\\n517–519, 519f, 520, 521\\nKolmogorov’s law of fragmentation,\\n253\\nKruskal–Wallis test, 543\\nL\\nLaplace, Pierre-Simon, 6\\nLateness probability, 649, 651, 657,\\n659–661\\nLeast squares estimators, 367–428\\ndistribution of, 371–377, 377f\\nfor Weibull distribution in life\\ntesting, 611–613\\nin multiple linear regression,\\n410–416, 421, 422\\nin polynomial regression, 406–409\\nof regression parameters, 367–370\\nweighted, 400–404, 404f, 405, 406\\nLeft-end inclusion convention, 16\\nLevel of signiﬁcance, see Signiﬁcance\\nlevel\\nLevels, in two-way analysis of\\nvariance, 470\\nLife distributions, estimation of,\\n255–257'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 683}, page_content='Index 679\\nLife testing, 591–613\\nexponential distribution in,\\n594–607\\nBayesian approach, 606, 607\\nsequential testing, 599–603\\nsimulation testing with stopping\\nat rth failure, 594–599\\nsimulation testing with stopping\\nby ﬁxed time, 603–605\\nhazard rate functions, 591–594\\nintroduction, 591\\ntwo-sample problem, 607–609\\nWeibull distribution in, 609, 610,\\n610f, 611–613\\nLikelihood function, 247\\nLine graph, 12, 13f\\nLinear regression equation, 365–367,\\nsee also Multiple linear\\nregression\\nassessment of, 395, 396f, 397\\nLinearity, transforming to, 396–398,\\n398f, 399, 400\\nLogarithms, for transforming to\\nlinearity, 396–398, 398f,\\n399, 400\\nLogistic regression models, for binary\\noutput data, 425, 426,\\n426f, 427, 428\\nLogistics distribution, of random\\nvariables, 209, 210\\nLogistics random variable, 209\\nLogistics regression, 663, 664\\nLogistics regression function, 426,\\n426f\\nLogit, 427\\nLognormal distribution, 253\\nLorenz curve, 43, 44, 44f, 45, 45f, 46,\\n46f, 47, 48\\nLower conﬁdence interval\\nfor difference in means of two\\nnormal distributions,\\n270–275\\nfor normal mean with unknown\\nvariance, 266–268\\nfor unknown mean, 262\\nfor unknown probability, 275, 276\\nfor variance of normal distribution,\\n276\\nLower control limits\\nfor exponentially weighted\\nmoving-average, 578–580,\\n580f\\nfor fraction defective, 568, 569\\nfor mean control charts, 557–559\\nfor moving-average, 574, 575, 575f\\nfor number of defects, 569–572\\nfor variance control charts, 565,\\n566, 566f, 567\\nM\\nMachine learning, 649–666\\nassessing the approaches, 660, 661\\nchoosing best probability, bandit\\nproblem, 664–666\\ndistance based estimators,\\nk-nearest neighbors rule,\\n657–660\\nintroduction to, 649, 650\\nlate ﬂight probabilities, 650, 651\\nnaive Bayes approach, 651–657\\nvariation of, 654–657\\nquantitative characterizing vectors,\\n662–664\\nMann-Whitney test, see Rank sum\\ntest\\nMarginal probability mass function,\\n108\\nMarkov’s inequality, random\\nvariables and, 139–141\\nMass function, see Probability mass\\nfunction\\nMatrix notation\\nfor multiple linear regression, 411,\\n412\\nfor polynomial regression, 409\\nMaximum likelihood estimators,\\n245–257, 271, 291, 509\\nevaluation of, 285–287\\nfor exponential distribution in life\\ntesting, 595, 596, 603–605\\nfor life distributions, 255–257\\nfor mean of exponential\\ndistribution, 280\\nfor Weibull distribution in life\\ntesting, 609–611\\nin logistic regression models, 427\\nin sequential testing for\\nexponential distribution\\nin life testing, 601, 602\\nleast squares estimators as, 374,\\n375\\nof Bernoulli parameter, 247–250\\nof difference in means of two\\nnormal distributions, 231\\nof normal population, 251–254\\nof Poisson parameter, 250, 251\\nweighted least squares estimators\\nas, 402\\nMean, see also Population means;\\nSample mean\\nconﬁdence interval estimators of\\nmean response, 386–389,\\n420–422\\nestimation of difference in means\\nof two normal\\ndistributions, 270–275\\nfor exponentially weighted\\nmoving-average, 576\\nfor moving-average, 574\\ngrand, 472, 473, 480\\nnormal, conﬁdence intervals for,\\n262, 263, 263f, 264–268\\nof Bernoulli random variable,\\nconﬁdence interval for,\\n275–279\\nof chi-square random variable, 205\\nof exponential distribution,\\nconﬁdence interval for,\\n280\\nof hypergeometric random\\nvariables, 167–169\\nof least squares estimators, 371,\\n372\\nof normal population, hypothesis\\ntesting concerning,\\n307–309, 309f, 310–326\\ncase of known variance, 307–319\\ncase of unknown variance,\\n319–326\\nof normal random variables,\\n180–182\\nof Poisson distribution, hypothesis\\ntesting for, 345–348\\nof uniform distribution, 254, 255\\nof uniform random variables, 173\\npermutation tests and, 635\\nPoisson distribution with\\nunknown value of,\\ngoodness of ﬁt tests for,\\n508–510\\npopulation, 222, 223, 223f\\nregression to, 381, 382, 382f, 383,\\n384, 384f, 385, 385f, 386\\ntesting equality of means of two\\nnormal populations,\\n326–330, 330f, 331–335\\ncase of known variance,\\n326–328'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 684}, page_content='680 Index\\ncase of unknown and equal\\nvariance, 333, 334\\ncase of unknown variance,\\n328–330, 330f, 331–333\\npaired t-test, 333–335\\nunknown\\nconﬁdence intervals for normal\\nmean with unknown\\nvariance, 262–269\\ncontrol charts for, 559–564\\nestimates of, 257–262\\nMean control chart, 556–558, 558f,\\n559–564\\ncase of unknown, 559–564\\nrecursive formula and, 561, 562\\nMean life, maximum likelihood\\nestimator of, 605\\nMean response\\nestimation of, 420, 421\\nstatistical inferences concerning,\\n386–389\\nMean square error\\nbootstrap method and, 627, 628\\nof point estimators, 281–287\\nMedian, sign test for, 531, 532, 532f\\nMemoryless, exponential random\\nvariables, 191\\nModal values, 23\\nMode, of density, 291\\nModels, assessment of, 395, 396f, 397\\nMoment generating functions\\nchi-square distribution, 201\\nchi-square random variable,\\n201–203, 203f, 204\\nexponential random variables and,\\n190\\ngamma distribution and, 199–201\\nnormal random variables and, 186\\nof Poisson random variables,\\n159–161\\nof random variables, 138, 139\\nMonte Carlo simulation, 267, 622,\\n623\\ndetermining runs in, 644, 645\\nMoving-average control charts,\\n573–575, 575f\\nexponentially weighted, 576, 577,\\n577f, 578–580, 580f\\nMultidimensional integrals,\\nsimulation of, 267, 268\\nMultiple comparisons, of sample\\nmeans, 466–468\\nMultiple linear regression, 410–424\\nMultiple probability distributions,\\nequality testing of,\\n541–543\\nMultiple regression equation, 366\\nMultivariate normal distribution, 414\\nMutually exclusive, in sample space,\\n65\\nN\\nNaive Bayes approach, 649, 652, 654\\nNaive Bayes estimated probability,\\n654\\nNatural and Political Observations\\nMade upon the Bilk of\\nMortality, 4, 5\\nNatural estimator, 651\\nNearest neighbor rules, 649, 662\\nNegatively correlated, 39\\nNeyman, Jerzy, 7\\n90 Percent prediction interval\\nfor variance of normal distribution,\\n270\\nof difference in means of two\\nnormal distributions, 290,\\n291\\n95 Percent conﬁdence interval\\nfor estimating unknown mean,\\n257–262\\nfor mean of exponential\\ndistribution, 280\\nfor normal mean with unknown\\nvariance, 264–268\\nfor regression parameters, 381\\nfor unknown probability, 277, 278\\nof difference in means of two\\nnormal distributions,\\n270–275\\nof mean response, 388\\n99 Percent prediction interval, 424\\n99 Percent conﬁdence interval\\nfor estimating unknown mean, 261\\nfor unknown probability, 266\\nNonparametric hypothesis tests,\\n529–547\\nintroduction to, 529\\nruns test for randomness, 544–547\\nsign test, 529–532, 532f, 533\\nsigned rank test, 533, 534, 534f,\\n535–538\\ntwo-sample problem, 538–543\\ntesting the equality of multiple\\nprobability distributions,\\n541–543\\nNonparametric inference, 221, 222\\nNonrandom sample, 3\\nNormal approximations, in\\npermutation tests,\\n634–637\\nNormal data sets, 33, 34, 34f, 35\\nNormal density function, 179, 179f\\nNormal distribution\\nconﬁdence interval for variance of,\\n269–271\\nestimation of difference in means\\nof, 270–275\\nNormal equations\\nin multiple linear regression, 411,\\n412\\nin polynomial regression, 407\\nof regressions, 367, 368\\nNormal histograms, 17, 33\\nNormal mean, with unknown\\nvariance, conﬁdence\\nintervals for, 262–269\\nNormal populations\\nmaximum likelihood estimator of,\\n251–253\\nmean of\\nhypothesis testing concerning,\\n307–309, 309f, 310–321,\\n321f, 322–326\\ntesting equality of means of two\\nnormal populations,\\n326–330, 330f, 331–335\\nsampling distributions from,\\n231–234\\njoint distribution, 232–234\\nsample mean distribution, 232\\nvariance of, hypothesis testing for,\\n336–338\\nNormal prior, choosing of, 290, 291\\nNormal random variables, 179–183,\\n183f, 184–188, 188f, 189\\nchi-square distribution, 201–203,\\n203f, 204, 205, 205f, 206\\ngeneration of, 643, 644\\nmean and variance of, 181\\nnormal density function, 179, 179f\\nstandard normal distribution and,\\n182, 183, 183f\\nt-distribution, 206, 206f, 207, 207f,\\n208, 208f\\nNotation\\ndot, in two-way analysis of\\nvariance, 471, 472\\nfor least squares estimators, 375'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 685}, page_content='Index 681\\nmatrix\\nfor polynomial regression, 409\\nin multiple linear regression,\\n411, 412\\nNull hypothesis, 306, 383\\npermutation tests and, 631–634\\nNumber of defect control charts,\\n569–572\\nO\\nObservational study, 344, 345\\nOC curve, see Operating\\ncharacteristic curve\\nOdds for success, 426\\nOdds of events, 69\\nOgives, 16–18, 18f, 19\\n100(1 −α) Percent conﬁdence\\ninterval of difference in\\nmeans of two normal\\ndistributions, 272–274\\n100(1 −α) Percent conﬁdence region,\\n276\\n100(1 −α) Percent conﬁdence\\ninterval of difference in\\nmeans of two normal\\ndistributions, 276t\\n100(1 −α) Percent prediction\\ninterval, 391, 423\\n100(1 −α) Percent conﬁdence\\ninterval of difference in\\nmeans of two normal\\ndistributions\\nfor estimating unknown mean,\\n260–262\\nfor exponential distribution in life\\ntesting, 597\\nfor mean of exponential\\ndistribution, 280\\nfor normal mean with unknown\\nvariance, 263–268\\nfor regression parameters\\nα, 386\\nβ, 380, 381\\nfor unknown probability, 276–278\\nfor variance of normal distribution,\\n271\\nin sequential testing for\\nexponential distribution\\nin life testing, 601, 602\\nof mean response, 388\\nOne-sided Chebyshev’s inequality,\\n32, 33\\nOne-sided critical region, 315\\nOne-sided hypothesis tests\\nfor mean of normal population,\\ncase of known variance,\\n314–317\\nfor testing equality of means of two\\nnormal populations, 330\\nOne-sided lower conﬁdence interval\\nfor normal mean with unknown\\nvariance, 265–268\\nfor unknown mean, 259–261\\nfor unknown probability, 279\\nfor variance of normal distribution,\\n271\\nof difference in means of two\\nnormal distributions,\\n270–275\\nOne-sided null hypothesis, sign test\\nand, 532, 533\\nOne-sided t-tests, for mean of normal\\npopulation with unknown\\nvariance, 323–326\\nOne-sided upper conﬁdence interval\\nfor unknown mean, 259–261\\nfor unknown probability, 279\\nfor variance of normal distribution,\\n271\\nOne-way analysis of variance, 454,\\n456–469\\nmultiple comparisons of sample\\nmeans, 466–468\\nwith unequal sample sizes, 468,\\n469\\nOperating characteristic (OC) curve,\\n311, 312f\\nfor one-sided hypothesis testing for\\nmean of normal\\npopulation, 315, 316\\nOut of control, 555, 557, 558, 558f,\\n559\\nOverlook probabilities, 85\\nP\\np-value\\nfor determining independence of\\ncharacteristics of\\npopulation member,\\n512–514\\nfor goodness of ﬁt tests when all\\nparameters are speciﬁed,\\n501, 502, 508\\nfor goodness of ﬁt tests when some\\nparameters are\\nunspeciﬁed, 509, 510\\nfor hypothesis testing\\nin Bernoulli populations,\\n339–342, 344\\nof equality of population means,\\n460\\nof mean of normal population,\\n310, 315, 318, 319,\\n321–324, 326\\nof mean of Poisson distribution,\\n346, 347\\nof regression parameters, β,\\n378–380\\nof regression to mean, 383, 384,\\n384f\\nof variance of normal\\npopulation, 336–338\\nwith multiple linear regression,\\n419\\nfor Kolmogorov–Smirnov\\ngoodness of ﬁt test, 519,\\n520\\nfor one-sided hypothesis testing for\\nmean of normal\\npopulation, 315–318\\nfor testing equality of means of two\\nnormal populations,\\n330–335\\nin sequential testing for\\nexponential distribution\\nin life testing, 603\\nin two-way analysis of variance,\\n477, 479, 483–487\\npermutation tests for, 631–639\\nnormal approximations in,\\n634–637\\ntwo sample, 637–639\\nrank sum test and, 541\\nsigned rank test for, 535, 538\\nsimulation for approximation of,\\n506, 507\\nPaired data sets, 36–38\\nsample correlation coefﬁcient and,\\n38–43\\nPaired t-test, 333–335, 531\\nParameter estimation, 245–292\\napproximate conﬁdence interval\\nfor mean of Bernoulli\\nrandom variable, 275–279\\nBayes estimator, 246, 287–292\\nconﬁdence interval for mean of\\nexponential distribution,\\n280'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 686}, page_content='682 Index\\nfor two-way analysis of variance,\\n470–473\\nfor Weibull distribution in life\\ntesting, 611, 612\\ninterval estimates, 245, 257–270,\\n318\\nintroduction, 245, 246\\nmaximum likelihood estimators,\\n246–257, 271, 291\\nof difference in means of two\\nnormal distributions,\\n270–275\\nof life distributions, 255–257\\npoint estimator evaluation,\\n281–287\\nParametric inference, 221, 222\\nPearson, Egon, 6\\nThe Pareto Distribution, 196–199\\nPearson, Karl, 6, 382, 506\\nPermutation, 71–73\\nPermutation tests, 631–639\\nimplementation of, 631, 632\\nnormal approximations in,\\n634–637\\nnull hypothesis and, 631–634\\ntwo sample, 637–639\\nPie chart, 14, 15f\\nPoint estimates, 245\\nPoint estimators\\nevaluation of, 281–287\\nfor hypothesis testing, 307, 308\\nof mean response, 386, 420\\nPoint prediction, 423\\nPoisson, S.D., 159\\nPoisson distribution\\nhypothesis testing concerning\\nmean of, 345–348\\nnumber of defects and, 569–572\\nvariance in, 405, 406\\nwith unknown mean, goodness of\\nﬁt tests for, 508–510\\nPoisson parameters\\nmaximum likelihood estimator of,\\n250, 251\\ntesting of relationship between,\\n346–348\\nPoisson probability mass function,\\n159, 159f, 160, 161, 164,\\n165\\nPoisson process, exponential random\\nvariables and, 193, 194,\\n194f, 195, 196\\nPoisson random variables, 158–165\\nbinomial random variables and,\\n160–163\\nconditional probability and,\\n163–165\\nmoment generating functions of,\\n159, 160\\nprobability mass function and,\\n158, 159, 159f, 160, 161,\\n164, 165\\nPolynomial regression, 406–408,\\n408f, 409, 409f\\nPooled estimator, 275, 303, 329\\nPopulation distributions\\nempirical distribution and, 624\\nequality of, hypothesis testing for,\\n516, 517\\nPopulation means, 222, 223, 223f\\nbootstrap method and, 623, 624,\\n628–631\\nconﬁdence interval for difference\\nin, 466–468\\ncontrol charts for, 573–583\\ncumulative sum, 581–583\\nexponentially weighted\\nmoving-average, 576, 577,\\n577f, 578–580, 580f\\nmoving-average, 573–575, 575f\\nhypothesis testing of equality of,\\n454, 456–469\\nmultiple, hypothesis testing of,\\n454–456\\nPopulation median, sign test for, 531,\\n532, 532f\\nPopulation variance, 222, 223\\nbootstrap method and, 623–625\\nPopulations\\ndeﬁnition of, 221\\nsamples and, 3\\nsampling distributions from\\nﬁnite, 234–238\\nnormal, 231–234\\nPositively correlated, 39\\nPosterior density function, 287, 291\\nPower-function, of hypothesis test,\\n312\\nPrediction interval\\nconﬁdence interval v., 391\\nof future response, 389–391\\nof response at input level x0, 391\\nof response in multiple linear\\nregression, 420–424\\nPrior distributions, 287–292, 606,\\n607\\nProbability, 63–89\\naxioms of, 67–69, 69f, 70\\nBayes’ formula, 79, 79f, 80–86\\nBernoulli random variables,\\n151–158\\nbinomial random variables,\\n152–157\\nbootstrap method and, 629–631\\ncentral limit theorem, 224–227,\\n227f, 228–230, 230f\\nchi-square distribution and, 203\\nconditional, 75–77, 77f, 78, 79,\\n115\\ncontinuous random variable and,\\n104\\ncounting and, 70–75\\ndistribution function and, 101\\nevents, 64–66\\nindependent, 86–89\\nexpectation, 117–121, 121f\\nexponential random variables and,\\n190, 192\\nfraction defective, 567, 568\\nintroduction to, 63, 64\\nof defects, 339–346\\nof lateness, 649, 651, 657, 659–661\\nof random variables, 99, 100\\nof uniform random variables, 171,\\n172, 172f\\noverlook, 85\\nPoisson random variables and,\\n158, 159, 159f, 160–166\\nrank sum tests, 540, 541\\nsample space, 64–66\\nwith equally likely outcomes,\\n70–75\\nsigned rank test and, 535–537\\nunknown, conﬁdence interval for,\\n275–279\\nVenn diagram and algebra of\\nevents, 66, 66f, 67\\nProbability density function, 103,\\n104f, 105\\ncumulative distribution function\\nand, 104, 104f, 105\\nexponential random variables and,\\n190\\njoint, 109, 110\\nof uniform random variables, 171,\\n172, 172f\\nupdated, 287'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 687}, page_content='Index 683\\nProbability distribution\\nof estimator of mean response,\\n387, 388\\nof sample, goodness of ﬁt tests for,\\n499–508\\nProbability distribution function\\njoint cumulative, 105, 113, 114\\nof populations, 221\\nPoisson, 158–166\\nrandom variable and expectation,\\n121–123\\nProbability mass function, 102, 102f,\\n103, 103f, 255–257\\nBernoulli random variables,\\n151–153, 153f\\nbinomial random variables,\\n151–153, 153f\\ncentral limit theorem and, 227f\\nconditional, 114–116\\ndiscrete random variables, 102,\\n103, 639–641\\nexpectation of, 117, 118\\nhypergeometric random variables,\\n167, 168\\nindividual and joint, 106, 108, 109\\nmarginal, 108\\nPoisson, 158, 159, 159f, 160, 161\\nPoisson random variables, 158,\\n159, 159f, 160, 161\\nProbability models, inferential\\nstatistics and, 2, 3\\nProbability theory, statistics and, 6\\nProbit model, 428\\nPrototypical model, 649\\nPseudo random numbers, 267, 620\\nQ\\nQuality control, 555–583\\nfraction defective control charts,\\n567–569\\nintroduction to, 555, 556\\nmean control chart, 556–558,\\n558f, 559–564\\nnumber of defect control charts,\\n569–572\\npopulation mean control charts,\\n573–583\\ncumulative sum, 581–583\\nexponentially weighted\\nmoving-average, 576,\\n577f, 580, 580f\\nmoving-average, 573–575, 575f\\nvariance control chart, 564–566,\\n566f, 567\\nR\\nR, command, 50, 51, 203, 211, 266,\\n335, 336, 338, 344, 378,\\n381, 393, 460, 464, 465,\\n505\\nR, use of, 48–52, 184, 189, 203, 204,\\n208, 209, 225, 234, 237,\\n266, 267, 270–275,\\n322–335, 368–423,\\n461–478, 504–510, 530,\\n537, 541, 663, 665\\nRandom error, in response to input\\nvariable, 365, 366, 371\\nRandom numbers, 619–623\\ndeﬁnition of, 174, 175\\ngeneration of, 619–622\\nMonte Carlo simulation approach,\\n622, 623\\npseudo, 620\\nuse of, 175–177, 177f\\nRandom sample, 4, 221, 234\\nruns test for, 544–547\\nRandom variables, 99–101, see also\\nspeciﬁc random variables\\nBernoulli and binomial, 151–153,\\n153f, 154–158\\ncentral limit theorem, 224–227,\\n227f, 228–230, 230f\\nChebyshev’s inequality, 139–142\\ncontinuous, 103, 104, 641–644\\ndensity function and, 122\\ndiscrete, 101, 639–641\\ndistribution function and, 101, 624\\nentropy of, 120\\nexpectation of function of,\\ncorollary of, 123, 124\\nexpected value of sums of,\\n124–128\\nexponential, 190–194, 194f, 195,\\n642\\ngamma distribution of, 199–201,\\n202f\\ngeneration of, 507, 627, 639–641\\nhypergeometric, 167–171\\nindependent, 111–114\\nindicator, 100, 101\\njointly distributed, 105–116\\nconditional distributions,\\n114–116\\nindependent, 111–114\\nlogistics distribution, 209, 210\\nMarkov’s inequality and, 139–141\\nmoment generating functions, 138,\\n139\\nnormal, 179–183, 183f, 184–188,\\n188f, 189, 643, 644\\nchi-square distribution,\\n201–205, 205f, 206\\nF-distribution, 208\\nt-distribution, 206, 206f, 207,\\n207f, 208, 208f, 209, 209f\\nPoisson, 158, 159, 159f, 160–166\\nprobability distribution function\\nand expectation, 121–123\\ntypes of, 102, 102f, 103, 103f, 104,\\n104f, 105\\nuniform, 171, 172, 172f, 173–179\\nvariance of, 128, 129, 173,\\n180–182, 206, 233, 455,\\n456\\nvariance of a sum of, 132–137\\nweak law of large numbers, 141\\nRank sum test, 529, 538–543\\ndistribution function of, 538\\nKruskal–Wallis test, 543\\np-value and, 541\\nprobability and, 540, 541\\nRate of distribution, 592\\nRayleigh density function, 593\\nRecursive formula, mean control\\nchart and, 561, 562\\nReferents, 345\\nRegression, 365–429\\nanalysis of residuals and assessing\\nmodels, 395, 396f, 397\\ncoefﬁcient of determination and\\nsample correlation\\ncoefﬁcient, 392–395\\ndistribution of least squares\\nestimators, 371–377, 377f\\nhistory of, 6\\nintroduction, 365, 366, 366f, 367\\nleast squares estimators of\\nregression parameters,\\n367–370\\nlogistic regression models for\\nbinary output data, 425,\\n426, 426f, 427–429'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 688}, page_content='684 Index\\nmultiple linear, 410–424\\npredicting future responses,\\n420–424\\npolynomial, 406–408, 408f, 409,\\n409f\\nstatistical inferences about\\nregression parameters,\\n377–382, 382f, 383, 384,\\n384f, 385, 385f, 386–392\\nα, 386\\nβ, 377–382, 382f, 383, 384,\\n384f, 385, 385f, 386\\nmean response, 386–389\\nprediction interval of future\\nresponse, 389–391\\nsummary of distribution results,\\n392\\nto mean, 381, 382, 382f, 383, 384,\\n384f, 385, 385f, 386\\ntransforming to linearity, 396–398,\\n398f, 399, 399f, 400\\nuse of dummy variables, 424, 425\\nweighted least squares, 400–404,\\n404f, 405, 406\\nRegression coefﬁcients, 365, 407\\nRegression fallacy, 386\\nRegression parameters\\nleast squares estimators of,\\n367–370\\nprediction interval of future\\nresponse, 389–391\\nstatistical inferences about, 377,\\n379, 382f, 384f, 385, 385f,\\n391, 392\\nα, 386\\nβ, 377–382, 382f, 383, 384,\\n384f, 385, 385f, 386\\nmean response, 386–389\\nsummary of distribution results,\\n392\\nRejection, of hypothesis, see\\nHypothesis testing\\nRelative frequency histogram, 16\\nRelative frequency tables and graphs,\\n14, 15, 15f\\nResidual standard error, 380, 419\\nResiduals, 373–375\\nanalysis of, 395, 396f, 397\\nin multiple linear regression, 416,\\n417, 419\\nstandardized, 395, 396f, 397\\nResponse variable, 365–367\\nprediction interval of future\\nresponse, 389–391\\nin multiple linear regression,\\n420–424\\nvariation in, 392–395\\nwith input variable, 400–404,\\n404f, 405, 406\\nRobustness, of hypothesis test, 318\\nRow factors\\nhypothesis testing for, 474–479\\nin two-way analysis of variance,\\n470\\ncolumn factor interaction with,\\n454, 479–487\\ndeviation from grand mean due\\nto, 472\\nRow sum of squares, 476, 477\\nRun, 544\\nRuns test for randomness, 529,\\n544–547\\nS\\nSample\\ndeﬁnition of, 221\\npopulations and, 3\\nSample 100p percentile, 26, 27\\nSample correlation coefﬁcient,\\n38–40, 40f, 41–43, 49\\nassociation v. causation, 41–43\\ncoefﬁcient of determination and,\\n392–395\\nproperties of, 39, 40, 42\\nSample mean, 19–24, 34, 48, 49, 662\\ncentral limit theorem for, 227–229\\ndistribution of, with chi-square\\nrandom variables, 233\\nfor exponential random variables,\\n230, 230f\\nfor normal population, 231, 232\\nmultiple comparisons of, 466–468\\nof normal data set, 33\\nsample variance distribution with,\\n232–234\\nSample median, 22–24, 27, 33, 34,\\n49\\nSample mode, 23\\nSample percentiles, 26, 27\\nSample quartiles, 27–29, 29f\\nSample size, one-way analysis of\\nvariance with unequal\\nsample sizes, 468, 469\\nSample spaces, 64–66\\nhaving equally likely outcomes,\\n70–75\\nSample standard deviation, 26, 231\\nSample variance, 24–26, 48, 230,\\n231, 662\\nfrom normal population, 231\\nsample mean distribution with,\\n232–234\\nSampling, 221\\nfrom ﬁnite populations, 234–238\\nfrom normal populations,\\n231–234\\njoint distribution, 232–234\\nsample mean distribution, 232\\nScatter diagram, 36–38, 38f, 49, 366,\\n366f, 370, 382, 382f, 385f,\\n395, 396f, 397, 407\\nSecond quartile, 27–29, 29f\\nSelection, of normal prior, 290–292\\nSequence of interarrival times, 195\\nSequential testing, for exponential\\ndistribution in life testing,\\n599, 600, 600f, 601–603\\nSign test, 529–532, 532f, 533\\nBernoulli random variables, 530\\nbinomial random variables, 530\\nfor population median, 531, 532,\\n532f\\none-sided null hypothesis and,\\n531–533\\npaired t-test v., 531\\nSigned rank test, 529, 533, 534, 534f,\\n535–538\\nfor distribution function, 533–538\\nfor p-value, 535–538\\nSigniﬁcance level, 306, 307\\nSigniﬁcance level α test\\nfor hypothesis testing\\nof mean of normal population,\\n319, 326\\nfor testing equality of means of two\\nnormal populations, 334\\nin two-way analysis of variance,\\n477, 486\\nfor determining independence of\\ncharacteristics of\\npopulation member,\\n512–514'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 689}, page_content='Index 685\\nfor hypothesis testing\\nin Bernoulli populations,\\n339–342\\nof equality of population means,\\n460, 469\\nof mean of normal population,\\n308–311, 315–317, 320,\\n323, 324\\nof mean of Poisson distribution,\\n345, 346\\nof variance of normal\\npopulation, 338\\nfor Kolmogorov—Smirnov\\ngoodness of ﬁt test, 521\\nfor testing equality of means of two\\nnormal populations,\\n327–330, 333\\nin two-way analysis of variance,\\n477, 483–485\\nSimple hypothesis, 306\\nSimple regression equation, 366\\nassessment of, 395, 396f, 397\\nSimulation\\nfor determination of critical region,\\n506–508\\nof single and multidimensional\\nintegrals, 261–263\\nSimulation run, 623\\nin Monte Carlo study, 644, 645\\nSimulation testing, for exponential\\ndistribution in life testing,\\n594–599, 603–605\\nSingle integrals, simulation of, 267,\\n268\\nSkewed data set, 33, 34f\\nSkewed random variables, 152, 153f\\nStandard deviation\\ndeﬁnition of, 132\\nmean control chart and, 563, 564\\nvariance control chart, 564, 565\\nStandard logistic, 210\\nStandard normal distribution, 182,\\n183, 183f, 187, 188, 188f,\\n189\\ncentral limit theorem and, 228\\nof mean control chart, 557\\nt-distribution and, 206, 206f, 207,\\n207f, 208, 208f\\nStandard normal distribution\\nfunction, 665\\nStandard normal random variable,\\n232\\ncentral limit theorem and, 224,\\n227, 228\\nStandardized residuals, 395, 396f,\\n397\\nStationary increment assumption,\\n194, 196\\nStatistical analysis, 1\\nStatistical inferences, about regression\\nparameters, 377–382,\\n382f, 383, 384, 384f, 385,\\n385f, 386–392\\nα, 386\\nβ, 377–382, 382f, 383, 384, 384f,\\n385, 385f, 386\\nmean response, 386–389\\nprediction interval of future\\nresponse, 389–391\\nsummary of distribution results,\\n392\\nStatistical theory, 1\\nStatistics\\napplication of, 6, 7\\ndeﬁnition of, 1, 6, 7, 221\\ndescriptive, 1, 2\\nhistory of, 4–7\\ninferential, 2, 3\\nintroduction to, 1–7\\nStem and leaf plots, 18, 19\\nof normal data set, 35\\nsample mean and, 23\\nsample median and, 23\\nSubjective interpretation, probability,\\n63\\nSuccess, odds for, 426\\nSum of squares\\nbetween samples, 459, 460, 463\\ncolumn, 477\\nerror, 475, 477\\nin two-way analysis of variance\\nwith interaction, 481–487\\nrow, 476, 477\\nwithin samples, 458, 461, 463, 469\\nSum of squares identity, 460\\nSum of squares of residuals,\\n373–375, 416, 417, 419\\nSurvival rate, 255, 256\\nT\\nt-density function, 206, 206f, 207,\\n263, 263f\\nt-distribution, 206, 206f, 207, 207f,\\n208, 208f, 234\\nT-method, 466–468\\nt-random variable, 263\\nt-tests, 319–321, 321f, 322–326\\none-sided, 323–326\\npaired, 333–335\\ntwo-sided, 321, 321f\\nuse of R in, 322, 325, 330–332\\nTest statistic\\nfor determining independence of\\ncharacteristics of\\npopulation member,\\n512–514\\nfor goodness of ﬁt tests when all\\nparameters are speciﬁed,\\n500–502, 504–507\\nfor goodness of ﬁt tests when some\\nparameters are\\nunspeciﬁed, 509, 510\\nfor hypothesis testing\\nin Bernoulli population, 339\\nof equality of population means,\\n459–461\\nof mean of normal population,\\n309, 310, 315, 317, 319,\\n321, 322, 324, 326\\nof regression parameters, 378\\nof regression to mean, 383\\nof variance of normal\\npopulation, 336, 338\\nfor one-sided hypothesis testing for\\nmean of normal\\npopulation, 315, 317\\nfor testing equality of means of two\\nnormal populations,\\n328–330, 333–335\\nfor testing independence in\\ncontingency tables,\\n514–516\\nin two-way analysis of variance,\\n477\\nKolmogorov–Smirnov, 517–519,\\n519f, 520, 521\\nTest vector, 656\\nTesting, see Goodness of ﬁt tests;\\nHypothesis testing; Life\\ntesting\\nTesting set, 660'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 690}, page_content='686 Index\\nTests of independence\\nin contingency tables, 510–514\\nin contingency tables having ﬁxed\\nmarginal totals, 514–517\\nThird quartile, 27–29\\nThreshold model, 428\\nTies, rank sum test and, 539\\nTotal-time-on-test statistic, 596–605,\\n607\\nTransformation, to linearity,\\n396–398, 398f, 399, 400\\nTreatment group, 174\\nTree diagram, random numbers of,\\n177, 177f\\nTwo-sample permutation tests,\\n637–639\\nTwo-sample problem, 529, 538–543\\ndistribution function of, 538\\nin life testing, 607–609\\np-value and, 541\\nprobability and, 540, 541\\nTwo-sided conﬁdence interval, 258,\\n261, 267\\nfor normal mean with unknown\\nvariance, 266, 267\\nfor unknown probability, 279\\nof difference in means of two\\nnormal distributions,\\n272–275\\nTwo-sided t-tests, for mean of normal\\npopulation with unknown\\nvariance, 319–321, 321f\\nTwo-way analysis of variance, 454\\nhypothesis testing for, 474–479\\nintroduction and parameter\\nestimation, 470–473\\nwith interaction, 455, 479–487\\nType I errors, 306, 307\\nType II errors, 306, 310–312, 312f, 313\\nU\\nUnbalanced case, in one-way analysis\\nof variance, 469\\nUnbiased estimator, 281–287\\nUniform distribution, estimating\\nmean of, 254, 255\\nUniform random variables, 171, 172,\\n172f, 173–179, 661, 665\\nmean and variance of, 173\\nprobability density function of,\\n171, 172, 172f\\nrandom numbers, 177–179\\nUnion\\nin Venn diagram, 66, 66f, 67, 67f\\nof sample space, 65\\nUnit normal distribution, see\\nStandard normal\\ndistribution\\nUnknown mean\\nconﬁdence intervals for normal\\nmean with unknown\\nvariance, 262–268\\nestimates of, 257–262\\nUnknown parameters, see Parameter\\nestimation\\nUnknown probability, conﬁdence\\ninterval for, 275–279\\nUnknown variance\\nconﬁdence intervals for normal\\nmean with, 262–268\\nhypothesis testing for mean of\\nnormal population with,\\n319–321, 321f, 322–326\\ntesting equality of means of two\\nnormal populations with,\\n328–330, 330f, 331–334\\nUpdated probability density\\nfunction, 287\\nUpper conﬁdence interval\\nfor difference in means of two\\nnormal distributions, 265\\nfor normal mean with unknown\\nvariance, 265, 266\\nfor unknown mean, 259, 260\\nfor unknown probability, 279\\nfor variance of normal distribution,\\n271\\nUpper control limits\\nfor exponentially weighted\\nmoving-average, 578–580,\\n580f\\nfor fraction defective, 568, 569\\nfor mean control charts, 557–559\\nfor moving-average, 574, 575, 575f\\nfor number of defects, 569–572\\nfor variance control charts, 565,\\n566, 566f, 567\\nUse of R, 48–52, 184, 189, 203, 204,\\n208, 209, 225, 234, 237,\\n266, 267, 270–275,\\n322–335, 368–423,\\n461–478, 504–510, 530,\\n537, 541, 663, 665\\nfor calculating binomial\\nprobabilities, 157, 158\\nfor calculating hypergeometric\\nprobabilities, 171\\nfor calculating Poisson\\nprobabilities, 166\\nfor computing Gini index, 51, 52\\nfor plotting binomial probabilities,\\n157\\nfor plotting Poisson mass\\nfunctions, 166\\nin t-test, 322, 325, 330–332\\nV\\nVariance, 128–132, see also Analysis\\nof variance; Population\\nvariance; Sample variance\\nconﬁdence intervals for normal\\nmean with, 262–268\\ncovariance, 132, 133\\ndeﬁnition of, 128\\ndistribution of, with chi-square\\nrandom variables, 233\\nestimators of, 455, 456\\nfor one-way analysis of variance,\\n454, 456–469\\nin two-way analysis of variance,\\n474–477\\nin two-way analysis of variance\\nwith interaction, 479–487\\nfor exponentially weighted\\nmoving-average, 577, 578\\nfor moving-average, 574\\nin response to input variable,\\n392–395, 400–404, 404f,\\n405, 406\\nknown\\nequality of means of two normal\\npopulations with,\\n326–328\\nhypothesis testing for mean of\\nnormal population with,\\n307–309, 309f, 310–312,\\n312f, 313–319\\nof a sum of random variables,\\n132–137\\nof chi-square random variable,\\n205, 233, 455, 456\\nof hypergeometric random\\nvariables, 167–169\\nof independent random variables,\\n233'),\n",
              " Document(metadata={'producer': 'Nuance PDF Create', 'creator': 'ImageToPDF', 'creationdate': '2020-10-24T06:49:05+01:00', 'source': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'file_path': 'data/pdf_files/introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf', 'total_pages': 692, 'format': 'PDF 1.5', 'title': 'Front Matter', 'author': 'Sheldon M. Ross', 'subject': 'Introduction to Probability and Statistics for Engineers and Scientists, Sixth Edition (2021) i-iii. doi:10.1016/B978-0-12-824346-6.00003-X', 'keywords': '', 'moddate': '2020-10-24T06:49:06+01:00', 'trapped': '', 'modDate': \"D:20201024064906+01'00'\", 'creationDate': \"D:20201024064905+01'00'\", 'page': 691}, page_content='Index 687\\nof indicator random variable, 129\\nof least squares estimators,\\n371–373, 414–416\\nof normal distribution, conﬁdence\\ninterval for, 269–271\\nof normal population, hypothesis\\ntesting for, 336–338\\nof normal random variables,\\n180–182\\nof random variables, 128–132,\\n134–137, 173, 180–182,\\n206, 233, 455, 456\\nof uniform random variables, 173\\npermutation tests and, 635\\npopulation, 222, 223\\nsample, 24–26, 230, 231\\nunknown\\nequality of means of two normal\\npopulations with,\\n328–330, 330f, 331–333\\nhypothesis testing for mean of\\nnormal population with,\\n319–321, 321f, 322–326\\nunknown and equal, testing\\nequality of means of two\\nnormal populations with,\\n333, 334\\nVariance control chart, 564–566,\\n566f, 567\\nVenn diagram, 66, 66f, 67, 67f, 68–70\\nprobability axioms and, 67–69,\\n69f, 70\\nW\\nWeak law of large numbers, 141\\nWeibull density function, 609, 610f\\nWeibull distribution, in life testing,\\n609, 610, 610f, 611–613\\nWeighted average, 21, 22\\nWeighted least squares estimators,\\n400–406\\nWilcoxon rank sum test, 541\\nWilcoxon signed rank test, 537, 538\\nWithin samples sum of squares, 458,\\n461, 463, 469'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 0}, page_content='Learning statistics with R:\\nA tutorial for psychology students and other beginners\\n(Version 0.3.1)\\nDaniel Navarro\\nUniversity of Adelaide\\ndaniel.navarro@adelaide.edu.au\\nOnline:\\nhttp://learningstatisticswithr.com\\nhttp://ua.edu.au/ccs/teaching/lsr\\nhttp://www.lulu.com/content/13570633\\nhttp://twitter.com/lsrbook'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 1}, page_content='1. Copyright notice\\n(a) c⃝2013 Daniel Joseph Navarro, All rights reserved.\\n(b) This material is subject to copyright. The copyright of this material, including, but not limited\\nto, the text, photographs, images, software (‘the Material’) is owned by Daniel Joseph Navarro\\n(‘the Author’).\\n(c) Except as speciﬁcally prescribed by the Copyright Act 1968, no part of the Material may in\\nany form or by any means (electronic, mechanical, microcopying, photocopying, recording or\\notherwise) be reproduced, stored in a retrieval system or transmitted without the Author’s\\nprior written permission.\\n(d) To avoid any doubt – except as noted in paragraph 4(a) – the Material must not be, with-\\nout limitation, edited, changed, transformed, published, republished, sold, distributed, redis-\\ntributed, broadcast, posted on the internet, compiled, shown or played in public (in any form\\nor media) without the Author’s prior written permission.\\n(e) The Author asserts his Moral Rights (as deﬁned by the Copyright Act 1968) in the Material.\\n2. Intellectual property rights\\n(a) ‘Intellectual Property’ for the purposes of paragraph 2(b), means “all copyright and all rights\\nin relation to inventions, registered and unregistered trademarks (including service marks),\\nregistered and unregistered designs, conﬁdential information and circuit layouts, and any other\\nrights resulting from intellectual activity in the industrial, scientiﬁc, literary and artistic ﬁelds\\nrecognised in domestic law and anywhere in the world”\\n(b) All Intellectual Property rights in the Material are owned by the Author. No licence or any\\nother rights are granted to any other person in respect of the Intellectual Property contained\\nin the Materials in Australia or anywhere else in the world.\\n3. No warranty\\n(a) The Author makes no warranty or representation that the Materials are correct, accurate,\\ncurrent, reliable, complete, or ﬁt for any particular purpose at all and the Author expressly\\ndisclaims any other warranties, express or implied either in fact or at law, to the extent\\npermitted by law.\\n(b) The user accepts sole responsibility and risk associated with the use of the Material. In no event\\nwill the Author be liable for any loss or damage including special, indirect or consequential\\ndamage, suﬀered by any person, resulting from or in connection with the Author’s provision\\nof the Material.\\n4. Preservation of GPL rights for R code\\n(a) No terms in this notice shall be construed as implying a limitation on the software distribution\\nrights granted by the GPL licences under which R is licensed.\\n(b) To avoid ambiguity, paragraph 4(a) means means that all R source code reproduced in the\\nMaterials but not written by the Author retains the original distribution rights. In addition,\\nit is the intention of the Author that the “lsr” R package with which this book is associated\\nbe treated as a distinct work from these Materials. The lsr package is freely available, and is\\ndistributed under the GPL. The Materials are not.\\nii'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 2}, page_content='This book was brought to you today by the letter ‘R’.\\niii'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 3}, page_content='.\\niv'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 4}, page_content='Table of Contents\\nPreface\\nix\\nI\\nBackground\\n1\\n1\\nWhy do we learn statistics?\\n3\\n1.1\\nOn the psychology of statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.2\\nThe cautionary tale of Simpson’s paradox\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n1.3\\nStatistics in psychology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.4\\nStatistics in everyday life . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n1.5\\nThere’s more to research methods than statistics\\n. . . . . . . . . . . . . . . . . . . . . .\\n10\\n2\\nA brief introduction to research design\\n11\\n2.1\\nIntroduction to psychological measurement\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n2.2\\nScales of measurement\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.3\\nAssessing the reliability of a measurement\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2.4\\nThe “role” of variables: predictors and outcomes\\n. . . . . . . . . . . . . . . . . . . . . .\\n19\\n2.5\\nExperimental and non-experimental research\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n20\\n2.6\\nAssessing the validity of a study\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n2.7\\nConfounds, artifacts and other threats to validity . . . . . . . . . . . . . . . . . . . . . .\\n24\\n2.8\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\nII\\nAn introduction to R\\n33\\n3\\nGetting started with R\\n35\\n3.1\\nInstalling R\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n3.2\\nTyping commands at the R console . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n3.3\\nDoing simple calculations with R\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n3.4\\nStoring a number as a variable\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n3.5\\nUsing functions to do calculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n50\\n3.6\\nStoring many numbers as a vector\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n3.7\\nStoring text data\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n3.8\\nStoring “true or false” data\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n3.9\\nIndexing vectors\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n3.10\\nQuitting R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n3.11\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n4\\nAdditional R concepts\\n67\\n4.1\\nUsing comments\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n4.2\\nInstalling and loading packages\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n4.3\\nManaging the workspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n75\\n4.4\\nNavigating the ﬁle system\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n4.5\\nLoading and saving data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n4.6\\nUseful things to know about variables\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n85\\n4.7\\nFactors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n4.8\\nData frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n92\\n4.9\\nLists\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n95\\nv'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 5}, page_content='4.10\\nFormulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n96\\n4.11\\nGeneric functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n4.12\\nGetting help\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n4.13\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\\nIII\\nWorking with data\\n103\\n5\\nDescriptive statistics\\n105\\n5.1\\nMeasures of central tendency\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\\n5.2\\nMeasures of variability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n5.3\\nSkew and kurtosis\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n5.4\\nGetting an overall summary of a variable\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n5.5\\nDescriptive statistics separately for each group\\n. . . . . . . . . . . . . . . . . . . . . . . 128\\n5.6\\nStandard scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\\n5.7\\nCorrelations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\\n5.8\\nHandling missing values\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\\n5.9\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n6\\nDrawing graphs\\n147\\n6.1\\nAn overview of R graphics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n6.2\\nAn introduction to plotting\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n6.3\\nHistograms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n6.4\\nStem and leaf plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n6.5\\nBoxplots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\\n6.6\\nScatterplots\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n6.7\\nBar graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\\n6.8\\nSaving image ﬁles using R\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\n6.9\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\\n7\\nPragmatic matters\\n185\\n7.1\\nTabulating and cross-tabulating data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n7.2\\nTransforming and recoding a variable\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\n7.3\\nA few more mathematical functions and operations . . . . . . . . . . . . . . . . . . . . . 193\\n7.4\\nExtracting a subset of a vector\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\\n7.5\\nExtracting a subset of a data frame\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\\n7.6\\nSorting, ﬂipping and merging data\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\\n7.7\\nReshaping a data frame . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\\n7.8\\nWorking with text\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\\n7.9\\nReading unusual data ﬁles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\\n7.10\\nCoercing data from one class to another\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 231\\n7.11\\nOther useful data structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\\n7.12\\nMiscellaneous topics\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\\n7.13\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\\n8\\nBasic programming\\n243\\n8.1\\nScripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\\n8.2\\nLoops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\\n8.3\\nConditional statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\\n8.4\\nWriting functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\\n8.5\\nImplicit loops\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\\n8.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\\nvi'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 6}, page_content='IV\\nStatistical theory\\n259\\n9\\nIntroduction to probability\\n261\\n9.1\\nProbability theory v. statistical inference\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 262\\n9.2\\nBasic probability theory\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n9.3\\nThe binomial distribution\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\\n9.4\\nThe normal distribution\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\\n9.5\\nOther useful distributions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\\n9.6\\nWhat does probability mean?\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\\n9.7\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\\n10 Estimating population parameters from a sample\\n285\\n10.1\\nSamples, populations and sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\\n10.2\\nEstimating population means and standard deviations\\n. . . . . . . . . . . . . . . . . . . 288\\n10.3\\nSampling distributions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292\\n10.4\\nThe central limit theorem\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292\\n10.5\\nEstimating a conﬁdence interval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295\\n10.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\\n11 Hypothesis testing\\n303\\n11.1\\nA menagerie of hypotheses\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\\n11.2\\nTwo types of errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306\\n11.3\\nTest statistics and sampling distributions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 308\\n11.4\\nMaking decisions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\\n11.5\\nThe p value of a test\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\\n11.6\\nReporting the results of a hypothesis test\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 314\\n11.7\\nRunning the hypothesis test in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\\n11.8\\nEﬀect size, sample size and power\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\\n11.9\\nSome issues to consider\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\\n11.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\\nV\\nStatistical tools\\n327\\n12 Categorical data analysis\\n329\\n12.1\\nThe χ2 goodness-of-ﬁt test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\\n12.2\\nThe χ2 test of independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\\n12.3\\nThe continuity correction\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\\n12.4\\nEﬀect size\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\\n12.5\\nAssumptions of the test(s) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\\n12.6\\nThe Fisher exact test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\\n12.7\\nThe McNemar test\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\\n12.8\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\\n13 Comparing two means\\n353\\n13.1\\nThe one-sample z-test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\\n13.2\\nThe one-sample t-test\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\\n13.3\\nThe independent samples t-test (Student test) . . . . . . . . . . . . . . . . . . . . . . . . 365\\n13.4\\nThe independent samples t-test (Welch test) . . . . . . . . . . . . . . . . . . . . . . . . . 374\\n13.5\\nThe paired-samples t-test\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\\n13.6\\nEﬀect size\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\\n13.7\\nChecking the normality of a sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\\nvii'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 7}, page_content='13.8\\nTesting non-normal data with Wilcoxon tests\\n. . . . . . . . . . . . . . . . . . . . . . . . 390\\n13.9\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\\n14 Comparing several means (one-way ANOVA)\\n395\\n14.1\\nAn illustrative data set\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\\n14.2\\nHow ANOVA works\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398\\n14.3\\nRunning an ANOVA in R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\\n14.4\\nEﬀect size\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\\n14.5\\nMultiple comparisons and post hoc tests . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\\n14.6\\nAssumptions of one-way ANOVA\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\\n14.7\\nChecking the homogeneity of variance assumption\\n. . . . . . . . . . . . . . . . . . . . . 417\\n14.8\\nRemoving the homogeneity of variance assumption\\n. . . . . . . . . . . . . . . . . . . . . 419\\n14.9\\nChecking the normality assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420\\n14.10 Removing the normality assumption\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\\n14.11 On the relationship between ANOVA and the Student t test . . . . . . . . . . . . . . . . 424\\n14.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\\n15 Linear regression\\n427\\n15.1\\nWhat is a linear regression model?\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427\\n15.2\\nEstimating a linear regression model\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\\n15.3\\nMultiple linear regression\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431\\n15.4\\nQuantifying the ﬁt of the regression model . . . . . . . . . . . . . . . . . . . . . . . . . . 434\\n15.5\\nHypothesis tests for regression models\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 436\\n15.6\\nRegarding regression coeﬃcients\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\\n15.7\\nAssumptions of regression\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443\\n15.8\\nModel checking\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443\\n15.9\\nModel selection\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\\n15.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464\\n16 Factorial ANOVA\\n465\\n16.1\\nFactorial ANOVA 1: balanced designs, no interactions\\n. . . . . . . . . . . . . . . . . . . 465\\n16.2\\nFactorial ANOVA 2: balanced designs, interactions allowed . . . . . . . . . . . . . . . . . 474\\n16.3\\nEﬀect size, estimated means, and conﬁdence intervals . . . . . . . . . . . . . . . . . . . . 481\\n16.4\\nAssumption checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485\\n16.5\\nThe F test as a model comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486\\n16.6\\nANOVA as a linear model\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489\\n16.7\\nDiﬀerent ways to specify contrasts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500\\n16.8\\nPost hoc tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505\\n16.9\\nThe method of planned comparisons\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507\\n16.10 Factorial ANOVA 3: unbalanced designs . . . . . . . . . . . . . . . . . . . . . . . . . . . 508\\n16.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520\\n17 Epilogue\\n521\\n17.1\\nThe undiscovered statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521\\n17.2\\nLearning the basics, and learning them in R\\n. . . . . . . . . . . . . . . . . . . . . . . . . 529\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531\\nviii'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 8}, page_content='Preface\\nThere’s a part of me that really doesn’t want to publish this book. It’s not ﬁnished.\\nAnd when I say that, I mean it. The referencing is spotty at best, the chapter summaries are just lists\\nof section titles, there’s no index, there are no exercises for the reader, the organisation is suboptimal,\\nand the coverage of topics is just not comprehensive enough for my liking. Additionally, there are sections\\nwith content that I’m not happy with, ﬁgures that really need to be redrawn, and I’ve had almost no time\\nto hunt down inconsistencies, typos, or errors. In other words, this book is not ﬁnished. If I didn’t have\\na looming teaching deadline and a baby due in a few weeks, I really wouldn’t be making this available at\\nall.\\nWhat this means is that if you are an academic looking for teaching materials, a Ph.D. student looking\\nto learn R, or just a member of the general public interested in statistics, I would advise you to be cautious.\\nWhat you’re looking at is a ﬁrst draft, and it may not serve your purposes. If we were living in the days\\nwhen publishing was expensive and the internet wasn’t around, I would never consider releasing a book\\nin this form. The thought of someong shelling out $80 for this (which is what a commercial publisher told\\nme it would retail for when they oﬀered to distribute it) makes me feel more than a little uncomfortable.\\nHowever, it’s the 21st century, so I can post the pdf on my website for free, and I can distribute hard\\ncopies via a print-on-demand service for less than half what a textbook publisher would charge. And so\\nmy guilt is assuaged, and I’m willing to share! With that in mind, you can obtain free soft copies and\\ncheap hard copies online, from the following webpages:\\nSoft copy:\\nua.edu.au/ccs/teaching/lsr\\nHard copy:\\nwww.lulu.com/content/13570633\\nEven so, the warning still stands: what you are looking at is Version 0.3 of a work in progress. If and\\nwhen it hits Version 1.0, I would be willing to stand behind the work and say, yes, this is a textbook that\\nI would encourage other people to use. At that point, I’ll probably start shamelessly ﬂogging the thing\\non the internet and generally acting like a tool. But until that day comes, I’d like it to be made clear\\nthat I’m really ambivalent about the work as it stands.\\nAll of the above being said, there is one group of people that I can enthusiastically endorse this book\\nto: the psychology students taking our undergraduate research methods classes (DRIP and DRIP:A) in\\n2013. For you, this book is ideal, because it was written to accompany your stats lectures. If a problem\\narises due to a shortcoming of these notes, I can and will adapt content on the ﬂy to ﬁx that problem.\\nEﬀectively, you’ve got a textbook written speciﬁcally for your classes, distributed for free (electronic\\ncopy) or at near-cost prices (hard copy). Better yet, the notes have been tested: Version 0.1 of these\\nnotes was used in the 2011 class, Version 0.2 was used in the 2012 class, and now you’re looking at the\\nnew and improved Version 0.3. I’m not saying these notes are titanium plated awesomeness on a stick –\\nthough if you wanted to say so on the student evaluation forms, then you’re totally welcome to – because\\nthey’re not. But I am saying that they’ve been tried out in previous years and they seem to work okay.\\nBesides, there’s a group of us around to troubleshoot if any problems come up, and you can guarantee\\nthat at least one of your lecturers has read the whole thing cover to cover!\\nOkay, with all that out of the way, I should say something about what the book aims to be. At its\\ncore, it is an introductory statistics textbook pitched primarily at psychology students. As such, it covers\\nthe standard topics that you’d expect of such a book: study design, descriptive statistics, the theory of\\nhypothesis testing, t-tests, χ2 tests, ANOVA and regression. However, there are also several chapters\\ndevoted to the R statistical package, including a chapter on data manipulation and another one on scripts\\nand programming. Moreover, when you look at the content presented in the book, you’ll notice a lot of\\ntopics that are traditionally swept under the carpet when teaching statistics to psychology students. The\\nBayesian/frequentist divide is openly disussed in the probability chapter, and the disagreement between\\nNeyman and Fisher about hypothesis testing makes an appearance. The diﬀerence between probability\\nix'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 9}, page_content='and density is discussed. A detailed treatment of Type I, II and III sums of squares for unbalanced\\nfactorial ANOVA is provided.\\nAnd if you have a look in the Epilogue, it should be clear that my\\nintention is to add a lot more advanced content.\\nMy reasons for pursuing this approach are pretty simple: the students can handle it, and they even\\nseem to enjoy it. Over the last few years I’ve been pleasantly surprised at just how little diﬃculty I’ve had\\nin getting undergraduate psych students to learn R. It’s certainly not easy for them, and I’ve found I need\\nto be a little charitable in setting marking standards, but they do eventually get there. Similarly, they\\ndon’t seem to have a lot of problems tolerating ambiguity and complexity in presentation of statistical\\nideas, as long as they are assured that the assessment standards will be set in a fashion that is appropriate\\nfor them. So if the students can handle it, why not teach it? The potential gains are pretty enticing.\\nIf they learn R, the students get access to CRAN, which is perhaps the largest and most comprehensive\\nlibrary of statistical tools in existence. And if they learn about probability theory in detail, it’s easier for\\nthem to switch from orthodox null hypothesis testing to Bayesian methods if they want to. Better yet,\\nthey learn data analysis skills that they can take to an employer without being dependent on expensive\\nand proprietary software.\\nSadly, this book isn’t the silver bullet that makes all this possible. It’s a work in progress, and maybe\\nwhen it is ﬁnished it will be a useful tool. One among many, I would think. There are a number of\\nother books that try to provide a basic introduction to statistics using R, and I’m not arrogant enough\\nto believe that mine is better. Still, I rather like the book, and maybe other people will ﬁnd it useful,\\nincomplete though it is.\\nDan Navarro\\nJanuary 13, 2013\\nx'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 10}, page_content='Part I.\\nBackground\\n- 1 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 11}, page_content=''),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 12}, page_content='1. Why do we learn statistics?\\n“Thou shalt not answer questionnaires\\nOr quizzes upon World Aﬀairs,\\nNor with compliance\\nTake any test. Thou shalt not sit\\nWith statisticians nor commit\\nA social science”\\n– W.H. Auden\\n1.1\\nOn the psychology of statistics\\nTo the surprise of many students, statistics is a fairly signiﬁcant part of a psychological education. To\\nthe surprise of no-one, statistics is very rarely the favourite part of one’s psychological education. After\\nall, if you really loved the idea of doing statistics, you’d probably be enrolled in a statistics class right\\nnow, not a psychology class. So, not surprisingly, there’s a pretty large proportion of the student base\\nthat isn’t happy about the fact that psychology has so much statistics in it. In view of this, I thought\\nthat the right place to start might be to answer some of the more common questions that people have\\nabout stats. . .\\nA big part of this issue at hand relates to the very idea of statistics. What is it? What’s it there\\nfor? And why are scientists so bloody obsessed with it? These are all good questions, when you think\\nabout it. So let’s start with the last one. As a group, scientists seem to be bizarrely ﬁxated on running\\nstatistical tests on everything. In fact, we use statistics so often that we sometimes forget to explain to\\npeople why we do. It’s a kind of article of faith among scientists – and especially social scientists – that\\nyour ﬁndings can’t be trusted until you’ve done some stats. Undergraduate students might be forgiven\\nfor thinking that we’re all completely mad, because no-one takes the time to answer one very simple\\nquestion:\\nWhy do you do statistics? Why don’t scientists just use common sense when evaluating evi-\\ndence?\\nIt’s a naive question in some ways, but most good questions are. There’s a lot of good answers to it,1 but\\nfor my money, the best answer is a really simple one: we don’t trust ourselves enough. We worry that\\n1Including the suggestion that common sense is in short supply among scientists.\\n- 3 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 13}, page_content='we’re human, and susceptible to all of the biases, temptations and frailties that all humans suﬀer from.\\nMuch of statistics is basically a safeguard. Using “common sense” to evaluate evidence means trusting\\ngut instincts, relying on verbal arguments and on using the raw power of human reason to come up with\\nthe right answer. Most scientists don’t think this approach is likely to work.\\nIn fact, come to think of it, this sounds a lot like a psychological question to me, and since I do work\\nin a psychology department, it seems like a good idea to dig a little deeper here. Is it really plausible to\\nthink that this “common sense” approach is very trustworthy? Verbal arguments have to be constructed\\nin language, and all languages have biases – some things are harder to say than others, and not necessarily\\nbecause they’re false (e.g., quantum electrodynamics is a good theory, but hard to explain in words).\\nThe instincts of our “gut” aren’t designed to solve scientiﬁc problems, they’re designed to handle day to\\nday inferences – and given that biological evolution is slower than cultural change, we should say that\\nthey’re designed to solve the day to day problems for a diﬀerent world than the one we live in. Most\\nfundamentally, reasoning sensibly requires people to engage in “induction”, making wise guesses and\\ngoing beyond the immediate evidence of the senses to make generalisations about the world. If you think\\nthat you can do that without being inﬂuenced by various distractors, well, I have a bridge in Brooklyn I’d\\nlike to sell you. Heck, as the next section shows, we can’t even solve “deductive” problems (ones where\\nno guessing is required) without being inﬂuenced by our pre-existing biases.\\n1.1.1\\nThe curse of belief bias\\nPeople are mostly pretty smart. We’re certainly smarter than the other species that we share the\\nplanet with (though many people might disagree). Our minds are quite amazing things, and we seem\\nto be capable of the most incredible feats of thought and reason. That doesn’t make us perfect though.\\nAnd among the many things that psychologists have shown over the years is that we really do ﬁnd it\\nhard to be neutral, to evaluate evidence impartially and without being swayed by pre-existing biases. A\\ngood example of this is the belief bias eﬀect in logical reasoning: if you ask people to decide whether a\\nparticular argument is logically valid (i.e., conclusion would be true if the premises were true), we tend\\nto be inﬂuenced by the believability of the conclusion, even when we shouldn’t. For instance, here’s a\\nvalid argument where the conclusion is believable:\\nNo cigarettes are inexpensive (Premise 1)\\nSome addictive things are inexpensive (Premise 2)\\nTherefore, some addictive things are not cigarettes (Conclusion)\\nAnd here’s a valid argument where the conclusion is not believable:\\nNo addictive things are inexpensive (Premise 1)\\nSome cigarettes are inexpensive (Premise 2)\\nTherefore, some cigarettes are not addictive (Conclusion)\\nThe logical structure of argument #2 is identical to the structure of argument #1, and they’re both valid.\\nHowever, in the second argument, there are good reasons to think that premise 1 is incorrect, and as a\\nresult it’s probably the case that the conclusion is also incorrect. But that’s entirely irrelevant to the\\ntopic at hand: an argument is deductively valid if the conclusion is a logical consequence of the premises.\\nThat is, a valid argument doesn’t have to involve true statements.\\nOn the other hand, here’s an invalid argument that has a believable conclusion:\\nNo addictive things are inexpensive (Premise 1)\\nSome cigarettes are inexpensive (Premise 2)\\nTherefore, some addictive things are not cigarettes (Conclusion)\\n- 4 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 14}, page_content='And ﬁnally, an invalid argument with an unbelievable conclusion:\\nNo cigarettes are inexpensive (Premise 1)\\nSome addictive things are inexpensive (Premise 2)\\nTherefore, some cigarettes are not addictive (Conclusion)\\nNow, suppose that people really are perfectly able to set aside their pre-existing biases about what is\\ntrue and what isn’t, and purely evaluate an argument on its logical merits. We’d expect 100% of people\\nto say that the valid arguments are valid, and 0% of people to say that the invalid arguments are valid.\\nSo if you ran an experiment looking at this, you’d expect to see data like this:\\nconclusion feels true\\nconclusion feels false\\nargument is valid\\n100% say “valid”\\n100% say “valid”\\nargument is invalid\\n0% say “valid”\\n0% say “valid”\\nIf the psychological data looked like this (or even a good approximation to this), we might feel safe in\\njust trusting our gut instincts. That is, it’d be perfectly okay just to let scientists evaluate data based on\\ntheir common sense, and not bother with all this murky statistics stuﬀ. However, you guys have taken\\npsych classes, and by now you probably know where this is going . . .\\nIn a classic study, Evans, Barston, and Pollard (1983) ran an experiment looking at exactly this.\\nWhat they found is that when pre-existing biases (i.e., beliefs) were in agreement with the structure of\\nthe data, everything went the way you’d hope:\\nconclusion feels true\\nconclusion feels false\\nargument is valid\\n92% say “valid”\\nargument is invalid\\n8% say “valid”\\nNot perfect, but that’s pretty good. But look what happens when our intuitive feelings about the truth\\nof the conclusion run against the logical structure of the argument:\\nconclusion feels true\\nconclusion feels false\\nargument is valid\\n92% say “valid”\\n46% say “valid”\\nargument is invalid\\n92% say “valid”\\n8% say “valid”\\nOh dear, that’s not as good.\\nApparently, when people are presented with a strong argument that\\ncontradicts our pre-existing beliefs, we ﬁnd it pretty hard to even perceive it to be a strong argument\\n(people only did so 46% of the time). Even worse, when people are presented with a weak argument that\\nagrees with our pre-existing biases, almost no-one can see that the argument is weak (people got that\\none wrong 92% of the time!)2\\nIf you think about it, it’s not as if these data are horribly damning. Overall, people did do better\\nthan chance at compensating for their prior biases, since about 60% of people’s judgements were correct\\n(you’d expect 50% by chance). Even so, if you were a professional “evaluator of evidence”, and someone\\ncame along and oﬀered you a magic tool that improves your chances of making the right decision from\\n60% to (say) 95%, you’d probably jump at it, right? Of course you would. Thankfully, we actually do\\nhave a tool that can do this. But it’s not magic, it’s statistics. So that’s reason #1 why scientists love\\n2In my more cynical moments I feel like this fact alone explains 95% of what I read on the internet.\\n- 5 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 15}, page_content='statistics. It’s just too easy for us to “believe what we want to believe”; so if we want to “believe in the\\ndata” instead, we’re going to need a bit of help to keep our personal biases under control. That’s what\\nstatistics does: it helps keep us honest.\\n1.2\\nThe cautionary tale of Simpson’s paradox\\nThe following is a true story. In 1973, the University of California, Berkeley got into a lot of trouble\\nover its admissions of students into postgraduate courses. For those of you who don’t know how the US\\nuniversity system works, it’s very diﬀerent to Australia. Unlike Australia, there’s no centralisation, and\\nuniversity admission is based on many diﬀerent factors besides grades. Moreover, each university makes\\nits own decisions about who to send oﬀers to, using its own decision process to do so. So, when the 1973\\nadmissions data looked like this. . .\\nNumber of applicants\\nPercent admitted\\nMales\\n8442\\n44%\\nFemales\\n4321\\n35%\\n. . . they got sued. Given that there were nearly 13,000 applicants, a diﬀerence of 9% in admission rates\\nbetween males and females is just way too big to be a coincidence. Pretty compelling data, right? And\\nif I were to say to you that these data actually reﬂect a weak bias in favour of females, you’d probably\\nthink that I was either crazy or sexist.\\nOddly, it’s actually sort of true . . . after Berkeley got sued, people started looking very carefully at\\nthe admissions data (Bickel, Hammel, & O’Connell, 1975). And remarkably, when they looked at it on\\na department by department basis, it turned out that most of the departments actually had a slightly\\nhigher success rate for female applicants than for male applicants. The table below shows the admission\\nﬁgures for the six largest departments (with the names of the departments removed for privacy reasons):\\nMales\\nFemales\\nDepartment\\nApplicants\\nPercent admitted\\nApplicants\\nPercent admitted\\nA\\n825\\n62%\\n108\\n82%\\nB\\n560\\n63%\\n25\\n68%\\nC\\n325\\n37%\\n593\\n34%\\nD\\n417\\n33%\\n375\\n35%\\nE\\n191\\n28%\\n393\\n24%\\nF\\n272\\n6%\\n341\\n7%\\nRemarkably, most departments had a higher rate of admissions for females than for males! Yet the overall\\nrate of admission across the university for females was lower than for males. How can this be? How can\\nboth of these statements be true at the same time?\\nHere’s what’s going on. Firstly, notice that the departments are not equal to one another in terms of\\ntheir admission percentages: some departments (e.g., engineering, chemistry) tended to admit a high per-\\ncentage of the qualiﬁed applicants, whereas others (e.g., English) tended to reject most of the candidates,\\neven if they were high quality. So, among the six departments shown above, notice that department A\\nis the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females\\ntended to apply to diﬀerent departments. If we rank the departments in terms of the total number of\\nmale applicants, we get AąBąDąCąFąE (the “easy” departments are in bold). On the whole, males\\ntended to apply to the departments that had high admission rates. Now compare this to how the female\\n- 6 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 16}, page_content='0\\n20\\n40\\n60\\n80\\n100\\n0\\n20\\n40\\n60\\n80\\n100\\nPercentage of female applicants\\nAdmission rate (both genders)\\nFigure 1.1: The Berkeley 1973 college admissions data. This ﬁgure plots the admission rate for the 85\\ndepartments that had at least one female applicant, as a function of the percentage of applicants that\\nwere female. The plot is a redrawing of Figure 1 from Bickel et al. (1975). Circles plot departments with\\nmore than 40 applicants; the area of the circle is proportional to the total number of applicants. The\\ncrosses plot department with fewer than 40 applicants.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\napplicants distributed themselves. Ranking the departments in terms of the total number of female ap-\\nplicants produces a quite diﬀerent ordering CąEąDąFąAąB. In other words, what these data seem\\nto be suggesting is that the female applicants tended to apply to “harder” departments. And in fact, if\\nwe look at all Figure 1.1 we see that this trend is systematic, and quite striking. This eﬀect is known\\nas Simpson’s paradox. It’s not common, but it does happen in real life, and most people are very\\nsurprised by it when they ﬁrst encounter it, and many people refuse to even believe that it’s real. It is\\nvery real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to\\nmake a much more important point . . . doing research is hard, and there are lots of subtle, counterintuitive\\ntraps lying in wait for the unwary. That’s reason #2 why scientists love statistics, and why we teach\\nresearch methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks\\nand crannies of complicated data.\\nBefore leaving this topic entirely, I want to point out something else really critical that is often over-\\nlooked in a research methods class. Statistics only solves part of the problem. Remember that we started\\nall this with the concern that Berkeley’s admissions processes might be unfairly biased against female\\n- 7 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 17}, page_content='applicants. When we looked at the “aggregated” data, it did seem like the university was discriminating\\nagainst women, but when we “disaggregate” and looked at the individual behaviour of all the depart-\\nments, it turned out that the actual departments were, if anything, slightly biased in favour of women.\\nThe gender bias in admissions was entirely caused by the fact that women tended to self-select for harder\\ndepartments. From a purely legal perspective, that puts the university in the clear. Postgraduate ad-\\nmissions are determined at the level of the individual department (and there are very good reasons to do\\nthat), and at the level of individual departments, the decisions are more or less unbiased (the weak bias\\nin favour of females at that level is small, and not consistent across departments). Since the university\\ncan’t dictate which departments people choose to apply to, and the decision making takes place at the\\nlevel of the department it can hardly be held accountable for any biases that those choices produce.\\nThat was the basis for my somewhat glib remarks, but that’s not exactly the whole story, is it? After\\nall, if we’re interested in this from a more sociological and psychological perspective, we might want\\nto ask why there are such strong gender diﬀerences in applications. Why do males tend to apply to\\nengineering more often than females, and why is this reversed for the English department? And why is\\nit it the case that the departments that tend to have a female-application bias tend to have lower overall\\nadmission rates than those departments that have a male-application bias? Might this not still reﬂect a\\ngender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically,\\nthat males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further\\nthat the reason for why the humanities departments have low admission rates is because the government\\ndoesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded\\nresearch projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the\\nhumanities? What if someone at a high level in the government cut the humanities funds because they\\nfelt that the humanities are “useless chick stuﬀ”. That seems pretty blatantly gender biased. None of\\nthis falls within the purview of statistics, but it matters to the research project. If you’re interested in the\\noverall structural eﬀects of subtle gender biases, then you probably want to look at both the aggregated\\nand disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re\\nprobably only interested in the disaggregated data.\\nIn short there are a lot of critical questions that you can’t answer with statistics, but the answers to\\nthose questions will have a huge impact on how you analyse and interpret data. And this is the reason\\nwhy you should always think of statistics as a tool to help you learn about your data, no more and no\\nless. It’s a powerful tool to that end, but there’s no substitute for careful thought.\\n1.3\\nStatistics in psychology\\nI hope that the discussion above helped explain why science in general is so focused on statistics. But I’m\\nguessing that you have a lot more questions about what role statistics plays in psychology, and speciﬁcally\\nwhy psychology classes always devote so many lectures to stats. So here’s my attempt to answer a few\\nof them...\\n• Why does psychology have so much statistics?\\nTo be perfectly honest, there’s a few diﬀerent reasons, some of which are better than others. The\\nmost important reason is that psychology is a statistical science. What I mean by that is that the\\n“things” that we study are people. Real, complicated, gloriously messy, infuriatingly perverse people.\\nThe “things” of physics include object like electrons, and while there are all sorts of complexities\\nthat arise in physics, electrons don’t have minds of their own. They don’t have opinions, they\\ndon’t diﬀer from each other in weird and arbitrary ways, they don’t get bored in the middle of an\\nexperiment, and they don’t get angry at the experimenter and then deliberately try to sabotage\\n- 8 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 18}, page_content='the data set (not that I’ve ever done that . . . ). At a fundamental level psychology is harder than\\nphysics.3\\nBasically, we teach statistics to you as psychologists because you need to be better at stats than\\nphysicists. There’s actually a saying used sometimes in physics, to the eﬀect that “if your experiment\\nneeds statistics, you should have done a better experiment”. They have the luxury of being able\\nto say that because their objects of study are pathetically simple in comparison to the vast mess\\nthat confronts social scientists. It’s not just psychology, really: most social sciences are desperately\\nreliant on statistics.\\nNot because we’re bad experimenters, but because we’ve picked a harder\\nproblem to solve. We teach you stats because you really, really need it.\\n• Can’t someone else do the statistics?\\nTo some extent, but not completely.\\nIt’s true that you don’t need to become a fully trained\\nstatistician just to do psychology, but you do need to reach a certain level of statistical competence.\\nIn my view, there’s three reasons that every psychological researcher ought to be able to do basic\\nstatistics:\\n– Firstly, there’s the fundamental reason: statistics is deeply intertwined with research design.\\nIf you want to be good at designing psychological studies, you need to at least understand the\\nbasics of stats.\\n– Secondly, if you want to be good at the psychological side of the research, then you need\\nto be able to understand the psychological literature, right? But almost every paper in the\\npsychological literature reports the results of statistical analyses. So if you really want to\\nunderstand the psychology, you need to be able to understand what other people did with\\ntheir data. And that means understanding a certain amount of statistics.\\n– Thirdly, there’s a big practical problem with being dependent on other people to do all your\\nstatistics: statistical analysis is expensive. If you ever get bored and want to look up how\\nmuch the Australian government charges for university fees, you’ll notice something interesting:\\nstatistics is designated as a “national priority” category, and so the fees are much, much lower\\nthan for any other area of study. This is because there’s a massive shortage of statisticians out\\nthere. So, from your perspective as a psychological researcher, the laws of supply and demand\\naren’t exactly on your side here! As a result, in almost any real life situation where you want to\\ndo psychological research, the cruel facts will be that you don’t have enough money to aﬀord a\\nstatistician. So the economics of the situation mean that you have to be pretty self-suﬃcient.\\nNote that a lot of these reasons generalise beyond researchers.\\nIf you want to be a practicing\\npsychologist and stay on top of the ﬁeld, it helps to be able to read the scientiﬁc literature, which\\nrelies pretty heavily on statistics.\\n• I don’t care about jobs, research, or clinical work. Do I need statistics?\\nOkay, now you’re just messing with me. Still, I think it should matter to you too. Statistics should\\nmatter to you in the same way that statistics should matter to everyone: we live in the 21st century,\\nand data are everywhere. Frankly, given the world in which we live these days, a basic knowledge\\nof statistics is pretty damn close to a survival tool! Which is the topic of the next section...\\n3Which might explain why physics is just a teensy bit further advanced as a science than we are.\\n- 9 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 19}, page_content='1.4\\nStatistics in everyday life\\n“We are drowning in information,\\nbut we are starved for knowledge”\\n– Various authors, original probably John Naisbitt\\nWhen I started writing up my lecture notes I took the 20 most recent news articles posted to the ABC\\nnews website. Of those 20 articles, it turned out that 8 of them involved a discussion of something that\\nI would call a statistical topic; 6 of those made a mistake. The most common error, if you’re curious,\\nwas failing to report baseline data (e.g., the article mentions that 5% of people in situation X have some\\ncharacteristic Y, but doesn’t say how common the characteristic is for everyone else!) The point I’m\\ntrying to make here isn’t that journalists are bad at statistics (though they almost always are), it’s that\\na basic knowledge of statistics is very helpful for trying to ﬁgure out when someone else is either making\\na mistake or even lying to you. In fact, one of the biggest things that a knowledge of statistics does to\\nyou is cause you to get angry at the newspaper or the internet on a far more frequent basis: you can ﬁnd\\na good example of this in Section 5.1.5. In later versions of this book I’ll try to include more anecdotes\\nalong those lines.\\n1.5\\nThere’s more to research methods than statistics\\nSo far, most of what I’ve talked about is statistics, and so you’d be forgiven for thinking that statistics\\nis all I care about in life. To be fair, you wouldn’t be far wrong, but research methodology is a broader\\nconcept than statistics. So most research methods courses will cover a lot of topics that relate much more\\nto the pragmatics of research design, and in particular the issues that you encounter when trying to do\\nresearch with humans. However, about 99% of student fears relate to the statistics part of the course, so\\nI’ve focused on the stats in this discussion, and hopefully I’ve convinced you that statistics matters, and\\nmore importantly, that it’s not to be feared. That being said, it’s pretty typical for introductory research\\nmethods classes to be very stats-heavy. This is not (usually) because the lecturers are evil people. Quite\\nthe contrary, in fact. Introductory classes focus a lot on the statistics because you almost always ﬁnd\\nyourself needing statistics before you need the other research methods training. Why? Because almost\\nall of your assignments in other classes will rely on statistical training, to a much greater extent than\\nthey rely on other methodological tools. It’s not common for undergraduate assignments to require you\\nto design your own study from the ground up (in which case you would need to know a lot about research\\ndesign), but it is common for assignments to ask you to analyse and interpret data that were collected in\\na study that someone else designed (in which case you need statistics). In that sense, from the perspective\\nof allowing you to do well in all your other classes, the statistics is more urgent.\\nBut note that “urgent” is diﬀerent from “important” – they both matter. I really do want to stress\\nthat research design is just as important as data analysis, and this book does spend a fair amount of\\ntime on it. However, while statistics has a kind of universality, and provides a set of core tools that\\nare useful for most types of psychological research, the research methods side isn’t quite so universal.\\nThere are some general principles that everyone should think about, but a lot of research design is very\\nidiosyncratic, and is speciﬁc to the area of research that you want to engage in. To the extent that it’s the\\ndetails that matter, those details don’t usually show up in an introductory stats and research methods\\nclass.\\n- 10 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 20}, page_content='2. A brief introduction to research design\\nIn this chapter, we’re going to start thinking about the basic ideas that go into designing a study, collecting\\ndata, checking whether your data collection works, and so on. It won’t give you enough information to\\nallow you to design studies of your own, but it will give you a lot of the basic tools that you need to assess\\nthe studies done by other people. However, since the focus of this book is much more on data analysis\\nthan on data collection, I’m only giving a very brief overview. Note that this chapter is “special” in\\ntwo ways. Firstly, it’s much more psychology-speciﬁc than the later chapters. Secondly, it focuses much\\nmore heavily on the scientiﬁc problem of research methodology, and much less on the statistical problem\\nof data analysis. Nevertheless, the two problems are related to one another, so it’s traditional for stats\\ntextbooks to discuss the problem in a little detail. This chapter relies heavily on Campbell and Stanley\\n(1963) for the discussion of study design, and Stevens (1946) for the discussion of scales of measurement.\\nLater versions will attempt to be more precise in the citations.\\n2.1\\nIntroduction to psychological measurement\\nThe ﬁrst thing to understand is data collection can be thought of as a kind of measurement. That is,\\nwhat we’re trying to do here is measure something about human behaviour or the human mind. What\\ndo I mean by “measurement”?\\n2.1.1\\nSome thoughts about psychological measurement\\nMeasurement itself is a subtle concept, but basically it comes down to ﬁnding some way of assigning\\nnumbers, or labels, or some other kind of well-deﬁned descriptions to “stuﬀ”. So, any of the following\\nwould count as a psychological measurement:\\n• My age is 33 years.\\n• I do not like anchovies.\\n• My chromosomal gender is male.\\n• My self-identiﬁed gender is male.\\nIn the short list above, the bolded part is “the thing to be measured”, and the italicised part is “the\\nmeasurement itself”. In fact, we can expand on this a little bit, by thinking about the set of possible\\nmeasurements that could have arisen in each case:\\n- 11 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 21}, page_content='• My age (in years) could have been 0, 1, 2, 3 . . . , etc. The upper bound on what my age could\\npossibly be is a bit fuzzy, but in practice you’d be safe in saying that the largest possible age is\\n150, since no human has ever lived that long.\\n• When asked if I like anchovies, I might have said that I do, or I do not, or I have no opinion, or\\nI sometimes do.\\n• My chromosomal gender is almost certainly going to be male (XY) or female (XX), but there\\nare a few other possibilities. I could also have Klinfelter’s syndrome (XXY), which is more similar\\nto male than to female. And I imagine there are other possibilities too.\\n• My self-identiﬁed gender is also very likely to be male or female, but it doesn’t have to agree\\nwith my chromosomal gender. I may also choose to identify with neither, or to explicitly call myself\\ntransgender.\\nAs you can see, for some things (like age) it seems fairly obvious what the set of possible measurements\\nshould be, whereas for other things it gets a bit tricky. But I want to point out that even in the case of\\nsomeone’s age, it’s much more subtle than this. For instance, in the example above, I assumed that it\\nwas okay to measure age in years. But if you’re a developmental psychologist, that’s way too crude, and\\nso you often measure age in years and months (if a child is 2 years and 11 months, this is usually written\\nas “2;11”). If you’re interested in newborns, you might want to measure age in days since birth, maybe\\neven hours since birth. In other words, the way in which you specify the allowable measurement values\\nis important.\\nLooking at this a bit more closely, you might also realise that the concept of “age” isn’t actually\\nall that precise. In general, when we say “age” we implicitly mean “the length of time since birth”.\\nBut that’s not always the right way to do it. Suppose you’re interested in how newborn babies control\\ntheir eye movements. If you’re interested in kids that young, you might also start to worry that “birth”\\nis not the only meaningful point in time to care about. If Baby Alice is born 3 weeks premature and\\nBaby Bianca is born 1 week late, would it really make sense to say that they are the “same age” if we\\nencountered them “2 hours after birth”? In one sense, yes: by social convention, we use birth as our\\nreference point for talking about age in everyday life, since it deﬁnes the amount of time the person has\\nbeen operating as an independent entity in the world, but from a scientiﬁc perspective that’s not the\\nonly thing we care about. When we think about the biology of human beings, it’s often useful to think of\\nourselves as organisms that have been growing and maturing since conception, and from that perspective\\nAlice and Bianca aren’t the same age at all. So you might want to deﬁne the concept of “age” in two\\ndiﬀerent ways: the length of time since conception, and the length of time since birth. When dealing\\nwith adults, it won’t make much diﬀerence, but when dealing with newborns it might.\\nMoving beyond these issues, there’s the question of methodology.\\nWhat speciﬁc “measurement\\nmethod” are you going to use to ﬁnd out someone’s age? As before, there are lots of diﬀerent possi-\\nbilities:\\n• You could just ask people “how old are you?” The method of self-report is fast, cheap and easy,\\nbut it only works with people old enough to understand the question, and some people lie about\\ntheir age.\\n• You could ask an authority (e.g., a parent) “how old is your child?” This method is fast, and when\\ndealing with kids it’s not all that hard since the parent is almost always around. It doesn’t work\\nas well if you want to know “age since conception”, since a lot of parents can’t say for sure when\\nconception took place. For that, you might need a diﬀerent authority (e.g., an obstetrician).\\n• You could look up oﬃcial records, like birth certiﬁcates. This is time consuming and annoying, but\\nit has it’s uses (e.g., if the person is now dead).\\n- 12 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 22}, page_content='2.1.2\\nOperationalisation: deﬁning your measurement\\nAll of the ideas discussed in the previous section all relate to the concept of operationalisation. To\\nbe a bit more precise about the idea, operationalisation is the process by which we take a meaningful but\\nsomewhat vague concept, and turn it into a precise measurement. The process of operationalisation can\\ninvolve several diﬀerent things:\\n• Being precise about what you are trying to measure. For instance, does “age” mean “time since\\nbirth” or “time since conception” in the context of your research?\\n• Determining what method you will use to measure it. Will you use self-report to measure age, ask\\na parent, or look up an oﬃcial record? If you’re using self-report, how will you phrase the question?\\n• Deﬁning the set of the allowable values that the measurement can take. Note that these values\\ndon’t always have to be numerical, though they often are. When measuring age, the values are\\nnumerical, but we still need to think carefully about what numbers are allowed. Do we want age\\nin years, years and months, days, hours? Etc. For other types of measurements (e.g., gender), the\\nvalues aren’t numerical. But, just as before, we need to think about what values are allowed. If\\nwe’re asking people to self-report their gender, what options to we allow them to choose between?\\nIs it enough to allow only “male” or “female”? Do you need an “other” option? Or should we not\\ngive people any speciﬁc options, and let them answer in their own words? And if you open up the\\nset of possible values to include all verbal response, how will you interpret their answers?\\nOperationalisation is a tricky business, and there’s no “one, true way” to do it. The way in which you\\nchoose to operationalise the informal concept of “age” or “gender” into a formal measurement depends on\\nwhat you need to use the measurement for. Often you’ll ﬁnd that the community of scientists who work in\\nyour area have some fairly well-established ideas for how to go about it. In other words, operationalisation\\nneeds to be thought through on a case by case basis. Nevertheless, while there a lot of issues that are\\nspeciﬁc to each individual research project, there are some aspects to it that are pretty general.\\nBefore moving on, I want to take a moment to clear up our terminology, and in the process introduce\\none more term. Here are four diﬀerent things that are closely related to each other:\\n• A theoretical construct. This is the thing that you’re trying to take a measurement of, like\\n“age”, “gender” or an “opinion”. A theoretical construct can’t be directly observed, and often\\nthey’re actually a bit vague.\\n• A measure. The measure refers to the method or the tool that you use to make your observations.\\nA question in a survey, a behavioural observation or a brain scan could all count as a measure.\\n• An operationalisation. The term “operationalisation” refers to the logical connection between\\nthe measure and the theoretical construct, or to the process by which we try to derive a measure\\nfrom a theoretical construct.\\n• A variable. Finally, a new term. A variable is what we end up with when we apply our measure\\nto something in the world. That is, variables are the actual “data” that we end up with in our data\\nsets.\\nIn practice, even scientists tend to blur the distinction between these things, but it’s very helpful to try\\nto understand the diﬀerences.\\n- 13 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 23}, page_content='2.2\\nScales of measurement\\nAs the previous section indicates, the outcome of a psychological measurement is called a variable. But\\nnot all variables are of the same qualitative type, and it’s very useful to understand what types there are.\\nA very useful concept for distinguishing between diﬀerent types of variables is what’s known as scales\\nof measurement.\\n2.2.1\\nNominal scale\\nA nominal scale variable is one in which there is no particular relationship between the diﬀerent\\npossibilities: for these kinds of variables it doesn’t make any sense to say that one of them is “bigger’\\nor “better” than any other one, and it absolutely doesn’t make any sense to average them. The classic\\nexample for this is “eye colour”. Eyes can be blue, green and brown, among other possibilities, but none\\nof them is any “better” than any other one. As a result, it would feel really weird to talk about an\\n“average eye colour”. Similarly, gender is nominal too: male isn’t better or worse than female, neither\\ndoes it make sense to try to talk about an “average gender”. In short, nominal scale variables are those\\nfor which the only thing you can say about the diﬀerent possibilities is that they are diﬀerent. That’s it.\\nLet’s take a slightly closer look at this. Suppose I was doing research on how people commute to and\\nfrom work. One variable I would have to measure would be what kind of transportation people use to\\nget to work. This “transport type” variable could have quite a few possible values, including: “train”,\\n“bus”, “car”, “bicycle”, etc. For now, let’s suppose that these four are the only possibilities, and suppose\\nthat when I ask 100 people how they got to work today, and I get this:\\nTransportation\\nNumber of people\\n(1) Train\\n12\\n(2) Bus\\n30\\n(3) Car\\n48\\n(4) Bicycle\\n10\\nSo, what’s the average transportation type? Obviously, the answer here is that there isn’t one. It’s a\\nsilly question to ask. You can say that travel by car is the most popular method, and travel by train is\\nthe least popular method, but that’s about all. Similarly, notice that the order in which I list the options\\nisn’t very interesting. I could have chosen to display the data like this\\nTransportation\\nNumber of people\\n(3) Car\\n48\\n(1) Train\\n12\\n(4) Bicycle\\n10\\n(2) Bus\\n30\\nand nothing really changes.\\n2.2.2\\nOrdinal scale\\nOrdinal scale variables have a bit more structure than nominal scale variables, but not by a lot. An\\nordinal scale variable is one in which there is a natural, meaningful way to order the diﬀerent possibilities,\\n- 14 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 24}, page_content='but you can’t do anything else. The usual example given of an ordinal variable is “ﬁnishing position in\\na race”. You can say that the person who ﬁnished ﬁrst was faster than the person who ﬁnished second,\\nbut you don’t know how much faster. As a consequence we know that 1st ą 2nd, and we know that 2nd\\ną 3rd, but the diﬀerence between 1st and 2nd might be much larger than the diﬀerence between 2nd and\\n3rd.\\nHere’s an more psychologically interesting example. Suppose I’m interested in people’s attitudes to\\nclimate change, and I ask them to pick one of these four statements that most closely matches their\\nbeliefs:\\n(1) Temperatures are rising, because of human activity\\n(2) Temperatures are rising, but we don’t know why\\n(3) Temperatures are rising, but not because of humans\\n(4) Temperatures are not rising\\nNotice that these four statements actually do have a natural ordering, in terms of “the extent to which\\nthey agree with the current science”. Statement 1 is a close match, statement 2 is a reasonable match,\\nstatement 3 isn’t a very good match, and statement 4 is in strong opposition to the science. So, in terms\\nof the thing I’m interested in (the extent to which people endorse the science), I can order the items as\\n1 ą 2 ą 3 ą 4. Since this ordering exists, it would be very weird to list the options like this. . .\\n(3) Temperatures are rising, but not because of humans\\n(1) Temperatures are rising, because of human activity\\n(4) Temperatures are not rising\\n(2) Temperatures are rising, but we don’t know why\\n. . . because it seems to violate the natural “structure” to the question.\\nSo, let’s suppose I asked 100 people these questions, and got the following answers:\\nResponse\\nNumber\\n(1) Temperatures are rising, because of human activity\\n51\\n(2) Temperatures are rising, but we don’t know why\\n20\\n(3) Temperatures are rising, but not because of humans\\n10\\n(4) Temperatures are not rising\\n19\\nWhen analysing these data, it seems quite reasonable to try to group (1), (2) and (3) together, and say\\nthat 81 of 100 people were willing to at least partially endorse the science. And it’s also quite reasonable\\nto group (2), (3) and (4) together and say that 49 of 100 people registered at least some disagreement\\nwith the dominant scientiﬁc view. However, it would be entirely bizarre to try to group (1), (2) and (4)\\ntogether and say that 90 of 100 people said. . . what? There’s nothing sensible that allows you to group\\nthose responses together at all.\\nThat said, notice that while we can use the natural ordering of these items to construct sensible\\ngroupings, what we can’t do is average them. For instance, in my simple example here, the “average”\\nresponse to the question is 1.97. If you can tell me what that means, I’d love to know. Because that\\nsounds like gibberish to me!\\n2.2.3\\nInterval scale\\nIn contrast to nominal and ordinal scale variables, interval scale and ratio scale variables are vari-\\nables for which the numerical value is genuinely meaningful. In the case of interval scale variables, the\\ndiﬀerences between the numbers are interpretable, but the variable doesn’t have a “natural” zero value.\\n- 15 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 25}, page_content='A good example of an interval scale variable is measuring temperature in degrees celsius. For instance,\\nif it was 15˝ yesterday and 18˝ today, then the 3˝ diﬀerence between the two is genuinely meaningful.\\nMoreover, that 3˝ diﬀerence is exactly the same as the 3˝ diﬀerence between 7˝ and 10˝. In short, addition\\nand subtraction are meaningful for interval scale variables.\\nHowever, notice that the 0˝ does not mean “no temperature at all”: it actually means “the temperature\\nat which water freezes”, which is pretty arbitrary.\\nAs a consequence, it becomes pointless to try to\\nmultiply and divide temperatures. It is wrong to say that 20˝ is twice as hot as 10˝, just as it is weird\\nand meaningless to try to claim that 20˝ is negative two times as hot as ´10˝.\\nAgain, lets look at a more psychological example.\\nSuppose I’m interested in looking at how the\\nattitudes of ﬁrst-year university students have changed over time. Obviously, I’m going to want to record\\nthe year in which each student started. This is an interval scale variable. A student who started in 2003\\ndid arrive 5 years before a student who started in 2008. However, it would be completely insane for me\\nto divide 2008 by 2003 and say that the second student started “1.0024 times later” than the ﬁrst one.\\nThat doesn’t make any sense at all.\\n2.2.4\\nRatio scale\\nThe fourth and ﬁnal type of variable to consider is a ratio scale variable, in which zero really means\\nzero, and it’s okay to multiply and divide. A good psychological example of a ratio scale variable is\\nresponse time (RT). In a lot of tasks it’s very common to record the amount of time somebody takes to\\nsolve a problem or answer a question, because it’s an indicator of how diﬃcult the task is. Suppose that\\nAlan takes 2.3 seconds to respond to a question, whereas Ben takes 3.1 seconds. As with an interval scale\\nvariable, addition and subtraction are both meaningful here. Ben really did take 3.1 ´ 2.3 “ 0.8 seconds\\nlonger than Alan did. However, notice that multiplication and division also make sense here too: Ben\\ntook 3.1{2.3 “ 1.35 times as long as Alan did to answer the question. And the reason why you can do\\nthis is that, for a ratio scale variable such as RT, “zero seconds” really does mean “no time at all”.\\n2.2.5\\nContinuous versus discrete variables\\nThere’s a second kind of distinction that you need to be aware of, regarding what types of variables you\\ncan run into. This is the distinction between continuous variables and discrete variables. The diﬀerence\\nbetween these is as follows:\\n• A continuous variable is one in which, for any two values that you can think of, it’s always\\nlogically possible to have another value in between.\\n• A discrete variable is, in eﬀect, a variable that isn’t continuous. For a discrete variable, it’s\\nsometimes the case that there’s nothing in the middle.\\nThese deﬁnitions probably seem a bit abstract, but they’re pretty simple once you see some examples.\\nFor instance, response time is continuous. If Alan takes 3.1 seconds and Ben takes 2.3 seconds to respond\\nto a question, then it’s possible for Cameron’s response time to lie in between, by taking 3.0 seconds.\\nAnd of course it would also be possible for David to take 3.031 seconds to respond, meaning that his RT\\nwould lie in between Cameron’s and Alan’s. And while in practice it might be impossible to measure\\nRT that precisely, it’s certainly possible in principle. Because we can always ﬁnd a new value for RT in\\nbetween any two other ones, we say that RT is continuous.\\nDiscrete variables occur when this rule is violated. For example, nominal scale variables are always\\ndiscrete: there isn’t a type of transportation that falls “in between” trains and bicycles, not in the strict\\nmathematical way that 2.3 falls in between 2 and 3. So transportation type is discrete. Similarly, ordinal\\nscale variables are always discrete: although “2nd place” does fall between “1st place” and “3rd place”,\\nthere’s nothing that can logically fall in between “1st place” and “2nd place”. Interval scale and ratio\\n- 16 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 26}, page_content='Table 2.1: The relationship between the scales of measurement and the discrete/continuity distinction.\\nCells with a tick mark correspond to things that are possible.\\ncontinuous\\ndiscrete\\nnominal\\n✓\\nordinal\\n✓\\ninterval\\n✓\\n✓\\nratio\\n✓\\n✓\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nscale variables can go either way. As we saw above, response time (a ratio scale variable) is continuous.\\nTemperature in degrees celsius (an interval scale variable) is also continuous. However, the year you went\\nto school (an interval scale variable) is discrete. There’s no year in between 2002 and 2003. The number of\\nquestions you get right on a true-or-false test (a ratio scale variable) is also discrete: since a true-or-false\\nquestion doesn’t allow you to be “partially correct”, there’s nothing in between 5/10 and 6/10. Table 2.1\\nsummarises the relationship between the scales of measurement and the discrete/continuity distinction.\\nCells with a tick mark correspond to things that are possible. I’m trying to hammer this point home,\\nbecause (a) some textbooks get this wrong, and (b) people very often say things like “discrete variable”\\nwhen they mean “nominal scale variable”. It’s very unfortunate.\\n2.2.6\\nSome complexities\\nOkay, I know you’re going to be shocked to hear this, but . . . the real world is much messier than\\nthis little classiﬁcation scheme suggests. Very few variables in real life actually fall into these nice neat\\ncategories, so you need to be kind of careful not to treat the scales of measurement as if they were\\nhard and fast rules. It doesn’t work like that: they’re guidelines, intended to help you think about the\\nsituations in which you should treat diﬀerent variables diﬀerently. Nothing more.\\nSo let’s take a classic example, maybe the classic example, of a psychological measurement tool: the\\nLikert scale. The humble Likert scale is the bread and butter tool of all survey design. You yourself\\nhave ﬁlled out hundreds, maybe thousands of them, and odds are you’ve even used one yourself. Suppose\\nwe have a survey question that looks like this:\\nWhich of the following best describes your opinion of the statement that “all pirates are\\nfreaking awesome” . . .\\nand then the options presented to the participant are these:\\n(1) Strongly disagree\\n(2) Disagree\\n(3) Neither agree nor disagree\\n(4) Agree\\n(5) Strongly agree\\nThis set of items is an example of a 5-point Likert scale: people are asked to choose among one of\\nseveral (in this case 5) clearly ordered possibilities, generally with a verbal descriptor given in each case.\\nHowever, it’s not necessary that all items be explicitly described. This is a perfectly good example of a\\n5-point Likert scale too:\\n(1) Strongly disagree\\n(2)\\n- 17 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 27}, page_content='(3)\\n(4)\\n(5) Strongly agree\\nLikert scales are very handy, if somewhat limited, tools. The question is, what kind of variable are\\nthey? They’re obviously discrete, since you can’t give a response of 2.5. They’re obviously not nominal\\nscale, since the items are ordered; and they’re not ratio scale either, since there’s no natural zero.\\nBut are they ordinal scale or interval scale? One argument says that we can’t really prove that the\\ndiﬀerence between “strongly agree” and “agree” is of the same size as the diﬀerence between “agree”\\nand “neither agree nor disagree”. In fact, in everyday life it’s pretty obvious that they’re not the same\\nat all. So this suggests that we ought to treat Likert scales as ordinal variables. On the other hand, in\\npractice most participants do seem to take the whole “on a scale from 1 to 5” part fairly seriously, and\\nthey tend to act as if the diﬀerences between the ﬁve response options were fairly similar to one another.\\nAs a consequence, a lot of researchers treat Likert scale data as if it were interval scale. It’s not interval\\nscale, but in practice it’s close enough that we usually think of it as being quasi-interval scale.\\n2.3\\nAssessing the reliability of a measurement\\nAt this point we’ve thought a little bit about how to operationalise a theoretical construct and thereby\\ncreate a psychological measure; and we’ve seen that by applying psychological measures we end up with\\nvariables, which can come in many diﬀerent types. At this point, we should start discussing the obvious\\nquestion: is the measurement any good? We’ll do this in terms of two related ideas: reliability and\\nvalidity. Put simply, the reliability of a measure tells you how precisely you are measuring something,\\nwhereas the validity of a measure tells you how accurate the measure is. In this section I’ll talk about\\nreliability; we’ll talk about validity in the next chapter.\\nReliability is actually a very simple concept: it refers to the repeatability or consistency of your\\nmeasurement. The measurement of my weight by means of a “bathroom scale” is very reliable: if I step\\non and oﬀthe scales over and over again, it’ll keep giving me the same answer. Measuring my intelligence\\nby means of “asking my mum” is very unreliable: some days she tells me I’m a bit thick, and other days\\nshe tells me I’m a complete moron. Notice that this concept of reliability is diﬀerent to the question of\\nwhether the measurements are correct (the correctness of a measurement relates to it’s validity). If I’m\\nholding a sack of potatos when I step on and oﬀof the bathroom scales, the measurement will still be\\nreliable: it will always give me the same answer. However, this highly reliable answer doesn’t match up to\\nmy true weight at all, therefore it’s wrong. In technical terms, this is a reliable but invalid measurement.\\nSimilarly, while my mum’s estimate of my intelligence is a bit unreliable, she might be right. Maybe I’m\\njust not too bright, and so while her estimate of my intelligence ﬂuctuates pretty wildly from day to day,\\nit’s basically right. So that would be an unreliable but valid measure. Of course, to some extent, notice\\nthat if my mum’s estimates are too unreliable, it’s going to be very hard to ﬁgure out which one of her\\nmany claims about my intelligence is actually the right one. To some extent, then, a very unreliable\\nmeasure tends to end up being invalid for practical purposes; so much so that many people would say\\nthat reliability is necessary (but not suﬃcient) to ensure validity.\\nOkay, now that we’re clear on the distinction between reliability and validity, let’s have a think about\\nthe diﬀerent ways in which we might measure reliability:\\n• Test-retest reliability. This relates to consistency over time: if we repeat the measurement at a\\nlater date, do we get a the same answer?\\n- 18 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 28}, page_content='• Inter-rater reliability. This relates to consistency across people: if someone else repeats the\\nmeasurement (e.g., someone else rates my intelligence) will they produce the same answer?\\n• Parallel forms reliability. This relates to consistency across theoretically-equivalent measure-\\nments: if I use a diﬀerent set of bathroom scales to measure my weight, does it give the same\\nanswer?\\n• Internal consistency reliability. If a measurement is constructed from lots of diﬀerent parts\\nthat perform similar functions (e.g., a personality questionnaire result is added up across several\\nquestions) do the individual parts tend to give similar answers.\\nNot all measurements need to possess all forms of reliability. For instance, educational assessment can\\nbe thought of as a form of measurement. One of the subjects that I teach, Computational Cognitive\\nScience, has an assessment structure that has a research component and an exam component (plus other\\nthings). The exam component is intended to measure something diﬀerent from the research component,\\nso the assessment as a whole has low internal consistency. However, within the exam there are several\\nquestions that are intended to (approximately) measure the same things, and those tend to produce\\nsimilar outcomes; so the exam on its own has a fairly high internal consistency. Which is as it should be.\\nYou should only demand reliability in those situations where you want to be measure the same thing!\\n2.4\\nThe “role” of variables: predictors and outcomes\\nOkay, I’ve got one last piece of terminology that I need to explain to you before moving away from\\nvariables. Normally, when we do some research we end up with lots of diﬀerent variables. Then, when we\\nanalyse our data we usually try to explain some of the variables in terms of some of the other variables.\\nIt’s important to keep the two roles “thing doing the explaining” and “thing being explained” distinct.\\nSo let’s be clear about this now. Firstly, we might as well get used to the idea of using mathematical\\nsymbols to describe variables, since it’s going to happen over and over again. Let’s denote the “to be\\nexplained” variable Y , and denote the variables “doing the explaining” as X1, X2, etc.\\nNow, when we doing an analysis, we have diﬀerent names for X and Y , since they play diﬀerent roles\\nin the analysis. The classical names for these roles are independent variable (IV) and dependent\\nvariable (DV). The IV is the variable that you use to do the explaining (i.e., X) and the DV is the variable\\nbeing explained (i.e., Y ). The logic behind these names goes like this: if there really is a relationship\\nbetween X and Y then we can say that Y depends on X, and if we have designed our study “properly”\\nthen X isn’t dependent on anything else. However, I personally ﬁnd those names horrible: they’re hard to\\nremember and they’re highly misleading, because (a) the IV is never actually “independent of everything\\nelse” and (b) if there’s no relationship, then the DV doesn’t actually depend on the IV. And in fact,\\nbecause I’m not the only person who thinks that IV and DV are just awful names, there are a number\\nof alternatives that I ﬁnd more appealing. The terms that I’ll use in these notes are predictors and\\noutcomes. The idea here is that what you’re trying to do is use X (the predictors) to make guesses\\nabout Y (the outcomes).1 This is summarised in Table 2.2.\\n1Annoyingly, though, there’s a lot of diﬀerent names used out there. I won’t list all of them – there would be no point\\nin doing that – other than to note that R often uses “response variable” where I’ve used “outcome”, and a traditionalist\\nwould use “dependent variable”. Sigh. This sort of terminological confusion is very common, I’m afraid.\\n- 19 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 29}, page_content='Table 2.2: The terminology used to distinguish between diﬀerent roles that a variable can play when\\nanalysing a data set. Note that this book will tend to avoid the classical terminology in favour of the\\nnewer names.\\nrole of the variable\\nclassical name\\nmodern name\\n“to be explained”\\ndependent variable (DV)\\noutcome\\n“to do the explaining”\\nindependent variable (IV)\\npredictor\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2.5\\nExperimental and non-experimental research\\nOne of the big distinctions that you should be aware of is the distinction between “experimental research”\\nand “non-experimental research”. When we make this distinction, what we’re really talking about is the\\ndegree of control that the researcher exercises over the people and events in the study.\\n2.5.1\\nExperimental research\\nThe key features of experimental research is that the researcher controls all aspects of the study,\\nespecially what participants experience during the study. In particular, the researcher manipulates or\\nvaries the predictor variables (IVs), and then allows the outcome variable (DV) to vary naturally. The\\nidea here is to deliberately vary the predictors (IVs) to see if they have any causal eﬀects on the outcomes.\\nMoreover, in order to ensure that there’s no chance that something other than the predictor variables\\nis causing the outcomes, everything else is kept constant or is in some other way “balanced” to ensure\\nthat they have no eﬀect on the results. In practice, it’s almost impossible to think of everything else that\\nmight have an inﬂuence on the outcome of an experiment, much less keep it constant. The standard\\nsolution to this is randomisation: that is, we randomly assign people to diﬀerent groups, and then give\\neach group a diﬀerent treatment (i.e., assign them diﬀerent values of the predictor variables). We’ll talk\\nmore about randomisation later in this course, but for now, it’s enough to say that what randomisation\\ndoes is minimise (but not eliminate) the chances that there are any systematic diﬀerence between groups.\\nLet’s consider a very simple, completely unrealistic and grossly unethical example.\\nSuppose you\\nwanted to ﬁnd out if smoking causes lung cancer. One way to do this would be to ﬁnd people who smoke\\nand people who don’t smoke, and look to see if smokers have a higher rate of lung cancer. This is not\\na proper experiment, since the researcher doesn’t have a lot of control over who is and isn’t a smoker.\\nAnd this really matters: for instance, it might be that people who choose to smoke cigarettes also tend\\nto have poor diets, or maybe they tend to work in asbestos mines, or whatever. The point here is that\\nthe groups (smokers and non-smokers) actually diﬀer on lots of things, not just smoking. So it might be\\nthat the higher incidence of lung cancer among smokers is caused by something else, not by smoking per\\nse. In technical terms, these other things (e.g. diet) are called “confounds”, and we’ll talk about those\\nin just a moment.\\nIn the meantime, let’s now consider what a proper experiment might look like. Recall that our concern\\nwas that smokers and non-smokers might diﬀer in lots of ways. The solution, as long as you have no\\nethics, is to control who smokes and who doesn’t. Speciﬁcally, if we randomly divide participants into\\ntwo groups, and force half of them to become smokers, then it’s very unlikely that the groups will diﬀer\\nin any respect other than the fact that half of them smoke. That way, if our smoking group gets cancer\\n- 20 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 30}, page_content='at a higher rate than the non-smoking group, then we can feel pretty conﬁdent that (a) smoking does\\ncause cancer and (b) we’re murderers.\\n2.5.2\\nNon-experimental research\\nNon-experimental research is a broad term that covers “any study in which the researcher doesn’t\\nhave quite as much control as they do in an experiment”. Obviously, control is something that scientists\\nlike to have, but as the previous example illustrates, there are lots of situations in which you can’t or\\nshouldn’t try to obtain that control. Since it’s grossly unethical (and almost certainly criminal) to force\\npeople to smoke in order to ﬁnd out if they get cancer, this is a good example of a situation in which\\nyou really shouldn’t try to obtain experimental control. But there are other reasons too. Even leaving\\naside the ethical issues, our “smoking experiment” does have a few other issues. For instance, when I\\nsuggested that we “force” half of the people to become smokers, I must have been talking about starting\\nwith a sample of non-smokers, and then forcing them to become smokers. While this sounds like the\\nkind of solid, evil experimental design that a mad scientist would love, it might not be a very sound\\nway of investigating the eﬀect in the real world. For instance, suppose that smoking only causes lung\\ncancer when people have poor diets, and suppose also that people who normally smoke do tend to have\\npoor diets. However, since the “smokers” in our experiment aren’t “natural” smokers (i.e., we forced\\nnon-smokers to become smokers; they didn’t take on all of the other normal, real life characteristics that\\nsmokers might tend to possess) they probably have better diets. As such, in this silly example they\\nwouldn’t get lung cancer, and our experiment will fail, because it violates the structure of the “natural”\\nworld (the technical name for this is an “artifactual” result; see later).\\nOne distinction worth making between two types of non-experimental research is the diﬀerence be-\\ntween quasi-experimental research and case studies. The example I discussed earlier – in which we\\nwanted to examine incidence of lung cancer among smokers and non-smokers, without trying to control\\nwho smokes and who doesn’t – is a quasi-experimental design. That is, it’s the same as an experiment,\\nbut we don’t control the predictors (IVs). We can still use statistics to analyse the results, it’s just that\\nwe have to be a lot more careful.\\nThe alternative approach, case studies, aims to provide a very detailed description of one or a few\\ninstances. In general, you can’t use statistics to analyse the results of case studies, and it’s usually very\\nhard to draw any general conclusions about “people in general” from a few isolated examples. However,\\ncase studies are very useful in some situations. Firstly, there are situations where you don’t have any\\nalternative: neuropsychology has this issue a lot. Sometimes, you just can’t ﬁnd a lot of people with\\nbrain damage in a speciﬁc area, so the only thing you can do is describe those cases that you do have in\\nas much detail and with as much care as you can. However, there’s also some genuine advantages to case\\nstudies: because you don’t have as many people to study, you have the ability to invest lots of time and\\neﬀort trying to understand the speciﬁc factors at play in each case. This is a very valuable thing to do.\\nAs a consequence, case studies can complement the more statistically-oriented approaches that you see in\\nexperimental and quasi-experimental designs. We won’t talk much about case studies in these lectures,\\nbut they are nevertheless very valuable tools!\\n2.6\\nAssessing the validity of a study\\nMore than any other thing, a scientist wants their research to be “valid”. The conceptual idea behind\\nvalidity is very simple: can you trust the results of your study? If not, the study is invalid. However,\\nwhile it’s easy to state, in practice it’s much harder to check validity than it is to check reliability. And\\nin all honesty, there’s no precise, clearly agreed upon notion of what validity actually is. In fact, there’s\\n- 21 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 31}, page_content='lots of diﬀerent kinds of validity, each of which raises it’s own issues, and not all forms of validity are\\nrelevant to all studies. I’m going to talk about ﬁve diﬀerent types:\\n• Internal validity\\n• External validity\\n• Construct validity\\n• Face validity\\n• Ecological validity\\nTo give you a quick guide as to what matters here. . . (1) Internal and external validity are the most\\nimportant, since they tie directly to the fundamental question of whether your study really works. (2)\\nConstruct validity asks whether you’re measuring what you think you are. (3) Face validity isn’t terribly\\nimportant except insofar as you care about “appearances”. (4) Ecological validity is a special case of face\\nvalidity that corresponds to a kind of appearance that you might care about a lot.\\n2.6.1\\nInternal validity\\nInternal validity refers to the extent to which you are able draw the correct conclusions about the\\ncausal relationships between variables. It’s called “internal” because it refers to the relationships between\\nthings “inside” the study. Let’s illustrate the concept with a simple example. Suppose you’re interested\\nin ﬁnding out whether a university education makes you write better. To do so, you get a group of ﬁrst\\nyear students, ask them to write a 1000 word essay, and count the number of spelling and grammatical\\nerrors they make. Then you ﬁnd some third-year students, who obviously have had more of a university\\neducation than the ﬁrst-years, and repeat the exercise. And let’s suppose it turns out that the third-year\\nstudents produce fewer errors. And so you conclude that a university education improves writing skills.\\nRight? Except... the big problem that you have with this experiment is that the third-year students are\\nolder, and they’ve had more experience with writing things. So it’s hard to know for sure what the causal\\nrelationship is: Do older people write better? Or people who have had more writing experience? Or\\npeople who have had more education? Which of the above is the true cause of the superior performance\\nof the third-years? Age? Experience? Education? You can’t tell. This is an example of a failure of\\ninternal validity, because your study doesn’t properly tease apart the causal relationships between the\\ndiﬀerent variables.\\n2.6.2\\nExternal validity\\nExternal validity relates to the generalisability of your ﬁndings. That is, to what extent do you\\nexpect to see the same pattern of results in “real life” as you saw in your study. To put it a bit more\\nprecisely, any study that you do in psychology will involve a fairly speciﬁc set of questions or tasks, will\\noccur in a speciﬁc environment, and will involve participants that are drawn from a particular subgroup.\\nSo, if it turns out that the results don’t actually generalise to people and situations beyond the ones that\\nyou studied, then what you’ve got is a lack of external validity.\\nThe classic example of this issue is the fact that a very large proportion of studies in psychology will\\nuse undergraduate psychology students as the participants. Obviously, however, the researchers don’t\\ncare only about psychology students; they care about people in general. Given that, a study that uses\\nonly psych students as participants always carries a risk of lacking external validity. That is, if there’s\\nsomething “special” about psychology students that makes them diﬀerent to the general populace in\\nsome relevant respect, then we may start worrying about a lack of external validity.\\n- 22 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 32}, page_content='That said, it is absolutely critical to realise that a study that uses only psychology students does not\\nnecessarily have a problem with external validity. I’ll talk about this again later, but it’s such a common\\nmistake that I’m going to mention it here. The external validity is threatened by the choice of population\\nif (a) the population from which you sample your participants is very narrow (e.g., psych students), and\\n(b) the narrow population that you sampled from is systematically diﬀerent from the general population,\\nin some respect that is relevant to the psychological phenomenon that you intend to study. The italicised\\npart is the bit that lots of people forget: it is true that psychology undergraduates diﬀer from the general\\npopulation in lots of ways, and so a study that uses only psych students may have problems with external\\nvalidity. However, if those diﬀerences aren’t very relevant to the phenomenon that you’re studying, then\\nthere’s nothing to worry about. To make this a bit more concrete, here’s two extreme examples:\\n• You want to measure “attitudes of the general public towards psychotherapy”, but all of your\\nparticipants are psychology students.\\nThis study would almost certainly have a problem with\\nexternal validity.\\n• You want to measure the eﬀectiveness of a visual illusion, and your participants are all psychology\\nstudents. This study is very unlikely to have a problem with external validity\\nHaving just spent the last couple of paragraphs focusing on the choice of participants (since that’s the\\nbig issue that everyone tends to worry most about), it’s worth remembering that external validity is a\\nbroader concept. The following are also examples of things that might pose a threat to external validity,\\ndepending on what kind of study you’re doing:\\n• People might answer a “psychology questionnaire” an a manner that doesn’t reﬂect what they\\nwould do in real life.\\n• Your lab experiment on (say) “human learning” has a diﬀerent structure to the learning problems\\npeople face in real life.\\n2.6.3\\nConstruct validity\\nConstruct validity is basically a question of whether you’re measuring what you want to be mea-\\nsuring. A measurement has good construct validity if it is actually measuring the correct theoretical\\nconstruct, and bad construct validity if it doesn’t. To give very simple (if ridiculous) example, suppose\\nI’m trying to investigate the rates with which university students cheat on their exams. And the way I\\nattempt to measure it is by asking the cheating students to stand up in the lecture theatre so that I can\\ncount them. When I do this with a class of 300 students, 0 people claim to be cheaters. So I therefore\\nconclude that the proportion of cheaters in my class is 0%. Clearly this is a bit ridiculous. But the point\\nhere is not that this is a very deep methodological example, but rather to explain what construct validity\\nis. The problem with my measure is that while I’m trying to measure “the proportion of people who\\ncheat” what I’m actually measuring is “the proportion of people stupid enough to own up to cheating, or\\nbloody minded enough to pretend that they do”. Obviously, these aren’t the same thing! So my study\\nhas gone wrong, because my measurement has very poor construct validity.\\n2.6.4\\nFace validity\\nFace validity simply refers to whether or not a measure “looks like” it’s doing what it’s supposed to,\\nnothing more. If I design a test of intelligence, and people look at it and they say “no, that test doesn’t\\nmeasure intelligence”, then the measure lacks face validity. It’s as simple as that. Obviously, face validity\\nisn’t very important from a pure scientiﬁc perspective. After all, what we care about is whether or not\\n- 23 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 33}, page_content='the measure actually does what it’s supposed to do, not whether it looks like it does what it’s supposed\\nto do. As a consequence, we generally don’t care very much about face validity. That said, the concept\\nof face validity serves three useful pragmatic purposes:\\n• Sometimes, an experienced scientist will have a “hunch” that a particular measure won’t work.\\nWhile these sorts of hunches have no strict evidentiary value, it’s often worth paying attention to\\nthem. Because often times people have knowledge that they can’t quite verbalise, so there might be\\nsomething to worry about even if you can’t quite say why. In other words, when someone you trust\\ncriticises the face validity of your study, it’s worth taking the time to think more carefully about\\nyour design to see if you can think of reasons why it might go awry. Mind you, if you don’t ﬁnd\\nany reason for concern, then you should probably not worry: after all, face validity really doesn’t\\nmatter much.\\n• Often (very often), completely uninformed people will also have a “hunch” that your research is\\ncrap. And they’ll criticise it on the internet or something. On close inspection, you’ll often notice\\nthat these criticisms are actually focused entirely on how the study “looks”, but not on anything\\ndeeper. The concept of face validity is useful for gently explaining to people that they need to\\nsubstantiate their arguments further.\\n• Expanding on the last point, if the beliefs of untrained people are critical (e.g., this is often the\\ncase for applied research where you actually want to convince policy makers of something or other)\\nthen you have to care about face validity. Simply because – whether you like it or not – a lot of\\npeople will use face validity as a proxy for real validity. If you want the government to change a\\nlaw on scientiﬁc, psychological grounds, then it won’t matter how good your studies “really” are. If\\nthey lack face validity, you’ll ﬁnd that politicians ignore you. Of course, it’s somewhat unfair that\\npolicy often depends more on appearance than fact, but that’s how things go.\\n2.6.5\\nEcological validity\\nEcological validity is a diﬀerent notion of validity, which is similar to external validity, but less\\nimportant. The idea is that, in order to be ecologically valid, the entire set up of the study should closely\\napproximate the real world scenario that is being investigated. In a sense, ecological validity is a kind\\nof face validity – it relates mostly to whether the study “looks” right, but with a bit more rigour to it.\\nTo be ecologically valid, the study has to look right in a fairly speciﬁc way. The idea behind it is the\\nintuition that a study that is ecologically valid is more likely to be externally valid. It’s no guarantee,\\nof course. But the nice thing about ecological validity is that it’s much easier to check whether a study\\nis ecologically valid than it is to check whether a study is externally valid. An simple example would\\nbe eyewitness identiﬁcation studies. Most of these studies tend to be done in a university setting, often\\nwith fairly simple array of faces to look at rather than a line up. The length of time between seeing the\\n“criminal” and being asked to identify the suspect in the “line up” is usually shorter. The “crime” isn’t\\nreal, so there’s no chance that the witness being scared, and there’s no police oﬃcers present, so there’s\\nnot as much chance of feeling pressured. These things all mean that the study deﬁnitely lacks ecological\\nvalidity. They might (but might not) mean that it also lacks external validity.\\n2.7\\nConfounds, artifacts and other threats to validity\\nIf we look at the issue of validity in the most general fashion, the two biggest worries that we have are\\nconfounds and artifact. These two terms are deﬁned in the following way:\\n- 24 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 34}, page_content='• Confound: A confound is an additional, often unmeasured variable2 that turns out to be related\\nto both the predictors and the outcomes. The existence of confounds threatens the internal validity\\nof the study because you can’t tell whether the predictor causes the outcome, or if the confounding\\nvariable causes it, etc.\\n• Artifact: A result is said to be “artifactual” if it only holds in the special situation that you\\nhappened to test in your study. The possibility that your result is an artifact describes a threat to\\nyour external validity, because it raises the possibility that you can’t generalise your results to the\\nactual population that you care about.\\nAs a general rule confounds are a bigger concern for non-experimental studies, precisely because they’re\\nnot proper experiments: by deﬁnition, you’re leaving lots of things uncontrolled, so there’s a lot of scope\\nfor confounds working their way into your study. Experimental research tends to be much less vulnerable\\nto confounds: the more control you have over what happens during the study, the more you can prevent\\nconfounds from appearing.\\nHowever, there’s always swings and roundabouts, and when we start thinking about artifacts rather\\nthan confounds, the shoe is very ﬁrmly on the other foot. For the most part, artifactual results tend to\\nbe a concern for experimental studies than for non-experimental studies. To see this, it helps to realise\\nthat the reason that a lot of studies are non-experimental is precisely because what the researcher is\\ntrying to do is examine human behaviour in a more naturalistic context. By working in a more real-world\\ncontext, you lose experimental control (making yourself vulnerable to confounds) but because you tend\\nto be studying human psychology “in the wild” you reduce the chances of getting an artifactual result.\\nOr, to put it another way, when you take psychology out of the wild and bring it into the lab (which we\\nusually have to do to gain our experimental control), you always run the risk of accidentally studying\\nsomething diﬀerent than you wanted to study: which is more or less the deﬁnition of an artifact.\\nBe warned though: the above is a rough guide only. It’s absolutely possible to have confounds in an\\nexperiment, and to get artifactual results with non-experimental studies. This can happen for all sorts\\nof reasons, not least of which is researcher error. In practice, it’s really hard to think everything through\\nahead of time, and even very good researchers make mistakes. But other times it’s unavoidable, simply\\nbecause the researcher has ethics (e.g., see “diﬀerential attrition”).\\nOkay. There’s a sense in which almost any threat to validity can be characterised as a confound or\\nan artifact: they’re pretty vague concepts. So let’s have a look at some of the most common examples. . .\\n2.7.1\\nHistory eﬀects\\nHistory eﬀects refer to the possibility that speciﬁc events may occur during the study itself that\\nmight inﬂuence the outcomes. For instance, something might happen in between a pre-test and a post-\\ntest. Or, in between testing participant 23 and participant 24. Alternatively, it might be that you’re\\nlooking at an older study, which was perfectly valid for it’s time, but the world has changed enough since\\nthen that the conclusions are no longer trustworthy. Examples of things that would count as history\\neﬀects:\\n• You’re interested in how people think about risk and uncertainty. You started your data collection\\nin December 2010. But ﬁnding participants and collecting data takes time, so you’re still ﬁnding\\nnew people in February 2011. Unfortunately for you (and even more unfortunately for others),\\nthe Queensland ﬂoods occurred in January 2011, causing billions of dollars of damage and killing\\n2The reason why I say that it’s unmeasured is that if you have measured it, then you can use some fancy statistical tricks\\nto deal with the confound. Because of the existence of these statistical solutions to the problem of confounds, we often refer\\nto a confound that we have measured and dealt with as a covariate. Dealing with covariates is a topic for a more advanced\\ncourse, but I thought I’d mention it in passing, since it’s kind of comforting to at least know that this stuﬀexists.\\n- 25 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 35}, page_content='many people. Not surprisingly, the people tested in February 2011 express quite diﬀerent beliefs\\nabout handling risk than the people tested in December 2010. Which (if any) of these reﬂects the\\n“true” beliefs of participants? I think the answer is probably both: the Queensland ﬂoods genuinely\\nchanged the beliefs of the Australian public, though possibly only temporarily. The key thing here\\nis that the “history” of the people tested in February is quite diﬀerent to people tested in December.\\n• You’re testing the psychological eﬀects of a new anti-anxiety drug. So what you do is measure\\nanxiety before administering the drug (e.g., by self-report, and taking physiological measures, let’s\\nsay), then you administer the drug, and then you take the same measures afterwards. In the middle,\\nhowever, because your labs are in Los Angeles, there’s an earthquake, which increases the anxiety\\nof the participants.\\n2.7.2\\nMaturation eﬀects\\nAs with history eﬀects, maturational eﬀects are fundamentally about change over time. However,\\nmaturation eﬀects aren’t in response to speciﬁc events. Rather, they relate to how people change on their\\nown over time: we get older, we get tired, we get bored, etc. Some examples of maturation eﬀects:\\n• When doing developmental psychology research, you need to be aware that children grow up quite\\nrapidly. So, suppose that you want to ﬁnd out whether some educational trick helps with vocabulary\\nsize among 3 year olds. One thing that you need to be aware of is that the vocabulary size of children\\nthat age is growing at an incredible rate (multiple words per day), all on its own. If you design\\nyour study without taking this maturational eﬀect into account, then you won’t be able to tell if\\nyour educational trick works.\\n• When running a very long experiment in the lab (say, something that goes for 3 hours), it’s very\\nlikely that people will begin to get bored and tired, and that this maturational eﬀect will cause\\nperformance to decline, regardless of anything else going on in the experiment\\n2.7.3\\nRepeated testing eﬀects\\nAn important type of history eﬀect is the eﬀect of repeated testing. Suppose I want to take two\\nmeasurements of some psychological construct (e.g., anxiety). One thing I might be worried about is if\\nthe ﬁrst measurement has an eﬀect on the second measurement. In other words, this is a history eﬀect\\nin which the “event” that inﬂuences the second measurement is the ﬁrst measurement itself! This is not\\nat all uncommon. Examples of this include:\\n• Learning and practice: e.g., “intelligence” at time 2 might appear to go up relative to time 1 because\\nparticipants learned the general rules of how to solve “intelligence-test-style” questions during the\\nﬁrst testing session.\\n• Familiarity with the testing situation: e.g., if people are nervous at time 1, this might make per-\\nformance go down; after sitting through the ﬁrst testing situation, they might calm down a lot\\nprecisely because they’ve seen what the testing looks like.\\n• Auxiliary changes caused by testing: e.g., if a questionnaire assessing mood is boring, then mood at\\nmeasurement at time 2 is more likely to become “bored”, precisely because of the boring measure-\\nment made at time 1.\\n- 26 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 36}, page_content='2.7.4\\nSelection bias\\nSelection bias is a pretty broad term. Suppose that you’re running an experiment with two groups\\nof participants, where each group gets a diﬀerent “treatment”, and you want to see if the diﬀerent\\ntreatments lead to diﬀerent outcomes. However, suppose that, despite your best eﬀorts, you’ve ended up\\nwith a gender imbalance across groups (say, group A has 80% females and group B has 50% females).\\nIt might sound like this could never happen, but trust me, it can. This is an example of a selection\\nbias, in which the people “selected into” the two groups have diﬀerent characteristics. If any of those\\ncharacteristics turns out to be relevant (say, your treatment works better on females than males) then\\nyou’re in a lot of trouble.\\n2.7.5\\nDiﬀerential attrition\\nOne quite subtle danger to be aware of is called diﬀerential attrition, which is a kind of selection\\nbias that is caused by the study itself. Suppose that, for the ﬁrst time ever in the history of psychology,\\nI manage to ﬁnd the perfectly balanced and representative sample of people. I start running “Dan’s\\nincredibly long and tedious experiment” on my perfect sample, but then, because my study is incredibly\\nlong and tedious, lots of people start dropping out. I can’t stop this: as we’ll discuss later in the chapter\\non research ethics, participants absolutely have the right to stop doing any experiment, any time, for\\nwhatever reason they feel like, and as researchers we are morally (and professionally) obliged to remind\\npeople that they do have this right. So, suppose that “Dan’s incredibly long and tedious experiment”\\nhas a very high drop out rate. What do you suppose the odds are that this drop out is random? Answer:\\nzero. Almost certainly, the people who remain are more conscientious, more tolerant of boredom etc than\\nthose that leave. To the extent that (say) conscientiousness is relevant to the psychological phenomenon\\nthat I care about, this attrition can decrease the validity of my results.\\nWhen thinking about the eﬀects of diﬀerential attrition, it is sometimes helpful to distinguish between\\ntwo diﬀerent types. The ﬁrst is homogeneous attrition, in which the attrition eﬀect is the same for\\nall groups, treatments or conditions. In the example I gave above, the diﬀerential attrition would be\\nhomogeneous if (and only if) the easily bored participants are dropping out of all of the conditions in\\nmy experiment at about the same rate. In general, the main eﬀect of homogeneous attrition is likely to\\nbe that it makes your sample unrepresentative. As such, the biggest worry that you’ll have is that the\\ngeneralisability of the results decreases: in other words, you lose external validity.\\nThe second type of diﬀerential attrition is heterogeneous attrition, in which the attrition eﬀect is\\ndiﬀerent for diﬀerent groups. This is a much bigger problem: not only do you have to worry about your\\nexternal validity, you also have to worry about your internal validity too. To see why this is the case, let’s\\nconsider a very dumb study in which I want to see if insulting people makes them act in a more obedient\\nway. Why anyone would actually want to study that I don’t know, but let’s suppose I really, deeply\\ncared about this. So, I design my experiment with two conditions. In the “treatment” condition, the\\nexperimenter insults the participant and then gives them a questionnaire designed to measure obedience.\\nIn the “control” condition, the experimenter engages in a bit of pointless chitchat and then gives them\\nthe questionnaire. Leaving aside the questionable scientiﬁc merits and dubious ethics of such a study,\\nlet’s have a think about what might go wrong here. As a general rule, when someone insults me to my\\nface, I tend to get much less co-operative. So, there’s a pretty good chance that a lot more people are\\ngoing to drop out of the treatment condition than the control condition. And this drop out isn’t going\\nto be random. The people most likely to drop out would probably be the people who don’t care all\\nthat much about the importance of obediently sitting through the experiment. Since the most bloody\\nminded and disobedient people all left the treatment group but not the control group, we’ve introduced\\na confound: the people who actually took the questionnaire in the treatment group were already more\\nlikely to be dutiful and obedient than the people in the control group. In short, in this study insulting\\npeople doesn’t make them more obedient: it makes the more disobedient people leave the experiment!\\n- 27 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 37}, page_content='The internal validity of this experiment is completely shot.\\n2.7.6\\nNon-response bias\\nNon-response bias is closely related to selection bias, and to diﬀerential attrition. The simplest\\nversion of the problem goes like this. You mail out a survey to 1000 people, and only 300 of them reply.\\nThe 300 people who replied are almost certainly not a random subsample. People who respond to surveys\\nare systematically diﬀerent to people who don’t. This introduces a problem when trying to generalise\\nfrom those 300 people who replied, to the population at large; since you now have a very non-random\\nsample. The issue of non-response bias is more general than this, though. Among the (say) 300 people\\nthat did respond to the survey, you might ﬁnd that not everyone answers every question. If (say) 80\\npeople chose not to answer one of your questions, does this introduce problems? As always, the answer\\nis maybe. If the question that wasn’t answered was on the last page of the questionnaire, and those 80\\nsurveys were returned with the last page missing, there’s a good chance that the missing data isn’t a\\nbig deal: probably the pages just fell oﬀ. However, if the question that 80 people didn’t answer was the\\nmost confrontational or invasive personal question in the questionnaire, then almost certainly you’ve got\\na problem. In essence, what you’re dealing with here is what’s called the problem of missing data. If\\nthe data that is missing was “lost” randomly, then it’s not a big problem. If it’s missing systematically,\\nthen it can be a big problem.\\n2.7.7\\nRegression to the mean\\nRegression to the mean is a curious variation on selection bias. It refers to any situation where\\nyou select data based on an extreme value on some measure. Because the measure has natural variation,\\nit almost certainly means that when you take a subsequent measurement, that later measurement will be\\nless extreme than the ﬁrst one, purely by chance.\\nHere’s an example. Suppose I’m interested in whether a psychology education has an adverse eﬀect\\non very smart kids. To do this, I ﬁnd the 20 psych I students with the best high school grades and look\\nat how well they’re doing at university. It turns out that they’re doing a lot better than average, but\\nthey’re not topping the class at university, even though they did top their classes at high school. What’s\\ngoing on? The natural ﬁrst thought is that this must mean that the psychology classes must be having\\nan adverse eﬀect on those students. However, while that might very well be the explanation, it’s more\\nlikely that what you’re seeing is an example of “regression to the mean”. To see how it works, let’s take a\\nmoment to think about what is required to get the best mark in a class, regardless of whether that class\\nbe at high school or at university. When you’ve got a big class, there are going to be lots of very smart\\npeople enrolled. To get the best mark you have to be very smart, work very hard, and be a bit lucky.\\nThe exam has to ask just the right questions for your idiosyncratic skills, and you have to not make any\\ndumb mistakes (we all do that sometimes) when answering them. And that’s the thing: intelligence and\\nhard work are transferrable from one class to the next. Luck isn’t. The people who got lucky in high\\nschool won’t be the same as the people who get lucky at university. That’s the very deﬁnition of “luck”.\\nThe consequence of this is that, when you select people at the very extreme values of one measurement\\n(the top 20 students), you’re selecting for hard work, skill and luck. But because the luck doesn’t transfer\\nto the second measurement (only the skill and work), these people will all be expected to drop a little bit\\nwhen you measure them a second time (at university). So their scores fall back a little bit, back towards\\neveryone else. This is regression to the mean.\\nRegression to the mean is surprisingly common. For instance, if two very tall people have kids, their\\nchildren will tend to be taller than average, but not as tall as the parents. The reverse happens with very\\nshort parents: two very short parents will tend to have short children, but nevertheless those kids will\\ntend to be taller than the parents. It can also be extremely subtle. Quite some time ago (sorry, haven’t\\ntracked down the references yet) there was some research done that suggested that people learn better\\n- 28 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 38}, page_content='from negative feedback than from positive feedback. However, the way that people tried to show this was\\nto give people positive reinforcement whenever they did good, and negative reinforcement when they did\\nbad. And what you see is that after the positive reinforcement, people tended to do worse; but after the\\nnegative reinforcement they tended to do better. But! Notice that there’s a selection bias here: when\\npeople do very well, you’re selecting for “high” values, and so you should expect (because of regression\\nto the mean) that performance on the next trial should be worse, regardless of whether reinforcement is\\ngiven. Similarly, after a bad trial, people will tend to improve all on their own.\\n2.7.8\\nExperimenter bias\\nExperimenter bias can come in multiple forms. The basic idea is that the experimenter, despite the\\nbest of intentions, can accidentally end up inﬂuencing the results of the experiment by subtly communi-\\ncating the “right answer” or the “desired behaviour” to the participants. Typically, this occurs because\\nthe experimenter has special knowledge that the participant does not – either the right answer to the\\nquestions being asked, or knowledge of the expected pattern of performance for the condition that the\\nparticipant is in, and so on. The classic example of this happening is the case study of “Clever Hans”,\\nwhich dates back to 1907. Clever Hans was a horse that apparently was able to read and count, and\\nperform other human like feats of intelligence. After Clever Hans became famous, psychologists started\\nexamining his behaviour more closely. It turned out that – not surprisingly – Hans didn’t know how to\\ndo maths. Rather, Hans was responding to the human observers around him. Because they did know\\nhow to count, and the horse had learned to change its behaviour when people changed theirs.\\nThe general solution to the problem of experimenter bias is to engage in double blind studies, where\\nneither the experimenter nor the participant knows which condition the participant is in, or knows what\\nthe desired behaviour is. This provides a very good solution to the problem, but it’s important to recognise\\nthat it’s not quite ideal, and hard to pull oﬀperfectly. For instance, the obvious way that I could try\\nto construct a double blind study is to have one of my Ph.D. students (one who doesn’t know anything\\nabout the experiment) run the study. That feels like it should be enough. The only person (me) who\\nknows all the details (e.g., correct answers to the questions, assignments of participants to conditions)\\nhas no interaction with the participants, and the person who does all the talking to people (the Ph.D.\\nstudent) doesn’t know anything. Except, that last part is very unlikely to be true. In order for the Ph.D.\\nstudent to run the study eﬀectively, they need to have been briefed by me, the researcher. And, as it\\nhappens, the Ph.D. student also knows me, and knows a bit about my general beliefs about people and\\npsychology (e.g., I tend to think humans are much smarter than psychologists give them credit for). As\\na result of all this, it’s almost impossible for the experimenter to avoid knowing a little bit about what\\nexpectations I have. And even a little bit of knowledge can have an eﬀect: suppose the experimenter\\naccidentally conveys the fact that the participants are expected to do well in this task. Well, there’s a\\nthing called the “Pygmalion eﬀect”: if you expect great things of people, they’ll rise to the occasion; but\\nif you expect them to fail, they’ll do that too. In other words, the expectations become a self-fulﬁlling\\nprophesy.\\n2.7.9\\nDemand eﬀects and reactivity\\nWhen talking about experimenter bias, the worry is that the experimenter’s knowledge or desires for\\nthe experiment are communicated to the participants, and that these eﬀect people’s behaviour. However,\\neven if you manage to stop this from happening, it’s almost impossible to stop people from knowing that\\nthey’re part of a psychological study. And the mere fact of knowing that someone is watching/studying\\nyou can have a pretty big eﬀect on behaviour. This is generally referred to as reactivity or demand\\neﬀects. The basic idea is captured by the Hawthorne eﬀect: people alter their performance because\\nof the attention that the study focuses on them.\\nThe eﬀect takes its name from a the “Hawthorne\\nWorks” factory outside of Chicago. A study done in the 1920s looking at the eﬀects of lighting on worker\\n- 29 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 39}, page_content='productivity at the factory turned out to be an eﬀect of the fact that the workers knew they were being\\nstudied, rather than the lighting.\\nTo get a bit more speciﬁc about some of the ways in which the mere fact of being in a study can change\\nhow people behave, it helps to think like a social psychologist and look at some of the roles that people\\nmight adopt during an experiment, but might not adopt if the corresponding events were occurring in\\nthe real world:\\n• The good participant tries to be too helpful to the researcher: he or she seeks to ﬁgure out the\\nexperimenter’s hypotheses and conﬁrm them.\\n• The negative participant does the exact opposite of the good participant: he or she seeks to break\\nor destroy the study or the hypothesis in some way.\\n• The faithful participant is unnaturally obedient: he or she seeks to follow instructions perfectly,\\nregardless of what might have happened in a more realistic setting.\\n• The apprehensive participant gets nervous about being tested or studied, so much so that his or her\\nbehaviour becomes highly unnatural, or overly socially desirable.\\n2.7.10\\nPlacebo eﬀects\\nThe placebo eﬀect is a speciﬁc type of demand eﬀect that we worry a lot about. It refers to the\\nsituation where the mere fact of being treated causes an improvement in outcomes. The classic example\\ncomes from clinical trials: if you give people a completely chemically inert drug and tell them that it’s\\na cure for a disease, they will tend to get better faster than people who aren’t treated at all. In other\\nwords, it is people’s belief that they are being treated that causes the improved outcomes, not the drug.\\n2.7.11\\nSituation, measurement and subpopulation eﬀects\\nIn some respects, these terms are a catch-all term for “all other threats to external validity”. They\\nrefer to the fact that the choice of subpopulation from which you draw your participants, the location,\\ntiming and manner in which you run your study (including who collects the data) and the tools that you\\nuse to make your measurements might all be inﬂuencing the results. Speciﬁcally, the worry is that these\\nthings might be inﬂuencing the results in such a way that the results won’t generalise to a wider array\\nof people, places and measures.\\n2.7.12\\nFraud, deception and self-deception\\nOne ﬁnal thing that I feel like I should mention. While reading what the textbooks often have to\\nsay about assessing the validity of the study, I couldn’t help but notice that they seem to make the\\nassumption that the researcher is honest. I ﬁnd this hilarious. While the vast majority of scientists are\\nhonest, in my experience at least, some are not.3 Not only that, as I mentioned earlier, scientists are\\nnot immune to belief bias – it’s easy for a researcher to end up deceiving themselves into believing the\\nwrong thing, and this can lead them to conduct subtly ﬂawed research, and then hide those ﬂaws when\\nthey write it up. So you need to consider not only the (probably unlikely) possibility of outright fraud,\\nbut also the (probably quite common) possibility that the research is unintentionally “slanted”. Because\\n3Some people might argue that if you’re not honest then you’re not a real scientist. Which does have some truth to\\nit I guess, but that’s disingenuous (google the “No true Scotsman” fallacy). The fact is that there are lots of people who\\nare employed ostensibly as scientists, and whose work has all of the trappings of science, but who are outright fraudulent.\\nPretending that they don’t exist by saying that they’re not scientists is just childish.\\n- 30 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 40}, page_content='I can’t ﬁnd anything on this in the textbook, here’s my own attempt to list a few ways in which these\\nissues can arise are:\\n• Data fabrication. Sometimes, people just make up the data. This is occasionally done with\\n“good” intentions.\\nFor instance, the researcher believes that the fabricated data do reﬂect the\\ntruth, and may actually reﬂect “slightly cleaned up” versions of actual data. On other occasions,\\nthe fraud is deliberate and malicious. Some high-proﬁle examples where data fabrication has been\\nalleged or shown include Cyril Burt (a psychologist who is thought to have fabricated some of\\nhis data), Andrew Wakeﬁeld (who has been accused of fabricating his data connecting the MMR\\nvaccine to autism) and Hwang Woo-suk (who falsiﬁed a lot of his data on stem cell research).\\n• Hoaxes. Hoaxes share a lot of similarities with data fabrication, but they diﬀer in the intended\\npurpose. A hoax is often a joke, and many of them are intended to be (eventually) discovered.\\nOften, the point of a hoax is to discredit someone or some ﬁeld. There’s quite a few well known\\nscientiﬁc hoaxes that have occurred over the years (e.g., Piltdown man) some of were deliberate\\nattempts to discredit particular ﬁelds of research (e.g., the Sokal aﬀair).\\n• Data misrepresentation. While fraud gets most of the headlines, it’s much more common in\\nmy experience to see data being misrepresented. When I say this, I’m not referring to newspapers\\ngetting it wrong (which they do, almost always). I’m referring to the fact that often, the data don’t\\nactually say what the researchers think they say. My guess is that, almost always, this isn’t the\\nresult of deliberate dishonesty, it’s due to a lack of sophistication in the data analyses. For instance,\\nthink back to the example of Simpson’s paradox that I discussed in the beginning of these notes.\\nIt’s very common to see people present “aggregated” data of some kind; and sometimes, when you\\ndig deeper and ﬁnd the raw data yourself, you ﬁnd that the aggregated data tell a diﬀerent story to\\nthe disaggregated data. Alternatively, you might ﬁnd that some aspect of the data is being hidden,\\nbecause it tells an inconvenient story (e.g., the researcher might choose not to refer to a particular\\nvariable). There’s a lot of variants on this; many of which are very hard to detect.\\n• Study “misdesign”. Okay, this one is subtle. Basically, the issue here is that a researcher designs\\na study that has built-in ﬂaws, and those ﬂaws are never reported in the paper. The data that are\\nreported are completely real, and are correctly analysed, but they are produced by a study that is\\nactually quite wrongly put together. The researcher really wants to ﬁnd a particular eﬀect, and so\\nthe study is set up in such a way as to make it “easy” to (artifactually) observe that eﬀect. One\\nsneaky way to do this – in case you’re feeling like dabbling in a bit of fraud yourself – is to design\\nan experiment in which it’s obvious to the participants what they’re “supposed” to be doing, and\\nthen let reactivity work its magic for you. If you want, you can add all the trappings of double\\nblind experimentation etc. It won’t make a diﬀerence, since the study materials themselves are\\nsubtly telling people what you want them to do. When you write up the results, the fraud won’t be\\nobvious to the reader: what’s obvious to the participant when they’re in the experimental context\\nisn’t always obvious to the person reading the paper. Of course, the way I’ve described this makes\\nit sound like it’s always fraud: probably there are cases where this is done deliberately, but in\\nmy experience the bigger concern has been with unintentional misdesign. The researcher believes\\n. . . and so the study just happens to end up with a built in ﬂaw, and that ﬂaw then magically erases\\nitself when the study is written up for publication.\\n• Data mining & post hoc hypothesising. Another way in which the authors of a study can\\nmore or less lie about what they found is by engaging in what’s referred to as “data mining”. As\\nwe’ll discuss later in the class, if you keep trying to analyse your data in lots of diﬀerent ways,\\nyou’ll eventually ﬁnd something that “looks” like a real eﬀect but isn’t. This is referred to as “data\\nmining”. It used to be quite rare because data analysis used to take weeks, but now that everyone\\nhas very powerful statistical software on their computers, it’s becoming very common. Data mining\\nper se isn’t “wrong”, but the more that you do it, the bigger the risk you’re taking. The thing that\\n- 31 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 41}, page_content='is wrong, and I suspect is very common, is unacknowledged data mining. That is, the researcher\\nrun every possible analysis known to humanity, ﬁnds the one that works, and then pretends that\\nthis was the only analysis that they ever conducted. Worse yet, they often “invent” a hypothesis\\nafter looking at the data, to cover up the data mining. To be clear: it’s not wrong to change your\\nbeliefs after looking at the data, and to reanalyse your data using your new “post hoc” hypotheses.\\nWhat is wrong (and, I suspect, common) is failing to acknowledge that you’ve done so. If you\\nacknowledge that you did it, then other researchers are able to take your behaviour into account.\\nIf you don’t, then they can’t. And that makes your behaviour deceptive. Bad!\\n• Publication bias & self-censoring.\\nFinally, a pervasive bias is “non-reporting” of negative\\nresults. This is almost impossible to prevent. Journals don’t publish every article that is submitted\\nto them: they prefer to publish articles that ﬁnd “something”. So, if 20 people run an experiment\\nlooking at whether reading Finnegan’s Wake causes insanity in humans, and 19 of them ﬁnd that\\nit doesn’t, which one do you think is going to get published? Obviously, it’s the one study that\\ndid ﬁnd that Finnegan’s Wake causes insanity4. This is an example of a publication bias: since\\nno-one ever published the 19 studies that didn’t ﬁnd an eﬀect, a naive reader would never know\\nthat they existed. Worse yet, most researchers “internalise” this bias, and end up self-censoring\\ntheir research. Knowing that negative results aren’t going to be accepted for publication, they never\\neven try to report them. As a friend of mine says “for every experiment that you get published, you\\nalso have 10 failures”. And she’s right. The catch is, while some (maybe most) of those studies are\\nfailures for boring reasons (e.g. you stuﬀed something up) others might be genuine “null” results\\nthat you ought to acknowledge when you write up the “good” experiment. And telling which is\\nwhich is often hard to do.\\nThere’s probably a lot more issues like this to think about, but that’ll do to start with. What I really\\nwant to point out is the blindingly obvious truth that real world science is conducted by actual humans,\\nand only the most gullible of people automatically assumes that everyone else is honest and impartial.\\nActual scientists aren’t that naive, but for some idiotic reason the world likes to pretend that we’re all\\nmorons when it comes to understanding basic human politics.\\n2.8\\nSummary\\n• Introduction to psychological measurement (Section 2.1)\\n• Scales of measurement and types of variables (Section 2.2)\\n• Reliability of a measurement (Section 2.3)\\n• Terminology: predictors and outcomes (Section 2.4)\\n• Experimental and non-experimental research designs (Section 2.5)\\n• Validity and its threats (Section 2.6)\\n4Clearly, the real eﬀect is that only insane people would even try to read Finnegan’s Wake.\\n- 32 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 42}, page_content='Part II.\\nAn introduction to R\\n- 33 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 43}, page_content=''),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 44}, page_content='3. Getting started with R\\nRobots are nice to work with. They mind their own business, and they never have anything\\nto say.\\n–Roger Zelazny\\nIn this chapter I’ll discuss how to get started in R. I’ll brieﬂy talk about how to download and install\\nR, but most of the chapter will be focused on getting you started typing R commands. Our goal in this\\nchapter is not to learn any statistical concepts: we’re just trying to learn the basics of how R works and\\nget comfortable interacting with the system. To do this, we’ll spend a bit of time using R as a simple\\ncalculator, since that’s the easiest thing to do with R. In doing so, you’ll get a bit of a feel for what it’s\\nlike to work in R. From there I’ll introduce some very basic programming ideas: in particular, I’ll talk\\nabout the idea of deﬁning variables to store information, and a few things that you can do with these\\nvariables.\\nHowever, before going into any of the speciﬁcs, it’s worth talking a little about why you might want\\nto use R at all. Given that you’re reading this, you’ve probably got your own reasons. However, if those\\nreasons are “because that’s what my stats class uses”, it might be worth explaining a little why your\\nlecturer has chosen to use R for the class. Of course, I don’t really know why other people choose R, so\\nI’m really talking about why I use it.\\n• It’s sort of obvious, but worth saying anyway: doing your statistics on a computer is faster, easier\\nand more powerful than doing statistics by hand. Computers excel at mindless repetitive tasks, and\\na lot of statistical calculations are both mindless and repetitive. For most people, the only reason\\nto ever do statistical calculations with pencil and paper is for learning purposes. In my class I do\\noccasionally suggest doing some calculations that way, but the only real value to it is pedagogical.\\nIt does help you to get a “feel” for statistics to do some calculations yourself, so it’s worth doing it\\nonce. But only once!\\n• Doing statistics in a spreadsheet (e.g., Microsoft Excel) is generally a bad idea in the long run.\\nAlthough many people are likely feel more familiar with them, spreadsheets are very limited in\\nterms of what analyses they allow you do. If you get into the habit of trying to do your real life\\ndata analysis using spreadsheets, then you’ve dug yourself into a very deep hole.\\n• Avoiding proprietary software is a very good idea. There are a lot of commercial packages out there\\nthat you can buy, some of which I like and some of which I don’t. They’re usually very glossy in\\ntheir appearance, and generally very powerful (much more powerful than spreadsheets). However,\\nthey’re also very expensive: usually, the company sells “student versions” (crippled versions of the\\nreal thing) very cheaply; they sell full powered “educational versions” at a price that makes me\\nwince; and they sell commercial licences with a staggeringly high price tag. The business model here\\nis to suck you in during your student days, and then leave you dependent on their tools when you\\ngo out into the real world. It’s hard to blame them for trying, but personally I’m not in favour of\\n- 35 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 45}, page_content='shelling out thousands of dollars if I can avoid it. And you can avoid it: if you make use of packages\\nlike R that are open source and free, you never get trapped having to pay exorbitant licensing fees.\\n• Something that you might not appreciate now, but will love later on if you do anything involving\\ndata analysis, is the fact that R is highly extensible. When you download and install R, you get\\nall the basic “packages”, and those are very powerful on their own. However, because R is so open\\nand so widely used, it’s become something of a standard tool in statistics, and so lots of people\\nwrite their own packages that extend the system. And these are freely available too. One of the\\nconsequences of this, I’ve noticed, is that if you open up an advanced textbook (a recent one, that\\nis) rather than introductory textbooks, is that a lot of them use R. In other words, if you learn how\\nto do your basic statistics in R, then you’re a lot closer to being able to use the state of the art\\nmethods than you would be if you’d started out with a “simpler” system: so if you want to become\\na genuine expert in psychological data analysis, learning R is a very good use of your time.\\n• Related to the previous point: R is a real programming language. As you get better at using R for\\ndata analysis, you’re also learning to program. To some people this might seem like a bad thing,\\nbut in truth, programming is a core research skill across a lot of the social and behavioural sciences.\\nThink about how many surveys and experiments are done online, or presented on computers. Think\\nabout all those online social environments (e.g., Facebook, World of Warcraft) which you might\\nbe interested in studying; and maybe collecting data from in an automated fashion. Think about\\nartiﬁcial intelligence systems, computer vision and speech recognition. If any of these are things\\nthat you think you might want to be involved in – as someone “doing research in psychology”, that\\nis – you’ll need to know a bit of programming. And if you don’t already know how to program,\\nthen learning how to do statistics using R is a nice way to start.\\nThose are the main reasons I use R. It’s not without its ﬂaws: it’s not easy to learn, and it has a few\\nvery annoying quirks to it that we’re all pretty much stuck with, but on the whole I think the strengths\\noutweigh the weakness; more so than any other option I’ve encountered so far.\\n3.1\\nInstalling R\\nOkay, enough with the sales pitch. Let’s get started. Just as with any piece of software, R needs to\\nbe installed on a “computer”, which is a magical box that does cool things and delivers free ponies. Or\\nsomething along those lines: I may be confusing computers with the iPad marketing campaigns. Anyway,\\nR is freely distributed online, and you can download it from the R homepage, which is:\\nhttp://cran.r-project.org/\\nAt the top of the page – under the heading “Download and Install R” – you’ll see separate links for\\nWindows users, Mac users, and Linux users. If you follow the relevant link, you’ll see that the online\\ninstructions are pretty self-explanatory, but I’ll walk you through the installation anyway. As of this\\nwriting, the current version of R is 2.14.0, but they usually issue (free) updates every six months, so\\nyou’ll probably have a newer version.1\\n1Although R is updated frequently, it doesn’t usually make much of a diﬀerence for the sort of work we’ll do in this\\nbook. In fact, during the writing of the book I upgraded twice, and didn’t have to change anything except these sections\\ndescribing the downloading.\\n- 36 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 46}, page_content='Figure 3.1: A screenshot of R running on a Windows machine. Apparently the version of R I was using\\nwhen I took this screenshot was 2.12.0, which tells me that I’ve been working on this book for over a\\nyear.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3.1.1\\nInstalling R on a Windows computer\\nIf you’re trying to install R on a computer running Windows and follow the links from the CRAN\\nhomepage, they should take you to a page that has a link that reads “Download R 2.14.0 for Windows”\\nfeaturing prominently on the page. When you click on the link, your browser should start downloading\\na ﬁle called R-2.14.0-win.exe, or whatever the equivalent version number is by the time you read this.\\nThe ﬁle for version 2.14.0 is about 45MB in size, so it may take some time depending on how fast your\\ninternet connection is. Once you’ve downloaded the ﬁle, double click to install it. As with any software\\nyou download online, Windows will ask you some questions about whether you trust the ﬁle and so on.\\nAfter you click through those, it’ll ask you where you want to install it, and what components you want\\nto install. The default values should be ﬁne for most people, so again, just click through. Once all that\\nis done, you should have R installed on your system. You can access it from the Start menu, or from the\\ndesktop if you asked it to add a shortcut there. When you open up R, you’ll see something that looks\\nvery similar to the screenshot in Figure 3.1.\\n3.1.2\\nInstalling R on a Mac\\nWhen you click on the Mac OS X link, you should ﬁnd yourself on a page with the title “R for Mac\\nOS X”. The vast majority of Mac users will have a fairly recent version of the operating system: as long\\nas you’re running Mac OS X 10.5 (Leopard) or higher, then you’ll be ﬁne.2 There’s a fairly prominent\\nlink on the page called “R-2.14.0.pkg”, which is the one you want if you’re running Leopard (10.5), Snow\\nLeopard (10.6) or Lion (10.7). Click on that link and you’ll start downloading the installer ﬁle, which is\\n2If you’re running an older version of the Mac OS, then you need to follow the link to the “old” page (http://cran.r\\n-project.org/bin/macosx/old/). You should be able to ﬁnd the installer ﬁle that you need at the bottom of the page.\\n- 37 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 47}, page_content='Figure 3.2: A screenshot of R running on a Mac.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n(not surprisingly) called R-2.14.0.pkg. It’s about 61MB in size, so the download can take a while on\\nslower internet connections.\\nOnce you’ve downloaded R-2.14.0.pkg, all you need to do is open it by double clicking on the package\\nﬁle. The installation should go smoothly from there: just follow all the instructions just like you usually\\ndo when you install something. Once it’s ﬁnished, you’ll ﬁnd a ﬁle called R.app in the Applications\\nfolder.3 This means you now have R installed on your computer. If you double click to open R, you’ll see\\nsomething like the screenshot in Figure 3.2.4\\n3.1.3\\nInstalling R on a Linux computer\\nIf you’re successfully managing to run a Linux box, regardless of what distribution, then you should\\nﬁnd the instructions on the website easy enough. You can compile R from source yourself if you want, or\\ninstall it through your package management system, which will probably have R in it. Alternatively, the\\n3Actually, you’ll ﬁnd two ﬁles, one called R.app and R64.app. The latter is the one you want for 64 bit machines. If you\\ndon’t know what that means, don’t worry: just ignore the pesky R64.app application.\\n4Tip for advanced Mac users. You can run R from the terminal if you want to. The command is just “R”. It behaves\\nlike the normal desktop version, except that help documentation behaves like a “man” page instead of opening in a new\\nwindow.\\n- 38 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 48}, page_content='CRAN site has precompiled binaries for Debian, Red Hat, Suse and Ubuntu and has separate instructions\\nfor each. Once you’ve got R installed, you can run it from the command line just by typing R. Don’t\\nforget that the commands are case sensitive, so typing r won’t work. However, if you’re feeling envious\\nof Windows and Mac users for their fancy GUIs, you can download Rstudio.\\n3.1.4\\nDownloading and installing Rstudio\\nAt this point you’re pretty much ready to go.\\nHowever, there’s one more step that I generally\\nrecommend that people go through: installing Rstudio. To understand what Rstudio is, you ﬁrst need\\nto understand a little bit more about R itself. The key thing to understand is the term R really refers\\nto the underlying statistical language, and not to the “graphical user interface” (GUI) that you use to\\ninteract with R. To see this, notice that the appearance of R is very diﬀerent on a Windows machine (see\\nFigure 3.1) than on a Mac (see Figure 3.2). Some Linux users won’t even have a GUI at all.\\nAll of these are perfectly reasonable ways of using R, but they’re not my favourite method, and at a\\npractical level, it can be convenient if everyone in the class is using the same interface. To that end, my\\nsuggestion is that you also download and install Rstudio. Rstudio provides a clean, professional interface\\ninto R that I ﬁnd much nicer to work with than either the Windows or Mac defaults. Like R itself, Rstudio\\nis free software: you can ﬁnd all the details on their webpage. In the meantime, you can download it\\nhere:\\nhttp://www.rstudio.org/\\nWhen you visit the Rstudio website, you’ll probably be struck by how much cleaner and simpler it is\\nthan the CRAN website. This is probably no coincidence: the people who design and distribute the core\\nR language itself are focused on technical stuﬀ. And sometimes they almost seem to forget that there’s\\nan actual human user at the end. The people who design and distribute Rstudio are focused on user\\ninterface. They want to make R as usable as possible. The two websites reﬂect that diﬀerence.\\nIn any case, when you click on the download button on the homepage it will ask you to choose whether\\nyou want the desktop version or the server version. At this stage you almost certainly want the desk-\\ntop version. After choosing the desktop version it will take you to a page (http://www.rstudio.org/\\ndownload/desktop) that shows several possible downloads: there’s a diﬀerent one for each operating\\nsystem. However, the nice people at Rstudio have designed the webpage so that it automatically recom-\\nmends the download that is most appropriate for your computer. Click on the appropriate link, and the\\nRstudio installer ﬁle will start downloading.\\nOnce it’s ﬁnished downloading, open the installer ﬁle in the usual way to install Rstudio. After it’s\\nﬁnished installing, you can start R by opening Rstudio. To illustrate what Rstudio looks like, Figure 3.3\\nshows a screenshot of an R session in progress. In this screenshot, you can see that it’s running on a\\ncomputer with the Windows 7 operating system, but it looks almost identical no matter what operating\\nsystem you have. The Mac version looks more like a Mac application (e.g., the menus are up the top\\nand the colour scheme is grey rather than blue), but it’s more or less identical. There are a few minor\\ndiﬀerences in where things are located in the menus (I’ll point them out as we go along) and in the\\nshortcut keys, because Rstudio is trying to “feel” like a proper Mac application or a proper Windows\\napplication, and this means that it has to change its behaviour a little bit depending on what computer\\nit’s running on. Even so, these diﬀerences are very small: I started out using the Mac version of Rstudio\\nand then started using the Windows version as well in order to write these notes.5\\nThe only “shortcoming” I’ve found with Rstudio is that – as of this writing – it’s still a work in\\nprogress. The current version as I type this is 0.94, which means that it’s oﬃcially still in beta testing.\\n5I don’t currently have any Linux versions running, so if you’re on a Linux machine you’ll have to work out any diﬀerences\\nyourself. By the time the next edition rolls around, I’ll have at least one Linux distro running as well, but that’s no help\\nto you right now. I’m mentioning this more as a promissory note and an apology\\n- 39 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 49}, page_content='Figure 3.3: An R session in progress running through Rstudio. The operating system here is Windows 7.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nEven so, the beta version of Rstudio still provides a better user experience than anything else I’ve tried:\\nit really is the best option available in my opinion. The “problem” is that they keep improving it. New\\nfeatures keep turning up the more recent releases, so there’s a good chance that by the time you read\\nthis book there will be a version out that has some really neat things that weren’t in the 0.94 version.\\n3.1.5\\nStarting up R\\nOne way or another, regardless of what operating system you’re using and regardless of whether you’re\\nusing Rstudio, or the default GUI, or even the command line, it’s time to open R and get started. When\\nyou do that, the ﬁrst thing you’ll see (assuming that you’re looking at the R console, that is) is a whole\\nlot of text that doesn’t make much sense. It should look something like this:\\nR version 2.13.1 (2011-07-08)\\nCopyright (C) 2011 The R Foundation for Statistical Computing\\nISBN 3-900051-07-0\\nPlatform: i386-pc-mingw32/i386 (32-bit)\\nR is free software and comes with ABSOLUTELY NO WARRANTY.\\nYou are welcome to redistribute it under certain conditions.\\nType ’license()’ or ’licence()’ for distribution details.\\nNatural language support but running in an English locale\\nR is a collaborative project with many contributors.\\nType ’contributors()’ for more information and\\n’citation()’ on how to cite R or R packages in publications.\\n- 40 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 50}, page_content='Type ’demo()’ for some demos, ’help()’ for on-line help, or\\n’help.start()’ for an HTML browser interface to help.\\nType ’q()’ to quit R.\\n>\\nMost of this text is pretty uninteresting, and when doing real data analysis you’ll never really pay much\\nattention to it. The important part of it is this...\\n>\\n... which has a ﬂashing cursor next to it. That’s the command prompt. When you see this, it means\\nthat R is waiting patiently for you to do something!\\n3.2\\nTyping commands at the R console\\nOne of the easiest things you can do with R is use it as a simple calculator, so it’s a good place to start.\\nFor instance, try typing 10 + 20, and hitting enter.6 When you do this, you’ve entered a command, and\\nR will “execute” that command. What you see on screen now will be this:\\n> 10 + 20\\n[1] 30\\nNot a lot of surprises in this extract. But there’s a few things worth talking about, even with such a\\nsimple example. Firstly, it’s important that you understand how to read the extract. In this example,\\nwhat I typed was the 10 + 20 part. I didn’t type the > symbol: that’s just the R command prompt and\\nisn’t part of the actual command. And neither did I type the [1] 30 part. That’s what R printed out in\\nresponse to my command.\\nSecondly, it’s important to understand how the output is formatted. Obviously, the correct answer to\\nthe sum 10 + 20 is 30, and not surprisingly R has printed that out as part of its response. But it’s also\\nprinted out this [1] part, which probably doesn’t make a lot of sense to you right now. You’re going to\\nsee that a lot. I’ll talk about what this means in a bit more detail later on, but for now you can think of\\n[1] 30 as if R were saying “the answer to the 1st question you asked is 30”. That’s not quite the truth,\\nbut it’s close enough for now. And in any case it’s not really very interesting at the moment: we only\\nasked R to calculate one thing, so obviously there’s only one answer printed on the screen. Later on this\\nwill change, and the [1] part will start to make a bit more sense. For now, I just don’t want you to get\\nconfused or concerned by it.\\n3.2.1\\nBe very careful to avoid typos\\nBefore we go on to talk about other types of calculations that we can do with R, there’s a few other\\nthings I want to point out. The ﬁrst thing is that, while R is good software, it’s still software. It’s pretty\\nstupid, and because it’s stupid it can’t handle typos. It takes it on faith that you meant to type exactly\\n6Seriously. If you’re in a position to do so, open up R and start typing. The simple act of typing it rather than “just\\nreading” makes a big diﬀerence. It makes the concepts more concrete, and it ties the abstract ideas (programming and\\nstatistics) to the actual context in which you need to use them. Statistics is something you do, not just something you read\\nabout in a textbook.\\n- 41 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 51}, page_content='what you did type. For example, suppose that you forgot to hit the shift key when trying to type +, and\\nas a result your command ended up being 10 = 20 rather than 10 + 20. Here’s what happens:\\n> 10 = 20\\nError in 10 = 20 : invalid (do_set) left-hand side to assignment\\nWhat’s happened here is that R has attempted to interpret 10 = 20 as a command, and spits out an error\\nmessage because the command doesn’t make any sense to it. When a human looks at this, and then\\nlooks down at his or her keyboard and sees that + and = are on the same key, it’s pretty obvious that the\\ncommand was a typo. But R doesn’t know this, so it gets upset. And, if you look at it from its perspective,\\nthis makes sense. All that R “knows” is that 10 is a legitimate number, 20 is a legitimate number, and\\n= is a legitimate part of the language too. In other words, from its perspective this really does look like\\nthe user meant to type 10 = 20, since all the individual parts of that statement are legitimate and it’s\\ntoo stupid to realise that this is probably a typo. Therefore, R takes it on faith that this is exactly what\\nyou meant... it only “discovers” that the command is nonsense when it tries to follow your instructions,\\ntypo and all. And then it whinges, and spits out an error.\\nEven more subtle is the fact that some typos won’t produce errors at all, because they happen to\\ncorrespond to “well-formed” R commands. For instance, suppose that not only did I forget to hit the\\nshift key when trying to type 10 + 20, I also managed to press the key next to one I meant do. The\\nresulting typo would produce the command 10 - 20. Clearly, R has no way of knowing that you meant\\nto add 20 to 10, not subtract 20 from 10, so what happens this time is this:\\n> 10 - 20\\n[1] -10\\nIn this case, R produces the right answer, but to the the wrong question.\\nTo some extent, I’m stating the obvious here, but it’s important. The people who wrote R are smart.\\nYou, the user, are smart. But R itself is dumb. And because it’s dumb, it has to be mindlessly obedient.\\nIt does exactly what you ask it to do. There is no equivalent to “autocorrect” in R, and for good reason.\\nWhen doing advanced stuﬀ– and even the simplest of statistics is pretty advanced in a lot of ways – it’s\\ndangerous to let a mindless automaton like R try to overrule the human user. But because of this, it’s\\nyour responsibility to be careful. Always make sure you type exactly what you mean. When dealing with\\ncomputers, it’s not enough to type “approximately” the right thing. In general, you absolutely must be\\nprecise in what you say to R ... like all machines it is too stupid to be anything other than absurdly\\nliteral in its interpretation.\\n3.2.2\\nR is (a bit) ﬂexible with spacing\\nOf course, now that I’ve been so uptight about the importance of always being precise, I should point\\nout that there are some exceptions. Or, more accurately, there are some situations in which R does show\\na bit more ﬂexibility than my previous description suggests. The ﬁrst thing R is smart enough to do is\\nignore redundant spacing. What I mean by this is that, when I typed 10 + 20 before, I could equally\\nhave done this\\n> 10\\n+ 20\\n[1] 30\\nor this\\n> 10+20\\n[1] 30\\n- 42 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 52}, page_content='and I would get exactly the same answer. However, that doesn’t mean that you can insert spaces in any\\nold place. When we looked at the startup documentation in Section 3.1.5 it suggested that you could\\ntype citation() to get some information about how to cite R. If I do so...\\n> citation()\\nTo cite R in publications use:\\nR Development Core Team (2011). R: A language and environment for statistical\\ncomputing. R Foundation for Statistical Computing, Vienna, Austria. ISBN\\n3-900051-07-0, URL http://www.R-project.org/.\\nBLAH BLAH BLAH\\nWe have invested a lot of time and effort in creating R, please cite it when using\\nit for data analysis. See also citation(\"pkgname\") for citing R packages.\\n... it tells me to cite the R manual (R Development Core Team, 2011). Obviously, the BLAH BLAH BLAH\\npart isn’t actually part of what R prints out: when you see that it means that I’ve choppoed out some\\nparts of the output that I don’t think are very interesting or relevant. I’ll do that a lot. Anyway, getting\\nback to my original point, let’s see what happens when I try changing the spacing. If I insert spaces in\\nbetween the word and the parentheses, or inside the parentheses themselves, then all is well. That is,\\neither of these two commands\\n> citation ()\\n> citation(\\n)\\nwill produce exactly the same response. However, what I can’t do is insert spaces in the middle of the\\nword. If I try to do this, R gets upset:\\n> citat ion()\\nError: unexpected symbol in \"citat ion\"\\nThroughout this book I’ll vary the way I use spacing a little bit, just to give you a feel for the diﬀerent\\nways in which spacing can be used. I’ll try not to do it too much though, since it’s generally considered\\nto be good practice to be consistent in how you format your commands.\\n3.2.3\\nR can sometimes tell that you’re not ﬁnished yet (but not often)\\nOne more thing I should point out. If you hit enter in a situation where it’s “obvious” to R that you\\nhaven’t actually ﬁnished typing the command, R is just smart enough to keep waiting. For example, if\\nyou type 10 + and then press enter, even R is smart enough to realise that you probably wanted to type\\nin another number. So here’s what happens:\\n> 10+\\n+\\nand there’s a blinking cursor next to the plus sign. What this means is that R is still waiting for you to\\nﬁnish. It “thinks” you’re still typing your command, so it hasn’t tried to execute it yet. In other words,\\nthis plus sign is actually another command prompt. It’s diﬀerent from the usual one (i.e., the > symbol)\\nto remind you that R is going to “add” whatever you type now to what you typed last time. For example,\\nif I then go on to type 3 and hit enter, what I get is this:\\n- 43 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 53}, page_content='> 10+\\n+ 20\\n[1] 30\\nAnd as far as R is concerned, this is exactly the same as if you had typed 10 + 20. Similarly, consider the\\ncitation() command that we talked about in the previous section. Suppose you hit enter after typing\\ncitation(. Once again, R is smart enough to realise that there must be more coming – since you need to\\nadd the ) character – so it waits. I can even hit enter several times and it will keep waiting:\\n> citation(\\n+\\n+\\n+ )\\nI’ll make use of this a lot in this book. A lot of the commands that we’ll have to type are pretty long,\\nand they’re visually a bit easier to read if I break it up over several lines.\\nThat being said, it’s not often the case that R is smart enough to tell that there’s more coming. For\\ninstance, in the same way that I can’t add a space in the middle of a word, I can’t hit enter in the middle\\nof a word either. If I hit enter after typing citat I get an error, because R thinks I’m interested in an\\n“object” called citat and can’t ﬁnd it:\\n> citat\\nError: object ’citat’ not found\\nWhat about if I typed citation and hit enter? In this case we get something very odd, something that\\nwe deﬁnitely don’t want, at least at this stage. Here’s what happens:\\n> citation\\nfunction (package = \"base\", lib.loc = NULL, auto = NULL)\\n{\\ndir <- system.file(package = package, lib.loc = lib.loc)\\nif (dir == \"\")\\nstop(gettextf(\"package ’%s’ not found\", package), domain = NA)\\nBLAH BLAH BLAH\\nwhere the BLAH BLAH BLAH goes on for rather a long time, and you don’t know enough R yet to understand\\nwhat all this gibberish actually means. This incomprehensible output can be quite intimidating to novice\\nusers, and unfortunately it’s very easy to forget to type the parentheses; so almost certainly you’ll do\\nthis by accident. Do not panic when this happens. Simply ignore the gibberish. As you become more\\nexperienced this gibberish will start to make sense, and you’ll ﬁnd it quite handy to print this stuﬀout.7\\nBut for now just try to remember to add the parentheses when typing your commands.\\n3.3\\nDoing simple calculations with R\\nOkay, now that we’ve discussed some of the tedious details associated with typing R commands, let’s\\nget back to learning how to use the most powerful piece of statistical software in the world as a $2\\ncalculator. So far, all we know how to do is addition. Clearly, a calculator that only did addition would\\n7For advanced users: yes, as you’ve probably guessed, R is printing out the source code for the function.\\n- 44 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 54}, page_content='Table 3.1: Basic arithmetic operations in R. These ﬁve operators are used very frequently throughout the\\ntext, so it’s important to be familiar with them at the outset. There are others as well, which I’ll discuss\\nin Chapter 7.\\noperation\\noperator\\nexample input\\nexample output\\naddition\\n+\\n10 + 2\\n12\\nsubtraction\\n-\\n9 - 3\\n6\\nmultiplication\\n*\\n5 * 5\\n25\\ndivision\\n/\\n10 / 3\\n3\\npower\\n^\\n5 ^ 2\\n25\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nbe a bit stupid, so I should tell you about how to perform other simple calculations using R. But ﬁrst,\\nsome more terminology. Addition is an example of an “operation” that you can perform (speciﬁcally,\\nan arithmetic operation), and the operator that performs it is +. To people with a programming or\\nmathematics background, this terminology probably feels pretty natural, but to other people it might feel\\nlike I’m trying to make something very simple (addition) sound more complicated than it is (by calling\\nit an arithmetic operation). To some extent, that’s true: if addition was the only operation that we were\\ninterested in, it’d be a bit silly to introduce all this extra terminology. However, as we go along, we’ll\\nstart using more and more diﬀerent kinds of operations, so it’s probably a good idea to get the language\\nstraight now, while we’re still talking about very familiar concepts like addition!\\n3.3.1\\nAdding, subtracting, multiplying and dividing\\nSo, now that we have the terminology, let’s learn how to perform some arithmetic operations in R. To that\\nend, Table 3.1 lists the operators that correspond to the basic arithmetic we learned in primary school:\\naddition, subtraction, multiplication and division. As you can see, R uses fairly standard symbols to\\ndenote each of the diﬀerent operations you might want to perform: addition is done using the + operator,\\nsubtraction is performed by the - operator, and so on. So if I wanted to ﬁnd out what 57 times 61 is\\n(and who wouldn’t?), I can use R instead of a calculator, like so:\\n> 57 * 61\\n[1] 3477\\nSo that’s handy.\\n3.3.2\\nTaking powers\\nThe ﬁrst four operations listed in Table 3.1 are things we all learned in primary school, but they aren’t\\nthe only arithmetic operations built into R. There are three other arithmetic operations that I should\\nprobably mention: taking powers, doing integer division, and calculating a modulus. Of the three, the\\nonly one that is of any real importance for the purposes of this book is taking powers, so I’ll discuss that\\none here: the other two are discussed in Chapter 7.\\nFor those of you who can still remember your high school maths, this should be familiar. But for some\\npeople high school maths was a long time ago, and others of us didn’t listen very hard in high school.\\nIt’s not complicated. As I’m sure everyone will probably remember the moment they read this, the act\\nof multiplying a number x by itself n times is called “raising x to the n-th power”. Mathematically, this\\n- 45 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 55}, page_content='is written as xn. Some values of n have special names: in particular x2 is called x-squared, and x3 is\\ncalled x-cubed. So, the 4th power of 5 is calculated like this:\\n54 “ 5 ˆ 5 ˆ 5 ˆ 5\\nOne way that we could calculate 54 in R would be to type in the complete multiplication as it is shown\\nin the equation above. That is, we could do this\\n> 5 * 5 * 5 * 5\\n[1] 625\\nbut it does seem a bit tedious. It would be very annoying indeed if you wanted to calculate 515, since the\\ncommand would end up being quite long. Therefore, to make our lives easier, we use the power operator\\ninstead. When we do that, our command to calculate 54 goes like this:\\n> 5 ^ 4\\n[1] 625\\nMuch easier.\\n3.3.3\\nDoing calculations in the right order\\nOkay. At this point, you know how to take one of the most powerful pieces of statistical software in\\nthe world, and use it as a $2 calculator. And as a bonus, you’ve learned a few very basic programming\\nconcepts. That’s not nothing (you could argue that you’ve just saved yourself $2) but on the other hand,\\nit’s not very much either. In order to use R more eﬀectively, we need to introduce more programming\\nconcepts.\\nIn most situations where you would want to use a calculator, you might want to do multiple calcula-\\ntions. R lets you do this, just by typing in longer commands.\\nIn fact, we’ve already seen an example of\\nthis earlier, when I typed in 5 * 5 * 5 * 5. However, let’s try a slightly diﬀerent example:\\n> 1 + 2 * 4\\n[1] 9\\nClearly, this isn’t a problem for R either. However, it’s worth stopping for a second, and thinking about\\nwhat R just did. Clearly, since it gave us an answer of 9 it must have multiplied 2 * 4 (to get an interim\\nanswer of 8) and then added 1 to that. But, suppose it had decided to just go from left to right: if R\\nhad decided instead to add 1+2 (to get an interim answer of 3) and then multiplied by 4, it would have\\ncome up with an answer of 12.\\nTo answer this, you need to know the order of operations that R uses. If you remember back to\\nyour high school maths classes, it’s actually the same order that you got taught when you were at school:\\nthe “bedmas” order.8 That is, ﬁrst calculate things inside Brackets (), then calculate Exponents ^, then\\nDivision / and Multiplication *, then Addition + and Subtraction -. So, to continue the example above,\\nif we want to force R to calculate the 1+2 part before the multiplication, all we would have to do is enclose\\nit in brackets:\\n> (1 + 2) * 4\\n[1] 12\\n8For advanced users: if you want a table showing the complete order of operator precedence in R, type ?Syntax. I haven’t\\nincluded it in this book since there are quite a few diﬀerent operators, and we don’t need that much detail.\\nBesides,\\nin practice most people seem to ﬁgure it out from seeing examples: until writing this book I never looked at the formal\\nstatement of operator precedence for any language I ever coded in, and never ran into any diﬃculties.\\n- 46 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 56}, page_content='This is a fairly useful thing to be able to do. The only other thing I should point out about order of\\noperations is what to expect when you have two operations that have the same priority: that is, how\\ndoes R resolve ties? For instance, multiplication and division are actually the same priority, but what\\nshould we expect when we give R a problem like 4 / 2 * 3 to solve? If it evaluates the multiplication\\nﬁrst and then the division, it would calculate a value of two-thirds. But if it evaluates the division ﬁrst\\nit calculates a value of 6. The answer, in this case, is that R goes from left to right, so in this case the\\ndivision step would come ﬁrst:\\n> 4 / 2 * 3\\n[1] 6\\nAll of the above being said, it’s helpful to remember that brackets always come ﬁrst. So, if you’re\\never unsure about what order R will do things in, an easy solution is to enclose the thing you want it to\\ndo ﬁrst in brackets. There’s nothing stopping you from typing (4 / 2) * 3. By enclosing the division in\\nbrackets we make it clear which thing is supposed to happen ﬁrst. In this instance you wouldn’t have\\nneeded to, since R would have done the division ﬁrst anyway, but when you’re ﬁrst starting out it’s better\\nto make sure R does what you want!\\n3.4\\nStoring a number as a variable\\nOne of the most important things to be able to do in R (or any programming language, for that matter) is\\nto store information in variables. Variables in R aren’t exactly the same thing as the variables we talked\\nabout in the last chapter on research methods, but they are similar. At a conceptual level you can think\\nof a variable as label for a certain piece of information, or even several diﬀerent pieces of information.\\nWhen doing statistical analysis in R all of your data (the variables you measured in your study) will be\\nstored as variables in R, but as well see later in the book you’ll ﬁnd that you end up creating variables for\\nother things too. However, before we delve into all the messy details of data sets and statistical analysis,\\nlet’s look at the very basics for how we create variables and work with them.\\n3.4.1\\nVariable assignment using ❁✲and ✲❃\\nSince we’ve been working with numbers so far, let’s start by creating variables to store our numbers.\\nAnd since most people like concrete examples, let’s invent one. Suppose I’m trying to calculate how\\nmuch money I’m going to make from this book. There’s several diﬀerent numbers I might want to store.\\nFirstly, I need to ﬁgure out how many copies I’ll sell. This isn’t exactly Harry Potter, so let’s assume\\nI’m only going to sell one copy per student in my class. That’s 350 sales, so let’s create a variable called\\nsales. What I want to do is assign a value to my variable sales, and that value should be 350. We do\\nthis by using the assignment operator, which is <-. Here’s how we do it:\\n> sales <- 350\\nWhen you hit enter, R doesn’t print out any output.9\\nIt just gives you another command prompt.\\nHowever, behind the scenes R has created a variable called sales and given it a value of 350. You can\\n9If you are using Rstudio, and the “workspace” panel is visible when you typed the command, then you probably saw\\nsomething happening there. That’s to be expected, and is quite helpful. However, there’s two things to note here (1) I\\nhaven’t yet explained what that panel does, so for now just ignore it, and (2) this is one of the helpful things Rstudio does,\\nnot a part of R itself.\\n- 47 -'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.07', 'creator': 'David M. Jones', 'creationdate': '2013-08-18T15:38:14+09:30', 'source': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'file_path': 'data/pdf_files/learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf', 'total_pages': 542, 'format': 'PDF 1.4', 'title': 'CMR17', 'author': 'Daniel Navarro', 'subject': '', 'keywords': '', 'moddate': '2013-08-18T15:38:14+09:30', 'trapped': '', 'modDate': \"D:20130818153814+09'30'\", 'creationDate': \"D:20130818153814+09'30'\", 'page': 57}, page_content='check that this has happened by asking R to print the variable on screen. And the simplest way to do\\nthat is to type the name of the variable and hit enter10\\n> sales\\n[1] 350\\nSo that’s nice to know. Anytime you can’t remember what R has got stored in a particular variable, you\\ncan just type the name of the variable and hit enter.\\nOkay, so now we know how to assign variables. Actually, there’s a bit more you should know. Firstly,\\none of the curious features of R is that there are several diﬀerent ways of making assignments.\\nIn\\naddition to the <- operator, we can also use -> and =, and it’s pretty important to understand the\\ndiﬀerences between them.11 Let’s start by considering ->, since that’s the easy one (we’ll discuss the use\\nof = in Section 3.5.1). As you might expect from just looking at the symbol, it’s almost identical to <-.\\nIt’s just that the arrow (i.e., the assignment) goes from left to right. So if I wanted to deﬁne my sales\\nvariable using ->, I would write it like this:\\n> 350 -> sales\\nThis has the same eﬀect: and it still means that I’m only going to sell 350 copies. Sigh. Apart from this\\nsuperﬁcial diﬀerence, <- and -> are identical. In fact, as far as R is concerned, they’re actually the same\\noperator, just in a “left form” and a “right form”.12\\n3.4.2\\nDoing calculations using variables\\nOkay, let’s get back to my original story. In my quest to become rich, I’ve written this textbook.\\nTo ﬁgure out how good a strategy is, I’ve started creating some variables in R. In addition to deﬁning\\na sales variable that counts the number of copies I’m going to sell, I can also create a variable called\\nroyalty, indicating how much money I get per copy. Let’s say that my royalties are about $7 per book:\\n> sales <- 350\\n> royalty <- 7\\nThe nice thing about variables (in fact, the whole point of having variables) is that we can do anything\\nwith a variable that we ought to be able to do with the information that it stores. That is, since R allows\\nme to multiply 350 by 7\\n> 350 * 7\\n[1] 2450\\nit also allows me to multiply sales by royalty\\n> sales * royalty\\n[1] 2450\\nAs far as R is concerned, the sales * royalty command is the same as the 350 * 7 command.\\nNot\\nsurprisingly, I can assign the output of this calculation to a new variable, which I’ll call revenue. And\\nwhen we do this, the new variable revenue gets the value 2450. So let’s do that, and then get R to print\\nout the value of revenue so that we can verify that it’s done what we asked:\\n10As we’ll discuss later, by doing this we are implicitly using the print() function.\\n11Actually, in keeping with the R tradition of providing you with a billion diﬀerent screwdrivers (even when you’re actually\\nlooking for a hammer) these aren’t the only options. There’s also the assign() function, and the <<- and ->> operators.\\nHowever, we won’t be using these at all in this book.\\n12A quick reminder: when using operators like <- and -> that span multiple characters, you can’t insert spaces in the\\nmiddle. That is, if you type - > or < -, R will interpret your command the wrong way. And I will cry.\\n- 48 -'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline : Data Ingestion to vector DB pipeline"
      ],
      "metadata": {
        "id": "NL7DE2lt4V7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Ingestion"
      ],
      "metadata": {
        "id": "KrU6yyIn_cXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import  PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "uIbTTWt67vg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read all the pdfs inside directory\n",
        "def process_all_pdfs(pdf_directory):\n",
        "  \"\"\"Process all pdfs in a directory\"\"\"\n",
        "\n",
        "  all_documents = []\n",
        "  pdf_dir = Path(pdf_directory)\n",
        "\n",
        "  pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
        "  print(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "  for pdf_file in pdf_files:\n",
        "    print(f'\\nProcessing : {pdf_file.name}')\n",
        "    try:\n",
        "      loader = PyMuPDFLoader(str(pdf_file))\n",
        "      documents = loader.load()\n",
        "\n",
        "      for doc in documents:\n",
        "        doc.metadata['source_file'] = pdf_file.name\n",
        "        doc.metadata['file_type'] = 'pdf'\n",
        "\n",
        "      all_documents.extend(documents)\n",
        "      print(f\"Loaded {len(documents)} pages\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error : {e}\")\n",
        "\n",
        "  print(f\"Total documents loaded: {len(all_documents)}\")\n",
        "  return all_documents\n"
      ],
      "metadata": {
        "id": "ZntWQIus-TRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pdf_documents = process_all_pdfs('data/pdf_files')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ0FrQil_fRh",
        "outputId": "5690507c-ed66-4a57-f6f9-161c3fc83cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 PDF files to process\n",
            "\n",
            "Processing : making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf\n",
            "Loaded 250 pages\n",
            "\n",
            "Processing : introduction_to_probability_and_statistics_for_engineers_and_scientists-ross-elsevier-2020.pdf\n",
            "Loaded 692 pages\n",
            "\n",
            "Processing : learning_statistics_with_r_a_tutorial_for_psychology_students_and_other_beginners-navarro--2013.pdf\n",
            "Loaded 542 pages\n",
            "\n",
            "Processing : ISLRv2_corrected_June_2023.pdf\n",
            "Loaded 615 pages\n",
            "Total documents loaded: 2099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking"
      ],
      "metadata": {
        "id": "syR_MhiZ_lBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitting get into chuncks\n",
        "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = chunk_size,\n",
        "      chunk_overlap = chunk_overlap,\n",
        "      length_function = len,\n",
        "      separators = [\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
        "  )\n",
        "\n",
        "  split_docs = text_splitter.split_documents(documents)\n",
        "  print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
        "\n",
        "  if split_docs:\n",
        "    print(f\"\\nExample chunk\")\n",
        "    print(f\"Content: {split_docs[0].page_content[:200]}\")\n",
        "    print(f\"Metadata: {split_docs[0].metadata}\")\n",
        "\n",
        "  return split_docs\n"
      ],
      "metadata": {
        "id": "NLbWRF6oJRUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = split_documents(all_pdf_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmbXeUE26T0m",
        "outputId": "199aa477-1a0e-4413-de66-9c0cfc972d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 2099 documents into 6034 chunks\n",
            "\n",
            "Example chunk\n",
            "Content: MAKING SENSE OF\n",
            "DATA I\n",
            "Metadata: {'producer': '', 'creator': '', 'creationdate': '2014-07-07T12:20:41+00:00', 'source': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_path': 'data/pdf_files/making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'total_pages': 250, 'format': 'PDF 1.7', 'title': 'Making Sense of Data I', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-07-09T16:01:25+02:00', 'trapped': '', 'modDate': \"D:20140709160125+02'00'\", 'creationDate': 'D:20140707122041Z', 'page': 2, 'source_file': 'making_sense_of_data_i_a_practical_guide_to_exploratory_data_analysis_and_data_mining-myatt_etal-wiley-2014.pdf', 'file_type': 'pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding"
      ],
      "metadata": {
        "id": "x_PEYcsk_V71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid                                               # for id of records in vector DB\n",
        "from typing import List,Dict,Any,Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "iUpVrGL0BhfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager():\n",
        "  \"\"\"Handles document generation using SentenceTransformer\"\"\"\n",
        "\n",
        "  def __init__(self,model_name : str = \"all-MiniLM-L6-v2\"):\n",
        "    self.model_name = model_name\n",
        "    self.model = None\n",
        "    self._load_model()\n",
        "\n",
        "  def _load_model(self):\n",
        "    \"\"\"\"Load the SentenceTransformer model\"\"\"\n",
        "    try:\n",
        "      print(f\"Loading embedding model: {self.model_name}\")\n",
        "      self.model = SentenceTransformer(self.model_name)\n",
        "      print(f\"Model loaded successfully.Embedding dimensions : {self.model.get_sentence_embedding_dimension()}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading model {self.model_name} : {e}\")\n",
        "      raise\n",
        "\n",
        "  def generate_embeddings(self,texts:List[str]) -> np.ndarray:\n",
        "    if not self.model:\n",
        "      raise ValueError(\"Model not loaded\")\n",
        "\n",
        "    print(f\"Generating embeddings for {len(texts)} texts..\")\n",
        "    embeddings = self.model.encode(texts,show_progress_bar=True)\n",
        "    print(f\"Generated embeddings with shape : {embeddings.shape}\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4IEmVRGGmN7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_manager = EmbeddingManager()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TKkiSWfwKmP",
        "outputId": "123b2ff9-902a-462c-f9c6-9aaa4d83903b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model: all-MiniLM-L6-v2\n",
            "Model loaded successfully.Embedding dimensions : 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VectorDB"
      ],
      "metadata": {
        "id": "yy612Re_w5dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore():\n",
        "\n",
        "  def __init__(self,collection_name : str = 'pdf_documents',persist_directory : str = 'data/vector_store'):\n",
        "    self.collection_name = collection_name\n",
        "    self.persist_directory = persist_directory\n",
        "    self.client = None\n",
        "    self.collection = None\n",
        "    self._initialize_store()\n",
        "\n",
        "  def _initialize_store(self):\n",
        "    \"\"\"\"Initialize ChromaDB and collection\"\"\"\n",
        "\n",
        "    try:\n",
        "      os.makedirs(self.persis_directory,exist_ok = True)\n",
        "      # Create persistent ChromaDB clinet\n",
        "      self.client = chromdb.PersistentClient(path = self.persist_directory)\n",
        "\n",
        "      # Get or create collection\n",
        "      self.collections = self.client.get_or_create_collection(\n",
        "          name = self.collection_name,\n",
        "          metadata = {'description':'PDF document embeddings for RAG'}\n",
        "      )\n",
        "\n",
        "      print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
        "      print(f\"Existing documents in collectio: {self.collection.count()}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error initializing vector store : {e}\")\n",
        "\n",
        "  def add_documents(self,documents : List[Any],embeddings:np.ndarray):\n",
        "\n",
        "    if(len(documents) != len(embeddings)):\n",
        "      raise ValueError(\"Number of documents must match number of embeddings\")\n",
        "\n",
        "    print(f\"Adding {len(documents)} documents to vector store\")\n",
        "\n",
        "    # Prepare data for chromadb\n",
        "    ids = []\n",
        "    metadatas = []\n",
        "    documents_text = []\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i ,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
        "      doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "      ids.append(doc_id)\n",
        "\n",
        "      # Prepare metadata\n",
        "      metadata = dict{doc.metadata}\n",
        "      metadata['doc_index'] = i\n",
        "      metadata['content_length'] = len(doc.page_content)\n",
        "      metadatas.append(metadata)\n",
        "\n",
        "      # Document content\n",
        "      documents_text.append(doc.page_content)\n",
        "\n",
        "      # Embedding\n",
        "      embeddings_list.append(embedding.tolist())\n",
        "\n",
        "      try:\n",
        "        self.collection.add(\n",
        "            ids=ids,\n",
        "            embeddings=embeddings_list,\n",
        "            metadatas = metadatas,\n",
        "            documents= documents_text\n",
        "        )\n",
        "\n",
        "        print(f\"Sucessfully added {len(documents)} documents to vector store\")\n",
        "        print(f\"Total documents in collection: {self.collection.count()}\")\n",
        "      except Exception as e:\n",
        "        print(f\"Error adding documents to vector store : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "7ESerXK3w-pe",
        "outputId": "0503fa47-73ec-4eca-bbd9-6b79fa779aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1414261166.py, line 47)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1414261166.py\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    metadata = dict{doc.metadata}\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}