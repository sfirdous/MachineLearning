{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Transformer-Based Models"
      ],
      "metadata": {
        "id": "g9bAUBC_dM42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# required libraries\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "rmh8c5jsdPly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Load and Inspect a Transformer Model\n",
        "\n"
      ],
      "metadata": {
        "id": "xuFB4pGEdQsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "wbLGUiu2ttE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"MODEL: {model_name}\")\n",
        "\n",
        "# Count total parameters\n",
        "total_params = 0\n",
        "for param in model.parameters():\n",
        "    total_params += param.numel()\n",
        "\n",
        "print(f\"\\nTotal Parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "WysQ5lGot0Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model size in MB\n",
        "model_size_mb = (total_params * 4) / (1024 ** 2)\n",
        "print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
        "\n",
        "# Print model configuration\n",
        "config = model.config\n",
        "print(f\"\\nNumber of Layers: {config.n_layers}\")\n",
        "print(f\"Hidden Size: {config.dim}\")\n",
        "print(f\"Attention Heads: {config.n_heads}\")\n",
        "print(f\"Max Sequence Length: {config.max_position_embeddings}\")"
      ],
      "metadata": {
        "id": "ivGbo2WZuBp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer information\n",
        "print(f\"\\nVocabulary Size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"Padding Token: {tokenizer.pad_token}\")"
      ],
      "metadata": {
        "id": "cME39FsjuGfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tokenization\n",
        "example_text = \"The transformer architecture revolutionized NLP!\"\n",
        "tokens = tokenizer.tokenize(example_text)\n",
        "token_ids = tokenizer.encode(example_text)\n",
        "\n",
        "print(f\"\\nExample Text: {example_text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "print(f\"Number of Tokens: {len(tokens)}\")"
      ],
      "metadata": {
        "id": "H0XxoucwuMz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Load Dataset and Build Classification Pipeline\n"
      ],
      "metadata": {
        "id": "ajU-j3zpuaDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "print(f\"\\nTrain samples: {len(dataset['train'])}\")\n",
        "print(f\"Test samples: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "id": "IW7rc7uWuhIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take smaller subset for faster training\n",
        "train_size = 5000\n",
        "test_size = 1000\n",
        "\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
        "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(test_size))\n",
        "\n",
        "print(f\"\\nUsing {train_size} training samples\")\n",
        "print(f\"Using {test_size} test samples\")"
      ],
      "metadata": {
        "id": "2_ea0Ovmuydh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tokenize dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "76PhQsUIUIIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)"
      ],
      "metadata": {
        "id": "_qIuMnCWu3DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTokenizing dataset...\")\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "B7-IevaGvJAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename label column\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "lSvUnBk3vPqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set format for PyTorch\n",
        "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {
        "id": "7-Tmhk5NvU5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Build PyTorch DataLoader"
      ],
      "metadata": {
        "id": "0QXk_3sHUQBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "print(f\"\\nBatch Size: {batch_size}\")\n",
        "print(f\"Training Batches: {len(train_loader)}\")\n",
        "print(f\"Test Batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "id": "4J-iIwPzvaOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model for classification\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "p0AopuX_Uyn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QO583-RYVDOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fine-tune the transformer"
      ],
      "metadata": {
        "id": "WO_zoDMsVSLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "learning_rate = 2e-5\n",
        "epochs = 3"
      ],
      "metadata": {
        "id": "ms_vXJyyVKpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Y__kGimKVUHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Log:\n"
      ],
      "metadata": {
        "id": "7gPQ0kQGVge_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Track training loss\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "epoch_times = []"
      ],
      "metadata": {
        "id": "fKeBu4yZVkcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Print progress every 50 batches\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Epoch {epoc\n",
        "                           h+1}/{epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    epoch_times.append(epoch_time)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
        "    print(f\"  Time: {epoch_time:.2f}s\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IupmVFzVuBG",
        "outputId": "2959c285-f8ea-46f8-de04-fca88bd7ba9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 | Batch 0/313 | Loss: 0.7069\n",
            "Epoch 1/3 | Batch 50/313 | Loss: 0.5330\n",
            "Epoch 1/3 | Batch 100/313 | Loss: 0.4596\n",
            "Epoch 1/3 | Batch 150/313 | Loss: 0.3216\n",
            "Epoch 1/3 | Batch 200/313 | Loss: 0.4301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Evaluate & Interpret"
      ],
      "metadata": {
        "id": "4zCPcFI_a_j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "misclassified = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Collect misclassified examples\n",
        "        for i in range(len(labels)):\n",
        "            if predictions[i] != labels[i]:\n",
        "                text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
        "                misclassified.append({\n",
        "                    \"text\": text[:200],\n",
        "                    \"true_label\": labels[i].item(),\n",
        "                    \"predicted_label\": predictions[i].item()\n",
        "                })"
      ],
      "metadata": {
        "id": "h6xPIEDUbIo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions)\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "J7jPNyIfbVsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first 5 misclassified examples\n",
        "print(f\"\\nMisclassified Examples (First 5):\")\n",
        "for idx, example in enumerate(misclassified[:5]):\n",
        "    print(f\"\\nExample {idx+1}:\")\n",
        "    print(f\"Text: {example['text']}...\")\n",
        "    print(f\"True Label: {example['true_label']} | Predicted: {example['predicted_label']}\")"
      ],
      "metadata": {
        "id": "YSj9yBjnbctb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Training Curves\n"
      ],
      "metadata": {
        "id": "oZDrsOZ5bjnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss curve\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs+1), train_losses, 'b-o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss over Epochs\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Accuracy curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs+1), val_accuracies, 'g-o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Validation Accuracy over Epochs\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dre_2qBsblPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Model Efficiency Analysis\n"
      ],
      "metadata": {
        "id": "kZ-jPiF9bqYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load second model for comparison\n",
        "comparison_model_name = \"bert-base-uncased\"\n",
        "comparison_model = AutoModelForSequenceClassification.from_pretrained(comparison_model_name, num_labels=num_labels)\n",
        "comparison_model.to(device)\n",
        "comparison_model.eval()"
      ],
      "metadata": {
        "id": "kJzY1XQbbrM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to measure inference time\n",
        "def measure_inference_time(model, dataloader, num_batches=20):\n",
        "    inference_times = []"
      ],
      "metadata": {
        "id": "xOexZ7bIbyrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            if idx >= num_batches:\n",
        "                break\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            start_time = time.time()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            inference_times.append(inference_time / len(input_ids))\n",
        "\n",
        "    return np.mean(inference_times)"
      ],
      "metadata": {
        "id": "_5m57GwNb3_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure for both models\n",
        "print(\"\\nMeasuring inference times...\")\n",
        "\n",
        "distilbert_time = measure_inference_time(model, test_loader)\n",
        "bert_time = measure_inference_time(comparison_model, test_loader)\n",
        "\n",
        "# Model parameters\n",
        "distilbert_params = sum(p.numel() for p in model.parameters())\n",
        "bert_params = sum(p.numel() for p in comparison_model.parameters())\n",
        "\n",
        "# Model sizes\n",
        "distilbert_size = (distilbert_params * 4) / (1024 ** 2)\n",
        "bert_size = (bert_params * 4) / (1024 ** 2)"
      ],
      "metadata": {
        "id": "u8Hvgyt1b7Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print comparison\n",
        "print(f\"\\nModel Comparison:\")\n",
        "print(f\"{'Metric':<30} {'DistilBERT':<20} {'BERT':<20}\")\n",
        "print(f\"{'Parameters':<30} {distilbert_params:,<20} {bert_params:,<20}\")\n",
        "print(f\"{'Model Size (MB)':<30} {distilbert_size:<20.2f}\n",
        "      {bert_size:<20.2f}\")\n",
        "print(f\"{'Inference Time (ms)':<30} {distilbert_time*1000:<20.2f} {bert_time*1000:<20.2f}\")\n",
        "print(f\"{'Speedup':<30} {bert_time/distilbert_time:<20.2f}x\")\n"
      ],
      "metadata": {
        "id": "XvpCdx5DcFFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}